{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901ccddd-a173-40cf-b0da-f4b6378d2733",
   "metadata": {},
   "source": [
    "# Dataset proces\n",
    "This file analyse, preprocess, and create dataset.\n",
    "\n",
    "- Analyse\n",
    "  \n",
    "On this stage we detect language, show all unique keywords,\n",
    "evaluate number of words in each field, count number of empty fields\n",
    "- Preproves\n",
    "\n",
    "On this stage we: split string of fields into array of words,\n",
    "stem (remove endings \"doing -> do\") them, lower case\n",
    "- Create datasets\n",
    "\n",
    "We created dataset with no stemming and with stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b634a-e59d-44ec-8595-b5a99d614d63",
   "metadata": {},
   "source": [
    "Download all needed libs:\n",
    "- langdetect (for detecting language in data understanding (eda) stage)\n",
    "- gensim for vocabulary and tokenization\n",
    "- nltk split by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f1d248-c897-42e8-b674-e55317450889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install langdetect\n",
    "# ! pip install gensim\n",
    "# !pip install sklearn\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6962ce4e-6668-4bc0-ace8-1bf914bacc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3a3d75-02d0-49a3-951a-744073a24d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4287a8-4f09-44a1-988c-e9e72a2f029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is constants\n",
    "Constants\n",
    "----------\n",
    "    DATA_PATH: str\n",
    "        path to store raw dataset with json metadata description\n",
    "    PROCESSED_DATA_PATH: str\n",
    "        path to store dataset created by this code\n",
    "    DATA_INDEX_PATH: str\n",
    "        path to store\n",
    "    SEARCH_DATA_PATH: str\n",
    "        path to search metadata dataset\n",
    "'''\n",
    "DATA_PATH = \"../datasets/raw_datasets/kaggle\"\n",
    "PROCESSED_DATA_PATH = \"../datasets/processed_datasets\"\n",
    "DATA_INDEX_PATH = \"../datasets/\"\n",
    "\n",
    "SEARCH_DATA_PATH = \"../datasets/raw_datasets/metadata-for-search\"\n",
    "SEARCH_DATA_INDEX_PATH = \"../datasets/raw_datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d023990-c5b5-4610-bcb5-f5fd108120b4",
   "metadata": {},
   "source": [
    "## data indexing\n",
    "index is csv file which contain:\n",
    "- path to each dataset \"path_dataset\",\n",
    "- path to each metadata of dataset \"path_metadata\",\n",
    "- author name of name of folder in DATA_PATH: \"user_folder_name\"\n",
    "- name of dataset name of folder DATA_PATH/\"user_folder_name\" in \"data_folder_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9a3bdeeb-03ad-48d8-8c56-a6270d78b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacker-news/hacker-news\n",
      "epa/epa-historical-air-quality\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_folder_name</th>\n",
       "      <th>data_folder_name</th>\n",
       "      <th>path_dataset</th>\n",
       "      <th>path_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sumon3455</td>\n",
       "      <td>bangla-sign-language-video-dataset</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>babaakki</td>\n",
       "      <td>imdb1000</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/b...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harry418</td>\n",
       "      <td>dataset-for-mask-detection</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/h...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>varunguddeti</td>\n",
       "      <td>redwinequality</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/v...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tevecsystems</td>\n",
       "      <td>retail-sales-forecasting</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/t...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>pandrii000</td>\n",
       "      <td>hituav-a-highaltitude-infrared-thermal-dataset</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/p...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>ignacioavas</td>\n",
       "      <td>alignmacrid-vae</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>carrie1</td>\n",
       "      <td>ecommerce-data</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/c...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>maxhorowitz</td>\n",
       "      <td>nflplaybyplay2009to2016</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>shibumohapatra</td>\n",
       "      <td>house-price</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1916 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_folder_name                                data_folder_name  \\\n",
       "0           sumon3455              bangla-sign-language-video-dataset   \n",
       "1            babaakki                                        imdb1000   \n",
       "2            harry418                      dataset-for-mask-detection   \n",
       "3        varunguddeti                                  redwinequality   \n",
       "4        tevecsystems                        retail-sales-forecasting   \n",
       "...               ...                                             ...   \n",
       "1911       pandrii000  hituav-a-highaltitude-infrared-thermal-dataset   \n",
       "1912      ignacioavas                                 alignmacrid-vae   \n",
       "1913          carrie1                                  ecommerce-data   \n",
       "1914      maxhorowitz                         nflplaybyplay2009to2016   \n",
       "1915   shibumohapatra                                     house-price   \n",
       "\n",
       "                                           path_dataset  \\\n",
       "0     ../datasets/raw_datasets/metadata-for-search/s...   \n",
       "1     ../datasets/raw_datasets/metadata-for-search/b...   \n",
       "2     ../datasets/raw_datasets/metadata-for-search/h...   \n",
       "3     ../datasets/raw_datasets/metadata-for-search/v...   \n",
       "4     ../datasets/raw_datasets/metadata-for-search/t...   \n",
       "...                                                 ...   \n",
       "1911  ../datasets/raw_datasets/metadata-for-search/p...   \n",
       "1912  ../datasets/raw_datasets/metadata-for-search/i...   \n",
       "1913  ../datasets/raw_datasets/metadata-for-search/c...   \n",
       "1914  ../datasets/raw_datasets/metadata-for-search/m...   \n",
       "1915  ../datasets/raw_datasets/metadata-for-search/s...   \n",
       "\n",
       "                                          path_metadata  \n",
       "0     ../datasets/raw_datasets/metadata-for-search/s...  \n",
       "1     ../datasets/raw_datasets/metadata-for-search/b...  \n",
       "2     ../datasets/raw_datasets/metadata-for-search/h...  \n",
       "3     ../datasets/raw_datasets/metadata-for-search/v...  \n",
       "4     ../datasets/raw_datasets/metadata-for-search/t...  \n",
       "...                                                 ...  \n",
       "1911  ../datasets/raw_datasets/metadata-for-search/p...  \n",
       "1912  ../datasets/raw_datasets/metadata-for-search/i...  \n",
       "1913  ../datasets/raw_datasets/metadata-for-search/c...  \n",
       "1914  ../datasets/raw_datasets/metadata-for-search/m...  \n",
       "1915  ../datasets/raw_datasets/metadata-for-search/s...  \n",
       "\n",
       "[1916 rows x 4 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_datasets_locations(SEARCH_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0076eb5a-26a4-4e59-ae44-d73fcffc50de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_folder_name</th>\n",
       "      <th>data_folder_name</th>\n",
       "      <th>path_dataset</th>\n",
       "      <th>path_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vijendersingh412</td>\n",
       "      <td>research-paper</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/vijendersingh4...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/vijendersingh4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>somumourya</td>\n",
       "      <td>news-recommendation-dataset</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/somumourya/new...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/somumourya/new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kamalesh1997</td>\n",
       "      <td>top-rated-movies-tmdb-data-set</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/kamalesh1997/t...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/kamalesh1997/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>charanjilagam</td>\n",
       "      <td>iplcleanedata</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/charanjilagam/...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/charanjilagam/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>michaelbryantds</td>\n",
       "      <td>fivethirtyeight-polls</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/michaelbryantd...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/michaelbryantd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>zaynshahbaz</td>\n",
       "      <td>pakistan-car-prices</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/zaynshahbaz/pa...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/zaynshahbaz/pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>adithyaawati</td>\n",
       "      <td>apartments-for-rent-classified</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/adithyaawati/a...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/adithyaawati/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>deepak007chaubey</td>\n",
       "      <td>flight-on-time-dataset</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/deepak007chaub...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/deepak007chaub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>deepak007chaubey</td>\n",
       "      <td>housevalueestimation</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/deepak007chaub...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/deepak007chaub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>nikatomashvili</td>\n",
       "      <td>steam-games-dataset</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/nikatomashvili...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/nikatomashvili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_folder_name                data_folder_name  \\\n",
       "0    vijendersingh412                  research-paper   \n",
       "1          somumourya     news-recommendation-dataset   \n",
       "2        kamalesh1997  top-rated-movies-tmdb-data-set   \n",
       "3       charanjilagam                   iplcleanedata   \n",
       "4     michaelbryantds           fivethirtyeight-polls   \n",
       "..                ...                             ...   \n",
       "654       zaynshahbaz             pakistan-car-prices   \n",
       "655      adithyaawati  apartments-for-rent-classified   \n",
       "656  deepak007chaubey          flight-on-time-dataset   \n",
       "657  deepak007chaubey            housevalueestimation   \n",
       "658    nikatomashvili             steam-games-dataset   \n",
       "\n",
       "                                          path_dataset  \\\n",
       "0    ../datasets/raw_datasets/kaggle/vijendersingh4...   \n",
       "1    ../datasets/raw_datasets/kaggle/somumourya/new...   \n",
       "2    ../datasets/raw_datasets/kaggle/kamalesh1997/t...   \n",
       "3    ../datasets/raw_datasets/kaggle/charanjilagam/...   \n",
       "4    ../datasets/raw_datasets/kaggle/michaelbryantd...   \n",
       "..                                                 ...   \n",
       "654  ../datasets/raw_datasets/kaggle/zaynshahbaz/pa...   \n",
       "655  ../datasets/raw_datasets/kaggle/adithyaawati/a...   \n",
       "656  ../datasets/raw_datasets/kaggle/deepak007chaub...   \n",
       "657  ../datasets/raw_datasets/kaggle/deepak007chaub...   \n",
       "658  ../datasets/raw_datasets/kaggle/nikatomashvili...   \n",
       "\n",
       "                                         path_metadata  \n",
       "0    ../datasets/raw_datasets/kaggle/vijendersingh4...  \n",
       "1    ../datasets/raw_datasets/kaggle/somumourya/new...  \n",
       "2    ../datasets/raw_datasets/kaggle/kamalesh1997/t...  \n",
       "3    ../datasets/raw_datasets/kaggle/charanjilagam/...  \n",
       "4    ../datasets/raw_datasets/kaggle/michaelbryantd...  \n",
       "..                                                 ...  \n",
       "654  ../datasets/raw_datasets/kaggle/zaynshahbaz/pa...  \n",
       "655  ../datasets/raw_datasets/kaggle/adithyaawati/a...  \n",
       "656  ../datasets/raw_datasets/kaggle/deepak007chaub...  \n",
       "657  ../datasets/raw_datasets/kaggle/deepak007chaub...  \n",
       "658  ../datasets/raw_datasets/kaggle/nikatomashvili...  \n",
       "\n",
       "[659 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # create and save index\n",
    "# data_index = get_datasets_locations()\n",
    "# data_index.to_csv(DATA_INDEX_PATH + \"data_index.csv\")\n",
    "# data_index.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c6ce2-29df-4e1a-8dc5-2dfc7128fcef",
   "metadata": {},
   "source": [
    "#### load search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c650e61c-327f-424d-9d5f-fa3bf55a0de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_data_index_hot = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info-hottest-rating.csv\")\n",
    "search_data_index_vote = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info.csv\")\n",
    "search_data_index_active = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info-active-rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8421931-6b7b-44c8-afd6-289c7693f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_data_index_hot['path to metadata'] = SEARCH_DATA_PATH + \"/\" + search_data_index_hot['path to metadata'] + \"/dataset-metadata.json\"\n",
    "search_data_index_vote['path to metadata'] = SEARCH_DATA_PATH + \"/\" + search_data_index_vote['path to metadata'] + \"/dataset-metadata.json\"\n",
    "search_data_index_active['path to metadata'] = SEARCH_DATA_PATH + \"/\" + search_data_index_active['path to metadata'] + \"/dataset-metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "870aad89-5c8c-4758-aa9e-76cc9642c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '../datasets/raw_datasets/metadata-for-search/hacker-news/hacker-news/dataset-metadata.json'\n",
    "target1 = '../datasets/raw_datasets/metadata-for-search/epa/epa-historical-air-quality/dataset-metadata.json'\n",
    "\n",
    "search_data_index_active = search_data_index_active[search_data_index_active['path to metadata'] != target1]\n",
    "search_data_index_vote = search_data_index_vote[search_data_index_vote['path to metadata'] != target1]\n",
    "search_data_index_hot = search_data_index_hot[search_data_index_hot['path to metadata'] != target1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "439dbee0-cbf9-498a-9930-b4a120e00ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '../datasets/raw_datasets/metadata-for-search/hacker-news/hacker-news/dataset-metadata.json'\n",
    "target2 = '../datasets/raw_datasets/metadata-for-search/hacker-news/hacker-news/dataset-metadata.json'\n",
    "\n",
    "search_data_index_active = search_data_index_active[search_data_index_active['path to metadata'] != target2]\n",
    "search_data_index_vote = search_data_index_vote[search_data_index_vote['path to metadata'] != target2]\n",
    "search_data_index_hot = search_data_index_hot[search_data_index_hot['path to metadata'] != target2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1e4ffccc-265f-494c-8de4-3ac48b8a98fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>path to metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                query name  rating_hottest  \\\n",
       "0  Top 1000 Movies Dataset               1   \n",
       "1  Top 1000 Movies Dataset               2   \n",
       "2  Top 1000 Movies Dataset               3   \n",
       "3  Top 1000 Movies Dataset               4   \n",
       "4  Top 1000 Movies Dataset               5   \n",
       "\n",
       "                                    path to metadata  \n",
       "0  ../datasets/raw_datasets/metadata-for-search/o...  \n",
       "1  ../datasets/raw_datasets/metadata-for-search/r...  \n",
       "2  ../datasets/raw_datasets/metadata-for-search/i...  \n",
       "3  ../datasets/raw_datasets/metadata-for-search/u...  \n",
       "4  ../datasets/raw_datasets/metadata-for-search/s...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_hot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c03611c9-29b9-4e3e-b899-ee5c39c88730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating</th>\n",
       "      <th>path to metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>online dating matches</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>online dating matches</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>online dating matches</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                query name  rating  \\\n",
       "0    online dating matches       1   \n",
       "1    online dating matches       2   \n",
       "2    online dating matches       3   \n",
       "3  Top 1000 Movies Dataset       1   \n",
       "4  Top 1000 Movies Dataset       2   \n",
       "\n",
       "                                    path to metadata  \n",
       "0  ../datasets/raw_datasets/metadata-for-search/s...  \n",
       "1  ../datasets/raw_datasets/metadata-for-search/s...  \n",
       "2  ../datasets/raw_datasets/metadata-for-search/u...  \n",
       "3  ../datasets/raw_datasets/metadata-for-search/h...  \n",
       "4  ../datasets/raw_datasets/metadata-for-search/s...  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_vote.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3780c856-0ec4-42d8-89ab-0852ae8e7ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_active</th>\n",
       "      <th>path to metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                query name  rating_active  \\\n",
       "0  Top 1000 Movies Dataset              1   \n",
       "1  Top 1000 Movies Dataset              2   \n",
       "2  Top 1000 Movies Dataset              3   \n",
       "3  Top 1000 Movies Dataset              4   \n",
       "4  Top 1000 Movies Dataset              5   \n",
       "\n",
       "                                    path to metadata  \n",
       "0  ../datasets/raw_datasets/metadata-for-search/r...  \n",
       "1  ../datasets/raw_datasets/metadata-for-search/e...  \n",
       "2  ../datasets/raw_datasets/metadata-for-search/s...  \n",
       "3  ../datasets/raw_datasets/metadata-for-search/k...  \n",
       "4  ../datasets/raw_datasets/metadata-for-search/d...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_active.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512be3fc-4e07-4c3d-ae29-f6d543e13b80",
   "metadata": {},
   "source": [
    "## data description load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc9e3d6-0535-464e-abc8-2cfe43b3b19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'sonialikhan/household-electric-power-consumption',\n",
       " 'id_no': 5061720,\n",
       " 'datasetSlugNullable': 'household-electric-power-consumption',\n",
       " 'ownerUserNullable': 'sonialikhan',\n",
       " 'usabilityRatingNullable': 0.625,\n",
       " 'titleNullable': 'Household Electric Power Consumption',\n",
       " 'subtitleNullable': 'time series analysis- regression / clustering',\n",
       " 'descriptionNullable': \"**About Dataset**\\nI need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\\nSorry about the format, it's in text file.\\nThanks in advance :)\\n\\n**Context: **\\nMeasurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\n\\n**Data Set Characteristics:**\\nMultivariate, Time-Series\\n\\n**Associated Tasks:**\\nRegression, Clustering\\n\\n**Data Set Information:**\\n\\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\\n**Notes:**\\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\n\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\n\\nAttribute Information:\\n1.date: Date in format dd/mm/yyyy\\n\\n2.time: time in format hh:mm:ss\\n\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\n\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\n\\n5.voltage: minute-averaged voltage (in volt)\\n\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\n\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\n\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\n\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\",\n",
       " 'datasetId': 5061720,\n",
       " 'datasetSlug': 'household-electric-power-consumption',\n",
       " 'hasDatasetSlug': True,\n",
       " 'ownerUser': 'sonialikhan',\n",
       " 'hasOwnerUser': True,\n",
       " 'usabilityRating': 0.625,\n",
       " 'hasUsabilityRating': True,\n",
       " 'totalViews': 161,\n",
       " 'totalVotes': 7,\n",
       " 'totalDownloads': 12,\n",
       " 'title': 'Household Electric Power Consumption',\n",
       " 'hasTitle': True,\n",
       " 'subtitle': 'time series analysis- regression / clustering',\n",
       " 'hasSubtitle': True,\n",
       " 'description': \"**About Dataset**\\nI need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\\nSorry about the format, it's in text file.\\nThanks in advance :)\\n\\n**Context: **\\nMeasurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\n\\n**Data Set Characteristics:**\\nMultivariate, Time-Series\\n\\n**Associated Tasks:**\\nRegression, Clustering\\n\\n**Data Set Information:**\\n\\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\\n**Notes:**\\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\n\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\n\\nAttribute Information:\\n1.date: Date in format dd/mm/yyyy\\n\\n2.time: time in format hh:mm:ss\\n\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\n\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\n\\n5.voltage: minute-averaged voltage (in volt)\\n\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\n\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\n\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\n\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\",\n",
       " 'hasDescription': True,\n",
       " 'isPrivate': False,\n",
       " 'keywords': ['electricity'],\n",
       " 'licenses': [{'nameNullable': 'CC0-1.0', 'name': 'CC0-1.0', 'hasName': True}],\n",
       " 'collaborators': [{'username': 'abdullah1502', 'role': 'writer'},\n",
       "  {'username': 'abdmental01', 'role': 'writer'},\n",
       "  {'username': 'muhammadibrahimqasmi', 'role': 'writer'},\n",
       "  {'username': 'sheemazain', 'role': 'writer'}],\n",
       " 'data': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_metadata(path:str)-> dict:\n",
    "    file = open(path)\n",
    "    return json.load(file)\n",
    "\n",
    "load_metadata('../datasets/raw_datasets/kaggle/sonialikhan/household-electric-power-consumption/dataset-metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3b259-9973-45ee-b812-740b77de8bd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## data sample (предпросмотр одного из файлов метаданных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146e2ccf-e60f-4c68-91b8-0cc3d1e96b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_index = pd.read_csv(DATA_INDEX_PATH+'/data_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9eabd29-e96d-4916-9472-41fbbaa56ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_folder_name</th>\n",
       "      <th>data_folder_name</th>\n",
       "      <th>path_dataset</th>\n",
       "      <th>path_metadata</th>\n",
       "      <th>is_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vijendersingh412</td>\n",
       "      <td>research-paper</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/vijendersingh4...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/vijendersingh4...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>somumourya</td>\n",
       "      <td>news-recommendation-dataset</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/somumourya/new...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/somumourya/new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>kamalesh1997</td>\n",
       "      <td>top-rated-movies-tmdb-data-set</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/kamalesh1997/t...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/kamalesh1997/t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>charanjilagam</td>\n",
       "      <td>iplcleanedata</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/charanjilagam/...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/charanjilagam/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>michaelbryantds</td>\n",
       "      <td>fivethirtyeight-polls</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/michaelbryantd...</td>\n",
       "      <td>../datasets/raw_datasets/kaggle/michaelbryantd...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  user_folder_name                data_folder_name  \\\n",
       "0           0  vijendersingh412                  research-paper   \n",
       "1           1        somumourya     news-recommendation-dataset   \n",
       "2           2      kamalesh1997  top-rated-movies-tmdb-data-set   \n",
       "3           3     charanjilagam                   iplcleanedata   \n",
       "4           4   michaelbryantds           fivethirtyeight-polls   \n",
       "\n",
       "                                        path_dataset  \\\n",
       "0  ../datasets/raw_datasets/kaggle/vijendersingh4...   \n",
       "1  ../datasets/raw_datasets/kaggle/somumourya/new...   \n",
       "2  ../datasets/raw_datasets/kaggle/kamalesh1997/t...   \n",
       "3  ../datasets/raw_datasets/kaggle/charanjilagam/...   \n",
       "4  ../datasets/raw_datasets/kaggle/michaelbryantd...   \n",
       "\n",
       "                                       path_metadata  is_empty  \n",
       "0  ../datasets/raw_datasets/kaggle/vijendersingh4...     False  \n",
       "1  ../datasets/raw_datasets/kaggle/somumourya/new...      True  \n",
       "2  ../datasets/raw_datasets/kaggle/kamalesh1997/t...     False  \n",
       "3  ../datasets/raw_datasets/kaggle/charanjilagam/...     False  \n",
       "4  ../datasets/raw_datasets/kaggle/michaelbryantd...     False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1669908f-f900-4843-9302-5d2fa6e8cf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'sonialikhan/household-electric-power-consumption',\n",
       " 'id_no': 5061720,\n",
       " 'datasetSlugNullable': 'household-electric-power-consumption',\n",
       " 'ownerUserNullable': 'sonialikhan',\n",
       " 'usabilityRatingNullable': 0.625,\n",
       " 'titleNullable': 'Household Electric Power Consumption',\n",
       " 'subtitleNullable': 'time series analysis- regression / clustering',\n",
       " 'descriptionNullable': \"**About Dataset**\\nI need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\\nSorry about the format, it's in text file.\\nThanks in advance :)\\n\\n**Context: **\\nMeasurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\n\\n**Data Set Characteristics:**\\nMultivariate, Time-Series\\n\\n**Associated Tasks:**\\nRegression, Clustering\\n\\n**Data Set Information:**\\n\\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\\n**Notes:**\\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\n\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\n\\nAttribute Information:\\n1.date: Date in format dd/mm/yyyy\\n\\n2.time: time in format hh:mm:ss\\n\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\n\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\n\\n5.voltage: minute-averaged voltage (in volt)\\n\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\n\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\n\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\n\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\",\n",
       " 'datasetId': 5061720,\n",
       " 'datasetSlug': 'household-electric-power-consumption',\n",
       " 'hasDatasetSlug': True,\n",
       " 'ownerUser': 'sonialikhan',\n",
       " 'hasOwnerUser': True,\n",
       " 'usabilityRating': 0.625,\n",
       " 'hasUsabilityRating': True,\n",
       " 'totalViews': 161,\n",
       " 'totalVotes': 7,\n",
       " 'totalDownloads': 12,\n",
       " 'title': 'Household Electric Power Consumption',\n",
       " 'hasTitle': True,\n",
       " 'subtitle': 'time series analysis- regression / clustering',\n",
       " 'hasSubtitle': True,\n",
       " 'description': \"**About Dataset**\\nI need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\\nSorry about the format, it's in text file.\\nThanks in advance :)\\n\\n**Context: **\\nMeasurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\n\\n**Data Set Characteristics:**\\nMultivariate, Time-Series\\n\\n**Associated Tasks:**\\nRegression, Clustering\\n\\n**Data Set Information:**\\n\\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\\n**Notes:**\\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\n\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\n\\nAttribute Information:\\n1.date: Date in format dd/mm/yyyy\\n\\n2.time: time in format hh:mm:ss\\n\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\n\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\n\\n5.voltage: minute-averaged voltage (in volt)\\n\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\n\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\n\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\n\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\",\n",
       " 'hasDescription': True,\n",
       " 'isPrivate': False,\n",
       " 'keywords': ['electricity'],\n",
       " 'licenses': [{'nameNullable': 'CC0-1.0', 'name': 'CC0-1.0', 'hasName': True}],\n",
       " 'collaborators': [{'username': 'abdullah1502', 'role': 'writer'},\n",
       "  {'username': 'abdmental01', 'role': 'writer'},\n",
       "  {'username': 'muhammadibrahimqasmi', 'role': 'writer'},\n",
       "  {'username': 'sheemazain', 'role': 'writer'}],\n",
       " 'data': []}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_metadata('../datasets/raw_datasets/kaggle/sonialikhan/household-electric-power-consumption/dataset-metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f069e4bf-71ec-4b9a-af56-b57b567b9ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'charanjilagam/iplcleanedata',\n",
       " 'id_no': 3505142,\n",
       " 'datasetSlugNullable': 'iplcleanedata',\n",
       " 'ownerUserNullable': 'charanjilagam',\n",
       " 'usabilityRatingNullable': 0.5882352941176471,\n",
       " 'titleNullable': 'IPL_WIN_PREDICTION (98% ACCURACY)',\n",
       " 'subtitleNullable': 'IPL WIN PREDICTION FROM 2008 TO 2016',\n",
       " 'descriptionNullable': 'With the preprocessed dataset at hand, we can now move forward with a variety of actions depending on the nature of the data and the specific goals of the analysis. Some common actions might include:\\n\\nExploratory Data Analysis (EDA): We can begin by exploring the dataset to gain insights and a better understanding of its structure, contents, and statistical properties. This can involve tasks such as computing summary statistics, visualizing distributions, detecting outliers, and identifying patterns or relationships between variables.\\n\\nFeature Engineering: If the dataset contains raw data or basic features, we can create new features that may be more informative or suitable for the specific analysis. This can involve mathematical transformations, combining existing features, or extracting relevant information from text or timestamps.\\n\\nModel Training: With the preprocessed dataset, we can proceed with training machine learning models to perform various tasks such as classification, regression, clustering, or recommendation. This typically involves splitting the data into training and testing sets, selecting appropriate models, and optimizing their parameters to achieve the best performance.\\n\\nModel Evaluation: Once the models are trained, we can evaluate their performance using appropriate metrics such as accuracy, precision, recall, or mean squared error. This allows us to assess how well the models generalize to unseen data and make informed decisions about their effectiveness.\\n\\nPredictions and Inference: Using the trained models, we can make predictions or perform inference on new or unseen data points. This can be valuable for tasks such as making predictions about future events, identifying anomalies, or generating recommendations based on user preferences.\\n\\nVisualization and Reporting: To communicate the findings and results effectively, we can create visualizations, reports, or interactive dashboards summarizing the analysis. This helps stakeholders understand the insights and make informed decisions based on the data.\\n\\nBy leveraging the preprocessed dataset, we can streamline our analysis and focus on extracting meaningful insights or solving specific problems without the need for extensive data cleaning and preprocessing steps.',\n",
       " 'datasetId': 3505142,\n",
       " 'datasetSlug': 'iplcleanedata',\n",
       " 'hasDatasetSlug': True,\n",
       " 'ownerUser': 'charanjilagam',\n",
       " 'hasOwnerUser': True,\n",
       " 'usabilityRating': 0.5882352941176471,\n",
       " 'hasUsabilityRating': True,\n",
       " 'totalViews': 483,\n",
       " 'totalVotes': 6,\n",
       " 'totalDownloads': 93,\n",
       " 'title': 'IPL_WIN_PREDICTION (98% ACCURACY)',\n",
       " 'hasTitle': True,\n",
       " 'subtitle': 'IPL WIN PREDICTION FROM 2008 TO 2016',\n",
       " 'hasSubtitle': True,\n",
       " 'description': 'With the preprocessed dataset at hand, we can now move forward with a variety of actions depending on the nature of the data and the specific goals of the analysis. Some common actions might include:\\n\\nExploratory Data Analysis (EDA): We can begin by exploring the dataset to gain insights and a better understanding of its structure, contents, and statistical properties. This can involve tasks such as computing summary statistics, visualizing distributions, detecting outliers, and identifying patterns or relationships between variables.\\n\\nFeature Engineering: If the dataset contains raw data or basic features, we can create new features that may be more informative or suitable for the specific analysis. This can involve mathematical transformations, combining existing features, or extracting relevant information from text or timestamps.\\n\\nModel Training: With the preprocessed dataset, we can proceed with training machine learning models to perform various tasks such as classification, regression, clustering, or recommendation. This typically involves splitting the data into training and testing sets, selecting appropriate models, and optimizing their parameters to achieve the best performance.\\n\\nModel Evaluation: Once the models are trained, we can evaluate their performance using appropriate metrics such as accuracy, precision, recall, or mean squared error. This allows us to assess how well the models generalize to unseen data and make informed decisions about their effectiveness.\\n\\nPredictions and Inference: Using the trained models, we can make predictions or perform inference on new or unseen data points. This can be valuable for tasks such as making predictions about future events, identifying anomalies, or generating recommendations based on user preferences.\\n\\nVisualization and Reporting: To communicate the findings and results effectively, we can create visualizations, reports, or interactive dashboards summarizing the analysis. This helps stakeholders understand the insights and make informed decisions based on the data.\\n\\nBy leveraging the preprocessed dataset, we can streamline our analysis and focus on extracting meaningful insights or solving specific problems without the need for extensive data cleaning and preprocessing steps.',\n",
       " 'hasDescription': True,\n",
       " 'isPrivate': False,\n",
       " 'keywords': ['cricket'],\n",
       " 'licenses': [{'nameNullable': 'unknown', 'name': 'unknown', 'hasName': True}],\n",
       " 'collaborators': [],\n",
       " 'data': []}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_metadata(data_index.path_metadata[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7e9b3-7346-4e7b-b847-934e21f3d3f8",
   "metadata": {},
   "source": [
    "## data analysis (EDA)\n",
    "\n",
    "- Проверить вездели одинаковые ключи (и заполнены ли они)\n",
    "- оценить колличество токенов\n",
    "- колличество уникалньых слов\n",
    "- стоп слова\n",
    "- лист уникальных слов в keyword, subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e9239c3-3105-42d0-b6ab-080e023a535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bad4ed1-95e0-42ec-bc55-804f59099c62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_index\u001b[49m\u001b[38;5;241m.\u001b[39mpath_metadata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_index' is not defined"
     ]
    }
   ],
   "source": [
    "len(data_index.path_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d3ef9d-f6a5-48ee-a0a2-d9dafb141044",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'detect' is not defined Language is not detected: vijendersingh412/research-paper\n",
      "name 'detect' is not defined Language is not detected: research-paper\n",
      "name 'detect' is not defined Language is not detected: vijendersingh412\n",
      "name 'detect' is not defined Language is not detected: Research Paper\n",
      "name 'detect' is not defined Language is not detected: NLP Based Research Papers\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset consists of top research papers in NLP domain with its metadata.xls file containing detailed information. \n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains description of research paper, its domain, its sub domain and link of origin to correct paper. Each research paper starts with unique number followed by underscore and name of research paper. The unique number is is assigned to Sno of metadata sheet.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is just a start of making a dataset for research purpose and using this dataset for recommendation system or solving other problems. You are welcome to contribute in this. And can also share the problem you are solving and I can help without any cost. \n",
      "\n",
      "### Problem Use Case\n",
      "\n",
      "Collaborating Filtering\n",
      "EDA on NLP research paper\n",
      "Document Classification\n",
      "Creating own Embedding for NLP domain applications\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The data is open to the world's largest data science community. Please share your doubts, problems and how we can make this better. ✌️ \n",
      "\n",
      "Open to direct chat @ https://in.linkedin.com/in/vijendersingh412 🤝 \n",
      "name 'detect' is not defined Language is not detected: research-paper\n",
      "name 'detect' is not defined Language is not detected: vijendersingh412\n",
      "name 'detect' is not defined Language is not detected: Research Paper\n",
      "name 'detect' is not defined Language is not detected: NLP Based Research Papers\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset consists of top research papers in NLP domain with its metadata.xls file containing detailed information. \n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains description of research paper, its domain, its sub domain and link of origin to correct paper. Each research paper starts with unique number followed by underscore and name of research paper. The unique number is is assigned to Sno of metadata sheet.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is just a start of making a dataset for research purpose and using this dataset for recommendation system or solving other problems. You are welcome to contribute in this. And can also share the problem you are solving and I can help without any cost. \n",
      "\n",
      "### Problem Use Case\n",
      "\n",
      "Collaborating Filtering\n",
      "EDA on NLP research paper\n",
      "Document Classification\n",
      "Creating own Embedding for NLP domain applications\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The data is open to the world's largest data science community. Please share your doubts, problems and how we can make this better. ✌️ \n",
      "\n",
      "Open to direct chat @ https://in.linkedin.com/in/vijendersingh412 🤝 \n",
      "name 'detect' is not defined Language is not detected: somumourya/news-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: news-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: somumourya\n",
      "name 'detect' is not defined Language is not detected: News_recommendation dataset\n",
      "name 'detect' is not defined Language is not detected: news-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: somumourya\n",
      "name 'detect' is not defined Language is not detected: News_recommendation dataset\n",
      "name 'detect' is not defined Language is not detected: kamalesh1997/top-rated-movies-tmdb-data-set\n",
      "name 'detect' is not defined Language is not detected: top-rated-movies-tmdb-data-set\n",
      "name 'detect' is not defined Language is not detected: kamalesh1997\n",
      "name 'detect' is not defined Language is not detected: Top Rated movies -TMDB data set\n",
      "name 'detect' is not defined Language is not detected: Dataset of Top rated movies from the TMDB collections\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "User can make use of the dataset in order to create a movie recommendation system out of top rated movies\n",
      "\n",
      "Source:\n",
      "Api link: https://api.themoviedb.org/3/movie/top_rated?&lt;\n",
      "name 'detect' is not defined Language is not detected: top-rated-movies-tmdb-data-set\n",
      "name 'detect' is not defined Language is not detected: kamalesh1997\n",
      "name 'detect' is not defined Language is not detected: Top Rated movies -TMDB data set\n",
      "name 'detect' is not defined Language is not detected: Dataset of Top rated movies from the TMDB collections\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "User can make use of the dataset in order to create a movie recommendation system out of top rated movies\n",
      "\n",
      "Source:\n",
      "Api link: https://api.themoviedb.org/3/movie/top_rated?&lt;\n",
      "name 'detect' is not defined Language is not detected: charanjilagam/iplcleanedata\n",
      "name 'detect' is not defined Language is not detected: iplcleanedata\n",
      "name 'detect' is not defined Language is not detected: charanjilagam\n",
      "name 'detect' is not defined Language is not detected: IPL_WIN_PREDICTION (98% ACCURACY)\n",
      "name 'detect' is not defined Language is not detected: IPL WIN PREDICTION FROM 2008 TO 2016\n",
      "name 'detect' is not defined Language is not detected: With the preprocessed dataset at hand, we can now move forward with a variety of actions depending on the nature of the data and the specific goals of the analysis. Some common actions might include:\n",
      "\n",
      "Exploratory Data Analysis (EDA): We can begin by exploring the dataset to gain insights and a better understanding of its structure, contents, and statistical properties. This can involve tasks such as computing summary statistics, visualizing distributions, detecting outliers, and identifying patterns or relationships between variables.\n",
      "\n",
      "Feature Engineering: If the dataset contains raw data or basic features, we can create new features that may be more informative or suitable for the specific analysis. This can involve mathematical transformations, combining existing features, or extracting relevant information from text or timestamps.\n",
      "\n",
      "Model Training: With the preprocessed dataset, we can proceed with training machine learning models to perform various tasks such as classification, regression, clustering, or recommendation. This typically involves splitting the data into training and testing sets, selecting appropriate models, and optimizing their parameters to achieve the best performance.\n",
      "\n",
      "Model Evaluation: Once the models are trained, we can evaluate their performance using appropriate metrics such as accuracy, precision, recall, or mean squared error. This allows us to assess how well the models generalize to unseen data and make informed decisions about their effectiveness.\n",
      "\n",
      "Predictions and Inference: Using the trained models, we can make predictions or perform inference on new or unseen data points. This can be valuable for tasks such as making predictions about future events, identifying anomalies, or generating recommendations based on user preferences.\n",
      "\n",
      "Visualization and Reporting: To communicate the findings and results effectively, we can create visualizations, reports, or interactive dashboards summarizing the analysis. This helps stakeholders understand the insights and make informed decisions based on the data.\n",
      "\n",
      "By leveraging the preprocessed dataset, we can streamline our analysis and focus on extracting meaningful insights or solving specific problems without the need for extensive data cleaning and preprocessing steps.\n",
      "name 'detect' is not defined Language is not detected: iplcleanedata\n",
      "name 'detect' is not defined Language is not detected: charanjilagam\n",
      "name 'detect' is not defined Language is not detected: IPL_WIN_PREDICTION (98% ACCURACY)\n",
      "name 'detect' is not defined Language is not detected: IPL WIN PREDICTION FROM 2008 TO 2016\n",
      "name 'detect' is not defined Language is not detected: With the preprocessed dataset at hand, we can now move forward with a variety of actions depending on the nature of the data and the specific goals of the analysis. Some common actions might include:\n",
      "\n",
      "Exploratory Data Analysis (EDA): We can begin by exploring the dataset to gain insights and a better understanding of its structure, contents, and statistical properties. This can involve tasks such as computing summary statistics, visualizing distributions, detecting outliers, and identifying patterns or relationships between variables.\n",
      "\n",
      "Feature Engineering: If the dataset contains raw data or basic features, we can create new features that may be more informative or suitable for the specific analysis. This can involve mathematical transformations, combining existing features, or extracting relevant information from text or timestamps.\n",
      "\n",
      "Model Training: With the preprocessed dataset, we can proceed with training machine learning models to perform various tasks such as classification, regression, clustering, or recommendation. This typically involves splitting the data into training and testing sets, selecting appropriate models, and optimizing their parameters to achieve the best performance.\n",
      "\n",
      "Model Evaluation: Once the models are trained, we can evaluate their performance using appropriate metrics such as accuracy, precision, recall, or mean squared error. This allows us to assess how well the models generalize to unseen data and make informed decisions about their effectiveness.\n",
      "\n",
      "Predictions and Inference: Using the trained models, we can make predictions or perform inference on new or unseen data points. This can be valuable for tasks such as making predictions about future events, identifying anomalies, or generating recommendations based on user preferences.\n",
      "\n",
      "Visualization and Reporting: To communicate the findings and results effectively, we can create visualizations, reports, or interactive dashboards summarizing the analysis. This helps stakeholders understand the insights and make informed decisions based on the data.\n",
      "\n",
      "By leveraging the preprocessed dataset, we can streamline our analysis and focus on extracting meaningful insights or solving specific problems without the need for extensive data cleaning and preprocessing steps.\n",
      "name 'detect' is not defined Language is not detected: michaelbryantds/fivethirtyeight-polls\n",
      "name 'detect' is not defined Language is not detected: fivethirtyeight-polls\n",
      "name 'detect' is not defined Language is not detected: michaelbryantds\n",
      "name 'detect' is not defined Language is not detected: FiveThirtyEight Election Polls Dataset\n",
      "name 'detect' is not defined Language is not detected: For presidential, house, senate, and governor elections\n",
      "name 'detect' is not defined Language is not detected: FiveThirtyEight, sometimes rendered as 538, is an American website that focuses on opinion poll analysis, politics, economics, and sports blogging in the United States. The website, which takes its name from the number of electors in the United States electoral college, 538 was founded on March 7, 2008, as a polling aggregation website with a blog created by analyst Nate Silver. In August 2010, the blog became a licensed feature of The New York Times online and renamed FiveThirtyEight: Nate Silver's Political Calculus. \n",
      "\n",
      "This dataset contains polls for presidential elections (1968-2020), and house, senate, and governor elections for 2022.\n",
      "\n",
      "Source: https://projects.fivethirtyeight.com/2022-election-forecast/\n",
      "\n",
      "### See README.md for more information.\n",
      "name 'detect' is not defined Language is not detected: fivethirtyeight-polls\n",
      "name 'detect' is not defined Language is not detected: michaelbryantds\n",
      "name 'detect' is not defined Language is not detected: FiveThirtyEight Election Polls Dataset\n",
      "name 'detect' is not defined Language is not detected: For presidential, house, senate, and governor elections\n",
      "name 'detect' is not defined Language is not detected: FiveThirtyEight, sometimes rendered as 538, is an American website that focuses on opinion poll analysis, politics, economics, and sports blogging in the United States. The website, which takes its name from the number of electors in the United States electoral college, 538 was founded on March 7, 2008, as a polling aggregation website with a blog created by analyst Nate Silver. In August 2010, the blog became a licensed feature of The New York Times online and renamed FiveThirtyEight: Nate Silver's Political Calculus. \n",
      "\n",
      "This dataset contains polls for presidential elections (1968-2020), and house, senate, and governor elections for 2022.\n",
      "\n",
      "Source: https://projects.fivethirtyeight.com/2022-election-forecast/\n",
      "\n",
      "### See README.md for more information.\n",
      "name 'detect' is not defined Language is not detected: akshayamali/netflix-clustering-and-recommendation\n",
      "name 'detect' is not defined Language is not detected: netflix-clustering-and-recommendation\n",
      "name 'detect' is not defined Language is not detected: akshayamali\n",
      "name 'detect' is not defined Language is not detected: NETFLIX CLUSTERING AND RECOMMENDATION\n",
      "name 'detect' is not defined Language is not detected: netflix-clustering-and-recommendation\n",
      "name 'detect' is not defined Language is not detected: akshayamali\n",
      "name 'detect' is not defined Language is not detected: NETFLIX CLUSTERING AND RECOMMENDATION\n",
      "name 'detect' is not defined Language is not detected: stephan/applied-ml-microcourse-ecommerce-recommendation\n",
      "name 'detect' is not defined Language is not detected: applied-ml-microcourse-ecommerce-recommendation\n",
      "name 'detect' is not defined Language is not detected: stephan\n",
      "name 'detect' is not defined Language is not detected: Applied ML Microcourse eCommerce Recommendation\n",
      "name 'detect' is not defined Language is not detected: applied-ml-microcourse-ecommerce-recommendation\n",
      "name 'detect' is not defined Language is not detected: stephan\n",
      "name 'detect' is not defined Language is not detected: Applied ML Microcourse eCommerce Recommendation\n",
      "name 'detect' is not defined Language is not detected: sonialikhan/household-electric-power-consumption\n",
      "name 'detect' is not defined Language is not detected: household-electric-power-consumption\n",
      "name 'detect' is not defined Language is not detected: sonialikhan\n",
      "name 'detect' is not defined Language is not detected: Household Electric Power Consumption\n",
      "name 'detect' is not defined Language is not detected: time series analysis- regression / clustering\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file.\n",
      "Thanks in advance :)\n",
      "\n",
      "**Context: **\n",
      "Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "Multivariate, Time-Series\n",
      "\n",
      "**Associated Tasks:**\n",
      "Regression, Clustering\n",
      "\n",
      "**Data Set Information:**\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\n",
      "**Notes:**\n",
      "1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "Attribute Information:\n",
      "1.date: Date in format dd/mm/yyyy\n",
      "\n",
      "2.time: time in format hh:mm:ss\n",
      "\n",
      "3.global_active_power: household global minute-averaged active power (in kilowatt)\n",
      "\n",
      "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt)\n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
      "\n",
      "7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
      "\n",
      "8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
      "\n",
      "9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: household-electric-power-consumption\n",
      "name 'detect' is not defined Language is not detected: sonialikhan\n",
      "name 'detect' is not defined Language is not detected: Household Electric Power Consumption\n",
      "name 'detect' is not defined Language is not detected: time series analysis- regression / clustering\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file.\n",
      "Thanks in advance :)\n",
      "\n",
      "**Context: **\n",
      "Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "Multivariate, Time-Series\n",
      "\n",
      "**Associated Tasks:**\n",
      "Regression, Clustering\n",
      "\n",
      "**Data Set Information:**\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\n",
      "**Notes:**\n",
      "1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "Attribute Information:\n",
      "1.date: Date in format dd/mm/yyyy\n",
      "\n",
      "2.time: time in format hh:mm:ss\n",
      "\n",
      "3.global_active_power: household global minute-averaged active power (in kilowatt)\n",
      "\n",
      "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt)\n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
      "\n",
      "7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
      "\n",
      "8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
      "\n",
      "9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: fplandes/covid19-granular-demographics-and-times-series\n",
      "name 'detect' is not defined Language is not detected: covid19-granular-demographics-and-times-series\n",
      "name 'detect' is not defined Language is not detected: fplandes\n",
      "name 'detect' is not defined Language is not detected: COVID-19 - FR - Predicting/Explaining the epidemic\n",
      "name 'detect' is not defined Language is not detected: Which socio-demographical indicators impact most the propagation?\n",
      "name 'detect' is not defined Language is not detected: ## 1. Context\n",
      "\n",
      "A variety of research is currently being developed in order to predict the future of the current covid-19 pandemic.\n",
      "Among all models, the SIR and other compartmental models are the main tool for explainable, extrapolation-robust predictions.\n",
      "\n",
      "Our idea is to provide the community with an extensive set of socio-demographics (and health) indicators, with the aim of explaining/predicting the variability in the pandemic dynamics.\n",
      "There is already a lot of literature covering these kind of indicators, for instance people study *contact matrices*, which account for the typical structure of contacts that various segments of the population have (in short: who has contacts with who?).\n",
      "\n",
      "However we haven't found systematic, Machine-Learning based studies of which indicators most impact the R0 (reproduction number) or other coefficients that are usually fitted to the (time-series) data of infected/hospitalized/deaths/etc.\n",
      "There are some nice initiatives similar to ours that have spawned on kaggle, and have been noticed (see the panel /datasets at https://www.kaggle.com/covid-19-contributions , in particular check out https://www.kaggle.com/jieyingwu/covid19-us-countylevel-summaries#counties.csv).\n",
      "\n",
      "Given the variety of indicators we provide here, we expect one should be able to predict the department-to-department variations of the empirical coefficient R0, but also of other rates, such has the rate of the process (infected-&gt;hospitalized), and to some extent, the rates of (hospitalized-&gt;resuscitation), or the rates of ([various states]-&gt; dead).\n",
      "This means the prediction deals both with the spread of the pandemic and the severity of its impact on people's lives and on the health system.\n",
      "\n",
      "Other data sources, to complete this repo (including worldwide data): https://modcov19.math.cnrs.fr/publicdata/#numeric\n",
      "\n",
      "## 2. Content\n",
      "\n",
      "Here we provide a training+test set of 100 'examples': the 100 departments of France (we had to exclude Mayotte for lack of reliable/available data). They can be considered homogeneous in the sense that all indicators have been recorded in the same way (see below). Likewise, the time-series of hospitalized/in resuscitation/returned home/deaths are measured in a consistent way among the different French departments, since procedures and instructions are very similar everywhere. This is the advantage of remaining in a single country (here, France).\n",
      "\n",
      "There are **two kind of data** we provide in the files.\n",
      "\n",
      "The **static** data is an aggregation of socio-demographics and health indicators taken from the last couple of years (2016-2019). It is mostly curated by INSEE, the French national statistics institution, but INSEE itself is only a statistics-precessing place, and their data comes from other French agencies and from some surveys INSEE performs itself (like the census data).\n",
      "It comes as a single file, but it is actually the result of our concatenation of several databases (7 of them). Each original database has a separate original source that we provide in the metadata. Sometimes this source itself is a link to the INSEE website, which then details which agencies originally produced the data.\n",
      "For several of these files, we had to preprocess data from the city-level into the larger departmental level. All the original files coming from public institutions, and the codes that we used to pre-process them, are available at this gitlab:  https://gitlab.inria.fr/flandes/covid-19-fr-socio-demographics.git\n",
      "\n",
      "\n",
      "The **time-series** (or dynamic) data comes from *Santé Publique France* (also called *Agence Nationale de Santé*, ANS), one of the French Public Health agencies, which gathers data from hospitals and from the Regional Health Agencies (*Agence Régionale de Santé*, ARS).\n",
      "An additional time-series is the lockdown (*confinement*) level time series, that we produced ourselves (from reading the news, basically). This one is a bit particular, in the sense that it makes no sense to predict it, instead the level of lockdown (which decreases starting on May 5th) impacts the epidemic spread, and can be known in advance.\n",
      "\n",
      "Note that the static data (many features) comes with feature names that start in a precise, regular way. We defined tags such that post-processing would be easy.\n",
      "\n",
      "**column names**\n",
      "\n",
      "- The first tag can be: `Nbre`, `Pop`, `RateIncome`, `RateMedian`, `RatePoverty`, `Rate[whatever]`\n",
      "- The other tags are (in this order):\n",
      "    - `sex=all`, `sex=F`, `sex=H` (F=Femme=Woman, H=homme=Man)\n",
      "    - `age=all`, `agemin=0_agemax=150` (or other values) (arbitrarily, the maximumof agemax is 150 years)\n",
      "- The suffix is a description of the feature\n",
      "\n",
      "There are *many more features than there are examples* in this data set, which means one has to be **extremely cautious** with **over-fitting**.\n",
      "\n",
      "\n",
      "## 3. Acknowledgements\n",
      "\n",
      "We thank INSEE and its partners for providing this wealth of data freely, and people working in the ARS and at ministries for providing the time-series health data.\n",
      "\n",
      "We also thank the ministry of research for providing permanent jobs, which allow to work on whatever we want (be 'agile') without the need for any bureaucracy (a.k.a. grant proposals). This kind of permanent jobs allowed for this project to exist (in the case of François Landes).\n",
      "\n",
      "We also thank flexible bosses who allow their postdocs to work on side projects.\n",
      "(?)\n",
      "\n",
      "F.L. thanks everyone who contributed to his (very limited) epidemiology cultural background, and indirectly encouraged this project (the ICUBAM team, esp. JP Nadal and François Husson).\n",
      "\n",
      "#### Credits\n",
      "\n",
      "Project initiated by **François Landes** and **Victor Alfonso**.\n",
      "\n",
      "## 4. Inspiration\n",
      "\n",
      "Concretely, we suggest a **multi-variate regression** (supervised) task.\n",
      "\n",
      "Precisely, for several time-series data (#of confirmed cases, #of people in hospitals, #of people in ICUs, #of death in hospitals, etc), we want to forecast the latest 7 days, for each department, based on the data excluding the last 7 days (and using the static data).\n",
      "In that sense, we have a training set with 100 examples (each department), with many more features (411 fixed-time socio-demographics and 5 time-series that cover various numbers of days).\n",
      "\n",
      "There are many possibilities to tackle this challenge: from linear or polynomial regressions to LSTMs or transformers, or combining with compartmental models: e.g. attempting to predict the effective reproduction number R0 in each department, for a given level of lockdown... the choice is yours !\n",
      "\n",
      "The idea of the challenge is that a model with high prediction accuracy can analyzed after training, so as to reveal the key features that impact most the spread and severity of outcomes of the pandemic. So high prediction accuracy is not so much the goal in itself than a means to achieve the goal.\n",
      "\n",
      "\n",
      "## 5. References\n",
      "\n",
      "\n",
      "#### At Kaggle:\n",
      "On Kaggle, similar **datasets** are:\n",
      "- listed in the the panel /datasets at https://www.kaggle.com/covid-19-contributions`\n",
      "- https://www.kaggle.com/kimjihoo/coronavirusdataset\n",
      "- in particular check out https://www.kaggle.com/jieyingwu/covid19-us-countylevel-summaries#counties.csv  They published an arxiv summarizing their idea:  http://arxiv.org/abs/2004.00756 and a github: https://github.com/JieYingWu/COVID-19_US_County-level_Summaries\n",
      "- or also https://www.kaggle.com/ringhilterra17/enrichednytimescovid19#covid19_us_county.csv\n",
      "\n",
      "On Kaggle, **relevant Kernels** are (probably there are many others?):\n",
      "- https://www.kaggle.com/lisphilar/covid-19-data-with-sir-model#SIR-to-SIR-F\n",
      "\n",
      "\n",
      "#### Elsewhere: some **relevant Litterature**:\n",
      "\n",
      "Very thorough research:\n",
      "    - www.epicx-lab.com/covid-19.html (multiple reports available there).\n",
      "    - even more generally: https://reacting.inserm.fr/\n",
      "\n",
      "- contact matrix:\n",
      "    - https://www.medrxiv.org/content/10.1101/2020.02.26.20028167v1 Estimation of country-level basic reproductive ratios for novel Coronavirus (COVID-19) using synthetic contact matrices\n",
      "    - https://arxiv.org/abs/2003.12055 Age-structured impact of social distancing on the COVID-19 epidemic in India\n",
      "- SIR models (esp. for France):\n",
      "\n",
      "    - https://arxiv.org/abs/1901.01144 A unified framework of epidemic spreading prediction by empirical mode decomposition based ensemble learning techniques\n",
      "- LSTM and time series prediction:\n",
      "    - https://arxiv.org/abs/2004.00959 Neural network based country wise risk prediction of COVID-19\n",
      "    - https://arxiv.org/abs/2004.13408\n",
      "- others:\n",
      "\n",
      "\n",
      "## 6. Other useful data sets\n",
      "\n",
      "See a list of time-series data: https://modcov19.math.cnrs.fr/publicdata/#numeric \n",
      "\n",
      "Here we list a couple or resources that we wish to add, as soon as we have time.\n",
      "If you want to **help us format these resources** into that of this dataset, you are welcome !\n",
      "- time-dependent data:\n",
      "    - total deaths (all causes, not just covid) (daily) https://www.insee.fr/fr/information/4481364 (historic data: data once per week)\n",
      "    - similar: https://www.insee.fr/fr/information/4470857#tableau-figure2_radio1  and in particular, https://www.insee.fr/fr/statistiques/fichier/4470857/2020-04-30_deces_sexe_age_lieu_csv.zip\n",
      "    - again, similar: https://www.data.gouv.fr/en/datasets/niveaux-dexces-de-mortalite-standardise-durant-lepidemie-de-covid-19/  and in particular, https://www.data.gouv.fr/en/datasets/r/055ebba4-89dc-4996-962e-71dde6aaf7a6\n",
      "    - Google's global mobility report : https://www.google.com/covid19/mobility/  and in particular, https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
      "\n",
      "- static data:\n",
      "    - equipments (health-related) https://www.insee.fr/fr/statistiques/3568611?sommaire=3568656  and in particular, https://www.insee.fr/fr/statistiques/fichier/3568611/equip-serv-sante-infra-2018.zip\n",
      "    - equipments (health-related) https://www.insee.fr/fr/statistiques/3568614?sommaire=3568656  and in particular, https://www.insee.fr/fr/statistiques/fichier/3568614/equip-serv-medical-para-infra-2018.zip\n",
      "    - equipments (general) https://www.insee.fr/fr/statistiques/3606476?sommaire=3568656#consulter  and in particular, https://www.insee.fr/fr/statistiques/fichier/3606476/bpe1318_csv.zip\n",
      "name 'detect' is not defined Language is not detected: covid19-granular-demographics-and-times-series\n",
      "name 'detect' is not defined Language is not detected: fplandes\n",
      "name 'detect' is not defined Language is not detected: COVID-19 - FR - Predicting/Explaining the epidemic\n",
      "name 'detect' is not defined Language is not detected: Which socio-demographical indicators impact most the propagation?\n",
      "name 'detect' is not defined Language is not detected: ## 1. Context\n",
      "\n",
      "A variety of research is currently being developed in order to predict the future of the current covid-19 pandemic.\n",
      "Among all models, the SIR and other compartmental models are the main tool for explainable, extrapolation-robust predictions.\n",
      "\n",
      "Our idea is to provide the community with an extensive set of socio-demographics (and health) indicators, with the aim of explaining/predicting the variability in the pandemic dynamics.\n",
      "There is already a lot of literature covering these kind of indicators, for instance people study *contact matrices*, which account for the typical structure of contacts that various segments of the population have (in short: who has contacts with who?).\n",
      "\n",
      "However we haven't found systematic, Machine-Learning based studies of which indicators most impact the R0 (reproduction number) or other coefficients that are usually fitted to the (time-series) data of infected/hospitalized/deaths/etc.\n",
      "There are some nice initiatives similar to ours that have spawned on kaggle, and have been noticed (see the panel /datasets at https://www.kaggle.com/covid-19-contributions , in particular check out https://www.kaggle.com/jieyingwu/covid19-us-countylevel-summaries#counties.csv).\n",
      "\n",
      "Given the variety of indicators we provide here, we expect one should be able to predict the department-to-department variations of the empirical coefficient R0, but also of other rates, such has the rate of the process (infected-&gt;hospitalized), and to some extent, the rates of (hospitalized-&gt;resuscitation), or the rates of ([various states]-&gt; dead).\n",
      "This means the prediction deals both with the spread of the pandemic and the severity of its impact on people's lives and on the health system.\n",
      "\n",
      "Other data sources, to complete this repo (including worldwide data): https://modcov19.math.cnrs.fr/publicdata/#numeric\n",
      "\n",
      "## 2. Content\n",
      "\n",
      "Here we provide a training+test set of 100 'examples': the 100 departments of France (we had to exclude Mayotte for lack of reliable/available data). They can be considered homogeneous in the sense that all indicators have been recorded in the same way (see below). Likewise, the time-series of hospitalized/in resuscitation/returned home/deaths are measured in a consistent way among the different French departments, since procedures and instructions are very similar everywhere. This is the advantage of remaining in a single country (here, France).\n",
      "\n",
      "There are **two kind of data** we provide in the files.\n",
      "\n",
      "The **static** data is an aggregation of socio-demographics and health indicators taken from the last couple of years (2016-2019). It is mostly curated by INSEE, the French national statistics institution, but INSEE itself is only a statistics-precessing place, and their data comes from other French agencies and from some surveys INSEE performs itself (like the census data).\n",
      "It comes as a single file, but it is actually the result of our concatenation of several databases (7 of them). Each original database has a separate original source that we provide in the metadata. Sometimes this source itself is a link to the INSEE website, which then details which agencies originally produced the data.\n",
      "For several of these files, we had to preprocess data from the city-level into the larger departmental level. All the original files coming from public institutions, and the codes that we used to pre-process them, are available at this gitlab:  https://gitlab.inria.fr/flandes/covid-19-fr-socio-demographics.git\n",
      "\n",
      "\n",
      "The **time-series** (or dynamic) data comes from *Santé Publique France* (also called *Agence Nationale de Santé*, ANS), one of the French Public Health agencies, which gathers data from hospitals and from the Regional Health Agencies (*Agence Régionale de Santé*, ARS).\n",
      "An additional time-series is the lockdown (*confinement*) level time series, that we produced ourselves (from reading the news, basically). This one is a bit particular, in the sense that it makes no sense to predict it, instead the level of lockdown (which decreases starting on May 5th) impacts the epidemic spread, and can be known in advance.\n",
      "\n",
      "Note that the static data (many features) comes with feature names that start in a precise, regular way. We defined tags such that post-processing would be easy.\n",
      "\n",
      "**column names**\n",
      "\n",
      "- The first tag can be: `Nbre`, `Pop`, `RateIncome`, `RateMedian`, `RatePoverty`, `Rate[whatever]`\n",
      "- The other tags are (in this order):\n",
      "    - `sex=all`, `sex=F`, `sex=H` (F=Femme=Woman, H=homme=Man)\n",
      "    - `age=all`, `agemin=0_agemax=150` (or other values) (arbitrarily, the maximumof agemax is 150 years)\n",
      "- The suffix is a description of the feature\n",
      "\n",
      "There are *many more features than there are examples* in this data set, which means one has to be **extremely cautious** with **over-fitting**.\n",
      "\n",
      "\n",
      "## 3. Acknowledgements\n",
      "\n",
      "We thank INSEE and its partners for providing this wealth of data freely, and people working in the ARS and at ministries for providing the time-series health data.\n",
      "\n",
      "We also thank the ministry of research for providing permanent jobs, which allow to work on whatever we want (be 'agile') without the need for any bureaucracy (a.k.a. grant proposals). This kind of permanent jobs allowed for this project to exist (in the case of François Landes).\n",
      "\n",
      "We also thank flexible bosses who allow their postdocs to work on side projects.\n",
      "(?)\n",
      "\n",
      "F.L. thanks everyone who contributed to his (very limited) epidemiology cultural background, and indirectly encouraged this project (the ICUBAM team, esp. JP Nadal and François Husson).\n",
      "\n",
      "#### Credits\n",
      "\n",
      "Project initiated by **François Landes** and **Victor Alfonso**.\n",
      "\n",
      "## 4. Inspiration\n",
      "\n",
      "Concretely, we suggest a **multi-variate regression** (supervised) task.\n",
      "\n",
      "Precisely, for several time-series data (#of confirmed cases, #of people in hospitals, #of people in ICUs, #of death in hospitals, etc), we want to forecast the latest 7 days, for each department, based on the data excluding the last 7 days (and using the static data).\n",
      "In that sense, we have a training set with 100 examples (each department), with many more features (411 fixed-time socio-demographics and 5 time-series that cover various numbers of days).\n",
      "\n",
      "There are many possibilities to tackle this challenge: from linear or polynomial regressions to LSTMs or transformers, or combining with compartmental models: e.g. attempting to predict the effective reproduction number R0 in each department, for a given level of lockdown... the choice is yours !\n",
      "\n",
      "The idea of the challenge is that a model with high prediction accuracy can analyzed after training, so as to reveal the key features that impact most the spread and severity of outcomes of the pandemic. So high prediction accuracy is not so much the goal in itself than a means to achieve the goal.\n",
      "\n",
      "\n",
      "## 5. References\n",
      "\n",
      "\n",
      "#### At Kaggle:\n",
      "On Kaggle, similar **datasets** are:\n",
      "- listed in the the panel /datasets at https://www.kaggle.com/covid-19-contributions`\n",
      "- https://www.kaggle.com/kimjihoo/coronavirusdataset\n",
      "- in particular check out https://www.kaggle.com/jieyingwu/covid19-us-countylevel-summaries#counties.csv  They published an arxiv summarizing their idea:  http://arxiv.org/abs/2004.00756 and a github: https://github.com/JieYingWu/COVID-19_US_County-level_Summaries\n",
      "- or also https://www.kaggle.com/ringhilterra17/enrichednytimescovid19#covid19_us_county.csv\n",
      "\n",
      "On Kaggle, **relevant Kernels** are (probably there are many others?):\n",
      "- https://www.kaggle.com/lisphilar/covid-19-data-with-sir-model#SIR-to-SIR-F\n",
      "\n",
      "\n",
      "#### Elsewhere: some **relevant Litterature**:\n",
      "\n",
      "Very thorough research:\n",
      "    - www.epicx-lab.com/covid-19.html (multiple reports available there).\n",
      "    - even more generally: https://reacting.inserm.fr/\n",
      "\n",
      "- contact matrix:\n",
      "    - https://www.medrxiv.org/content/10.1101/2020.02.26.20028167v1 Estimation of country-level basic reproductive ratios for novel Coronavirus (COVID-19) using synthetic contact matrices\n",
      "    - https://arxiv.org/abs/2003.12055 Age-structured impact of social distancing on the COVID-19 epidemic in India\n",
      "- SIR models (esp. for France):\n",
      "\n",
      "    - https://arxiv.org/abs/1901.01144 A unified framework of epidemic spreading prediction by empirical mode decomposition based ensemble learning techniques\n",
      "- LSTM and time series prediction:\n",
      "    - https://arxiv.org/abs/2004.00959 Neural network based country wise risk prediction of COVID-19\n",
      "    - https://arxiv.org/abs/2004.13408\n",
      "- others:\n",
      "\n",
      "\n",
      "## 6. Other useful data sets\n",
      "\n",
      "See a list of time-series data: https://modcov19.math.cnrs.fr/publicdata/#numeric \n",
      "\n",
      "Here we list a couple or resources that we wish to add, as soon as we have time.\n",
      "If you want to **help us format these resources** into that of this dataset, you are welcome !\n",
      "- time-dependent data:\n",
      "    - total deaths (all causes, not just covid) (daily) https://www.insee.fr/fr/information/4481364 (historic data: data once per week)\n",
      "    - similar: https://www.insee.fr/fr/information/4470857#tableau-figure2_radio1  and in particular, https://www.insee.fr/fr/statistiques/fichier/4470857/2020-04-30_deces_sexe_age_lieu_csv.zip\n",
      "    - again, similar: https://www.data.gouv.fr/en/datasets/niveaux-dexces-de-mortalite-standardise-durant-lepidemie-de-covid-19/  and in particular, https://www.data.gouv.fr/en/datasets/r/055ebba4-89dc-4996-962e-71dde6aaf7a6\n",
      "    - Google's global mobility report : https://www.google.com/covid19/mobility/  and in particular, https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
      "\n",
      "- static data:\n",
      "    - equipments (health-related) https://www.insee.fr/fr/statistiques/3568611?sommaire=3568656  and in particular, https://www.insee.fr/fr/statistiques/fichier/3568611/equip-serv-sante-infra-2018.zip\n",
      "    - equipments (health-related) https://www.insee.fr/fr/statistiques/3568614?sommaire=3568656  and in particular, https://www.insee.fr/fr/statistiques/fichier/3568614/equip-serv-medical-para-infra-2018.zip\n",
      "    - equipments (general) https://www.insee.fr/fr/statistiques/3606476?sommaire=3568656#consulter  and in particular, https://www.insee.fr/fr/statistiques/fichier/3606476/bpe1318_csv.zip\n",
      "name 'detect' is not defined Language is not detected: pavellexyr/one-million-reddit-questions\n",
      "name 'detect' is not defined Language is not detected: one-million-reddit-questions\n",
      "name 'detect' is not defined Language is not detected: pavellexyr\n",
      "name 'detect' is not defined Language is not detected: One Million Reddit Questions\n",
      "name 'detect' is not defined Language is not detected: One million questions from /r/AskReddit, going back from September 2021.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Ah, questions. One of the most important parts of natural dialogue. Automated question answering has been a long-standing problem in the NLP field. To help solve it, we present you with this dataset.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The following dataset comprises one million questions from /r/AskReddit, procured using [SocialGrep](https://socialgrep.com/datasets/one-million-reddit-questions?utm_source=kaggle&utm_medium=link&utm_campaign=onemillionquestions).\n",
      "The questions are labelled with date of creation and their score.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank [Etienne Girardet](https://unsplash.com/@etiennegirardet) for generously providing us with a background image for this dataset.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- What makes a popular Reddit question?\n",
      "- What makes a good Reddit question?\n",
      "- Can Reddit teach us more about how to ask questions properly?\n",
      "name 'detect' is not defined Language is not detected: one-million-reddit-questions\n",
      "name 'detect' is not defined Language is not detected: pavellexyr\n",
      "name 'detect' is not defined Language is not detected: One Million Reddit Questions\n",
      "name 'detect' is not defined Language is not detected: One million questions from /r/AskReddit, going back from September 2021.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Ah, questions. One of the most important parts of natural dialogue. Automated question answering has been a long-standing problem in the NLP field. To help solve it, we present you with this dataset.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The following dataset comprises one million questions from /r/AskReddit, procured using [SocialGrep](https://socialgrep.com/datasets/one-million-reddit-questions?utm_source=kaggle&utm_medium=link&utm_campaign=onemillionquestions).\n",
      "The questions are labelled with date of creation and their score.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank [Etienne Girardet](https://unsplash.com/@etiennegirardet) for generously providing us with a background image for this dataset.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- What makes a popular Reddit question?\n",
      "- What makes a good Reddit question?\n",
      "- Can Reddit teach us more about how to ask questions properly?\n",
      "name 'detect' is not defined Language is not detected: tanushreegupta2411/pdfs-llm\n",
      "name 'detect' is not defined Language is not detected: pdfs-llm\n",
      "name 'detect' is not defined Language is not detected: tanushreegupta2411\n",
      "name 'detect' is not defined Language is not detected: PDFs LLM\n",
      "name 'detect' is not defined Language is not detected: pdfs-llm\n",
      "name 'detect' is not defined Language is not detected: tanushreegupta2411\n",
      "name 'detect' is not defined Language is not detected: PDFs LLM\n",
      "name 'detect' is not defined Language is not detected: /the-shakeups-of-10-competitions\n",
      "name 'detect' is not defined Language is not detected: the-shakeups-of-10-competitions\n",
      "name 'detect' is not defined Language is not detected: The Shakeups of 10 Competitions\n",
      "name 'detect' is not defined Language is not detected: the-shakeups-of-10-competitions\n",
      "name 'detect' is not defined Language is not detected: The Shakeups of 10 Competitions\n",
      "name 'detect' is not defined Language is not detected: ossingh6/ott-recommendation-sonyrise\n",
      "name 'detect' is not defined Language is not detected: ott-recommendation-sonyrise\n",
      "name 'detect' is not defined Language is not detected: ossingh6\n",
      "name 'detect' is not defined Language is not detected: OTT_Recommendation_SonyRISE\n",
      "name 'detect' is not defined Language is not detected: ott-recommendation-sonyrise\n",
      "name 'detect' is not defined Language is not detected: ossingh6\n",
      "name 'detect' is not defined Language is not detected: OTT_Recommendation_SonyRISE\n",
      "name 'detect' is not defined Language is not detected: nipunarora8/most-liked-comments-on-youtube\n",
      "name 'detect' is not defined Language is not detected: most-liked-comments-on-youtube\n",
      "name 'detect' is not defined Language is not detected: nipunarora8\n",
      "name 'detect' is not defined Language is not detected: MOST LIKED COMMENTS ON YOUTUBE\n",
      "name 'detect' is not defined Language is not detected: Youtube Comments Dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I was finding a specific dataset but never got one.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This is a text dataset focussing on the top comments on the best youtube videos (views&gt;1B)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I wanna thank youtube api for helping me, lol\n",
      "and mongo db where I stored all the raw data. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I shared this dataset to see how the world will react and what will people do with this dataset. \n",
      "I hope this helps me learn more about NLP and ML\n",
      "name 'detect' is not defined Language is not detected: most-liked-comments-on-youtube\n",
      "name 'detect' is not defined Language is not detected: nipunarora8\n",
      "name 'detect' is not defined Language is not detected: MOST LIKED COMMENTS ON YOUTUBE\n",
      "name 'detect' is not defined Language is not detected: Youtube Comments Dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I was finding a specific dataset but never got one.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This is a text dataset focussing on the top comments on the best youtube videos (views&gt;1B)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I wanna thank youtube api for helping me, lol\n",
      "and mongo db where I stored all the raw data. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I shared this dataset to see how the world will react and what will people do with this dataset. \n",
      "I hope this helps me learn more about NLP and ML\n",
      "name 'detect' is not defined Language is not detected: bharatkumar0925/indian-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: indian-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: bharatkumar0925\n",
      "name 'detect' is not defined Language is not detected: Indian Movies dataset\n",
      "name 'detect' is not defined Language is not detected: More than 3000 indian movies, good for making recommendation system.\n",
      "name 'detect' is not defined Language is not detected: ### About the Dataset:\n",
      "\n",
      "This dataset contains more than 3000 movies, which can be beneficial for recommendation systems. Two files are available: `indian-movies.csv` and `clean-data.csv`. While both files contain the same data, `indian-movies.csv` includes raw data that can also be downloaded from [here](https://www.kaggle.com/datasets/prabhakarz/tmdb-15000-movies-dataset-with-credits). The `clean-data.csv` file includes cleaned information, such as extracted actors and character names from the `cast` column, and directors and producers from the `crew` column. Additionally, unnecessary or less relevant information has been removed. The data has been cleaned by removing trailing and leading spaces, and converting all data to lowercase, among other steps.\n",
      "\n",
      "### Uses of this Dataset:\n",
      "\n",
      "This dataset can serve various purposes, including:\n",
      "\n",
      "- **Movie Analysis:**\n",
      "  - What are the most popular movies?\n",
      "  - Who are the most successful actors/directors in terms of popular movies?\n",
      "  - What is the relation between popular movies and title length?\n",
      "  - Which month's movie releases are more successful?\n",
      "  - Which genres are more frequently produced than others?\n",
      "  Note: Define your criteria for popular movies first.\n",
      "\n",
      "- **Recommendation Systems**\n",
      "- **Prediction of Popularity Score, Vote Average, and Vote Count**\n",
      "\n",
      "### Data Information of `clean-data.csv`:\n",
      "\n",
      "- `movie_id`: ID of the movie\n",
      "- `overview`: Overview of the movie\n",
      "- `popularity`: Popularity score from TMDB\n",
      "- `release_date`: Release date\n",
      "- `title`: Title of the movie\n",
      "- `vote_average`: Average vote\n",
      "- `vote_count`: Total votes\n",
      "- `genres`: Genre of the movie\n",
      "- `keywords`: Movie keywords\n",
      "- `month`: Release month\n",
      "- `year`: Release year\n",
      "- `director`: Director of the movie\n",
      "- `producer`: Producer of the movie\n",
      "- `actors`: Top 3 actors of the movie\n",
      "- `characters`: Top 3 characters of the movie\n",
      "- `tags`: Combination of all relevant attributes of the movie, such as overview, keywords, actors, etc.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: indian-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: bharatkumar0925\n",
      "name 'detect' is not defined Language is not detected: Indian Movies dataset\n",
      "name 'detect' is not defined Language is not detected: More than 3000 indian movies, good for making recommendation system.\n",
      "name 'detect' is not defined Language is not detected: ### About the Dataset:\n",
      "\n",
      "This dataset contains more than 3000 movies, which can be beneficial for recommendation systems. Two files are available: `indian-movies.csv` and `clean-data.csv`. While both files contain the same data, `indian-movies.csv` includes raw data that can also be downloaded from [here](https://www.kaggle.com/datasets/prabhakarz/tmdb-15000-movies-dataset-with-credits). The `clean-data.csv` file includes cleaned information, such as extracted actors and character names from the `cast` column, and directors and producers from the `crew` column. Additionally, unnecessary or less relevant information has been removed. The data has been cleaned by removing trailing and leading spaces, and converting all data to lowercase, among other steps.\n",
      "\n",
      "### Uses of this Dataset:\n",
      "\n",
      "This dataset can serve various purposes, including:\n",
      "\n",
      "- **Movie Analysis:**\n",
      "  - What are the most popular movies?\n",
      "  - Who are the most successful actors/directors in terms of popular movies?\n",
      "  - What is the relation between popular movies and title length?\n",
      "  - Which month's movie releases are more successful?\n",
      "  - Which genres are more frequently produced than others?\n",
      "  Note: Define your criteria for popular movies first.\n",
      "\n",
      "- **Recommendation Systems**\n",
      "- **Prediction of Popularity Score, Vote Average, and Vote Count**\n",
      "\n",
      "### Data Information of `clean-data.csv`:\n",
      "\n",
      "- `movie_id`: ID of the movie\n",
      "- `overview`: Overview of the movie\n",
      "- `popularity`: Popularity score from TMDB\n",
      "- `release_date`: Release date\n",
      "- `title`: Title of the movie\n",
      "- `vote_average`: Average vote\n",
      "- `vote_count`: Total votes\n",
      "- `genres`: Genre of the movie\n",
      "- `keywords`: Movie keywords\n",
      "- `month`: Release month\n",
      "- `year`: Release year\n",
      "- `director`: Director of the movie\n",
      "- `producer`: Producer of the movie\n",
      "- `actors`: Top 3 actors of the movie\n",
      "- `characters`: Top 3 characters of the movie\n",
      "- `tags`: Combination of all relevant attributes of the movie, such as overview, keywords, actors, etc.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nileshthonte/crab-age-regression-dataset\n",
      "name 'detect' is not defined Language is not detected: crab-age-regression-dataset\n",
      "name 'detect' is not defined Language is not detected: nileshthonte\n",
      "name 'detect' is not defined Language is not detected: Crab_age_regression_dataset\n",
      "name 'detect' is not defined Language is not detected: crab-age-regression-dataset\n",
      "name 'detect' is not defined Language is not detected: nileshthonte\n",
      "name 'detect' is not defined Language is not detected: Crab_age_regression_dataset\n",
      "name 'detect' is not defined Language is not detected: gruffgemini/steam-games-dataset\n",
      "name 'detect' is not defined Language is not detected: steam-games-dataset\n",
      "name 'detect' is not defined Language is not detected: gruffgemini\n",
      "name 'detect' is not defined Language is not detected: Steam games dataset\n",
      "name 'detect' is not defined Language is not detected: User tags info and data from howlongtobeat.com on about 60000 Steam games\n",
      "name 'detect' is not defined Language is not detected: The dataset includes information on about 60000 video games from Steam platform. The key information is user tags, which describe the nature and prominent features of the game. The data also includes information about the average length of the game (taken from howlongtobeat.com).\n",
      "\n",
      "The dataset can be a great help in building a recommendation system that doesn't rely on peer preferences.\n",
      "name 'detect' is not defined Language is not detected: steam-games-dataset\n",
      "name 'detect' is not defined Language is not detected: gruffgemini\n",
      "name 'detect' is not defined Language is not detected: Steam games dataset\n",
      "name 'detect' is not defined Language is not detected: User tags info and data from howlongtobeat.com on about 60000 Steam games\n",
      "name 'detect' is not defined Language is not detected: The dataset includes information on about 60000 video games from Steam platform. The key information is user tags, which describe the nature and prominent features of the game. The data also includes information about the average length of the game (taken from howlongtobeat.com).\n",
      "\n",
      "The dataset can be a great help in building a recommendation system that doesn't rely on peer preferences.\n",
      "name 'detect' is not defined Language is not detected: abhishtagatya/my-anime-list-2021\n",
      "name 'detect' is not defined Language is not detected: my-anime-list-2021\n",
      "name 'detect' is not defined Language is not detected: abhishtagatya\n",
      "name 'detect' is not defined Language is not detected: MyAnimeList 2021\n",
      "name 'detect' is not defined Language is not detected: MyAnimeList Dataset Updated to 2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "MyAnimeList 2021 Dataset is a dataset created from scraping JikanPy API for MyAnimeList anime data. The following data is going to be used for personal projects for Anime Recommendation and Analysis.\n",
      "\n",
      "### Content\n",
      "\n",
      "Data collection of JikanPy API takes roughly 12+ Hours of API Scraping with the division approach of the index to search. With a huge list of roughly 18.000+ anime listed to date, I analyzed the data deeply to gather insights and created a recommendation engine for future applications.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset wouldn't have been possible without the huge library of memes provided for content from [ImgFlip](https://www.ImgFlip.com) \n",
      "\n",
      "This dataset wouldn't have been possible without the huge library of animes provided for content from [MyAnimeList](https://myanimelist.net) and [JikanPy Library](https://jikan.moe/)\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "As an aspiring Data Scientist learning and analyzing anime data. I find it exciting to gather more insights into anime from data collected by MyAnimeList. To analyze the origin, genres, audience, and etc. takes the approach in the notebook I've created.\n",
      "name 'detect' is not defined Language is not detected: my-anime-list-2021\n",
      "name 'detect' is not defined Language is not detected: abhishtagatya\n",
      "name 'detect' is not defined Language is not detected: MyAnimeList 2021\n",
      "name 'detect' is not defined Language is not detected: MyAnimeList Dataset Updated to 2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "MyAnimeList 2021 Dataset is a dataset created from scraping JikanPy API for MyAnimeList anime data. The following data is going to be used for personal projects for Anime Recommendation and Analysis.\n",
      "\n",
      "### Content\n",
      "\n",
      "Data collection of JikanPy API takes roughly 12+ Hours of API Scraping with the division approach of the index to search. With a huge list of roughly 18.000+ anime listed to date, I analyzed the data deeply to gather insights and created a recommendation engine for future applications.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset wouldn't have been possible without the huge library of memes provided for content from [ImgFlip](https://www.ImgFlip.com) \n",
      "\n",
      "This dataset wouldn't have been possible without the huge library of animes provided for content from [MyAnimeList](https://myanimelist.net) and [JikanPy Library](https://jikan.moe/)\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "As an aspiring Data Scientist learning and analyzing anime data. I find it exciting to gather more insights into anime from data collected by MyAnimeList. To analyze the origin, genres, audience, and etc. takes the approach in the notebook I've created.\n",
      "name 'detect' is not defined Language is not detected: cdr0101/netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: cdr0101\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset\n",
      "name 'detect' is not defined Language is not detected: From blockbuster movies to binge-worthy TV shows, this collection offers insights into the rich content library available on the popular streaming platform. Gain access to detailed information about each title, including genre, release year, cast, director, and synopsis. Whether you're a data enthusiast, a movie buff, or a TV aficionado, this dataset provides valuable resources for analysis, recommendation systems, and exploring trends in the ever-evolving landscape of digital entertainment. \n",
      "name 'detect' is not defined Language is not detected: netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: cdr0101\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset\n",
      "name 'detect' is not defined Language is not detected: From blockbuster movies to binge-worthy TV shows, this collection offers insights into the rich content library available on the popular streaming platform. Gain access to detailed information about each title, including genre, release year, cast, director, and synopsis. Whether you're a data enthusiast, a movie buff, or a TV aficionado, this dataset provides valuable resources for analysis, recommendation systems, and exploring trends in the ever-evolving landscape of digital entertainment. \n",
      "name 'detect' is not defined Language is not detected: aleksandrglotov/car-prices-poland\n",
      "name 'detect' is not defined Language is not detected: car-prices-poland\n",
      "name 'detect' is not defined Language is not detected: aleksandrglotov\n",
      "name 'detect' is not defined Language is not detected: Car Prices Poland\n",
      "name 'detect' is not defined Language is not detected: Predict sale prices for cars in Poland \n",
      "name 'detect' is not defined Language is not detected: Hello\n",
      "I am a Python developer and train the ability to parse, analyze data and create neural networks. I decided to share the dataset I'm currently working with. Maybe it will be useful to someone. Why a car? I love cars! As well as programming. If you are interested in me as a specialist, please contact me glotoffalexandr@gmail.com (I'm looking for a new job)\n",
      "\n",
      "The dataset was assembled in January 2022. Data from a well-known car sale site in Poland (which is public). Selenium and request were used for parsing (python of course)\n",
      "\n",
      "The dataset contains information about the make, model, generation, year of production, mileage, engine type and volume, localization and price\n",
      "\n",
      "I have ideas for expanding the model: add body type, configuration, color, power, etc.\n",
      "\n",
      "I see many ways to use models created on the basis of this dataset, I will describe them in notebooks\n",
      "name 'detect' is not defined Language is not detected: car-prices-poland\n",
      "name 'detect' is not defined Language is not detected: aleksandrglotov\n",
      "name 'detect' is not defined Language is not detected: Car Prices Poland\n",
      "name 'detect' is not defined Language is not detected: Predict sale prices for cars in Poland \n",
      "name 'detect' is not defined Language is not detected: Hello\n",
      "I am a Python developer and train the ability to parse, analyze data and create neural networks. I decided to share the dataset I'm currently working with. Maybe it will be useful to someone. Why a car? I love cars! As well as programming. If you are interested in me as a specialist, please contact me glotoffalexandr@gmail.com (I'm looking for a new job)\n",
      "\n",
      "The dataset was assembled in January 2022. Data from a well-known car sale site in Poland (which is public). Selenium and request were used for parsing (python of course)\n",
      "\n",
      "The dataset contains information about the make, model, generation, year of production, mileage, engine type and volume, localization and price\n",
      "\n",
      "I have ideas for expanding the model: add body type, configuration, color, power, etc.\n",
      "\n",
      "I see many ways to use models created on the basis of this dataset, I will describe them in notebooks\n",
      "name 'detect' is not defined Language is not detected: lakshmi25npathi/online-retail-dataset\n",
      "name 'detect' is not defined Language is not detected: online-retail-dataset\n",
      "name 'detect' is not defined Language is not detected: lakshmi25npathi\n",
      "name 'detect' is not defined Language is not detected: Online retail dataset \n",
      "name 'detect' is not defined Language is not detected: A real online retail transaction data set of two years\n",
      "name 'detect' is not defined Language is not detected: Abstract: A real online retail transaction data set of two years.\n",
      "\n",
      "Data Set Information:\n",
      "This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n",
      "\n",
      "Attribute Information:\n",
      "InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
      "StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
      "Description: Product (item) name. Nominal.\n",
      "Quantity: The quantities of each product (item) per transaction. Numeric.\n",
      "InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
      "UnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£).\n",
      "CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
      "Country: Country name. Nominal. The name of the country where a customer resides.\n",
      "\n",
      "Source:\n",
      "Dr. Daqing Chen, Course Director: MSc Data Science. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
      "\n",
      "Please find more information refer the below link,\n",
      "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
      "name 'detect' is not defined Language is not detected: online-retail-dataset\n",
      "name 'detect' is not defined Language is not detected: lakshmi25npathi\n",
      "name 'detect' is not defined Language is not detected: Online retail dataset \n",
      "name 'detect' is not defined Language is not detected: A real online retail transaction data set of two years\n",
      "name 'detect' is not defined Language is not detected: Abstract: A real online retail transaction data set of two years.\n",
      "\n",
      "Data Set Information:\n",
      "This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n",
      "\n",
      "Attribute Information:\n",
      "InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
      "StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
      "Description: Product (item) name. Nominal.\n",
      "Quantity: The quantities of each product (item) per transaction. Numeric.\n",
      "InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
      "UnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£).\n",
      "CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
      "Country: Country name. Nominal. The name of the country where a customer resides.\n",
      "\n",
      "Source:\n",
      "Dr. Daqing Chen, Course Director: MSc Data Science. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
      "\n",
      "Please find more information refer the below link,\n",
      "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
      "name 'detect' is not defined Language is not detected: gsagar12/dspp1\n",
      "name 'detect' is not defined Language is not detected: dspp1\n",
      "name 'detect' is not defined Language is not detected: gsagar12\n",
      "name 'detect' is not defined Language is not detected: Customer Subscription Data\n",
      "name 'detect' is not defined Language is not detected: Customer purchase and call center volume data for a subscription based product\n",
      "name 'detect' is not defined Language is not detected: This data is about a subscription-based digital product offering for financial advisory that includes newsletters, webinars, and investment recommendations. The offering has a couple of varieties, annual subscription, and digital subscription. The product also provides daytime support for customers to reach out to a care team that can help them with any product-related questions and signup/cancellation-related queries. \n",
      "\n",
      "*The data set contains the following information*\n",
      "- customer sign-up and cancellation dates at the product level\n",
      "- call center activity \n",
      "- customer demographics\n",
      "- product pricing info\n",
      "\n",
      "**Few of business questions to answer**\n",
      "1. Forecast 2022 year revenue if the company expects to add the same number of new customers as the previous year \n",
      "\n",
      "2. In 2022, one of the company's key initiatives is to improve the customer experience, one of the capabilities is superior customer care service, and they are thinking about enhancing in-product support better but that takes beyond 2022 to happen. It is very important to forecast call center case volume accurately so that they can make appropriate staffing decisions. Need to predict daily call volumes and make recommendations on how to staff on an hourly basis.\n",
      "name 'detect' is not defined Language is not detected: dspp1\n",
      "name 'detect' is not defined Language is not detected: gsagar12\n",
      "name 'detect' is not defined Language is not detected: Customer Subscription Data\n",
      "name 'detect' is not defined Language is not detected: Customer purchase and call center volume data for a subscription based product\n",
      "name 'detect' is not defined Language is not detected: This data is about a subscription-based digital product offering for financial advisory that includes newsletters, webinars, and investment recommendations. The offering has a couple of varieties, annual subscription, and digital subscription. The product also provides daytime support for customers to reach out to a care team that can help them with any product-related questions and signup/cancellation-related queries. \n",
      "\n",
      "*The data set contains the following information*\n",
      "- customer sign-up and cancellation dates at the product level\n",
      "- call center activity \n",
      "- customer demographics\n",
      "- product pricing info\n",
      "\n",
      "**Few of business questions to answer**\n",
      "1. Forecast 2022 year revenue if the company expects to add the same number of new customers as the previous year \n",
      "\n",
      "2. In 2022, one of the company's key initiatives is to improve the customer experience, one of the capabilities is superior customer care service, and they are thinking about enhancing in-product support better but that takes beyond 2022 to happen. It is very important to forecast call center case volume accurately so that they can make appropriate staffing decisions. Need to predict daily call volumes and make recommendations on how to staff on an hourly basis.\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital/aemo-qld-australia-electricity-demand-2018-2022\n",
      "name 'detect' is not defined Language is not detected: aemo-qld-australia-electricity-demand-2018-2022\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: QLD, AUS, Electricity Price & Demand 2018-2023\n",
      "name 'detect' is not defined Language is not detected: 30 min electricity demand and price data for QLD, Australia from 2018 to 2023\n",
      "name 'detect' is not defined Language is not detected: This data was downloaded from the [Australian Energy Market Operator (AEMO)](https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data), and contains 30 minute increments of electricity demand and price in Queensland, Australia, from 2018 to July 2023. This data was made publicly available by AEMO, as per their [Copyright Permission](https://aemo.com.au/privacy-and-legal-notices/copyright-permissions).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aemo-qld-australia-electricity-demand-2018-2022\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: QLD, AUS, Electricity Price & Demand 2018-2023\n",
      "name 'detect' is not defined Language is not detected: 30 min electricity demand and price data for QLD, Australia from 2018 to 2023\n",
      "name 'detect' is not defined Language is not detected: This data was downloaded from the [Australian Energy Market Operator (AEMO)](https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data), and contains 30 minute increments of electricity demand and price in Queensland, Australia, from 2018 to July 2023. This data was made publicly available by AEMO, as per their [Copyright Permission](https://aemo.com.au/privacy-and-legal-notices/copyright-permissions).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital/recipe-reviews-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: recipe-reviews-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews and User Feedback Dataset\n",
      "name 'detect' is not defined Language is not detected: Recipe name, its ranking on the top 100 recipes list, a unique recipe code, etc \n",
      "name 'detect' is not defined Language is not detected: **Overview**\n",
      "\n",
      "The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in sentiment analysis, user behavior analysis, recipe recommendation systems, and more. It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain.\n",
      "\n",
      "**Introductory Paper**\n",
      "\n",
      "[Textual Taste Buds: A Profound Exploration of Emotion Identification in Food Recipes through BERT and AttBiRNN Models](https://www.ijnrd.org/papers/IJNRD2310077.pdf)\n",
      "By Amir Ali, Stanislaw Matuszewski, Jacek Czupyt, Usman Ahmad. 2023\n",
      "Published in International Journal of Novel Research and Development\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2003977%2F4a1dd7a94f0a503d99a9324352434142%2FScreenshot%20from%202024-01-19%2013-33-04.png?generation=1705635221554249&alt=media)\n",
      "name 'detect' is not defined Language is not detected: recipe-reviews-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews and User Feedback Dataset\n",
      "name 'detect' is not defined Language is not detected: Recipe name, its ranking on the top 100 recipes list, a unique recipe code, etc \n",
      "name 'detect' is not defined Language is not detected: **Overview**\n",
      "\n",
      "The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in sentiment analysis, user behavior analysis, recipe recommendation systems, and more. It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain.\n",
      "\n",
      "**Introductory Paper**\n",
      "\n",
      "[Textual Taste Buds: A Profound Exploration of Emotion Identification in Food Recipes through BERT and AttBiRNN Models](https://www.ijnrd.org/papers/IJNRD2310077.pdf)\n",
      "By Amir Ali, Stanislaw Matuszewski, Jacek Czupyt, Usman Ahmad. 2023\n",
      "Published in International Journal of Novel Research and Development\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2003977%2F4a1dd7a94f0a503d99a9324352434142%2FScreenshot%20from%202024-01-19%2013-33-04.png?generation=1705635221554249&alt=media)\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital/top-10000-spotify-songs-1960-now\n",
      "name 'detect' is not defined Language is not detected: top-10000-spotify-songs-1960-now\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: Top 10000 Songs on Spotify 1960-Now\n",
      "name 'detect' is not defined Language is not detected: The best and biggest songs from ARIA & Billboard charts spanning 7 decades.\n",
      "name 'detect' is not defined Language is not detected: The \"Top 10000 Spotify Songs - ARIA and Billboard Charts\" is a comprehensive collection of 10,000 of the most popular songs that have dominated the music scene from 1960 to the present day. This dataset was curated based on rankings from both the ARIA (Australian Recording Industry Association) and Billboard charts, ensuring a diverse representation of songs that have achieved immense commercial success and cultural significance.\n",
      "\n",
      "The dataset encompasses various music genres and showcases the evolution of musical trends over the years, providing valuable insights into the ever-changing landscape of popular music. It includes tracks from iconic artists and bands, representing a mix of timeless classics and contemporary hits that have left a lasting impact on music lovers worldwide.\n",
      "\n",
      "Researchers, music enthusiasts, and data analysts can use this dataset for a wide range of applications, such as analyzing trends in music popularity, studying the influence of specific artists or albums, exploring genre shifts, and building recommendation systems based on historical music preferences.\n",
      "name 'detect' is not defined Language is not detected: top-10000-spotify-songs-1960-now\n",
      "name 'detect' is not defined Language is not detected: joebeachcapital\n",
      "name 'detect' is not defined Language is not detected: Top 10000 Songs on Spotify 1960-Now\n",
      "name 'detect' is not defined Language is not detected: The best and biggest songs from ARIA & Billboard charts spanning 7 decades.\n",
      "name 'detect' is not defined Language is not detected: The \"Top 10000 Spotify Songs - ARIA and Billboard Charts\" is a comprehensive collection of 10,000 of the most popular songs that have dominated the music scene from 1960 to the present day. This dataset was curated based on rankings from both the ARIA (Australian Recording Industry Association) and Billboard charts, ensuring a diverse representation of songs that have achieved immense commercial success and cultural significance.\n",
      "\n",
      "The dataset encompasses various music genres and showcases the evolution of musical trends over the years, providing valuable insights into the ever-changing landscape of popular music. It includes tracks from iconic artists and bands, representing a mix of timeless classics and contemporary hits that have left a lasting impact on music lovers worldwide.\n",
      "\n",
      "Researchers, music enthusiasts, and data analysts can use this dataset for a wide range of applications, such as analyzing trends in music popularity, studying the influence of specific artists or albums, exploring genre shifts, and building recommendation systems based on historical music preferences.\n",
      "name 'detect' is not defined Language is not detected: adityaramachandran27/nasa-near-earth-objects-information\n",
      "name 'detect' is not defined Language is not detected: nasa-near-earth-objects-information\n",
      "name 'detect' is not defined Language is not detected: adityaramachandran27\n",
      "name 'detect' is not defined Language is not detected: NASA Near Earth Objects Information \n",
      "name 'detect' is not defined Language is not detected: Information about all the NEOs by NASA\n",
      "name 'detect' is not defined Language is not detected: # Context \n",
      "\n",
      "Near-Earth objects are asteroids and comets with orbits that bring them to within 120 million miles (195 million kilometers) of the Sun, which means they can circulate through the Earth’s orbital neighborhood. Most near-Earth objects are asteroids that range in size from about 10 feet (a few meters) to nearly 25 miles (40 kilometers) across.\n",
      "\n",
      "The orbit of each object is computed by finding the elliptical path through space that best fits all the available observations, which often span many orbits over many years or decades. As more observations are made, the accuracy of an object's orbit improves dramatically, and it becomes possible to predict where an object will be years or even decades into the future – and whether it could come close to Earth.\n",
      "\n",
      "# Content \n",
      "This dataset was made using the data from the NASA Open API website \n",
      "This dataset has various information about the NEOs like:\n",
      "- NEO ID\n",
      "- Name\n",
      "- Designation\n",
      "- Absolute Magnitude\n",
      "- The minimum and the maximum estimated diameter in kilometers \n",
      "- Orbit ID\n",
      "- Orbit Class \n",
      "- Perihelion and Aphelion Distance\n",
      "- First and last observation date\n",
      "\n",
      "#Acknowledgements\n",
      "[NASA Open API\n",
      "](https://api.nasa.gov/)\n",
      "[Jet Propulsion Laboratory\n",
      "](https://www.jpl.nasa.gov/asteroid-watch)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nasa-near-earth-objects-information\n",
      "name 'detect' is not defined Language is not detected: adityaramachandran27\n",
      "name 'detect' is not defined Language is not detected: NASA Near Earth Objects Information \n",
      "name 'detect' is not defined Language is not detected: Information about all the NEOs by NASA\n",
      "name 'detect' is not defined Language is not detected: # Context \n",
      "\n",
      "Near-Earth objects are asteroids and comets with orbits that bring them to within 120 million miles (195 million kilometers) of the Sun, which means they can circulate through the Earth’s orbital neighborhood. Most near-Earth objects are asteroids that range in size from about 10 feet (a few meters) to nearly 25 miles (40 kilometers) across.\n",
      "\n",
      "The orbit of each object is computed by finding the elliptical path through space that best fits all the available observations, which often span many orbits over many years or decades. As more observations are made, the accuracy of an object's orbit improves dramatically, and it becomes possible to predict where an object will be years or even decades into the future – and whether it could come close to Earth.\n",
      "\n",
      "# Content \n",
      "This dataset was made using the data from the NASA Open API website \n",
      "This dataset has various information about the NEOs like:\n",
      "- NEO ID\n",
      "- Name\n",
      "- Designation\n",
      "- Absolute Magnitude\n",
      "- The minimum and the maximum estimated diameter in kilometers \n",
      "- Orbit ID\n",
      "- Orbit Class \n",
      "- Perihelion and Aphelion Distance\n",
      "- First and last observation date\n",
      "\n",
      "#Acknowledgements\n",
      "[NASA Open API\n",
      "](https://api.nasa.gov/)\n",
      "[Jet Propulsion Laboratory\n",
      "](https://www.jpl.nasa.gov/asteroid-watch)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: jvthunder/urban-traffic-flow-csv\n",
      "name 'detect' is not defined Language is not detected: urban-traffic-flow-csv\n",
      "name 'detect' is not defined Language is not detected: jvthunder\n",
      "name 'detect' is not defined Language is not detected: Urban Traffic Flow CSV\n",
      "name 'detect' is not defined Language is not detected: Forecast the Traffic Flow using Time Series and Graph Connections\n",
      "name 'detect' is not defined Language is not detected: This dataset originated from [UCI Traffic Flow Forecasting Data Set](https://archive.ics.uci.edu/ml/datasets/Traffic+Flow+Forecasting#). The original file is in matlab format and I have changed it into csv file in [this notebook](https://www.kaggle.com/jvthunder/urban-traffic-time-series-forecasting). \n",
      "\n",
      "There is 3 files:\n",
      "- train.csv (Contains 1261 timesteps for 36 locations calculated with 15 minutes intervals)\n",
      "- test.csv (Contains 840 timesteps for 36 locations calculated with 15 minutes intervals)\n",
      "- adj_matrix.csv (Contains the location's adjacency matrix, so you can include graph connectivity features)\n",
      "\n",
      "The features in the train.csv are:\n",
      "- timestep (the timestep of data)\n",
      "- location (the location of data)\n",
      "- traffic (the traffic rate at the point of time (from 0 to 1))\n",
      "- prev_i (the i-th previous traffic rate)\n",
      "- hour_i (one hot encoding of the current hour)\n",
      "- feata_i, featb_i, featc_i (these columns I am not sure of, but the sum values suggests they are one hot encodings (only 0 or 1 values) of a certain value, you can include or not include this for your predictions)\n",
      "\n",
      "\n",
      "One of the tasks you can do with this dataset is to predict the `traffic` columns.\n",
      "name 'detect' is not defined Language is not detected: urban-traffic-flow-csv\n",
      "name 'detect' is not defined Language is not detected: jvthunder\n",
      "name 'detect' is not defined Language is not detected: Urban Traffic Flow CSV\n",
      "name 'detect' is not defined Language is not detected: Forecast the Traffic Flow using Time Series and Graph Connections\n",
      "name 'detect' is not defined Language is not detected: This dataset originated from [UCI Traffic Flow Forecasting Data Set](https://archive.ics.uci.edu/ml/datasets/Traffic+Flow+Forecasting#). The original file is in matlab format and I have changed it into csv file in [this notebook](https://www.kaggle.com/jvthunder/urban-traffic-time-series-forecasting). \n",
      "\n",
      "There is 3 files:\n",
      "- train.csv (Contains 1261 timesteps for 36 locations calculated with 15 minutes intervals)\n",
      "- test.csv (Contains 840 timesteps for 36 locations calculated with 15 minutes intervals)\n",
      "- adj_matrix.csv (Contains the location's adjacency matrix, so you can include graph connectivity features)\n",
      "\n",
      "The features in the train.csv are:\n",
      "- timestep (the timestep of data)\n",
      "- location (the location of data)\n",
      "- traffic (the traffic rate at the point of time (from 0 to 1))\n",
      "- prev_i (the i-th previous traffic rate)\n",
      "- hour_i (one hot encoding of the current hour)\n",
      "- feata_i, featb_i, featc_i (these columns I am not sure of, but the sum values suggests they are one hot encodings (only 0 or 1 values) of a certain value, you can include or not include this for your predictions)\n",
      "\n",
      "\n",
      "One of the tasks you can do with this dataset is to predict the `traffic` columns.\n",
      "name 'detect' is not defined Language is not detected: sureshmecad/independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: sureshmecad\n",
      "name 'detect' is not defined Language is not detected: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Topic Modeling for Research Articles\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Since the lockdown was announced in the country back in March, we started with a 1 day hackathon called Janatahack inspired from Janata cerfew to start our war against the pandemic.  Looking at the amazing response and demand for more, we continued the hackathons over the weekends every week. Janatahack today is a phenomena where loads of esteemed members of our community regularly participate to showcase their machine learning skills by sharing their approaches and more important to learn how to apply machine learning and predictive analytics to new domains such as agriculture, Banking, IOT, forecasting and so on. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Hackathon\n",
      "name 'detect' is not defined Language is not detected: independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: sureshmecad\n",
      "name 'detect' is not defined Language is not detected: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Topic Modeling for Research Articles\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Since the lockdown was announced in the country back in March, we started with a 1 day hackathon called Janatahack inspired from Janata cerfew to start our war against the pandemic.  Looking at the amazing response and demand for more, we continued the hackathons over the weekends every week. Janatahack today is a phenomena where loads of esteemed members of our community regularly participate to showcase their machine learning skills by sharing their approaches and more important to learn how to apply machine learning and predictive analytics to new domains such as agriculture, Banking, IOT, forecasting and so on. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Hackathon\n",
      "name 'detect' is not defined Language is not detected: flenderson/sales-analysis\n",
      "name 'detect' is not defined Language is not detected: sales-analysis\n",
      "name 'detect' is not defined Language is not detected: flenderson\n",
      "name 'detect' is not defined Language is not detected: Historical Sales and Active Inventory\n",
      "name 'detect' is not defined Language is not detected: Records of sold and unsold products and their characteristics\n",
      "name 'detect' is not defined Language is not detected: # Context \n",
      "\n",
      "Attached is a set of products in which we are trying to determine which products we should continue to sell, and which products to remove from our inventory. The file contains BOTH historical sales data AND active inventory, which can be discerned with the column titled \"File Type\". \n",
      "\n",
      "We suspect that data science applied to the set--such as a decision tree analysis or logistic regression, or some other machine learning model---can help us generate a value (i.e., probability score) for each product, that can be used as the main determinant evaluating the inventory. Each row in the file represents one product. \n",
      "\n",
      "It is important to note that we have MANY products in our inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year.\n",
      "\n",
      "\n",
      "# Content\n",
      "\n",
      "The file contains historical sales data (identified with the column titled File_Type) along with current active inventory that is in need of evaluation (i.e., File Type = \"Active\"). The historical data shows sales for the past 6 months. The binary target (1 = sale, 0 = no sale in past six months) is likely the primary target that should drive the analysis. \n",
      "\n",
      "The other columns contain numeric and categorical attributes that we deem relevant to sales. \n",
      "\n",
      "Note that some of the historical sales SKUs are ALSO included in the active inventory. \n",
      "\n",
      "A few comments about the attributes included, as we realize we may have some attributes that are unnecessary or may need to be explained. \n",
      "\n",
      "- SKU_number: This is the unique identifier for each product.\n",
      "\n",
      "- Order: Just a sequential counter. Can be ignored.\n",
      "\n",
      "- SoldFlag: 1 = sold in past 6 mos. 0 = Not sold\n",
      "\n",
      "- MarketingType = Two categories of how we market the product. This should probably be ignored, or better yet, each type should be considered independently. \n",
      "\n",
      "- New_Release_Flag = Any product that has had a future release (i.e.,  Release Number > 1)\n",
      "\n",
      "# Inspiration\n",
      "\n",
      "(1) What is the best model to use that will provide us with a probability estimate of a sale for each SKU? We are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add, as we are able). \n",
      "\n",
      "(2) Is it possible to provide a scored file (i.e., a probability score for each SKU in the file), and to provide an evaluation of the accuracy of the selected model? \n",
      "\n",
      "(3) What are the next steps we should take?\n",
      "\n",
      "Thanks very much for any suggestions you may provide.\n",
      "name 'detect' is not defined Language is not detected: sales-analysis\n",
      "name 'detect' is not defined Language is not detected: flenderson\n",
      "name 'detect' is not defined Language is not detected: Historical Sales and Active Inventory\n",
      "name 'detect' is not defined Language is not detected: Records of sold and unsold products and their characteristics\n",
      "name 'detect' is not defined Language is not detected: # Context \n",
      "\n",
      "Attached is a set of products in which we are trying to determine which products we should continue to sell, and which products to remove from our inventory. The file contains BOTH historical sales data AND active inventory, which can be discerned with the column titled \"File Type\". \n",
      "\n",
      "We suspect that data science applied to the set--such as a decision tree analysis or logistic regression, or some other machine learning model---can help us generate a value (i.e., probability score) for each product, that can be used as the main determinant evaluating the inventory. Each row in the file represents one product. \n",
      "\n",
      "It is important to note that we have MANY products in our inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year.\n",
      "\n",
      "\n",
      "# Content\n",
      "\n",
      "The file contains historical sales data (identified with the column titled File_Type) along with current active inventory that is in need of evaluation (i.e., File Type = \"Active\"). The historical data shows sales for the past 6 months. The binary target (1 = sale, 0 = no sale in past six months) is likely the primary target that should drive the analysis. \n",
      "\n",
      "The other columns contain numeric and categorical attributes that we deem relevant to sales. \n",
      "\n",
      "Note that some of the historical sales SKUs are ALSO included in the active inventory. \n",
      "\n",
      "A few comments about the attributes included, as we realize we may have some attributes that are unnecessary or may need to be explained. \n",
      "\n",
      "- SKU_number: This is the unique identifier for each product.\n",
      "\n",
      "- Order: Just a sequential counter. Can be ignored.\n",
      "\n",
      "- SoldFlag: 1 = sold in past 6 mos. 0 = Not sold\n",
      "\n",
      "- MarketingType = Two categories of how we market the product. This should probably be ignored, or better yet, each type should be considered independently. \n",
      "\n",
      "- New_Release_Flag = Any product that has had a future release (i.e.,  Release Number > 1)\n",
      "\n",
      "# Inspiration\n",
      "\n",
      "(1) What is the best model to use that will provide us with a probability estimate of a sale for each SKU? We are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add, as we are able). \n",
      "\n",
      "(2) Is it possible to provide a scored file (i.e., a probability score for each SKU in the file), and to provide an evaluation of the accuracy of the selected model? \n",
      "\n",
      "(3) What are the next steps we should take?\n",
      "\n",
      "Thanks very much for any suggestions you may provide.\n",
      "name 'detect' is not defined Language is not detected: vishalmane109/anime-recommendations-database\n",
      "name 'detect' is not defined Language is not detected: anime-recommendations-database\n",
      "name 'detect' is not defined Language is not detected: vishalmane109\n",
      "name 'detect' is not defined Language is not detected: Anime  Database for Recommendation system\n",
      "name 'detect' is not defined Language is not detected: anime information database for developing recommendation system or data analysis\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains a total of 16737 unique animes. The reason for creating this dataset is the requirement of a  clean dataset of Anime. I found a few datasets on anime, most of the datasets had the major anime but some dataset 1) doesn't have 'Genre' or 'Synopsis' of anime. For content-based recommendation, it is helpful if we have more information about anime 2) have duplicate data 3) missing data is represented by different notations.\n",
      "\n",
      "\n",
      "### Content\n",
      "Anime_id    :anime Id (as per  myanimelist.net)\n",
      "Title            : name of anime\n",
      "Genre         :Main genre   \n",
      "Synopsis    :Brief Discription\n",
      "Type            \n",
      "Producer \n",
      "Studio\n",
      "Rating            :Rating of anime as pe myanimelist.net/ \n",
      "ScoredBy      : Total no user scored given anime\n",
      "Popularity     :Rank of anime based on popularity\n",
      "Members      :No of members added given anime on their list\n",
      "Episodes      : No. of episodes\n",
      "Source     \n",
      "Aired\n",
      "Link\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset is a combination of 2 datasets\n",
      "\n",
      "1. https://docs.google.com/spreadsheets/d/1brguO5nGfXS-Fr1Xcf3pqPTQoBUPGLTYM_EMAA9yJFw/export?format=csv&id=1brguO5nGfXS-Fr1Xcf3pqPTQoBUPGLTYM_EMAA9yJFw&gid=0 \n",
      "2. https://www.kaggle.com/CooperUnion/anime-recommendations-database\n",
      "\n",
      "name 'detect' is not defined Language is not detected: anime-recommendations-database\n",
      "name 'detect' is not defined Language is not detected: vishalmane109\n",
      "name 'detect' is not defined Language is not detected: Anime  Database for Recommendation system\n",
      "name 'detect' is not defined Language is not detected: anime information database for developing recommendation system or data analysis\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains a total of 16737 unique animes. The reason for creating this dataset is the requirement of a  clean dataset of Anime. I found a few datasets on anime, most of the datasets had the major anime but some dataset 1) doesn't have 'Genre' or 'Synopsis' of anime. For content-based recommendation, it is helpful if we have more information about anime 2) have duplicate data 3) missing data is represented by different notations.\n",
      "\n",
      "\n",
      "### Content\n",
      "Anime_id    :anime Id (as per  myanimelist.net)\n",
      "Title            : name of anime\n",
      "Genre         :Main genre   \n",
      "Synopsis    :Brief Discription\n",
      "Type            \n",
      "Producer \n",
      "Studio\n",
      "Rating            :Rating of anime as pe myanimelist.net/ \n",
      "ScoredBy      : Total no user scored given anime\n",
      "Popularity     :Rank of anime based on popularity\n",
      "Members      :No of members added given anime on their list\n",
      "Episodes      : No. of episodes\n",
      "Source     \n",
      "Aired\n",
      "Link\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset is a combination of 2 datasets\n",
      "\n",
      "1. https://docs.google.com/spreadsheets/d/1brguO5nGfXS-Fr1Xcf3pqPTQoBUPGLTYM_EMAA9yJFw/export?format=csv&id=1brguO5nGfXS-Fr1Xcf3pqPTQoBUPGLTYM_EMAA9yJFw&gid=0 \n",
      "2. https://www.kaggle.com/CooperUnion/anime-recommendations-database\n",
      "\n",
      "name 'detect' is not defined Language is not detected: xionglang/recommendation\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: xionglang\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: xionglang\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: raghavgarg12/vortex2\n",
      "name 'detect' is not defined Language is not detected: vortex2\n",
      "name 'detect' is not defined Language is not detected: raghavgarg12\n",
      "name 'detect' is not defined Language is not detected: Vortex\n",
      "name 'detect' is not defined Language is not detected: Heart disease and stroke mortality in the United States\n",
      "name 'detect' is not defined Language is not detected: Introduction:\n",
      "The dataset documents rates and trends in heart disease and stroke mortality in the United States from 2000 to 2019. It provides county-level estimates of heart disease and stroke death rates.The data is segmented by age group, race/ethnicity, and sex to comprehensively analyse mortality patterns and trends.\n",
      "\n",
      "Dataset Characteristics:\n",
      "The dataset encompasses mortality rates and heart disease and stroke trends across various demographic groups and time periods. This dataset has been sourced from the official website of Centre for Disease Control and Prevention(CDC),US Government under ODC(1.0) license.\n",
      "\n",
      "The dataset presents the occurrences of various cardiovascular diseases like coronary heart diseases, stroke, hypertension etc. among people of different age groups, ethnicities and gender in different counties of the US over a period of 20 years.\n",
      "name 'detect' is not defined Language is not detected: vortex2\n",
      "name 'detect' is not defined Language is not detected: raghavgarg12\n",
      "name 'detect' is not defined Language is not detected: Vortex\n",
      "name 'detect' is not defined Language is not detected: Heart disease and stroke mortality in the United States\n",
      "name 'detect' is not defined Language is not detected: Introduction:\n",
      "The dataset documents rates and trends in heart disease and stroke mortality in the United States from 2000 to 2019. It provides county-level estimates of heart disease and stroke death rates.The data is segmented by age group, race/ethnicity, and sex to comprehensively analyse mortality patterns and trends.\n",
      "\n",
      "Dataset Characteristics:\n",
      "The dataset encompasses mortality rates and heart disease and stroke trends across various demographic groups and time periods. This dataset has been sourced from the official website of Centre for Disease Control and Prevention(CDC),US Government under ODC(1.0) license.\n",
      "\n",
      "The dataset presents the occurrences of various cardiovascular diseases like coronary heart diseases, stroke, hypertension etc. among people of different age groups, ethnicities and gender in different counties of the US over a period of 20 years.\n",
      "name 'detect' is not defined Language is not detected: erfanloghmani/myket-android-application-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: myket-android-application-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: erfanloghmani\n",
      "name 'detect' is not defined Language is not detected: Myket Android Application Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Application install interactions of users in Myket android application market\n",
      "name 'detect' is not defined Language is not detected: # Myket Android Application Install Dataset\n",
      "\n",
      "This dataset contains information on application install interactions of users in the [Myket](https://myket.ir/) android application market. The dataset was created for the purpose of evaluating interaction prediction models, requiring user and item identifiers along with timestamps of the interactions. Hence, the dataset can be used for interaction prediction and building a recommendation system. Furthermore, the data forms a dynamic network of interactions, and we can also perform network representation learning on the nodes in the network, which are users and applications.\n",
      "\n",
      "## Data Creation\n",
      "\n",
      "The dataset was initially generated by the Myket data team, and later cleaned and subsampled by Erfan Loghmani a master student at Sharif University of Technology at the time. The data team focused on a two-week period and randomly sampled 1/3 of the users with interactions during that period. They then selected install and update interactions for three months before and after the two-week period, resulting in interactions spanning about 6 months and two weeks.\n",
      "\n",
      "We further subsampled and cleaned the data to focus on application download interactions. We identified the top 8000 most installed applications and selected interactions related to them. We retained users with more than 32 interactions, resulting in 280,391 users. From this group, we randomly selected 10,000 users, and the data was filtered to include only interactions for these users. The detailed procedure can be found in [here](create_data.ipynb).\n",
      "\n",
      "## Data Structure\n",
      "\n",
      "The dataset has two main files.\n",
      "\n",
      "- `myket.csv`: This file contains the interaction information and follows the same format as the datasets used in the \"[JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks](https://github.com/claws-lab/jodie)\" (ACM SIGKDD 2019) project. However, this data does not contain state labels and interaction features, resulting in associated columns being all zero.\n",
      "- `app_info_sample.csv`: This file comprises features associated with applications present in the sample. For each individual application, information such as the approximate number of installs, average rating, count of ratings, and category are included. These features provide insights into the applications present in the dataset.\n",
      "\n",
      "## Dataset Details\n",
      "\n",
      "- Total Instances: 694,121 install interaction instances\n",
      "- Instances Format: Triplets of user_id, app_name, timestamp\n",
      "- 10,000 users and 7,988 android applications\n",
      "- Item features for 7,606 applications\n",
      "\n",
      "For a detailed summary of the data's statistics, including information on users, applications, and interactions, please refer to the Python notebook available at [summary-stats.ipynb](summary-stats.ipynb). The notebook provides an overview of the dataset's characteristics and can be helpful for understanding the data's structure before using it for research or analysis.\n",
      "\n",
      "### Top 20 Most Installed Applications\n",
      "\n",
      "| Package Name                       | Count of Interactions |\n",
      "| ---------------------------------- | --------------------- |\n",
      "| com.instagram.android              | 15292                 |\n",
      "| ir.resaneh1.iptv                   | 12143                 |\n",
      "| com.tencent.ig                     | 7919                  |\n",
      "| com.ForgeGames.SpecialForcesGroup2 | 7797                  |\n",
      "| ir.nomogame.ClutchGame             | 6193                  |\n",
      "| com.dts.freefireth                 | 6041                  |\n",
      "| com.whatsapp                       | 5876                  |\n",
      "| com.supercell.clashofclans         | 5817                  |\n",
      "| com.mojang.minecraftpe             | 5649                  |\n",
      "| com.lenovo.anyshare.gps            | 5076                  |\n",
      "| ir.medu.shad                       | 4673                  |\n",
      "| com.firsttouchgames.dls3           | 4641                  |\n",
      "| com.activision.callofduty.shooter  | 4357                  |\n",
      "| com.tencent.iglite                 | 4126                  |\n",
      "| com.aparat                         | 3598                  |\n",
      "| com.kiloo.subwaysurf               | 3135                  |\n",
      "| com.supercell.clashroyale          | 2793                  |\n",
      "| co.palang.QuizOfKings              | 2589                  |\n",
      "| com.nazdika.app                    | 2436                  |\n",
      "| com.digikala                       | 2413                  |\n",
      "\n",
      "## Comparison with SNAP Datasets\n",
      "\n",
      "The Myket dataset introduced in this repository exhibits distinct characteristics compared to the real-world datasets used by the project. The table below provides a comparative overview of the key dataset characteristics:\n",
      "\n",
      "| Dataset         | #Users           | #Items          | #Interactions | Average Interactions per User | Average Unique Items per User |\n",
      "| --------------- | ---------------- | --------------- | ------------- | ----------------------------- | ----------------------------- |\n",
      "| **Myket** | **10,000** | **7,988** | 694,121       | 69.4                          | 54.6                          |\n",
      "| LastFM          | 980              | 1,000           | 1,293,103     | 1,319.5                       | 158.2                         |\n",
      "| Reddit          | **10,000** | 984             | 672,447       | 67.2                          | 7.9                           |\n",
      "| Wikipedia       | 8,227            | 1,000           | 157,474       | 19.1                          | 2.2                           |\n",
      "| MOOC            | 7,047            | 97              | 411,749       | 58.4                          | 25.3                          |\n",
      "\n",
      "The Myket dataset stands out by having an ample number of both users and items, highlighting its relevance for real-world, large-scale applications. Unlike LastFM, Reddit, and Wikipedia datasets, where users exhibit repetitive item interactions, the Myket dataset contains a comparatively lower amount of repetitive interactions. This unique characteristic reflects the diverse nature of user behaviors in the Android application market environment.\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this dataset in your research, please cite the following [preprint](https://arxiv.org/abs/2308.06862):\n",
      "\n",
      "```\n",
      "@misc{loghmani2023effect,\n",
      "      title={Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks}, \n",
      "      author={Erfan Loghmani and MohammadAmin Fazli},\n",
      "      year={2023},\n",
      "      eprint={2308.06862},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.LG}\n",
      "}\n",
      "```\n",
      "\n",
      "License: MIT\n",
      "name 'detect' is not defined Language is not detected: myket-android-application-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: erfanloghmani\n",
      "name 'detect' is not defined Language is not detected: Myket Android Application Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Application install interactions of users in Myket android application market\n",
      "name 'detect' is not defined Language is not detected: # Myket Android Application Install Dataset\n",
      "\n",
      "This dataset contains information on application install interactions of users in the [Myket](https://myket.ir/) android application market. The dataset was created for the purpose of evaluating interaction prediction models, requiring user and item identifiers along with timestamps of the interactions. Hence, the dataset can be used for interaction prediction and building a recommendation system. Furthermore, the data forms a dynamic network of interactions, and we can also perform network representation learning on the nodes in the network, which are users and applications.\n",
      "\n",
      "## Data Creation\n",
      "\n",
      "The dataset was initially generated by the Myket data team, and later cleaned and subsampled by Erfan Loghmani a master student at Sharif University of Technology at the time. The data team focused on a two-week period and randomly sampled 1/3 of the users with interactions during that period. They then selected install and update interactions for three months before and after the two-week period, resulting in interactions spanning about 6 months and two weeks.\n",
      "\n",
      "We further subsampled and cleaned the data to focus on application download interactions. We identified the top 8000 most installed applications and selected interactions related to them. We retained users with more than 32 interactions, resulting in 280,391 users. From this group, we randomly selected 10,000 users, and the data was filtered to include only interactions for these users. The detailed procedure can be found in [here](create_data.ipynb).\n",
      "\n",
      "## Data Structure\n",
      "\n",
      "The dataset has two main files.\n",
      "\n",
      "- `myket.csv`: This file contains the interaction information and follows the same format as the datasets used in the \"[JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks](https://github.com/claws-lab/jodie)\" (ACM SIGKDD 2019) project. However, this data does not contain state labels and interaction features, resulting in associated columns being all zero.\n",
      "- `app_info_sample.csv`: This file comprises features associated with applications present in the sample. For each individual application, information such as the approximate number of installs, average rating, count of ratings, and category are included. These features provide insights into the applications present in the dataset.\n",
      "\n",
      "## Dataset Details\n",
      "\n",
      "- Total Instances: 694,121 install interaction instances\n",
      "- Instances Format: Triplets of user_id, app_name, timestamp\n",
      "- 10,000 users and 7,988 android applications\n",
      "- Item features for 7,606 applications\n",
      "\n",
      "For a detailed summary of the data's statistics, including information on users, applications, and interactions, please refer to the Python notebook available at [summary-stats.ipynb](summary-stats.ipynb). The notebook provides an overview of the dataset's characteristics and can be helpful for understanding the data's structure before using it for research or analysis.\n",
      "\n",
      "### Top 20 Most Installed Applications\n",
      "\n",
      "| Package Name                       | Count of Interactions |\n",
      "| ---------------------------------- | --------------------- |\n",
      "| com.instagram.android              | 15292                 |\n",
      "| ir.resaneh1.iptv                   | 12143                 |\n",
      "| com.tencent.ig                     | 7919                  |\n",
      "| com.ForgeGames.SpecialForcesGroup2 | 7797                  |\n",
      "| ir.nomogame.ClutchGame             | 6193                  |\n",
      "| com.dts.freefireth                 | 6041                  |\n",
      "| com.whatsapp                       | 5876                  |\n",
      "| com.supercell.clashofclans         | 5817                  |\n",
      "| com.mojang.minecraftpe             | 5649                  |\n",
      "| com.lenovo.anyshare.gps            | 5076                  |\n",
      "| ir.medu.shad                       | 4673                  |\n",
      "| com.firsttouchgames.dls3           | 4641                  |\n",
      "| com.activision.callofduty.shooter  | 4357                  |\n",
      "| com.tencent.iglite                 | 4126                  |\n",
      "| com.aparat                         | 3598                  |\n",
      "| com.kiloo.subwaysurf               | 3135                  |\n",
      "| com.supercell.clashroyale          | 2793                  |\n",
      "| co.palang.QuizOfKings              | 2589                  |\n",
      "| com.nazdika.app                    | 2436                  |\n",
      "| com.digikala                       | 2413                  |\n",
      "\n",
      "## Comparison with SNAP Datasets\n",
      "\n",
      "The Myket dataset introduced in this repository exhibits distinct characteristics compared to the real-world datasets used by the project. The table below provides a comparative overview of the key dataset characteristics:\n",
      "\n",
      "| Dataset         | #Users           | #Items          | #Interactions | Average Interactions per User | Average Unique Items per User |\n",
      "| --------------- | ---------------- | --------------- | ------------- | ----------------------------- | ----------------------------- |\n",
      "| **Myket** | **10,000** | **7,988** | 694,121       | 69.4                          | 54.6                          |\n",
      "| LastFM          | 980              | 1,000           | 1,293,103     | 1,319.5                       | 158.2                         |\n",
      "| Reddit          | **10,000** | 984             | 672,447       | 67.2                          | 7.9                           |\n",
      "| Wikipedia       | 8,227            | 1,000           | 157,474       | 19.1                          | 2.2                           |\n",
      "| MOOC            | 7,047            | 97              | 411,749       | 58.4                          | 25.3                          |\n",
      "\n",
      "The Myket dataset stands out by having an ample number of both users and items, highlighting its relevance for real-world, large-scale applications. Unlike LastFM, Reddit, and Wikipedia datasets, where users exhibit repetitive item interactions, the Myket dataset contains a comparatively lower amount of repetitive interactions. This unique characteristic reflects the diverse nature of user behaviors in the Android application market environment.\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this dataset in your research, please cite the following [preprint](https://arxiv.org/abs/2308.06862):\n",
      "\n",
      "```\n",
      "@misc{loghmani2023effect,\n",
      "      title={Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks}, \n",
      "      author={Erfan Loghmani and MohammadAmin Fazli},\n",
      "      year={2023},\n",
      "      eprint={2308.06862},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.LG}\n",
      "}\n",
      "```\n",
      "\n",
      "License: MIT\n",
      "name 'detect' is not defined Language is not detected: smartpunchteam/boxpunch-dataset\n",
      "name 'detect' is not defined Language is not detected: boxpunch-dataset\n",
      "name 'detect' is not defined Language is not detected: smartpunchteam\n",
      "name 'detect' is not defined Language is not detected: Boxpunch dataset\n",
      "name 'detect' is not defined Language is not detected: Accelerometer raw data measured from a Smartwatch\n",
      "name 'detect' is not defined Language is not detected: <h1>Accelerometer raw data of 7606 box punches - annotated on a smartwatch</h1>\n",
      "<hr>\n",
      "<h2>Short Description</h2>\n",
      "This dataset contains all annotated box punches. It consists of:\n",
      "<ul>\n",
      "<li>4 different classes (frontal punch, side-cut, hook, no-action)</li>\n",
      "<li>left / right orientated</li>\n",
      "<li>of 8 different subjects (boxers)</li>\n",
      "</ul>\n",
      "\n",
      "This dataset is part of the \"smartPunch\" project were box punch data measured from a smartwatch app is classified by machine learning algorithms like decision trees, SVMs, logistical regression and k-nearest-neighbor.\n",
      "<hr>\n",
      "<h2>Gettings started</h2>\n",
      "See the detailed description of this dataset and the structure at the github repository for that dataset. Its available at: \n",
      "<br>\n",
      "See also the other project files like the Database, server application and smartwatch app files and - the machine learning files. All the stuff is delivered with some tutorials which will help you to understand the hole project and challenge the box punch classification using the smartwatch app and the machine learning algorithms. <br>\n",
      "__Happy Coding :-)__\n",
      "name 'detect' is not defined Language is not detected: boxpunch-dataset\n",
      "name 'detect' is not defined Language is not detected: smartpunchteam\n",
      "name 'detect' is not defined Language is not detected: Boxpunch dataset\n",
      "name 'detect' is not defined Language is not detected: Accelerometer raw data measured from a Smartwatch\n",
      "name 'detect' is not defined Language is not detected: <h1>Accelerometer raw data of 7606 box punches - annotated on a smartwatch</h1>\n",
      "<hr>\n",
      "<h2>Short Description</h2>\n",
      "This dataset contains all annotated box punches. It consists of:\n",
      "<ul>\n",
      "<li>4 different classes (frontal punch, side-cut, hook, no-action)</li>\n",
      "<li>left / right orientated</li>\n",
      "<li>of 8 different subjects (boxers)</li>\n",
      "</ul>\n",
      "\n",
      "This dataset is part of the \"smartPunch\" project were box punch data measured from a smartwatch app is classified by machine learning algorithms like decision trees, SVMs, logistical regression and k-nearest-neighbor.\n",
      "<hr>\n",
      "<h2>Gettings started</h2>\n",
      "See the detailed description of this dataset and the structure at the github repository for that dataset. Its available at: \n",
      "<br>\n",
      "See also the other project files like the Database, server application and smartwatch app files and - the machine learning files. All the stuff is delivered with some tutorials which will help you to understand the hole project and challenge the box punch classification using the smartwatch app and the machine learning algorithms. <br>\n",
      "__Happy Coding :-)__\n",
      "name 'detect' is not defined Language is not detected: undersc0re/flight-delay-and-causes\n",
      "name 'detect' is not defined Language is not detected: flight-delay-and-causes\n",
      "name 'detect' is not defined Language is not detected: undersc0re\n",
      "name 'detect' is not defined Language is not detected: Flight Delay and Causes\n",
      "name 'detect' is not defined Language is not detected: This Dataset contains Flights trip and multiple cause of delay\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Using this data you can find what caused the delay for flight whether it's Security delay, NAS delay or Carrier delay, etc. \n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Data Dictionary\n",
      "\n",
      "1. DayOfWeek  → 1 (Monday) - 7 (Sunday)\n",
      "2. Date       → Scheduled date \n",
      "3. DepTime    → Actual departure time (local, hhmm)\n",
      "4. ArrTime    →  Actual arrival time (local, hhmm)\n",
      "5. CRSArrTime  →  Scheduled arrival time (local, hhmm)\n",
      "6. UniqueCarrier  → Unique carrier code\n",
      "7. Airline   → Airline company\n",
      "8. FlightNum  →   flight number\n",
      "9. TailNum  →   plane tail number\n",
      "10. ActualElapsedTime  →  Actual time an airplane spends in the air(in minutes) with TaxiIn/Out\n",
      "11. CRSElapsedTime  →   CRS Elapsed Time of Flight (estimated elapse time), in minutes\n",
      "12. AirTime  → Flight Time (in minutes)\n",
      "13. ArrDelay →  Difference in minutes between scheduled and actual arrival time\n",
      "14. Origin  →   Origin IATA(International Air Transport Association) airport code   \n",
      "15. Org_Airport → Origin Airport Name\n",
      "16. Dest    → Destination IATA code\n",
      "17. Dest_Airport → Destination Airport Name\n",
      "18. Distance → Distance between airports (miles) \n",
      "19. TaxiIn →  Wheels down and arrival at the destination airport gate, in minutes\n",
      "20. TaxiOut →  The time elapsed between departure from the origin airport gate and wheels off, in minutes\n",
      "21. Cancelled → Was the flight canceled?\n",
      "22. CancellationCode → Reason for cancellation\n",
      "23. Diverted       →  1 = yes, 0 = no\n",
      "24. CarrierDelay → Flight delay due to carrier(e.g. maintenance or crew problems, aircraft cleaning, fueling, etc), 0 = No, yes = (in minutes)\n",
      "25. WeatherDelay  → Flight delay due to weather, 0 = No, yes = (in minutes)\n",
      "26. NASDelay     → Flight delay by NSA(National Aviation System), 0 = No, yes = (in minutes)\n",
      "27. SecurityDelay   → Flight delay by this reason, 0 = No, yes = (in minutes)\n",
      "28. LateAircraftDelay  → Flight delay by this reason, 0 = No, yes = (in minutes)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "Inspired from others DataSet in the same domain. So, tried to create one balanced dataset ready to use for beginners. This is a public dataset so it's not Licensed by anyone. \n",
      "name 'detect' is not defined Language is not detected: flight-delay-and-causes\n",
      "name 'detect' is not defined Language is not detected: undersc0re\n",
      "name 'detect' is not defined Language is not detected: Flight Delay and Causes\n",
      "name 'detect' is not defined Language is not detected: This Dataset contains Flights trip and multiple cause of delay\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Using this data you can find what caused the delay for flight whether it's Security delay, NAS delay or Carrier delay, etc. \n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Data Dictionary\n",
      "\n",
      "1. DayOfWeek  → 1 (Monday) - 7 (Sunday)\n",
      "2. Date       → Scheduled date \n",
      "3. DepTime    → Actual departure time (local, hhmm)\n",
      "4. ArrTime    →  Actual arrival time (local, hhmm)\n",
      "5. CRSArrTime  →  Scheduled arrival time (local, hhmm)\n",
      "6. UniqueCarrier  → Unique carrier code\n",
      "7. Airline   → Airline company\n",
      "8. FlightNum  →   flight number\n",
      "9. TailNum  →   plane tail number\n",
      "10. ActualElapsedTime  →  Actual time an airplane spends in the air(in minutes) with TaxiIn/Out\n",
      "11. CRSElapsedTime  →   CRS Elapsed Time of Flight (estimated elapse time), in minutes\n",
      "12. AirTime  → Flight Time (in minutes)\n",
      "13. ArrDelay →  Difference in minutes between scheduled and actual arrival time\n",
      "14. Origin  →   Origin IATA(International Air Transport Association) airport code   \n",
      "15. Org_Airport → Origin Airport Name\n",
      "16. Dest    → Destination IATA code\n",
      "17. Dest_Airport → Destination Airport Name\n",
      "18. Distance → Distance between airports (miles) \n",
      "19. TaxiIn →  Wheels down and arrival at the destination airport gate, in minutes\n",
      "20. TaxiOut →  The time elapsed between departure from the origin airport gate and wheels off, in minutes\n",
      "21. Cancelled → Was the flight canceled?\n",
      "22. CancellationCode → Reason for cancellation\n",
      "23. Diverted       →  1 = yes, 0 = no\n",
      "24. CarrierDelay → Flight delay due to carrier(e.g. maintenance or crew problems, aircraft cleaning, fueling, etc), 0 = No, yes = (in minutes)\n",
      "25. WeatherDelay  → Flight delay due to weather, 0 = No, yes = (in minutes)\n",
      "26. NASDelay     → Flight delay by NSA(National Aviation System), 0 = No, yes = (in minutes)\n",
      "27. SecurityDelay   → Flight delay by this reason, 0 = No, yes = (in minutes)\n",
      "28. LateAircraftDelay  → Flight delay by this reason, 0 = No, yes = (in minutes)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "Inspired from others DataSet in the same domain. So, tried to create one balanced dataset ready to use for beginners. This is a public dataset so it's not Licensed by anyone. \n",
      "name 'detect' is not defined Language is not detected: yash0956/fakenews\n",
      "name 'detect' is not defined Language is not detected: fakenews\n",
      "name 'detect' is not defined Language is not detected: yash0956\n",
      "name 'detect' is not defined Language is not detected: Source based Fake News Classification\n",
      "name 'detect' is not defined Language is not detected: Classification of news by type and labels\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "Social media is a vast pool of content, and among all the content available for users to access, news is an element that is accessed most frequently. These news can be posted by politicians, news channels, newspaper websites, or even common civilians. These posts have to be checked for their authenticity, since spreading misinformation has been a real concern in today’s times, and many firms are taking steps to make the common people aware of the consequences of spread misinformation. The measure of authenticity of the news posted online cannot be definitively measured, since the manual classification of news is tedious and time-consuming, and is also subject to bias.\n",
      "\n",
      "\n",
      "**Content**\n",
      "\n",
      "Data preprocessing has been done on the dataset Getting Real about Fake News and skew has been eliminated.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: fakenews\n",
      "name 'detect' is not defined Language is not detected: yash0956\n",
      "name 'detect' is not defined Language is not detected: Source based Fake News Classification\n",
      "name 'detect' is not defined Language is not detected: Classification of news by type and labels\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "Social media is a vast pool of content, and among all the content available for users to access, news is an element that is accessed most frequently. These news can be posted by politicians, news channels, newspaper websites, or even common civilians. These posts have to be checked for their authenticity, since spreading misinformation has been a real concern in today’s times, and many firms are taking steps to make the common people aware of the consequences of spread misinformation. The measure of authenticity of the news posted online cannot be definitively measured, since the manual classification of news is tedious and time-consuming, and is also subject to bias.\n",
      "\n",
      "\n",
      "**Content**\n",
      "\n",
      "Data preprocessing has been done on the dataset Getting Real about Fake News and skew has been eliminated.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: teeyee314/ucf-building-meter-reading\n",
      "name 'detect' is not defined Language is not detected: ucf-building-meter-reading\n",
      "name 'detect' is not defined Language is not detected: teeyee314\n",
      "name 'detect' is not defined Language is not detected: UCF building energy consumption\n",
      "name 'detect' is not defined Language is not detected: UCF Buildings 0-104 Electric and Chilled Water meter readings\n",
      "name 'detect' is not defined Language is not detected: Electrical  and Chilled Water meter readings - University of Central Florida (UCF) (2017-2018)\n",
      "\n",
      "This dataset was scraped from https://www.oeis.ucf.edu/buildings and compiled for the purpose of the ASHRAE Energy Prediction competition. These are test (Ground Truth) labels that lower public LB directly. There is a building_id, timestamp, meter, and meter_reading column to merge directly onto test submission file.\n",
      "name 'detect' is not defined Language is not detected: ucf-building-meter-reading\n",
      "name 'detect' is not defined Language is not detected: teeyee314\n",
      "name 'detect' is not defined Language is not detected: UCF building energy consumption\n",
      "name 'detect' is not defined Language is not detected: UCF Buildings 0-104 Electric and Chilled Water meter readings\n",
      "name 'detect' is not defined Language is not detected: Electrical  and Chilled Water meter readings - University of Central Florida (UCF) (2017-2018)\n",
      "\n",
      "This dataset was scraped from https://www.oeis.ucf.edu/buildings and compiled for the purpose of the ASHRAE Energy Prediction competition. These are test (Ground Truth) labels that lower public LB directly. There is a building_id, timestamp, meter, and meter_reading column to merge directly onto test submission file.\n",
      "name 'detect' is not defined Language is not detected: minnieliang/rec-system\n",
      "name 'detect' is not defined Language is not detected: rec-system\n",
      "name 'detect' is not defined Language is not detected: minnieliang\n",
      "name 'detect' is not defined Language is not detected: pinterest recommendation data\n",
      "name 'detect' is not defined Language is not detected: great for recommender systems/collaborative memory networks!\n",
      "name 'detect' is not defined Language is not detected: The structure of the data in the npz file is as follows: \n",
      "`train_data = [[user id, item id], ...] `\n",
      "`test_data = {userid: (pos_id, [neg_id1, neg_id2, ...]), ...}`\n",
      "name 'detect' is not defined Language is not detected: rec-system\n",
      "name 'detect' is not defined Language is not detected: minnieliang\n",
      "name 'detect' is not defined Language is not detected: pinterest recommendation data\n",
      "name 'detect' is not defined Language is not detected: great for recommender systems/collaborative memory networks!\n",
      "name 'detect' is not defined Language is not detected: The structure of the data in the npz file is as follows: \n",
      "`train_data = [[user id, item id], ...] `\n",
      "`test_data = {userid: (pos_id, [neg_id1, neg_id2, ...]), ...}`\n",
      "name 'detect' is not defined Language is not detected: datascientist97/e-commerece-sales-data-2024\n",
      "name 'detect' is not defined Language is not detected: e-commerece-sales-data-2024\n",
      "name 'detect' is not defined Language is not detected: datascientist97\n",
      "name 'detect' is not defined Language is not detected: E-commerece Sales Data 2024\n",
      "name 'detect' is not defined Language is not detected: A E-commerce Sales Dataset: User Profiles, Product Details, and transactions\n",
      "name 'detect' is not defined Language is not detected: Make notebooks on Machine Learning applying different Algorithms like Linear Regression ,SVM , Decision Tree and make notebook public here this will help me alot!\n",
      "About Dataset\n",
      "😍Upvote Please and share this would help me Thank You!\n",
      "**I'M Looking For Data Analyst Inturnship email at @datascientist097@gmail.com** \n",
      "\n",
      "Description:\n",
      "The E-commerce Sales Data dataset provides a comprehensive collection of information related to user profiles, product details, and user-product interactions. It is a valuable resource for understanding customer behavior, preferences, and purchasing trends on an e-commerce platform.\n",
      "\n",
      "Dataset Structure:\n",
      "\n",
      "User Sheet: This sheet contains user profiles, including details such as user ID, name, age, location, and other relevant information. It helps in understanding the demographics and characteristics of the platform's users.\n",
      "\n",
      "Product Sheet: The product sheet offers insights into the various products available on the e-commerce platform. It includes product IDs, names, categories, prices, descriptions, and other product-specific attributes.\n",
      "\n",
      "Interactions Sheet: The interactions sheet is a crucial component of the dataset, capturing the interactions between users and products. It records details of user actions, such as product views, purchases, reviews, and ratings. This data is essential for building recommendation systems and understanding user preferences.\n",
      "\n",
      "Potential Use Cases:\n",
      "\n",
      "Recommendation Systems: With the user-product interaction data, this dataset is ideal for building recommendation systems. It allows the development of personalized product recommendations to enhance the user experience.\n",
      "\n",
      "Market Basket Analysis: The dataset can be used for market basket analysis to understand which products are frequently purchased together, aiding in inventory management and targeted marketing.\n",
      "\n",
      "User Behavior Analysis: By analyzing user interactions, you can gain insights into user behavior, such as popular product categories, browsing patterns, and the impact of user reviews and ratings on purchasing decisions.\n",
      "\n",
      "Targeted Marketing: The dataset can inform marketing strategies, enabling businesses to tailor promotions and advertisements to specific user segments and product categories.\n",
      "\n",
      "This E-commerce Sales Data dataset is a valuable resource for e-commerce platforms and data scientists seeking to optimize the shopping experience, enhance customer satisfaction, and drive business growth through data-driven insights.\n",
      "name 'detect' is not defined Language is not detected: e-commerece-sales-data-2024\n",
      "name 'detect' is not defined Language is not detected: datascientist97\n",
      "name 'detect' is not defined Language is not detected: E-commerece Sales Data 2024\n",
      "name 'detect' is not defined Language is not detected: A E-commerce Sales Dataset: User Profiles, Product Details, and transactions\n",
      "name 'detect' is not defined Language is not detected: Make notebooks on Machine Learning applying different Algorithms like Linear Regression ,SVM , Decision Tree and make notebook public here this will help me alot!\n",
      "About Dataset\n",
      "😍Upvote Please and share this would help me Thank You!\n",
      "**I'M Looking For Data Analyst Inturnship email at @datascientist097@gmail.com** \n",
      "\n",
      "Description:\n",
      "The E-commerce Sales Data dataset provides a comprehensive collection of information related to user profiles, product details, and user-product interactions. It is a valuable resource for understanding customer behavior, preferences, and purchasing trends on an e-commerce platform.\n",
      "\n",
      "Dataset Structure:\n",
      "\n",
      "User Sheet: This sheet contains user profiles, including details such as user ID, name, age, location, and other relevant information. It helps in understanding the demographics and characteristics of the platform's users.\n",
      "\n",
      "Product Sheet: The product sheet offers insights into the various products available on the e-commerce platform. It includes product IDs, names, categories, prices, descriptions, and other product-specific attributes.\n",
      "\n",
      "Interactions Sheet: The interactions sheet is a crucial component of the dataset, capturing the interactions between users and products. It records details of user actions, such as product views, purchases, reviews, and ratings. This data is essential for building recommendation systems and understanding user preferences.\n",
      "\n",
      "Potential Use Cases:\n",
      "\n",
      "Recommendation Systems: With the user-product interaction data, this dataset is ideal for building recommendation systems. It allows the development of personalized product recommendations to enhance the user experience.\n",
      "\n",
      "Market Basket Analysis: The dataset can be used for market basket analysis to understand which products are frequently purchased together, aiding in inventory management and targeted marketing.\n",
      "\n",
      "User Behavior Analysis: By analyzing user interactions, you can gain insights into user behavior, such as popular product categories, browsing patterns, and the impact of user reviews and ratings on purchasing decisions.\n",
      "\n",
      "Targeted Marketing: The dataset can inform marketing strategies, enabling businesses to tailor promotions and advertisements to specific user segments and product categories.\n",
      "\n",
      "This E-commerce Sales Data dataset is a valuable resource for e-commerce platforms and data scientists seeking to optimize the shopping experience, enhance customer satisfaction, and drive business growth through data-driven insights.\n",
      "name 'detect' is not defined Language is not detected: furqanrustam118/calling-apps-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: calling-apps-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: furqanrustam118\n",
      "name 'detect' is not defined Language is not detected: Calling Apps Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: App Reviews Dataset for Sentiment Analysis\n",
      "name 'detect' is not defined Language is not detected: This dataset is extracted from the google play store for research purposes.\n",
      "\n",
      "Please cite the paper for this article \n",
      "Aslam, N.; Xia, K.; Rustam, F.; Hameed, A.; Ashraf, I. Using Aspect-Level Sentiments for Calling App Recommendation with Hybrid Deep-Learning Models. Appl. Sci. 2022, 12, 8522. https://doi.org/10.3390/app12178522\n",
      "\n",
      "The title \"Using Aspect-Level Sentiments for Calling App Recommendation with Hybrid Deep-Learning Models\"\n",
      "\n",
      "The rapid and wide proliferation of mobile phones has led to accelerated demand for mobile applications (apps). Consequently, a large number of mobile apps have been developed and deployed on the Google and Apple Play stores. Calling apps hold special importance in this regard by offering the services of sharing messages, making video calls, and sending audio messages, free of cost. Although each app has its own set of features, different apps can provide higher levels of satisfaction for the user, and aspect analysis is often overlooked by existing studies. This study presents an aspect-level analysis of IMO, Skype, Telegram, WeChat, and WhatsApp regarding the services offered for the account, app, call, message, update, video, and working features. A large collected dataset from the Google Play store is utilized for aspect extraction and analysis using the Latent Dirichlet Allocation (LDA) model. Apps are analyzed using LDA-extracted aspects and recommended regarding users’ priorities of call, message, and video requirements. Sentiment analysis is adopted to analyze user sentiments regarding apps as well as to aid in the aspect analysis. For sentiment analysis, a novel ensemble model of a gated recurrent unit and convolutional neural network is presented, which obtains a 94% accuracy score.\n",
      "name 'detect' is not defined Language is not detected: calling-apps-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: furqanrustam118\n",
      "name 'detect' is not defined Language is not detected: Calling Apps Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: App Reviews Dataset for Sentiment Analysis\n",
      "name 'detect' is not defined Language is not detected: This dataset is extracted from the google play store for research purposes.\n",
      "\n",
      "Please cite the paper for this article \n",
      "Aslam, N.; Xia, K.; Rustam, F.; Hameed, A.; Ashraf, I. Using Aspect-Level Sentiments for Calling App Recommendation with Hybrid Deep-Learning Models. Appl. Sci. 2022, 12, 8522. https://doi.org/10.3390/app12178522\n",
      "\n",
      "The title \"Using Aspect-Level Sentiments for Calling App Recommendation with Hybrid Deep-Learning Models\"\n",
      "\n",
      "The rapid and wide proliferation of mobile phones has led to accelerated demand for mobile applications (apps). Consequently, a large number of mobile apps have been developed and deployed on the Google and Apple Play stores. Calling apps hold special importance in this regard by offering the services of sharing messages, making video calls, and sending audio messages, free of cost. Although each app has its own set of features, different apps can provide higher levels of satisfaction for the user, and aspect analysis is often overlooked by existing studies. This study presents an aspect-level analysis of IMO, Skype, Telegram, WeChat, and WhatsApp regarding the services offered for the account, app, call, message, update, video, and working features. A large collected dataset from the Google Play store is utilized for aspect extraction and analysis using the Latent Dirichlet Allocation (LDA) model. Apps are analyzed using LDA-extracted aspects and recommended regarding users’ priorities of call, message, and video requirements. Sentiment analysis is adopted to analyze user sentiments regarding apps as well as to aid in the aspect analysis. For sentiment analysis, a novel ensemble model of a gated recurrent unit and convolutional neural network is presented, which obtains a 94% accuracy score.\n",
      "name 'detect' is not defined Language is not detected: rupals/gpu-runtime\n",
      "name 'detect' is not defined Language is not detected: gpu-runtime\n",
      "name 'detect' is not defined Language is not detected: rupals\n",
      "name 'detect' is not defined Language is not detected: GPU Kernel Performance Dataset\n",
      "name 'detect' is not defined Language is not detected: Linear and Logistic Regression by implementing gradient decent algorithm\n",
      "name 'detect' is not defined Language is not detected: In this assignment, we will be implementing linear and logistic regression on a given dataset. In addition, we will experiment with design and feature choices.\n",
      "\n",
      "We will be using the SGEMM GPU kernel performance Data Set available for download at [https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance](https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)\n",
      "\n",
      "**Goal:**\n",
      "Implement a linear regression model on the dataset to predict the GPU run time. Use the average of four runs as the target variable. You are not allowed to use any available implementation of the regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). Use the sum of squared error normalized by 2*number of samples [J(β0, β1) = (1/2m)[Σ(yᶺ(i) – y(i))2] as your cost and error measures, where m is number of samples. You should use all 14 features.\n",
      "\n",
      "Also implement a logistic regression model as described in Part 4. Again, you are not allowed to use any available implementation of the logistic regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). You should use the logistic regression cost/error function from the class. In addition you can also use accuracy/ROC/etc.\n",
      "\n",
      "**Tasks:**\n",
      "*Part 1: *Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\n",
      "*Part 2:* Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\n",
      "*Part 3:* Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\n",
      "*Part 4:* Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\n",
      "\n",
      "**Experimentation:**\n",
      "*1. *Experiment with various parameters for linear and logistic regression (e.g. learning rate ∝) and report on your findings as how the error/accuracy varies for train and test sets with varying these parameters. Plot the results. Report the best values of the parameters.\n",
      "*2.* Experiment with various thresholds for convergence for linear and logistic regression. Plot error results for train and test sets as a function of threshold and describe how varying the threshold affects error. Pick your best threshold and plot train and test error (in one figure) as a function of number of gradient descent iterations.\n",
      "*3.* Pick eight features randomly and retrain your models only on these ten features. Compare train and test error results for the case of using your original set of features (14) and eight random features. Report the ten randomly selected features.\n",
      "*4.* Now pick eight features that you think are best suited to predict the output, and retrain your models using these ten features. Compare to the case of using your original set of features and to the random features case. Did your choice of features provide better results than picking random features? Why? Did your choice of features provide better results than using all features? Why?\n",
      "\n",
      "**Source:**\n",
      "\n",
      "Enrique G. Paredes (egparedes '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n",
      "Rafael Ballester-Ripoll (rballester '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n",
      "\n",
      "**Citation:**\n",
      "\n",
      "If you use this data set, please cite one or both of these:\n",
      "\n",
      "- Rafael Ballester-Ripoll, Enrique G. Paredes, Renato Pajarola.\n",
      "Sobol Tensor Trains for Global Sensitivity Analysis.\n",
      "In arXiv Computer Science / Numerical Analysis e-prints, 2017\n",
      "\n",
      "- Cedric Nugteren and Valeriu Codreanu.\n",
      "CLTune: A Generic Auto-Tuner for OpenCL Kernels.\n",
      "In: MCSoC: 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip. IEEE, 2015\n",
      "name 'detect' is not defined Language is not detected: gpu-runtime\n",
      "name 'detect' is not defined Language is not detected: rupals\n",
      "name 'detect' is not defined Language is not detected: GPU Kernel Performance Dataset\n",
      "name 'detect' is not defined Language is not detected: Linear and Logistic Regression by implementing gradient decent algorithm\n",
      "name 'detect' is not defined Language is not detected: In this assignment, we will be implementing linear and logistic regression on a given dataset. In addition, we will experiment with design and feature choices.\n",
      "\n",
      "We will be using the SGEMM GPU kernel performance Data Set available for download at [https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance](https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)\n",
      "\n",
      "**Goal:**\n",
      "Implement a linear regression model on the dataset to predict the GPU run time. Use the average of four runs as the target variable. You are not allowed to use any available implementation of the regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). Use the sum of squared error normalized by 2*number of samples [J(β0, β1) = (1/2m)[Σ(yᶺ(i) – y(i))2] as your cost and error measures, where m is number of samples. You should use all 14 features.\n",
      "\n",
      "Also implement a logistic regression model as described in Part 4. Again, you are not allowed to use any available implementation of the logistic regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). You should use the logistic regression cost/error function from the class. In addition you can also use accuracy/ROC/etc.\n",
      "\n",
      "**Tasks:**\n",
      "*Part 1: *Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\n",
      "*Part 2:* Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\n",
      "*Part 3:* Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\n",
      "*Part 4:* Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\n",
      "\n",
      "**Experimentation:**\n",
      "*1. *Experiment with various parameters for linear and logistic regression (e.g. learning rate ∝) and report on your findings as how the error/accuracy varies for train and test sets with varying these parameters. Plot the results. Report the best values of the parameters.\n",
      "*2.* Experiment with various thresholds for convergence for linear and logistic regression. Plot error results for train and test sets as a function of threshold and describe how varying the threshold affects error. Pick your best threshold and plot train and test error (in one figure) as a function of number of gradient descent iterations.\n",
      "*3.* Pick eight features randomly and retrain your models only on these ten features. Compare train and test error results for the case of using your original set of features (14) and eight random features. Report the ten randomly selected features.\n",
      "*4.* Now pick eight features that you think are best suited to predict the output, and retrain your models using these ten features. Compare to the case of using your original set of features and to the random features case. Did your choice of features provide better results than picking random features? Why? Did your choice of features provide better results than using all features? Why?\n",
      "\n",
      "**Source:**\n",
      "\n",
      "Enrique G. Paredes (egparedes '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n",
      "Rafael Ballester-Ripoll (rballester '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n",
      "\n",
      "**Citation:**\n",
      "\n",
      "If you use this data set, please cite one or both of these:\n",
      "\n",
      "- Rafael Ballester-Ripoll, Enrique G. Paredes, Renato Pajarola.\n",
      "Sobol Tensor Trains for Global Sensitivity Analysis.\n",
      "In arXiv Computer Science / Numerical Analysis e-prints, 2017\n",
      "\n",
      "- Cedric Nugteren and Valeriu Codreanu.\n",
      "CLTune: A Generic Auto-Tuner for OpenCL Kernels.\n",
      "In: MCSoC: 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip. IEEE, 2015\n",
      "name 'detect' is not defined Language is not detected: amariaziz/tunisian-stock-market\n",
      "name 'detect' is not defined Language is not detected: tunisian-stock-market\n",
      "name 'detect' is not defined Language is not detected: amariaziz\n",
      "name 'detect' is not defined Language is not detected: Tunisian Stock Market\n",
      "name 'detect' is not defined Language is not detected: Curated list of all tunisian stocks historical data starting from the year 2010\n",
      "name 'detect' is not defined Language is not detected: ## About Dataset\n",
      "### Context\n",
      "I web-scraped historical data for all listed companies from 01-01-2010 to 01-30-2022 from [ilboursa](https://ilboursa.com/).\n",
      "### Why\n",
      "On the official Tunisian stock exchange, you can actually download historical data as .csv but links for data before 2016 don't work\n",
      "### Content\n",
      "Dataset consists of the following files:\n",
      "* **stock_list.csv**:  contains tuples of (Company Name, Ticker)\n",
      "* **ALL_DATA.csv**\n",
      "* **individual/**: a .csv file for each ticker\n",
      "* **stocks_market_cap.csv**: contains the market cap for each company\n",
      "### Applications\n",
      "You can do a couple of things using this data\n",
      "* Exploratory Data Analysis\n",
      "* Explore covid-19 effect on the market\n",
      "* One day ahead prediction: Rolling Linear Regression, ARIMA, Neural Networks, LSTM\n",
      "* Momentum/Mean-Reversion Strategies\n",
      "* Security clustering, portfolio construction/hedging\n",
      "* Rising and falling companies\n",
      "name 'detect' is not defined Language is not detected: tunisian-stock-market\n",
      "name 'detect' is not defined Language is not detected: amariaziz\n",
      "name 'detect' is not defined Language is not detected: Tunisian Stock Market\n",
      "name 'detect' is not defined Language is not detected: Curated list of all tunisian stocks historical data starting from the year 2010\n",
      "name 'detect' is not defined Language is not detected: ## About Dataset\n",
      "### Context\n",
      "I web-scraped historical data for all listed companies from 01-01-2010 to 01-30-2022 from [ilboursa](https://ilboursa.com/).\n",
      "### Why\n",
      "On the official Tunisian stock exchange, you can actually download historical data as .csv but links for data before 2016 don't work\n",
      "### Content\n",
      "Dataset consists of the following files:\n",
      "* **stock_list.csv**:  contains tuples of (Company Name, Ticker)\n",
      "* **ALL_DATA.csv**\n",
      "* **individual/**: a .csv file for each ticker\n",
      "* **stocks_market_cap.csv**: contains the market cap for each company\n",
      "### Applications\n",
      "You can do a couple of things using this data\n",
      "* Exploratory Data Analysis\n",
      "* Explore covid-19 effect on the market\n",
      "* One day ahead prediction: Rolling Linear Regression, ARIMA, Neural Networks, LSTM\n",
      "* Momentum/Mean-Reversion Strategies\n",
      "* Security clustering, portfolio construction/hedging\n",
      "* Rising and falling companies\n",
      "name 'detect' is not defined Language is not detected: umzi01/all-car-images-dataset-indian-cars\n",
      "name 'detect' is not defined Language is not detected: all-car-images-dataset-indian-cars\n",
      "name 'detect' is not defined Language is not detected: umzi01\n",
      "name 'detect' is not defined Language is not detected: All Car Images Dataset (Indian Cars)\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Collection of High-Quality Images of Indian Cars\n",
      "name 'detect' is not defined Language is not detected: Are you a car enthusiast or a data scientist working on computer vision projects? This All Car Images Dataset offers a comprehensive collection of high-quality images showcasing a wide range of cars in India. Whether you're interested in exploring the diverse automotive landscape of India or seeking data for your machine learning and computer vision projects, this dataset has you covered.\n",
      "\n",
      "This extensive dataset comprises  all the images, featuring various Indian car models captured from different angles and in diverse settings. The images encompass popular car brands, such as Maruti Suzuki, Hyundai, Tata Motors, Mahindra, Honda, and many others. From sleek sedans to rugged SUVs, this dataset offers an array of car types to satisfy your research and analysis needs\n",
      "name 'detect' is not defined Language is not detected: all-car-images-dataset-indian-cars\n",
      "name 'detect' is not defined Language is not detected: umzi01\n",
      "name 'detect' is not defined Language is not detected: All Car Images Dataset (Indian Cars)\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Collection of High-Quality Images of Indian Cars\n",
      "name 'detect' is not defined Language is not detected: Are you a car enthusiast or a data scientist working on computer vision projects? This All Car Images Dataset offers a comprehensive collection of high-quality images showcasing a wide range of cars in India. Whether you're interested in exploring the diverse automotive landscape of India or seeking data for your machine learning and computer vision projects, this dataset has you covered.\n",
      "\n",
      "This extensive dataset comprises  all the images, featuring various Indian car models captured from different angles and in diverse settings. The images encompass popular car brands, such as Maruti Suzuki, Hyundai, Tata Motors, Mahindra, Honda, and many others. From sleek sedans to rugged SUVs, this dataset offers an array of car types to satisfy your research and analysis needs\n",
      "name 'detect' is not defined Language is not detected: georgearun/hackerearth-tis-still-the-season-to-be-jolly\n",
      "name 'detect' is not defined Language is not detected: hackerearth-tis-still-the-season-to-be-jolly\n",
      "name 'detect' is not defined Language is not detected: georgearun\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Tis STILL the season to be jolly\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Deep Learning Challenge to classify images\n",
      "name 'detect' is not defined Language is not detected: # **Problem statement**\n",
      "\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays. \n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a snowman, a Christmas tree, flights, and the like. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# **Dataset**\n",
      "\n",
      "The dataset consists of over 6000 images.\n",
      "\n",
      "The benefits of practicing this problem by using Machine Learning/Deep Learning techniques are as follows:\n",
      "\n",
      "This challenge encourages you to apply your Deep Learning skills to build a model that classifies an image based on holiday-related elements.\n",
      "This challenge will help you to enhance your knowledge of multi-label classification, which is one of the basic building blocks of Machine Learning.\n",
      "We challenge you to develop a model that successfully identifies holiday-related elements in images and classifies them accordingly.\n",
      "name 'detect' is not defined Language is not detected: hackerearth-tis-still-the-season-to-be-jolly\n",
      "name 'detect' is not defined Language is not detected: georgearun\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Tis STILL the season to be jolly\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Deep Learning Challenge to classify images\n",
      "name 'detect' is not defined Language is not detected: # **Problem statement**\n",
      "\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays. \n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a snowman, a Christmas tree, flights, and the like. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# **Dataset**\n",
      "\n",
      "The dataset consists of over 6000 images.\n",
      "\n",
      "The benefits of practicing this problem by using Machine Learning/Deep Learning techniques are as follows:\n",
      "\n",
      "This challenge encourages you to apply your Deep Learning skills to build a model that classifies an image based on holiday-related elements.\n",
      "This challenge will help you to enhance your knowledge of multi-label classification, which is one of the basic building blocks of Machine Learning.\n",
      "We challenge you to develop a model that successfully identifies holiday-related elements in images and classifies them accordingly.\n",
      "name 'detect' is not defined Language is not detected: marquis03/afac2023-time-series-prediction-in-finance\n",
      "name 'detect' is not defined Language is not detected: afac2023-time-series-prediction-in-finance\n",
      "name 'detect' is not defined Language is not detected: marquis03\n",
      "name 'detect' is not defined Language is not detected: AFAC2023 - Time Series Prediction in Finance\n",
      "name 'detect' is not defined Language is not detected: AFAC2023 - Financial Scenario Understanding - Time Series Prediction in Finance\n",
      "name 'detect' is not defined Language is not detected: **(1) Competition data**\n",
      "\n",
      "The redemption amount of the fund is not only related to the periodicity and volatility of the timing itself, but also closely related to the income performance of the fund and user behavior, which is more susceptible to market fluctuations. Moreover, due to the attribution to the next trading day during holidays, the asynchronous timing is increased. Therefore, the following data is provided as modeling data:\n",
      "\n",
      "**A. Fund redemption data and fund characteristics information table**\n",
      "\n",
      "The table contains the forecast target of this game, the subscription amount (apply_amt), redemption amount (redeem_amt) and net subscription amount (net_in_amt) of the fund on each trading day, where net subscription = subscription - redemption.\n",
      "\n",
      "At the same time, the table gives the fund information and user access information (exposure uv on each page, etc.) of the fund on each trading day. Some data in the table is missing, please pay attention to it.\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | product_pid | Product id | String |\n",
      "| 2 | transaction_date | Transaction date | String |\n",
      "| 3 | apply_amt | Application amount | Double |\n",
      "| 4 | redeem_amt | Redemption amount | Double |\n",
      "| 5 | net_in_amt | Net inflow amount = Application amount - Redemption amount | Double |\n",
      "| 6 | uv_fundown | Fund position page exposure uv | Double |\n",
      "| 7 | uv_stableown | Stable position page exposure uv | Double |\n",
      "| 8 | uv_fundopt | Fund optional page exposure uv | Double |\n",
      "| 9 | uv fundmarket | Fund market page exposure uv | Double |\n",
      "| 10 | uv_termmarket | Term market page exposure uv | Double |\n",
      "| 11 | during_days | Fund holding period | Bigint |\n",
      "| 12 | total_net_value | Total net value | Double |\n",
      "\n",
      "**B. Market information table**\n",
      "\n",
      "The table shows the one-year Yield curve of Commercial Bank Interbank Deposit (AAA).\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | enddate | Date | String |\n",
      "| 2 | yield | Yield(%) | Double |\n",
      "\n",
      "**C. Time information table**\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | stat_date | Date | String |\n",
      "| 2 | is_trade | Whether it is a trading day | Bigint |\n",
      "| 3 | next_trade_date | Next trading day | String |\n",
      "| 4 | last_trade_date | Last trading day | String |\n",
      "| 5 | is_week_end | Whether it is the last trading day of the week | Bigint |\n",
      "| 6 | is_month_end | Whether it is the last trading day of the month | Bigint |\n",
      "| 7 | is_quarter_end | Whether it is the last trading day of the quarter | Bigint |\n",
      "| 8 | is_year_end | Whether it is the last trading day of the year | Bigint |\n",
      "| 9 | trade_day_rank | Trading day ranking | Bigint |\n",
      "\n",
      "**(2) Data files**\n",
      "\n",
      "Competition data is provided as a.CSV file.\n",
      "name 'detect' is not defined Language is not detected: afac2023-time-series-prediction-in-finance\n",
      "name 'detect' is not defined Language is not detected: marquis03\n",
      "name 'detect' is not defined Language is not detected: AFAC2023 - Time Series Prediction in Finance\n",
      "name 'detect' is not defined Language is not detected: AFAC2023 - Financial Scenario Understanding - Time Series Prediction in Finance\n",
      "name 'detect' is not defined Language is not detected: **(1) Competition data**\n",
      "\n",
      "The redemption amount of the fund is not only related to the periodicity and volatility of the timing itself, but also closely related to the income performance of the fund and user behavior, which is more susceptible to market fluctuations. Moreover, due to the attribution to the next trading day during holidays, the asynchronous timing is increased. Therefore, the following data is provided as modeling data:\n",
      "\n",
      "**A. Fund redemption data and fund characteristics information table**\n",
      "\n",
      "The table contains the forecast target of this game, the subscription amount (apply_amt), redemption amount (redeem_amt) and net subscription amount (net_in_amt) of the fund on each trading day, where net subscription = subscription - redemption.\n",
      "\n",
      "At the same time, the table gives the fund information and user access information (exposure uv on each page, etc.) of the fund on each trading day. Some data in the table is missing, please pay attention to it.\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | product_pid | Product id | String |\n",
      "| 2 | transaction_date | Transaction date | String |\n",
      "| 3 | apply_amt | Application amount | Double |\n",
      "| 4 | redeem_amt | Redemption amount | Double |\n",
      "| 5 | net_in_amt | Net inflow amount = Application amount - Redemption amount | Double |\n",
      "| 6 | uv_fundown | Fund position page exposure uv | Double |\n",
      "| 7 | uv_stableown | Stable position page exposure uv | Double |\n",
      "| 8 | uv_fundopt | Fund optional page exposure uv | Double |\n",
      "| 9 | uv fundmarket | Fund market page exposure uv | Double |\n",
      "| 10 | uv_termmarket | Term market page exposure uv | Double |\n",
      "| 11 | during_days | Fund holding period | Bigint |\n",
      "| 12 | total_net_value | Total net value | Double |\n",
      "\n",
      "**B. Market information table**\n",
      "\n",
      "The table shows the one-year Yield curve of Commercial Bank Interbank Deposit (AAA).\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | enddate | Date | String |\n",
      "| 2 | yield | Yield(%) | Double |\n",
      "\n",
      "**C. Time information table**\n",
      "\n",
      "| Serial number | Fields | Description | Type |\n",
      "| --- | --- | --- | --- |\n",
      "| 1 | stat_date | Date | String |\n",
      "| 2 | is_trade | Whether it is a trading day | Bigint |\n",
      "| 3 | next_trade_date | Next trading day | String |\n",
      "| 4 | last_trade_date | Last trading day | String |\n",
      "| 5 | is_week_end | Whether it is the last trading day of the week | Bigint |\n",
      "| 6 | is_month_end | Whether it is the last trading day of the month | Bigint |\n",
      "| 7 | is_quarter_end | Whether it is the last trading day of the quarter | Bigint |\n",
      "| 8 | is_year_end | Whether it is the last trading day of the year | Bigint |\n",
      "| 9 | trade_day_rank | Trading day ranking | Bigint |\n",
      "\n",
      "**(2) Data files**\n",
      "\n",
      "Competition data is provided as a.CSV file.\n",
      "name 'detect' is not defined Language is not detected: crispen5gar/recipes3k\n",
      "name 'detect' is not defined Language is not detected: recipes3k\n",
      "name 'detect' is not defined Language is not detected: crispen5gar\n",
      "name 'detect' is not defined Language is not detected: Food Recipes\n",
      "name 'detect' is not defined Language is not detected: This dataset contains some food recipes from different categories.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains a total `~3k` different recipes from all recipes categories.\n",
      "\n",
      "### Content\n",
      "There are 5 different recipes categories and each category has it's own file `.json` file. These categories are:\n",
      "\n",
      "* recipes (general recipes) -&gt; `recipes.json`\n",
      "* inspiration -&gt; `inspiration.json`\n",
      "* baking -&gt; `baking.json`\n",
      "* health -&gt; `health.json`\n",
      "* budget -&gt; `budget.json`\n",
      "\n",
      "\n",
      "Each `.json` document from these categories contains the following fields:\n",
      "\n",
      "* `id`:  type of `string` which is the recipe id\n",
      "* `url`: the url to the recipe website\n",
      "* `name`: the name of the recipe\n",
      "* `author`: the author of the recipe\n",
      "* `rattings`: type of `number` which is the rating of the recipe between `0` and `5`.\n",
      "* `description`: the recipe description\n",
      "* `ingredients`: `array` of ingredients used\n",
      "* `steps`: steps to follow when preparing this recipe\n",
      "* `nutrients`: an `object` containing the nutrients in this recipe\n",
      "* `times: an `object` containing preparation and cooking time\n",
      "* `serves`: type of `number` showing how many people does this recipe serves\n",
      "* `difficult`: is the recipe easy or difficult to prepare\n",
      "* `vote_count` : `number` telling us how many people voted for this recipe as good.\n",
      "* `subcategory`: a `string` telling us the subcategory of the recipe eg. \"Bread\"\n",
      "* `dish_type`: a `string` telling us the dish_type of the recipe eg. \"Bread roll recipes\"\n",
      "* `maincategory`: the category of the recipe eg. \"baking\"\n",
      "\n",
      "### Dataset Usage\n",
      "* This dataset is flexible you can use it in any way, one of the task is to create a recipe recommendation system for each category using neural networks.\n",
      "* You can use this dataset to cook\n",
      "\n",
      "### Acknowledgements\n",
      "* The data is for research purposes only and belongs to [www.bbcgoodfood.com](https://www.bbcgoodfood.com).\n",
      "* The data has been collected with the help of [these notebooks](https://github.com/CrispenGari/web-scrapping-python/tree/main/bs4/00_RECIPES)\n",
      "name 'detect' is not defined Language is not detected: recipes3k\n",
      "name 'detect' is not defined Language is not detected: crispen5gar\n",
      "name 'detect' is not defined Language is not detected: Food Recipes\n",
      "name 'detect' is not defined Language is not detected: This dataset contains some food recipes from different categories.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains a total `~3k` different recipes from all recipes categories.\n",
      "\n",
      "### Content\n",
      "There are 5 different recipes categories and each category has it's own file `.json` file. These categories are:\n",
      "\n",
      "* recipes (general recipes) -&gt; `recipes.json`\n",
      "* inspiration -&gt; `inspiration.json`\n",
      "* baking -&gt; `baking.json`\n",
      "* health -&gt; `health.json`\n",
      "* budget -&gt; `budget.json`\n",
      "\n",
      "\n",
      "Each `.json` document from these categories contains the following fields:\n",
      "\n",
      "* `id`:  type of `string` which is the recipe id\n",
      "* `url`: the url to the recipe website\n",
      "* `name`: the name of the recipe\n",
      "* `author`: the author of the recipe\n",
      "* `rattings`: type of `number` which is the rating of the recipe between `0` and `5`.\n",
      "* `description`: the recipe description\n",
      "* `ingredients`: `array` of ingredients used\n",
      "* `steps`: steps to follow when preparing this recipe\n",
      "* `nutrients`: an `object` containing the nutrients in this recipe\n",
      "* `times: an `object` containing preparation and cooking time\n",
      "* `serves`: type of `number` showing how many people does this recipe serves\n",
      "* `difficult`: is the recipe easy or difficult to prepare\n",
      "* `vote_count` : `number` telling us how many people voted for this recipe as good.\n",
      "* `subcategory`: a `string` telling us the subcategory of the recipe eg. \"Bread\"\n",
      "* `dish_type`: a `string` telling us the dish_type of the recipe eg. \"Bread roll recipes\"\n",
      "* `maincategory`: the category of the recipe eg. \"baking\"\n",
      "\n",
      "### Dataset Usage\n",
      "* This dataset is flexible you can use it in any way, one of the task is to create a recipe recommendation system for each category using neural networks.\n",
      "* You can use this dataset to cook\n",
      "\n",
      "### Acknowledgements\n",
      "* The data is for research purposes only and belongs to [www.bbcgoodfood.com](https://www.bbcgoodfood.com).\n",
      "* The data has been collected with the help of [these notebooks](https://github.com/CrispenGari/web-scrapping-python/tree/main/bs4/00_RECIPES)\n",
      "name 'detect' is not defined Language is not detected: anandaramg/global-superstore\n",
      "name 'detect' is not defined Language is not detected: global-superstore\n",
      "name 'detect' is not defined Language is not detected: anandaramg\n",
      "name 'detect' is not defined Language is not detected: Global Superstore\n",
      "name 'detect' is not defined Language is not detected: global super store analysis & prediction\n",
      "name 'detect' is not defined Language is not detected: ![mg](https://media.allure.com/photos/585bf909f32978df093421c0/16:9/w_1920,c_limit/walmart-hair-products.jpg)\n",
      "\n",
      "Walmart is one of the world's largest and most well-known retail chains, often referred to as a \"Global Superstore.\" Founded in 1962 by Sam Walton in Rogers, Arkansas, Walmart has grown to become a multinational retail giant with a presence in various countries. It's known for its vast and diverse product offerings, including groceries, electronics, clothing, household goods, and more.\n",
      "\n",
      "Here are some key points that highlight Walmart's status as a Global Superstore:\n",
      "\n",
      "- International Presence: Walmart operates in multiple countries across the globe, making it a truly global retailer. Its international expansion began in the 1990s, and it now has a presence in numerous countries, including the United States, Mexico, Canada, the United Kingdom, China, India, Brazil, and many others.\n",
      "\n",
      "- Extensive Product Range: Walmart offers an extensive and diverse range of products, from everyday groceries to electronics, apparel, furniture, automotive supplies, and more. This vast selection of goods under one roof makes it a one-stop shop for a wide variety of consumer needs.\n",
      "\n",
      "- Competitive Pricing: Walmart is known for its commitment to offering low prices, often leveraging its buying power to negotiate favorable deals with suppliers and pass those savings on to customers. This has helped it attract price-conscious shoppers and maintain a competitive edge.\n",
      "\n",
      "- E-commerce and Online Presence: In addition to its physical stores, Walmart has a significant online presence through its e-commerce platform. It has invested heavily in expanding its digital retail capabilities, including online ordering, home delivery, and curbside pickup, to meet the changing shopping preferences of consumers.\n",
      "\n",
      "- Private Label Brands: Walmart offers a range of private-label or store-brand products, providing more affordable alternatives to well-known brands. These private label products cover a wide range of categories and help draw budget-conscious customers.\n",
      "\n",
      "- Sustainability Efforts: Walmart has made significant efforts to enhance its sustainability practices. It aims to reduce its environmental footprint by using renewable energy, improving energy efficiency, and implementing sustainable sourcing practices.\n",
      "\n",
      "- Community Engagement: Walmart is actively involved in philanthropic activities and community engagement. The company supports various social and charitable initiatives, including education, hunger relief, and disaster relief efforts.\n",
      "\n",
      "- Employment and Economic Impact: Walmart is one of the world's largest employers, providing jobs to millions of people across its global operations. Its presence in many communities contributes to local economies and often influences pricing and employment trends in those areas.\n",
      "\n",
      "- Supply Chain and Logistics: Walmart is renowned for its efficient supply chain and distribution systems, which allow it to stock products, restock shelves, and move goods efficiently. This logistical expertise is a key element of its success as a Global Superstore.\n",
      "\n",
      "Overall, Walmart's status as a Global Superstore is characterized by its massive reach, diverse product offerings, competitive pricing, and continuous efforts to adapt to changing consumer preferences and demands. Its global footprint and influence in the retail industry make it a prominent player in the world of commerce.\n",
      "name 'detect' is not defined Language is not detected: global-superstore\n",
      "name 'detect' is not defined Language is not detected: anandaramg\n",
      "name 'detect' is not defined Language is not detected: Global Superstore\n",
      "name 'detect' is not defined Language is not detected: global super store analysis & prediction\n",
      "name 'detect' is not defined Language is not detected: ![mg](https://media.allure.com/photos/585bf909f32978df093421c0/16:9/w_1920,c_limit/walmart-hair-products.jpg)\n",
      "\n",
      "Walmart is one of the world's largest and most well-known retail chains, often referred to as a \"Global Superstore.\" Founded in 1962 by Sam Walton in Rogers, Arkansas, Walmart has grown to become a multinational retail giant with a presence in various countries. It's known for its vast and diverse product offerings, including groceries, electronics, clothing, household goods, and more.\n",
      "\n",
      "Here are some key points that highlight Walmart's status as a Global Superstore:\n",
      "\n",
      "- International Presence: Walmart operates in multiple countries across the globe, making it a truly global retailer. Its international expansion began in the 1990s, and it now has a presence in numerous countries, including the United States, Mexico, Canada, the United Kingdom, China, India, Brazil, and many others.\n",
      "\n",
      "- Extensive Product Range: Walmart offers an extensive and diverse range of products, from everyday groceries to electronics, apparel, furniture, automotive supplies, and more. This vast selection of goods under one roof makes it a one-stop shop for a wide variety of consumer needs.\n",
      "\n",
      "- Competitive Pricing: Walmart is known for its commitment to offering low prices, often leveraging its buying power to negotiate favorable deals with suppliers and pass those savings on to customers. This has helped it attract price-conscious shoppers and maintain a competitive edge.\n",
      "\n",
      "- E-commerce and Online Presence: In addition to its physical stores, Walmart has a significant online presence through its e-commerce platform. It has invested heavily in expanding its digital retail capabilities, including online ordering, home delivery, and curbside pickup, to meet the changing shopping preferences of consumers.\n",
      "\n",
      "- Private Label Brands: Walmart offers a range of private-label or store-brand products, providing more affordable alternatives to well-known brands. These private label products cover a wide range of categories and help draw budget-conscious customers.\n",
      "\n",
      "- Sustainability Efforts: Walmart has made significant efforts to enhance its sustainability practices. It aims to reduce its environmental footprint by using renewable energy, improving energy efficiency, and implementing sustainable sourcing practices.\n",
      "\n",
      "- Community Engagement: Walmart is actively involved in philanthropic activities and community engagement. The company supports various social and charitable initiatives, including education, hunger relief, and disaster relief efforts.\n",
      "\n",
      "- Employment and Economic Impact: Walmart is one of the world's largest employers, providing jobs to millions of people across its global operations. Its presence in many communities contributes to local economies and often influences pricing and employment trends in those areas.\n",
      "\n",
      "- Supply Chain and Logistics: Walmart is renowned for its efficient supply chain and distribution systems, which allow it to stock products, restock shelves, and move goods efficiently. This logistical expertise is a key element of its success as a Global Superstore.\n",
      "\n",
      "Overall, Walmart's status as a Global Superstore is characterized by its massive reach, diverse product offerings, competitive pricing, and continuous efforts to adapt to changing consumer preferences and demands. Its global footprint and influence in the retail industry make it a prominent player in the world of commerce.\n",
      "name 'detect' is not defined Language is not detected: anandaramg/apartment-cost-in-new-york-city\n",
      "name 'detect' is not defined Language is not detected: apartment-cost-in-new-york-city\n",
      "name 'detect' is not defined Language is not detected: anandaramg\n",
      "name 'detect' is not defined Language is not detected: Housing Cost in New York\n",
      "name 'detect' is not defined Language is not detected: Cost of Housing/Apartment across New York City\n",
      "name 'detect' is not defined Language is not detected: \n",
      "![img](https://t2.gstatic.com/licensed-image?q=tbn:ANd9GcQIJZO61HT7jnkXHFugvCckGSEYA1d41EQGf80Qy1oPJ9yi8zm2TqPC-jewOVBFvLd_)\n",
      "\n",
      "The NYC Housing dataset contains information about the New York City Housing and Preservation Department's (HPD) affordable housing development projects. It includes data on building characteristics, affordability levels, location, and ownership information for all properties in the dataset.\n",
      "\n",
      "The dataset consists of several files, including Building Data, Project Data, and HPD Contacts. The Building Data file contains information on individual buildings, such as the building's address, number of units, and building type. The Project Data file contains information on the development projects that contain these buildings, including information on the funding programs used to develop the projects and the affordability levels of the units. The HPD Contacts file contains contact information for HPD employees responsible for the management of each project.\n",
      "\n",
      "The NYC Housing dataset is a valuable resource for researchers, policymakers, and developers interested in affordable housing in New York City. It can be used to analyze trends in affordable housing development, identify neighborhoods with high levels of affordable housing, and evaluate the effectiveness of various affordable housing programs.\n",
      "name 'detect' is not defined Language is not detected: apartment-cost-in-new-york-city\n",
      "name 'detect' is not defined Language is not detected: anandaramg\n",
      "name 'detect' is not defined Language is not detected: Housing Cost in New York\n",
      "name 'detect' is not defined Language is not detected: Cost of Housing/Apartment across New York City\n",
      "name 'detect' is not defined Language is not detected: \n",
      "![img](https://t2.gstatic.com/licensed-image?q=tbn:ANd9GcQIJZO61HT7jnkXHFugvCckGSEYA1d41EQGf80Qy1oPJ9yi8zm2TqPC-jewOVBFvLd_)\n",
      "\n",
      "The NYC Housing dataset contains information about the New York City Housing and Preservation Department's (HPD) affordable housing development projects. It includes data on building characteristics, affordability levels, location, and ownership information for all properties in the dataset.\n",
      "\n",
      "The dataset consists of several files, including Building Data, Project Data, and HPD Contacts. The Building Data file contains information on individual buildings, such as the building's address, number of units, and building type. The Project Data file contains information on the development projects that contain these buildings, including information on the funding programs used to develop the projects and the affordability levels of the units. The HPD Contacts file contains contact information for HPD employees responsible for the management of each project.\n",
      "\n",
      "The NYC Housing dataset is a valuable resource for researchers, policymakers, and developers interested in affordable housing in New York City. It can be used to analyze trends in affordable housing development, identify neighborhoods with high levels of affordable housing, and evaluate the effectiveness of various affordable housing programs.\n",
      "name 'detect' is not defined Language is not detected: arunklenin/ps4e4-ensemble-ancillary\n",
      "name 'detect' is not defined Language is not detected: ps4e4-ensemble-ancillary\n",
      "name 'detect' is not defined Language is not detected: arunklenin\n",
      "name 'detect' is not defined Language is not detected: PS4E4 | Ensemble Ancillary\n",
      "name 'detect' is not defined Language is not detected: Collation of multiple non-blended version results from my public notebook\n",
      "name 'detect' is not defined Language is not detected: The Dataset is contains submissions from different versions of my notebook using various feature engineering & modeling ideas. Each submission is not blended with any other submission, the idea is to use these to generalize result. \n",
      "\n",
      "Naming format \"sub_pure_\"+score(in decimals)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ps4e4-ensemble-ancillary\n",
      "name 'detect' is not defined Language is not detected: arunklenin\n",
      "name 'detect' is not defined Language is not detected: PS4E4 | Ensemble Ancillary\n",
      "name 'detect' is not defined Language is not detected: Collation of multiple non-blended version results from my public notebook\n",
      "name 'detect' is not defined Language is not detected: The Dataset is contains submissions from different versions of my notebook using various feature engineering & modeling ideas. Each submission is not blended with any other submission, the idea is to use these to generalize result. \n",
      "\n",
      "Naming format \"sub_pure_\"+score(in decimals)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: harishedison/jena-weather-dataset\n",
      "name 'detect' is not defined Language is not detected: jena-weather-dataset\n",
      "name 'detect' is not defined Language is not detected: harishedison\n",
      "name 'detect' is not defined Language is not detected: Jena Weather Dataset\n",
      "name 'detect' is not defined Language is not detected: Weather data of Jena Weather Station of Max Planck Institute for Biogeochemistry\n",
      "name 'detect' is not defined Language is not detected: Jena Climate is a weather time-series dataset recorded at the Weather Station of the Max Planck Institute for Biogeochemistry in Jena, Germany.\n",
      "\n",
      "Jena Weather dataset is made up of 14 different quantities (such as air temperature, atmospheric pressure, humidity, wind direction, and so on) that were recorded every 10 minutes, over several years. This dataset covers data from January 1st, 2010 to August 31st, 2022.\n",
      "name 'detect' is not defined Language is not detected: jena-weather-dataset\n",
      "name 'detect' is not defined Language is not detected: harishedison\n",
      "name 'detect' is not defined Language is not detected: Jena Weather Dataset\n",
      "name 'detect' is not defined Language is not detected: Weather data of Jena Weather Station of Max Planck Institute for Biogeochemistry\n",
      "name 'detect' is not defined Language is not detected: Jena Climate is a weather time-series dataset recorded at the Weather Station of the Max Planck Institute for Biogeochemistry in Jena, Germany.\n",
      "\n",
      "Jena Weather dataset is made up of 14 different quantities (such as air temperature, atmospheric pressure, humidity, wind direction, and so on) that were recorded every 10 minutes, over several years. This dataset covers data from January 1st, 2010 to August 31st, 2022.\n",
      "name 'detect' is not defined Language is not detected: mayurdaa/tmdb-movies-data\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-data\n",
      "name 'detect' is not defined Language is not detected: mayurdaa\n",
      "name 'detect' is not defined Language is not detected: tmdb movies data\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Collection\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information on 10,000 movies, covering various genres and release periods. The dataset is designed to facilitate research and analysis in the field of movie studies, recommendation systems, and data science. Each record provides key details about a movie, allowing users to explore and derive insights from the dataset.\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-data\n",
      "name 'detect' is not defined Language is not detected: mayurdaa\n",
      "name 'detect' is not defined Language is not detected: tmdb movies data\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Collection\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information on 10,000 movies, covering various genres and release periods. The dataset is designed to facilitate research and analysis in the field of movie studies, recommendation systems, and data science. Each record provides key details about a movie, allowing users to explore and derive insights from the dataset.\n",
      "name 'detect' is not defined Language is not detected: andreagarritano/wikipedia-article-networks\n",
      "name 'detect' is not defined Language is not detected: wikipedia-article-networks\n",
      "name 'detect' is not defined Language is not detected: andreagarritano\n",
      "name 'detect' is not defined Language is not detected: Wikipedia Article Networks\n",
      "name 'detect' is not defined Language is not detected: ## Wikipedia Article Networks\n",
      "\n",
      "### Description\n",
      "The data was collected from the English Wikipedia (December 2018). These datasets represent page-page networks on specific topics (chameleons, crocodiles and squirrels). Nodes represent articles and edges are mutual links between them. The edges csv files contain the edges - nodes are indexed from 0. The features json files contain the features of articles - each key is a page id, and node features are given as lists. The presence of a feature in the feature list means that an informative noun appeared in the text of the Wikipedia article. The target csv contains the node identifiers and the average monthly traffic between October 2017 and November 2018 for each page.  For each page-page network we listed the number of nodes an edges with some other descriptive statistics.\n",
      "\n",
      "### Properties\n",
      "\n",
      "- **Directed:** No.\n",
      "- **Node features:** Yes.\n",
      "- **Edge features:** No.\n",
      "- **Node labels:** Yes. Continuous target.\n",
      "- **Temporal:** No.\n",
      "\n",
      "| Dataset | **Chameleon** | **Crocodile** | **Squirrel** |\n",
      "\n",
      "| **Nodes** |2,277   | 11,631  |  5,201 |\n",
      "\n",
      "| **Edges** | 31,421  |170,918 |  198,493 |\n",
      "\n",
      "| **Density** |  0.012 | 0.003  | 0.015 |\n",
      "\n",
      "| **Transitvity** | 0.314| 0.026 | 0.348 |\n",
      "\n",
      "\n",
      "### Possible Tasks\n",
      "\n",
      "- **Regression**\n",
      "- **Link prediction**\n",
      "- **Community detection**\n",
      "- **Network visualization**\n",
      "\n",
      "**Paper**:\n",
      "Multi-scale Attributed Node Embedding. Benedek Rozemberczki, Carl Allen, and Rik Sarkar. arXiv, 2019. https://arxiv.org/abs/1909.13021\n",
      "name 'detect' is not defined Language is not detected: wikipedia-article-networks\n",
      "name 'detect' is not defined Language is not detected: andreagarritano\n",
      "name 'detect' is not defined Language is not detected: Wikipedia Article Networks\n",
      "name 'detect' is not defined Language is not detected: ## Wikipedia Article Networks\n",
      "\n",
      "### Description\n",
      "The data was collected from the English Wikipedia (December 2018). These datasets represent page-page networks on specific topics (chameleons, crocodiles and squirrels). Nodes represent articles and edges are mutual links between them. The edges csv files contain the edges - nodes are indexed from 0. The features json files contain the features of articles - each key is a page id, and node features are given as lists. The presence of a feature in the feature list means that an informative noun appeared in the text of the Wikipedia article. The target csv contains the node identifiers and the average monthly traffic between October 2017 and November 2018 for each page.  For each page-page network we listed the number of nodes an edges with some other descriptive statistics.\n",
      "\n",
      "### Properties\n",
      "\n",
      "- **Directed:** No.\n",
      "- **Node features:** Yes.\n",
      "- **Edge features:** No.\n",
      "- **Node labels:** Yes. Continuous target.\n",
      "- **Temporal:** No.\n",
      "\n",
      "| Dataset | **Chameleon** | **Crocodile** | **Squirrel** |\n",
      "\n",
      "| **Nodes** |2,277   | 11,631  |  5,201 |\n",
      "\n",
      "| **Edges** | 31,421  |170,918 |  198,493 |\n",
      "\n",
      "| **Density** |  0.012 | 0.003  | 0.015 |\n",
      "\n",
      "| **Transitvity** | 0.314| 0.026 | 0.348 |\n",
      "\n",
      "\n",
      "### Possible Tasks\n",
      "\n",
      "- **Regression**\n",
      "- **Link prediction**\n",
      "- **Community detection**\n",
      "- **Network visualization**\n",
      "\n",
      "**Paper**:\n",
      "Multi-scale Attributed Node Embedding. Benedek Rozemberczki, Carl Allen, and Rik Sarkar. arXiv, 2019. https://arxiv.org/abs/1909.13021\n",
      "name 'detect' is not defined Language is not detected: jawadkhattak/us-flight-delay-from-january-2017-july-2022\n",
      "name 'detect' is not defined Language is not detected: us-flight-delay-from-january-2017-july-2022\n",
      "name 'detect' is not defined Language is not detected: jawadkhattak\n",
      "name 'detect' is not defined Language is not detected: Flight Delay from January 2017 - July 2022\n",
      "name 'detect' is not defined Language is not detected: Predicting the future flight delay based on the current data\n",
      "name 'detect' is not defined Language is not detected: This is flights delay data in the U.S. For more information about the data, go to URL below. \n",
      " \n",
      "source = https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
      "\n",
      "year\n",
      "Year data collected\n",
      "\n",
      "month\n",
      "Numeric representation of the month\n",
      "\n",
      "carrier\n",
      "Carrier.\n",
      "\n",
      "carrier_name\n",
      "Carrier Name.\n",
      "\n",
      "airport\n",
      "Airport code.\n",
      "\n",
      "airport_name\n",
      "Name of airport.\n",
      "\n",
      "arr_flights\n",
      "Number of flights arriving at airport\n",
      "\n",
      "arr_del15\n",
      "Number of flights more than 15 minutes late\n",
      "\n",
      "carrier_ct\n",
      "Number of flights delayed due to air carrier. (e.g. no crew)\n",
      "\n",
      "weather_ct\n",
      "Number of flights due to weather.\n",
      "\n",
      "nas_ct\n",
      "Number of flights delayed due to National Aviation System (e.g. heavy air traffic).\n",
      "\n",
      "security_ct\n",
      "Number of flights canceled due to a security breach.\n",
      "\n",
      "late_aircraft_ct\n",
      "Number of flights delayed as a result of another flight on the same aircraft delayed\n",
      "\n",
      "arr_cancelled\n",
      "Number of cancelled flights\n",
      "\n",
      "arr_diverted\n",
      "Number of flights that were diverted\n",
      "\n",
      "arr_delay\n",
      "Total time (minutes) of delayed flight.\n",
      "\n",
      "carrier_delay\n",
      "Total time (minutes) of delay due to air carrier\n",
      "\n",
      "weather_delay\n",
      "Total time (minutes) of delay due to inclement weather.\n",
      "\n",
      "nas_delay\n",
      "Total time (minutes) of delay due to National Aviation System.\n",
      "\n",
      "security_delay\n",
      "Total time (minutes) of delay as a result of a security issue .\n",
      "\n",
      "late_aircraft_delay\n",
      "Total time (minutes) of delay flights as a result of a previous flight on the same airplane being late.\n",
      " \n",
      "name 'detect' is not defined Language is not detected: us-flight-delay-from-january-2017-july-2022\n",
      "name 'detect' is not defined Language is not detected: jawadkhattak\n",
      "name 'detect' is not defined Language is not detected: Flight Delay from January 2017 - July 2022\n",
      "name 'detect' is not defined Language is not detected: Predicting the future flight delay based on the current data\n",
      "name 'detect' is not defined Language is not detected: This is flights delay data in the U.S. For more information about the data, go to URL below. \n",
      " \n",
      "source = https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
      "\n",
      "year\n",
      "Year data collected\n",
      "\n",
      "month\n",
      "Numeric representation of the month\n",
      "\n",
      "carrier\n",
      "Carrier.\n",
      "\n",
      "carrier_name\n",
      "Carrier Name.\n",
      "\n",
      "airport\n",
      "Airport code.\n",
      "\n",
      "airport_name\n",
      "Name of airport.\n",
      "\n",
      "arr_flights\n",
      "Number of flights arriving at airport\n",
      "\n",
      "arr_del15\n",
      "Number of flights more than 15 minutes late\n",
      "\n",
      "carrier_ct\n",
      "Number of flights delayed due to air carrier. (e.g. no crew)\n",
      "\n",
      "weather_ct\n",
      "Number of flights due to weather.\n",
      "\n",
      "nas_ct\n",
      "Number of flights delayed due to National Aviation System (e.g. heavy air traffic).\n",
      "\n",
      "security_ct\n",
      "Number of flights canceled due to a security breach.\n",
      "\n",
      "late_aircraft_ct\n",
      "Number of flights delayed as a result of another flight on the same aircraft delayed\n",
      "\n",
      "arr_cancelled\n",
      "Number of cancelled flights\n",
      "\n",
      "arr_diverted\n",
      "Number of flights that were diverted\n",
      "\n",
      "arr_delay\n",
      "Total time (minutes) of delayed flight.\n",
      "\n",
      "carrier_delay\n",
      "Total time (minutes) of delay due to air carrier\n",
      "\n",
      "weather_delay\n",
      "Total time (minutes) of delay due to inclement weather.\n",
      "\n",
      "nas_delay\n",
      "Total time (minutes) of delay due to National Aviation System.\n",
      "\n",
      "security_delay\n",
      "Total time (minutes) of delay as a result of a security issue .\n",
      "\n",
      "late_aircraft_delay\n",
      "Total time (minutes) of delay flights as a result of a previous flight on the same airplane being late.\n",
      " \n",
      "name 'detect' is not defined Language is not detected: jarmos/stack-overflow-developer-survey-2011-2022\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survey-2011-2022\n",
      "name 'detect' is not defined Language is not detected: jarmos\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survey 2011-2022\n",
      "name 'detect' is not defined Language is not detected: The entire Stack Overflow historical survey compiled under one source!\n",
      "name 'detect' is not defined Language is not detected: The [Stack Overflow Annual Developer Survey](https://insights.stackoverflow.com/survey) is an online questionnaire format survey organised by the guys at Stack Overflow. It consists of responses of Software Engineers & Software Developers from different backgrounds around the world. And this dataset is a compiled set of all those historical responses right from 2011 to 2022 (and the near future!).\n",
      "\n",
      "The contents of this dataset **ARE NOT** owned by me and all copyrights should be accredited to the original authors at Stack Overflow instead. The work involved in this dataset is simply a compilation of all the data made publicly available by the guys at Stack Overflow instead.\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survey-2011-2022\n",
      "name 'detect' is not defined Language is not detected: jarmos\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survey 2011-2022\n",
      "name 'detect' is not defined Language is not detected: The entire Stack Overflow historical survey compiled under one source!\n",
      "name 'detect' is not defined Language is not detected: The [Stack Overflow Annual Developer Survey](https://insights.stackoverflow.com/survey) is an online questionnaire format survey organised by the guys at Stack Overflow. It consists of responses of Software Engineers & Software Developers from different backgrounds around the world. And this dataset is a compiled set of all those historical responses right from 2011 to 2022 (and the near future!).\n",
      "\n",
      "The contents of this dataset **ARE NOT** owned by me and all copyrights should be accredited to the original authors at Stack Overflow instead. The work involved in this dataset is simply a compilation of all the data made publicly available by the guys at Stack Overflow instead.\n",
      "name 'detect' is not defined Language is not detected: alexanderfrosati/goodbooks-10k-updated\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k-updated\n",
      "name 'detect' is not defined Language is not detected: alexanderfrosati\n",
      "name 'detect' is not defined Language is not detected: Goodbooks 10k Updated\n",
      "name 'detect' is not defined Language is not detected: The updated version of [this dataset](https://www.kaggle.com/zygmunt/goodbooks-10k).\n",
      "\n",
      "Taken from [here.](https://github.com/zygmuntz/goodbooks-10k)\n",
      "\n",
      "Original description below:\n",
      "\n",
      "\n",
      "There have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now.\n",
      "\n",
      "This dataset contains ratings for ten thousand popular books. As to the source, let's say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.\n",
      "\n",
      "Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.\n",
      "\n",
      "There are also books marked to read by the users, book metadata (author, year, etc.) and tags.\n",
      "\n",
      "## **Contents**\n",
      "\n",
      "**ratings.csv** contains ratings.\n",
      "\n",
      "**to_read.csv** provides IDs of the books marked \"to read\" by each user, as user_id,book_id pairs.\n",
      "\n",
      "**books.csv** has metadata for each book (goodreads IDs, authors, title, average rating, etc.).\n",
      "\n",
      "The metadata have been extracted from goodreads XML files, available in the third version of this dataset as **books_xml.tar.gz. **The archive contains 10000 XML files. One of them is available as sample_book.xml. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.\n",
      "\n",
      "**book_tags.csv** contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.\n",
      "\n",
      "**tags.csv** translates tag IDs to names.\n",
      "\n",
      "See the notebook for some basic stats of the dataset.\n",
      "\n",
      "## **goodreads IDs**\n",
      "\n",
      "Each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.\n",
      "\n",
      "You can use the goodreads book and work IDs to create URLs as follows:\n",
      "\n",
      "https://www.goodreads.com/book/show/2767052\n",
      "\n",
      "https://www.goodreads.com/work/editions/2792775\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k-updated\n",
      "name 'detect' is not defined Language is not detected: alexanderfrosati\n",
      "name 'detect' is not defined Language is not detected: Goodbooks 10k Updated\n",
      "name 'detect' is not defined Language is not detected: The updated version of [this dataset](https://www.kaggle.com/zygmunt/goodbooks-10k).\n",
      "\n",
      "Taken from [here.](https://github.com/zygmuntz/goodbooks-10k)\n",
      "\n",
      "Original description below:\n",
      "\n",
      "\n",
      "There have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now.\n",
      "\n",
      "This dataset contains ratings for ten thousand popular books. As to the source, let's say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.\n",
      "\n",
      "Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.\n",
      "\n",
      "There are also books marked to read by the users, book metadata (author, year, etc.) and tags.\n",
      "\n",
      "## **Contents**\n",
      "\n",
      "**ratings.csv** contains ratings.\n",
      "\n",
      "**to_read.csv** provides IDs of the books marked \"to read\" by each user, as user_id,book_id pairs.\n",
      "\n",
      "**books.csv** has metadata for each book (goodreads IDs, authors, title, average rating, etc.).\n",
      "\n",
      "The metadata have been extracted from goodreads XML files, available in the third version of this dataset as **books_xml.tar.gz. **The archive contains 10000 XML files. One of them is available as sample_book.xml. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.\n",
      "\n",
      "**book_tags.csv** contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.\n",
      "\n",
      "**tags.csv** translates tag IDs to names.\n",
      "\n",
      "See the notebook for some basic stats of the dataset.\n",
      "\n",
      "## **goodreads IDs**\n",
      "\n",
      "Each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.\n",
      "\n",
      "You can use the goodreads book and work IDs to create URLs as follows:\n",
      "\n",
      "https://www.goodreads.com/book/show/2767052\n",
      "\n",
      "https://www.goodreads.com/work/editions/2792775\n",
      "name 'detect' is not defined Language is not detected: danyalhyder/the-new-chapterbook-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: the-new-chapterbook-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: danyalhyder\n",
      "name 'detect' is not defined Language is not detected: The New Chapter(book recommendation system)\n",
      "name 'detect' is not defined Language is not detected: the-new-chapterbook-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: danyalhyder\n",
      "name 'detect' is not defined Language is not detected: The New Chapter(book recommendation system)\n",
      "name 'detect' is not defined Language is not detected: spittman1248/cdc-data-nutrition-physical-activity-obesity\n",
      "name 'detect' is not defined Language is not detected: cdc-data-nutrition-physical-activity-obesity\n",
      "name 'detect' is not defined Language is not detected: spittman1248\n",
      "name 'detect' is not defined Language is not detected: CDC Data: Nutrition, Physical Activity, & Obesity\n",
      "name 'detect' is not defined Language is not detected: Obesity Trends in US\n",
      "name 'detect' is not defined Language is not detected: This dataset includes data on adult's diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. This data is used for DNPAO's Data, Trends, and Maps database, which provides national and state specific data on obesity, nutrition, physical activity, and breastfeeding.  I was particularly curious on whether socioeconomic status has an impact on obesity.  In my analysis, I compare the obesity rate in each state, and then perform a linear regression on the obesity rate for each educational status and the income bracket.  \n",
      "\n",
      "name 'detect' is not defined Language is not detected: cdc-data-nutrition-physical-activity-obesity\n",
      "name 'detect' is not defined Language is not detected: spittman1248\n",
      "name 'detect' is not defined Language is not detected: CDC Data: Nutrition, Physical Activity, & Obesity\n",
      "name 'detect' is not defined Language is not detected: Obesity Trends in US\n",
      "name 'detect' is not defined Language is not detected: This dataset includes data on adult's diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. This data is used for DNPAO's Data, Trends, and Maps database, which provides national and state specific data on obesity, nutrition, physical activity, and breastfeeding.  I was particularly curious on whether socioeconomic status has an impact on obesity.  In my analysis, I compare the obesity rate in each state, and then perform a linear regression on the obesity rate for each educational status and the income bracket.  \n",
      "\n",
      "name 'detect' is not defined Language is not detected: trinadhsingaladevi/recommendation-corpus\n",
      "name 'detect' is not defined Language is not detected: recommendation-corpus\n",
      "name 'detect' is not defined Language is not detected: trinadhsingaladevi\n",
      "name 'detect' is not defined Language is not detected: recommendation_corpus\n",
      "name 'detect' is not defined Language is not detected: recommendation-corpus\n",
      "name 'detect' is not defined Language is not detected: trinadhsingaladevi\n",
      "name 'detect' is not defined Language is not detected: recommendation_corpus\n",
      "name 'detect' is not defined Language is not detected: dsxavier/not-the-price-of-books\n",
      "name 'detect' is not defined Language is not detected: not-the-price-of-books\n",
      "name 'detect' is not defined Language is not detected: dsxavier\n",
      "name 'detect' is not defined Language is not detected: Not the price of books\n",
      "name 'detect' is not defined Language is not detected: Datasets that used in the workshop of Not ONLY the price of books\n",
      "name 'detect' is not defined Language is not detected: This dataset is about books' information and aims to help for creating a model for predicting the prices of unseen books based on given features; `title, author, edition, reviews, ratings, synopsis, genre, bookcategory`\n",
      "name 'detect' is not defined Language is not detected: not-the-price-of-books\n",
      "name 'detect' is not defined Language is not detected: dsxavier\n",
      "name 'detect' is not defined Language is not detected: Not the price of books\n",
      "name 'detect' is not defined Language is not detected: Datasets that used in the workshop of Not ONLY the price of books\n",
      "name 'detect' is not defined Language is not detected: This dataset is about books' information and aims to help for creating a model for predicting the prices of unseen books based on given features; `title, author, edition, reviews, ratings, synopsis, genre, bookcategory`\n",
      "name 'detect' is not defined Language is not detected: muonneutrino/us-census-demographic-data\n",
      "name 'detect' is not defined Language is not detected: us-census-demographic-data\n",
      "name 'detect' is not defined Language is not detected: muonneutrino\n",
      "name 'detect' is not defined Language is not detected: US Census Demographic Data\n",
      "name 'detect' is not defined Language is not detected: Demographic and Economic Data for Tracts and Counties\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset expands on my earlier [New York City Census Data][1] dataset. It includes data from the entire country instead of just New York City. The expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets.\n",
      "\n",
      "### Content\n",
      "\n",
      "The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder [website][2]. Currently, I include two data files:\n",
      "\n",
      " 1. acs2015_census_tract_data.csv: Data for each census tract in the US, including DC and Puerto Rico.\n",
      " 2. acs2015_county_data.csv: Data for each county or county equivalent in the US, including DC and Puerto Rico.\n",
      "\n",
      "The two files have the same structure, with just a small difference in the name of the id column. Counties are political subdivisions, and the boundaries of some have been set for centuries. Census tracts, however, are defined by the census bureau and will have a much more consistent size. A typical census tract has around 5000 or so residents.\n",
      "\n",
      "The Census Bureau updates the estimates approximately every year. At least some of the 2016 data is already available, so I will likely update this in the near future.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data here were collected by the US Census Bureau. As a product of the US federal government, this is not subject to copyright within the US.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "There are many questions that we could try to answer with the data here. Can we predict things such as the state (classification) or household income (regression)? What kinds of clusters can we find in the data? What other datasets can be improved by the addition of census data? \n",
      "\n",
      "  [1]: https://www.kaggle.com/muonneutrino/new-york-city-census-data\n",
      "  [2]: https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml\n",
      "name 'detect' is not defined Language is not detected: us-census-demographic-data\n",
      "name 'detect' is not defined Language is not detected: muonneutrino\n",
      "name 'detect' is not defined Language is not detected: US Census Demographic Data\n",
      "name 'detect' is not defined Language is not detected: Demographic and Economic Data for Tracts and Counties\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset expands on my earlier [New York City Census Data][1] dataset. It includes data from the entire country instead of just New York City. The expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets.\n",
      "\n",
      "### Content\n",
      "\n",
      "The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder [website][2]. Currently, I include two data files:\n",
      "\n",
      " 1. acs2015_census_tract_data.csv: Data for each census tract in the US, including DC and Puerto Rico.\n",
      " 2. acs2015_county_data.csv: Data for each county or county equivalent in the US, including DC and Puerto Rico.\n",
      "\n",
      "The two files have the same structure, with just a small difference in the name of the id column. Counties are political subdivisions, and the boundaries of some have been set for centuries. Census tracts, however, are defined by the census bureau and will have a much more consistent size. A typical census tract has around 5000 or so residents.\n",
      "\n",
      "The Census Bureau updates the estimates approximately every year. At least some of the 2016 data is already available, so I will likely update this in the near future.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data here were collected by the US Census Bureau. As a product of the US federal government, this is not subject to copyright within the US.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "There are many questions that we could try to answer with the data here. Can we predict things such as the state (classification) or household income (regression)? What kinds of clusters can we find in the data? What other datasets can be improved by the addition of census data? \n",
      "\n",
      "  [1]: https://www.kaggle.com/muonneutrino/new-york-city-census-data\n",
      "  [2]: https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml\n",
      "name 'detect' is not defined Language is not detected: rohan4050/movie-recommendation-data\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-data\n",
      "name 'detect' is not defined Language is not detected: rohan4050\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation Data\n",
      "name 'detect' is not defined Language is not detected: Movie Recommender Dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Recommending user movies based on different parameters by taking input as movie name and giving the output as movie suggestions along with similarity score.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are various metadata available in the above datasets, the user can accordingly choose a data from the above bunch of datasets and design his movie recommendation engine.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I would like to thank my College faculties for encouraging me to develop this project due to which I was able to create this dataset.\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-data\n",
      "name 'detect' is not defined Language is not detected: rohan4050\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation Data\n",
      "name 'detect' is not defined Language is not detected: Movie Recommender Dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Recommending user movies based on different parameters by taking input as movie name and giving the output as movie suggestions along with similarity score.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are various metadata available in the above datasets, the user can accordingly choose a data from the above bunch of datasets and design his movie recommendation engine.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I would like to thank my College faculties for encouraging me to develop this project due to which I was able to create this dataset.\n",
      "name 'detect' is not defined Language is not detected: mahdiehhajian/museum-universe-data-file\n",
      "name 'detect' is not defined Language is not detected: museum-universe-data-file\n",
      "name 'detect' is not defined Language is not detected: mahdiehhajian\n",
      "name 'detect' is not defined Language is not detected: Museum Universe Data File\n",
      "name 'detect' is not defined Language is not detected: The Museum Universe Data File is an evolving list of museums \n",
      "name 'detect' is not defined Language is not detected: DESCRIPTION\n",
      "The Museum Universe Data File is an evolving list of museums and related organizations in the United States.\n",
      "SUMMARY\n",
      "The Museum Universe Data File is an evolving list of museums and related organizations in the United States. It includes basic information on aquariums, arboretums, botanical gardens, art museums, children’s museums, general museums, historic houses and sites, history museums, nature centers, natural history and anthropology museums, planetariums, science and technology centers, specialized museums, and zoos.\n",
      "\n",
      "References and Resources\n",
      "Museum Universe Data File FY 2015 Q3 - Most Current as of 2018-11-20\n",
      "Canonical Source\n",
      "Interactive Dataset\n",
      "Data Table\n",
      "Documentation (PDF)\n",
      "GeoJSON\n",
      "CSV Data File (ZIP)\n",
      "API\n",
      "JSON\n",
      "name 'detect' is not defined Language is not detected: museum-universe-data-file\n",
      "name 'detect' is not defined Language is not detected: mahdiehhajian\n",
      "name 'detect' is not defined Language is not detected: Museum Universe Data File\n",
      "name 'detect' is not defined Language is not detected: The Museum Universe Data File is an evolving list of museums \n",
      "name 'detect' is not defined Language is not detected: DESCRIPTION\n",
      "The Museum Universe Data File is an evolving list of museums and related organizations in the United States.\n",
      "SUMMARY\n",
      "The Museum Universe Data File is an evolving list of museums and related organizations in the United States. It includes basic information on aquariums, arboretums, botanical gardens, art museums, children’s museums, general museums, historic houses and sites, history museums, nature centers, natural history and anthropology museums, planetariums, science and technology centers, specialized museums, and zoos.\n",
      "\n",
      "References and Resources\n",
      "Museum Universe Data File FY 2015 Q3 - Most Current as of 2018-11-20\n",
      "Canonical Source\n",
      "Interactive Dataset\n",
      "Data Table\n",
      "Documentation (PDF)\n",
      "GeoJSON\n",
      "CSV Data File (ZIP)\n",
      "API\n",
      "JSON\n",
      "name 'detect' is not defined Language is not detected: shubchat/1002-short-stories-from-project-guttenberg\n",
      "name 'detect' is not defined Language is not detected: 1002-short-stories-from-project-guttenberg\n",
      "name 'detect' is not defined Language is not detected: shubchat\n",
      "name 'detect' is not defined Language is not detected: 1002 short stories from project guttenberg\n",
      "name 'detect' is not defined Language is not detected: Short stories from project guttenberg with metadata info(Author,title,language) \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset was extracted from the wonderful portal of [Project Guttenberg](https://www.gutenberg.org/) to develop a short story recommendation engine . The idea that was being explored was, that we have Spotify for music and Netflix for movies but can we develop a great recommendation application for reading. The dataset is supposed to be used to experiment with various similarity algorithms and metrics to help develop a great user short story reading recommendation and open it for free to a wider audience.\n",
      "The project is open on Github at https://github.com/shubchat/Readnet. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "My deep thanks and respect for the team of volunteers at https://www.gutenberg.org/ .It is their inspiring effort to aggregate and maintain the open literature that is serving millions of people who love reading but don't have access to resources. If you are using this dataset please consider donating to this wonderful cause at https://www.gutenberg.org/wiki/Gutenberg:Project_Gutenberg_Needs_Your_Donation\n",
      "\n",
      "## Permissions on how to\n",
      "Please refer to the guidance from project Gutenberg at https://www.gutenberg.org/wiki/Gutenberg:Permission_How-To for details.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: 1002-short-stories-from-project-guttenberg\n",
      "name 'detect' is not defined Language is not detected: shubchat\n",
      "name 'detect' is not defined Language is not detected: 1002 short stories from project guttenberg\n",
      "name 'detect' is not defined Language is not detected: Short stories from project guttenberg with metadata info(Author,title,language) \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset was extracted from the wonderful portal of [Project Guttenberg](https://www.gutenberg.org/) to develop a short story recommendation engine . The idea that was being explored was, that we have Spotify for music and Netflix for movies but can we develop a great recommendation application for reading. The dataset is supposed to be used to experiment with various similarity algorithms and metrics to help develop a great user short story reading recommendation and open it for free to a wider audience.\n",
      "The project is open on Github at https://github.com/shubchat/Readnet. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "My deep thanks and respect for the team of volunteers at https://www.gutenberg.org/ .It is their inspiring effort to aggregate and maintain the open literature that is serving millions of people who love reading but don't have access to resources. If you are using this dataset please consider donating to this wonderful cause at https://www.gutenberg.org/wiki/Gutenberg:Project_Gutenberg_Needs_Your_Donation\n",
      "\n",
      "## Permissions on how to\n",
      "Please refer to the guidance from project Gutenberg at https://www.gutenberg.org/wiki/Gutenberg:Permission_How-To for details.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: amanverma1999/a-comprehensive-dataset-for-ddos-attack\n",
      "name 'detect' is not defined Language is not detected: a-comprehensive-dataset-for-ddos-attack\n",
      "name 'detect' is not defined Language is not detected: amanverma1999\n",
      "name 'detect' is not defined Language is not detected: A comprehensive dataset for DDOS attack\n",
      "name 'detect' is not defined Language is not detected: ### Introduction\n",
      "\n",
      "A DDoS (Distributed Denial-of-Service) attack is a malicious attempt to disrupt the normal traffic of a targeted server, service or network by overwhelming the target or its surrounding infrastructure with a flood of Internet traffic. Below Dataset consists of few of DDoS Attacks.The most common types are incorporated in this dataset are:-TCP Flood Attack,UDP Flood Attack and ICMP (Ping) Flood Attack.These attacks constitute the most common threats that are advancing at an alarming rate, in their sophistication and frequency. \n",
      "\n",
      "\n",
      "### Content of Dataset \n",
      "\n",
      "Dataset consists of  various features/columns like [\"duration\", \"protocoltype\", \"service\", \"flag\", \"srcbytes\", \"dstbytes\", \"land\", \"wrongfragment\",\"urgent\", \"hot\",\"numfailedlogins\", \"loggedin\", \"numcompromised\", \"rootshell\", \"suattempted\",\"numroot\", \"numfilecreations\", \"numshells\",\"numaccessfiles\",\"dsthostsrvserrorrate\", \"dsthostrerrorrate\", \"dsthostsrvrerrorrate\", \"result\".etc .But I found these features :- \"service\",\"count\",\"srvcount\",\"srcbytes\",\"serrorrate\"  useful for TCP Attacks,\"dstbytes\",\"service\",\"srcbytes\",\"dsthostsrvcount\",\"count\" for UDP attacks, and 'duration','service','srcbytes','wrongfragment','count','urgent','numcompromised','srvcount'for ICMP attacks.\n",
      "\n",
      "### Acknowledgement\n",
      "\n",
      "The data was collected from many sources, then it was merged and preprocessed. Due to very few and raw datasets available for generalised models, specific data was collected for each of our models. Datasets like\n",
      "https://explore.openaire.eu/search/publication?articleId=od_18::28e41a0b8b48aee3824dfa74f6fbcf9d\n",
      "https://www.kaggle.com/xwolf12/network-traffic-android-malware\n",
      "https://www.impactcybertrust.org/dataset_view?idDataset=479\n",
      "https://search.datacite.org/works/10.25549/usctheses-m150\n",
      "https://data.mendeley.com/datasets/psjxnzsxyx/2\n",
      "These datasets and a few others were combined to prepare the final  data-set.\n",
      "name 'detect' is not defined Language is not detected: a-comprehensive-dataset-for-ddos-attack\n",
      "name 'detect' is not defined Language is not detected: amanverma1999\n",
      "name 'detect' is not defined Language is not detected: A comprehensive dataset for DDOS attack\n",
      "name 'detect' is not defined Language is not detected: ### Introduction\n",
      "\n",
      "A DDoS (Distributed Denial-of-Service) attack is a malicious attempt to disrupt the normal traffic of a targeted server, service or network by overwhelming the target or its surrounding infrastructure with a flood of Internet traffic. Below Dataset consists of few of DDoS Attacks.The most common types are incorporated in this dataset are:-TCP Flood Attack,UDP Flood Attack and ICMP (Ping) Flood Attack.These attacks constitute the most common threats that are advancing at an alarming rate, in their sophistication and frequency. \n",
      "\n",
      "\n",
      "### Content of Dataset \n",
      "\n",
      "Dataset consists of  various features/columns like [\"duration\", \"protocoltype\", \"service\", \"flag\", \"srcbytes\", \"dstbytes\", \"land\", \"wrongfragment\",\"urgent\", \"hot\",\"numfailedlogins\", \"loggedin\", \"numcompromised\", \"rootshell\", \"suattempted\",\"numroot\", \"numfilecreations\", \"numshells\",\"numaccessfiles\",\"dsthostsrvserrorrate\", \"dsthostrerrorrate\", \"dsthostsrvrerrorrate\", \"result\".etc .But I found these features :- \"service\",\"count\",\"srvcount\",\"srcbytes\",\"serrorrate\"  useful for TCP Attacks,\"dstbytes\",\"service\",\"srcbytes\",\"dsthostsrvcount\",\"count\" for UDP attacks, and 'duration','service','srcbytes','wrongfragment','count','urgent','numcompromised','srvcount'for ICMP attacks.\n",
      "\n",
      "### Acknowledgement\n",
      "\n",
      "The data was collected from many sources, then it was merged and preprocessed. Due to very few and raw datasets available for generalised models, specific data was collected for each of our models. Datasets like\n",
      "https://explore.openaire.eu/search/publication?articleId=od_18::28e41a0b8b48aee3824dfa74f6fbcf9d\n",
      "https://www.kaggle.com/xwolf12/network-traffic-android-malware\n",
      "https://www.impactcybertrust.org/dataset_view?idDataset=479\n",
      "https://search.datacite.org/works/10.25549/usctheses-m150\n",
      "https://data.mendeley.com/datasets/psjxnzsxyx/2\n",
      "These datasets and a few others were combined to prepare the final  data-set.\n",
      "name 'detect' is not defined Language is not detected: alessiasimone/lego-sets-and-price-1955-2023\n",
      "name 'detect' is not defined Language is not detected: lego-sets-and-price-1955-2023\n",
      "name 'detect' is not defined Language is not detected: alessiasimone\n",
      "name 'detect' is not defined Language is not detected: Lego sets and price [1955 - 2023]\n",
      "name 'detect' is not defined Language is not detected: Unifyied Rebrickable database with price and star ratings added informations\n",
      "name 'detect' is not defined Language is not detected: This dataset has been created by merging two types of dataset: \n",
      "- [rebrickable dataset](https://rebrickable.com/downloads/) downloaded on 2023.11.14 \n",
      "- informations about price, reviews and star rating for each set coming from [this](https://www.kaggle.com/datasets/mterzolo/lego-sets) dataset\n",
      "\n",
      "Preprocessing steps have been computer with Tableau Prep (the whole steps are represented in the image below), and involved cleaning, joining and aggregating data. \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8628282%2F9500c26d4e8b02ce1098b83acdeee228%2Fdata%20prep.png?generation=1700243606255489&alt=media)\n",
      "name 'detect' is not defined Language is not detected: lego-sets-and-price-1955-2023\n",
      "name 'detect' is not defined Language is not detected: alessiasimone\n",
      "name 'detect' is not defined Language is not detected: Lego sets and price [1955 - 2023]\n",
      "name 'detect' is not defined Language is not detected: Unifyied Rebrickable database with price and star ratings added informations\n",
      "name 'detect' is not defined Language is not detected: This dataset has been created by merging two types of dataset: \n",
      "- [rebrickable dataset](https://rebrickable.com/downloads/) downloaded on 2023.11.14 \n",
      "- informations about price, reviews and star rating for each set coming from [this](https://www.kaggle.com/datasets/mterzolo/lego-sets) dataset\n",
      "\n",
      "Preprocessing steps have been computer with Tableau Prep (the whole steps are represented in the image below), and involved cleaning, joining and aggregating data. \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8628282%2F9500c26d4e8b02ce1098b83acdeee228%2Fdata%20prep.png?generation=1700243606255489&alt=media)\n",
      "name 'detect' is not defined Language is not detected: nikhil741/hackerearth-holiday-season\n",
      "name 'detect' is not defined Language is not detected: hackerearth-holiday-season\n",
      "name 'detect' is not defined Language is not detected: nikhil741\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Holiday season\n",
      "name 'detect' is not defined Language is not detected: HackerEarth dataset on classic computer vision classification challenge.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a snowman, a Christmas tree, flights, and the like.\n",
      "name 'detect' is not defined Language is not detected: hackerearth-holiday-season\n",
      "name 'detect' is not defined Language is not detected: nikhil741\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Holiday season\n",
      "name 'detect' is not defined Language is not detected: HackerEarth dataset on classic computer vision classification challenge.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a snowman, a Christmas tree, flights, and the like.\n",
      "name 'detect' is not defined Language is not detected: smritisingh1997/movielens-10m-dataset\n",
      "name 'detect' is not defined Language is not detected: movielens-10m-dataset\n",
      "name 'detect' is not defined Language is not detected: smritisingh1997\n",
      "name 'detect' is not defined Language is not detected: MovieLens 10M Dataset\n",
      "name 'detect' is not defined Language is not detected: Can be used to build recommendation system using RBM\n",
      "name 'detect' is not defined Language is not detected: Build a RBM using this dataset to predict whether a particular user will like a movie or not.\n",
      "This data set contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service. Users were selected at random for inclusion. All users selected had rated at least 20 movies. Unlike previous MovieLens data sets, no demographic information is included. Each user is represented by an id, and no other information is provided. The data are contained in three files, movies.dat, ratings.dat and tags.dat. Also included are scripts for generating subsets of the data to support five-fold cross-validation of rating predictions.\n",
      "\n",
      "User Ids\n",
      "Movielens users were selected at random for inclusion. Their ids have been anonymized.\n",
      "\n",
      "Users were selected separately for inclusion in the ratings and tags data sets, which implies that user ids may appear in one set but not the other.\n",
      "\n",
      "The anonymized values are consistent between the ratings and tags data files. That is, user id n, if it appears in both files, refers to the same real MovieLens user.\n",
      "\n",
      "Ratings Data File Structure\n",
      "All ratings are contained in the file ratings.dat. Each line of this file represents one rating of one movie by one user, and has the following format:\n",
      "\n",
      "UserID::MovieID::Rating::Timestamp\n",
      "\n",
      "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
      "\n",
      "Ratings are made on a 5-star scale, with half-star increments.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "Tags Data File Structure\n",
      "All tags are contained in the file tags.dat. Each line of this file represents one tag applied to one movie by one user, and has the following format:\n",
      "\n",
      "UserID::MovieID::Tag::Timestamp\n",
      "\n",
      "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
      "\n",
      "Tags are user generated metadata about movies. Each tag is typically a single word, or short phrase. The meaning, value and purpose of a particular tag is determined by each user.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "Movies Data File Structure\n",
      "Movie information is contained in the file movies.dat. Each line of this file represents one movie, and has the following format:\n",
      "\n",
      "MovieID::Title::Genres\n",
      "\n",
      "MovieID is the real MovieLens id.\n",
      "\n",
      "Movie titles, by policy, should be entered identically to those found in IMDB, including year of release. However, they are entered manually, so errors and inconsistencies may exist.\n",
      "\n",
      "Genres are a pipe-separated list, and are selected from the following:\n",
      "\n",
      "Action\n",
      "Adventure\n",
      "Animation\n",
      "Children's\n",
      "Comedy\n",
      "Crime\n",
      "Documentary\n",
      "Drama\n",
      "Fantasy\n",
      "Film-Noir\n",
      "Horror\n",
      "Musical\n",
      "Mystery\n",
      "Romance\n",
      "Sci-Fi\n",
      "Thriller\n",
      "War\n",
      "Western\n",
      "name 'detect' is not defined Language is not detected: movielens-10m-dataset\n",
      "name 'detect' is not defined Language is not detected: smritisingh1997\n",
      "name 'detect' is not defined Language is not detected: MovieLens 10M Dataset\n",
      "name 'detect' is not defined Language is not detected: Can be used to build recommendation system using RBM\n",
      "name 'detect' is not defined Language is not detected: Build a RBM using this dataset to predict whether a particular user will like a movie or not.\n",
      "This data set contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service. Users were selected at random for inclusion. All users selected had rated at least 20 movies. Unlike previous MovieLens data sets, no demographic information is included. Each user is represented by an id, and no other information is provided. The data are contained in three files, movies.dat, ratings.dat and tags.dat. Also included are scripts for generating subsets of the data to support five-fold cross-validation of rating predictions.\n",
      "\n",
      "User Ids\n",
      "Movielens users were selected at random for inclusion. Their ids have been anonymized.\n",
      "\n",
      "Users were selected separately for inclusion in the ratings and tags data sets, which implies that user ids may appear in one set but not the other.\n",
      "\n",
      "The anonymized values are consistent between the ratings and tags data files. That is, user id n, if it appears in both files, refers to the same real MovieLens user.\n",
      "\n",
      "Ratings Data File Structure\n",
      "All ratings are contained in the file ratings.dat. Each line of this file represents one rating of one movie by one user, and has the following format:\n",
      "\n",
      "UserID::MovieID::Rating::Timestamp\n",
      "\n",
      "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
      "\n",
      "Ratings are made on a 5-star scale, with half-star increments.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "Tags Data File Structure\n",
      "All tags are contained in the file tags.dat. Each line of this file represents one tag applied to one movie by one user, and has the following format:\n",
      "\n",
      "UserID::MovieID::Tag::Timestamp\n",
      "\n",
      "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
      "\n",
      "Tags are user generated metadata about movies. Each tag is typically a single word, or short phrase. The meaning, value and purpose of a particular tag is determined by each user.\n",
      "\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
      "\n",
      "Movies Data File Structure\n",
      "Movie information is contained in the file movies.dat. Each line of this file represents one movie, and has the following format:\n",
      "\n",
      "MovieID::Title::Genres\n",
      "\n",
      "MovieID is the real MovieLens id.\n",
      "\n",
      "Movie titles, by policy, should be entered identically to those found in IMDB, including year of release. However, they are entered manually, so errors and inconsistencies may exist.\n",
      "\n",
      "Genres are a pipe-separated list, and are selected from the following:\n",
      "\n",
      "Action\n",
      "Adventure\n",
      "Animation\n",
      "Children's\n",
      "Comedy\n",
      "Crime\n",
      "Documentary\n",
      "Drama\n",
      "Fantasy\n",
      "Film-Noir\n",
      "Horror\n",
      "Musical\n",
      "Mystery\n",
      "Romance\n",
      "Sci-Fi\n",
      "Thriller\n",
      "War\n",
      "Western\n",
      "name 'detect' is not defined Language is not detected: muqarrishzaib/tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: muqarrishzaib\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Metadata on ~10,000 movies from TMDB\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "Recommendation systems are used everywhere nowadays. Netflix, Amazon Prime, YouTube, Online shopping sites, etc. Datasets like this are a great way to start working on a Recommendation system.\n",
      "The Dataset was created from the official API provided by TMDB.\n",
      "# Content\n",
      "What's inside is more than just rows and columns. This is the dataset for 10,000 Popular movies based on the TMDB ratings. Ideal database to start off with Recommendation algorithms.\n",
      "## Columns in the file include :\n",
      "- id\n",
      "- original_language\n",
      "- original_title\n",
      "- overview\n",
      "- popularity\n",
      "- release_date\n",
      "- title\n",
      "- vote_average\n",
      "- vote_count\n",
      "# Inspiration\n",
      "This dataset was assembled as part of a semester Project in Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata to build various types of Recommender Systems.\n",
      "\n",
      "**Some of the things you can do with this dataset:**\n",
      "Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content-Based and Collaborative Filtering Based Recommendation Engines.\n",
      "# Acknowledgments\n",
      "This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb.\n",
      "Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself [here](https://www.themoviedb.org/documentation/api).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: muqarrishzaib\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Metadata on ~10,000 movies from TMDB\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "Recommendation systems are used everywhere nowadays. Netflix, Amazon Prime, YouTube, Online shopping sites, etc. Datasets like this are a great way to start working on a Recommendation system.\n",
      "The Dataset was created from the official API provided by TMDB.\n",
      "# Content\n",
      "What's inside is more than just rows and columns. This is the dataset for 10,000 Popular movies based on the TMDB ratings. Ideal database to start off with Recommendation algorithms.\n",
      "## Columns in the file include :\n",
      "- id\n",
      "- original_language\n",
      "- original_title\n",
      "- overview\n",
      "- popularity\n",
      "- release_date\n",
      "- title\n",
      "- vote_average\n",
      "- vote_count\n",
      "# Inspiration\n",
      "This dataset was assembled as part of a semester Project in Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata to build various types of Recommender Systems.\n",
      "\n",
      "**Some of the things you can do with this dataset:**\n",
      "Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content-Based and Collaborative Filtering Based Recommendation Engines.\n",
      "# Acknowledgments\n",
      "This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb.\n",
      "Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself [here](https://www.themoviedb.org/documentation/api).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ernestojaguilar/shortterm-electricity-load-forecasting-panama\n",
      "name 'detect' is not defined Language is not detected: shortterm-electricity-load-forecasting-panama\n",
      "name 'detect' is not defined Language is not detected: ernestojaguilar\n",
      "name 'detect' is not defined Language is not detected: Short-term electricity load forecasting (Panama)\n",
      "name 'detect' is not defined Language is not detected: Panama case study (years 2015 to 2020)\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "These datasets are framed on predicting the short-term electricity, this forecasting problem is known in the research field as short-term load forecasting (STLF). These datasets address the STLF problem for the Panama power system, in which the forecasting horizon is one week, with hourly steps, which is a total of 168 hours. These datasets are useful to train and test forecasting models and compare their results with the power system operator official forecast ([take a look at real-time electricity load](https://sitr.cnd.com.pa/m/pub/sin.html)). The datasets include historical load, a vast set of weather variables, holidays, and historical load weekly forecast features. More information regarding these datasets context, a literature review of forecasting techniques suitable for this dataset, and results after testing a set of Machine Learning; are available in the article **Short-Term Electricity Load Forecasting with Machine Learning**. (Aguilar Madrid, E.; Antonio, N. Short-Term Electricity Load Forecasting with Machine Learning. *Information* 2021, 12, 50. https://doi.org/10.3390/info12020050)\n",
      "\n",
      "### Objectives\n",
      "The main objectives around these datasets are: \n",
      "1. Evaluate the power system operator official forecasts (weekly pre-dispatch forecast) against the real load, on weekly basis. \n",
      "2. Develop, train and test forecasting models to improve the operator official weekly forecasts (168 hours), in different scenarios. \n",
      "\n",
      "### Considerations to compare results\n",
      "The following considerations should be kept to compare forecasting results with the weekly pre-dispatch forecast:\n",
      "1. Saturday is the first day of each weekly forecast; for instance, Friday is the last day.\n",
      "2. The first full-week starting on Saturday should be considered as the first week of the year, to number the weeks.\n",
      "3. A 72 hours gap of unseen records should be considered before the first day to forecast. In other words, next week forecast should be done with records until each Tuesday last hour.\n",
      "4. Make sure to train and test keeping the chronological order of records.\n",
      "\n",
      "### Data sources\n",
      "Data sources provide hourly records, from January 2015 until June 2020. The data composition is the following:\n",
      "1. Historical electricity load, available on [daily post-dispatch reports](https://www.cnd.com.pa/index.php/informes/categoria/informes-de-operaciones?tipo=60), from the grid operator (ETESA, CND). \n",
      "2. Historical weekly forecasts available on [weekly pre-dispatch reports](https://www.cnd.com.pa/index.php/informes/categoria/informes-de-operaciones?tipo=68&anio=2019&semana=0), both from ETESA, CND.\n",
      "3. Calendar information related to school periods, from Panama's Ministry of Education, published in [official gazette](https://www.gacetaoficial.gob.pa/Busqueda-Avanzada).\n",
      "4. Calendar information related to holidays, from \"When on Earth?\" [website](https://www.whenonearth.com/calendar/panama/2020).\n",
      "5. Weather variables, such as temperature, relative humidity, precipitation, and wind speed, for three main cities in Panama, from [Earthdata](https://disc.gsfc.nasa.gov/datasets/M2T1NXSLV_5.12.4/summary).\n",
      "\n",
      "The original data sources provide the post-dispatch electricity load in individual Excel files on a daily basis and weekly pre-dispatch electricity load forecast data in individual Excel files on a weekly basis, both with hourly granularity. Holidays and school periods data is sparse, along with websites and PDF files. Weather data is available on daily NetCDF files. \n",
      "\n",
      "### Datasets\n",
      "For simplicity, the published datasets are already pre-processed by merging all data sources on the date-time index: \n",
      "1. A CSV file containing all records in a single continuous dataset with all variables.\n",
      "2. A CSV file containing the load forecast from weekly pre-dispatch reports.\n",
      "3. Two Excel files containing suggested regressors and 14 pairs of training/testing datasets as described in the PDF file.\n",
      "\n",
      "These 14 pairs of raining/testing datasets are selected according to these testing criteria:\n",
      "1. A testing week for each month before the lockdown due to COVID-19.\n",
      "2. Select testing weeks containing holidays.\n",
      "3. Plus, two testing weeks during the lockdown.\n",
      "\n",
      "### Less pre-processed data\n",
      "- Less pre-processed data regarding these datasets can be found in [this](http://dx.doi.org/10.17632/tcmmj4t6f4.1) data repository.\n",
      "name 'detect' is not defined Language is not detected: shortterm-electricity-load-forecasting-panama\n",
      "name 'detect' is not defined Language is not detected: ernestojaguilar\n",
      "name 'detect' is not defined Language is not detected: Short-term electricity load forecasting (Panama)\n",
      "name 'detect' is not defined Language is not detected: Panama case study (years 2015 to 2020)\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "These datasets are framed on predicting the short-term electricity, this forecasting problem is known in the research field as short-term load forecasting (STLF). These datasets address the STLF problem for the Panama power system, in which the forecasting horizon is one week, with hourly steps, which is a total of 168 hours. These datasets are useful to train and test forecasting models and compare their results with the power system operator official forecast ([take a look at real-time electricity load](https://sitr.cnd.com.pa/m/pub/sin.html)). The datasets include historical load, a vast set of weather variables, holidays, and historical load weekly forecast features. More information regarding these datasets context, a literature review of forecasting techniques suitable for this dataset, and results after testing a set of Machine Learning; are available in the article **Short-Term Electricity Load Forecasting with Machine Learning**. (Aguilar Madrid, E.; Antonio, N. Short-Term Electricity Load Forecasting with Machine Learning. *Information* 2021, 12, 50. https://doi.org/10.3390/info12020050)\n",
      "\n",
      "### Objectives\n",
      "The main objectives around these datasets are: \n",
      "1. Evaluate the power system operator official forecasts (weekly pre-dispatch forecast) against the real load, on weekly basis. \n",
      "2. Develop, train and test forecasting models to improve the operator official weekly forecasts (168 hours), in different scenarios. \n",
      "\n",
      "### Considerations to compare results\n",
      "The following considerations should be kept to compare forecasting results with the weekly pre-dispatch forecast:\n",
      "1. Saturday is the first day of each weekly forecast; for instance, Friday is the last day.\n",
      "2. The first full-week starting on Saturday should be considered as the first week of the year, to number the weeks.\n",
      "3. A 72 hours gap of unseen records should be considered before the first day to forecast. In other words, next week forecast should be done with records until each Tuesday last hour.\n",
      "4. Make sure to train and test keeping the chronological order of records.\n",
      "\n",
      "### Data sources\n",
      "Data sources provide hourly records, from January 2015 until June 2020. The data composition is the following:\n",
      "1. Historical electricity load, available on [daily post-dispatch reports](https://www.cnd.com.pa/index.php/informes/categoria/informes-de-operaciones?tipo=60), from the grid operator (ETESA, CND). \n",
      "2. Historical weekly forecasts available on [weekly pre-dispatch reports](https://www.cnd.com.pa/index.php/informes/categoria/informes-de-operaciones?tipo=68&anio=2019&semana=0), both from ETESA, CND.\n",
      "3. Calendar information related to school periods, from Panama's Ministry of Education, published in [official gazette](https://www.gacetaoficial.gob.pa/Busqueda-Avanzada).\n",
      "4. Calendar information related to holidays, from \"When on Earth?\" [website](https://www.whenonearth.com/calendar/panama/2020).\n",
      "5. Weather variables, such as temperature, relative humidity, precipitation, and wind speed, for three main cities in Panama, from [Earthdata](https://disc.gsfc.nasa.gov/datasets/M2T1NXSLV_5.12.4/summary).\n",
      "\n",
      "The original data sources provide the post-dispatch electricity load in individual Excel files on a daily basis and weekly pre-dispatch electricity load forecast data in individual Excel files on a weekly basis, both with hourly granularity. Holidays and school periods data is sparse, along with websites and PDF files. Weather data is available on daily NetCDF files. \n",
      "\n",
      "### Datasets\n",
      "For simplicity, the published datasets are already pre-processed by merging all data sources on the date-time index: \n",
      "1. A CSV file containing all records in a single continuous dataset with all variables.\n",
      "2. A CSV file containing the load forecast from weekly pre-dispatch reports.\n",
      "3. Two Excel files containing suggested regressors and 14 pairs of training/testing datasets as described in the PDF file.\n",
      "\n",
      "These 14 pairs of raining/testing datasets are selected according to these testing criteria:\n",
      "1. A testing week for each month before the lockdown due to COVID-19.\n",
      "2. Select testing weeks containing holidays.\n",
      "3. Plus, two testing weeks during the lockdown.\n",
      "\n",
      "### Less pre-processed data\n",
      "- Less pre-processed data regarding these datasets can be found in [this](http://dx.doi.org/10.17632/tcmmj4t6f4.1) data repository.\n",
      "name 'detect' is not defined Language is not detected: shreesudame/covid19-dataset\n",
      "name 'detect' is not defined Language is not detected: covid19-dataset\n",
      "name 'detect' is not defined Language is not detected: shreesudame\n",
      "name 'detect' is not defined Language is not detected: COVID-19 dataset\n",
      "name 'detect' is not defined Language is not detected:  Exp. global data of covid-19 cases ,tests, deaths, vaccination,hospitalizations\n",
      "name 'detect' is not defined Language is not detected: Explore the global data on confirmed COVID-19 cases FROM 2020 TO 2022\n",
      "\n",
      "\n",
      "CONTENT:\n",
      "What is the daily number of confirmed cases?\n",
      "Daily confirmed cases per million people\n",
      "What is the cumulative number of confirmed cases?\n",
      "Cumulative confirmed cases per million people\n",
      "Weekly and biweekly cases: where are confirmed cases increasing or falling?\n",
      "Global comparison: where are confirmed cases increasing most rapidly?\n",
      "\n",
      "#  COMPLETE DATASET AVAILABLE ON :\n",
      "- Explore https://ourworldindata.org/\n",
      "name 'detect' is not defined Language is not detected: covid19-dataset\n",
      "name 'detect' is not defined Language is not detected: shreesudame\n",
      "name 'detect' is not defined Language is not detected: COVID-19 dataset\n",
      "name 'detect' is not defined Language is not detected:  Exp. global data of covid-19 cases ,tests, deaths, vaccination,hospitalizations\n",
      "name 'detect' is not defined Language is not detected: Explore the global data on confirmed COVID-19 cases FROM 2020 TO 2022\n",
      "\n",
      "\n",
      "CONTENT:\n",
      "What is the daily number of confirmed cases?\n",
      "Daily confirmed cases per million people\n",
      "What is the cumulative number of confirmed cases?\n",
      "Cumulative confirmed cases per million people\n",
      "Weekly and biweekly cases: where are confirmed cases increasing or falling?\n",
      "Global comparison: where are confirmed cases increasing most rapidly?\n",
      "\n",
      "#  COMPLETE DATASET AVAILABLE ON :\n",
      "- Explore https://ourworldindata.org/\n",
      "name 'detect' is not defined Language is not detected: validmodel/amazon-top-cell-phones-and-accessories-qa\n",
      "name 'detect' is not defined Language is not detected: amazon-top-cell-phones-and-accessories-qa\n",
      "name 'detect' is not defined Language is not detected: validmodel\n",
      "name 'detect' is not defined Language is not detected: Amazon Top cell Phones and Accessories QA\n",
      "name 'detect' is not defined Language is not detected:  Answers for Amazon Customer Queries , followed with Yes or No.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains question and answers for Amazon cell phones and Accessories .\n",
      "Citation :- Addressing complex and subjective product-related queries with customer reviews .\n",
      "\n",
      "This dataset can be used to build the chatbot or recommendation of answers for questions based on similarity score.\n",
      "name 'detect' is not defined Language is not detected: amazon-top-cell-phones-and-accessories-qa\n",
      "name 'detect' is not defined Language is not detected: validmodel\n",
      "name 'detect' is not defined Language is not detected: Amazon Top cell Phones and Accessories QA\n",
      "name 'detect' is not defined Language is not detected:  Answers for Amazon Customer Queries , followed with Yes or No.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains question and answers for Amazon cell phones and Accessories .\n",
      "Citation :- Addressing complex and subjective product-related queries with customer reviews .\n",
      "\n",
      "This dataset can be used to build the chatbot or recommendation of answers for questions based on similarity score.\n",
      "name 'detect' is not defined Language is not detected: hossamahmedsalah/anime-mal-dataset\n",
      "name 'detect' is not defined Language is not detected: anime-mal-dataset\n",
      "name 'detect' is not defined Language is not detected: hossamahmedsalah\n",
      "name 'detect' is not defined Language is not detected: Anime MAL dataset\n",
      "name 'detect' is not defined Language is not detected: My Anime List dataset \n",
      "name 'detect' is not defined Language is not detected: MyAnimeList is a popular online platform that allows users to create a list of anime and manga they have watched or read, rate them, and write reviews. The MyAnimeList dataset available on Kaggle is a collection of information about various anime titles and their corresponding attributes, such as title, genre, rating, popularity, and episode count.\n",
      " The columns include information about the anime title, the type of anime (TV show, movie, OVA, etc.), the genre(s) it belongs to, the studio that produced it, the source material (whether it is an original work or an adaptation), and the season and year of release.\n",
      "\n",
      "In addition to the basic information, the dataset also includes ratings and popularity metrics, such as the number of users who have rated the anime and the average rating score, as well as the number of members who have added the anime to their list and the number of favorites. Moreover, the dataset includes information about the anime's episodes, duration, and opening and ending themes.\n",
      "\n",
      "This dataset could be useful for various applications, such as building recommendation systems, conducting research on anime trends, and analyzing the relationship between various attributes (e.g., genre and popularity). Overall, the MyAnimeList dataset is an invaluable resource for anyone interested in anime and manga, and it provides a wealth of information that can be leveraged for various data-driven analyses.\n",
      "name 'detect' is not defined Language is not detected: anime-mal-dataset\n",
      "name 'detect' is not defined Language is not detected: hossamahmedsalah\n",
      "name 'detect' is not defined Language is not detected: Anime MAL dataset\n",
      "name 'detect' is not defined Language is not detected: My Anime List dataset \n",
      "name 'detect' is not defined Language is not detected: MyAnimeList is a popular online platform that allows users to create a list of anime and manga they have watched or read, rate them, and write reviews. The MyAnimeList dataset available on Kaggle is a collection of information about various anime titles and their corresponding attributes, such as title, genre, rating, popularity, and episode count.\n",
      " The columns include information about the anime title, the type of anime (TV show, movie, OVA, etc.), the genre(s) it belongs to, the studio that produced it, the source material (whether it is an original work or an adaptation), and the season and year of release.\n",
      "\n",
      "In addition to the basic information, the dataset also includes ratings and popularity metrics, such as the number of users who have rated the anime and the average rating score, as well as the number of members who have added the anime to their list and the number of favorites. Moreover, the dataset includes information about the anime's episodes, duration, and opening and ending themes.\n",
      "\n",
      "This dataset could be useful for various applications, such as building recommendation systems, conducting research on anime trends, and analyzing the relationship between various attributes (e.g., genre and popularity). Overall, the MyAnimeList dataset is an invaluable resource for anyone interested in anime and manga, and it provides a wealth of information that can be leveraged for various data-driven analyses.\n",
      "name 'detect' is not defined Language is not detected: bibintb/movie-dataset\n",
      "name 'detect' is not defined Language is not detected: movie-dataset\n",
      "name 'detect' is not defined Language is not detected: bibintb\n",
      "name 'detect' is not defined Language is not detected: Movie dataset\n",
      "name 'detect' is not defined Language is not detected: Movie dataset with details\n",
      "name 'detect' is not defined Language is not detected: Content:\n",
      "\n",
      "Useful for basic prediction of movie recommendation system.\n",
      "\n",
      "Acknowledgements:\n",
      "\n",
      "The  dataset got from GitHub repo of codeheroku.  \n",
      "\n",
      "Sources : https://github.com/codeheroku/Introduction-to-Machine-Learning/blob/master/Building%20a%20Movie%20Recommendation%20Engine/movie_dataset.csv \n",
      "name 'detect' is not defined Language is not detected: movie-dataset\n",
      "name 'detect' is not defined Language is not detected: bibintb\n",
      "name 'detect' is not defined Language is not detected: Movie dataset\n",
      "name 'detect' is not defined Language is not detected: Movie dataset with details\n",
      "name 'detect' is not defined Language is not detected: Content:\n",
      "\n",
      "Useful for basic prediction of movie recommendation system.\n",
      "\n",
      "Acknowledgements:\n",
      "\n",
      "The  dataset got from GitHub repo of codeheroku.  \n",
      "\n",
      "Sources : https://github.com/codeheroku/Introduction-to-Machine-Learning/blob/master/Building%20a%20Movie%20Recommendation%20Engine/movie_dataset.csv \n",
      "name 'detect' is not defined Language is not detected: /electric-power-consumption-data-set\n",
      "name 'detect' is not defined Language is not detected: electric-power-consumption-data-set\n",
      "name 'detect' is not defined Language is not detected: Household Electric Power Consumption\n",
      "name 'detect' is not defined Language is not detected: time series analysis- regression / clustering\n",
      "name 'detect' is not defined Language is not detected: I need help to analyze this data set **with R code**, if someone can help me I'd appreciate a lot and **I'd send some money** for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file. \n",
      "**Thanks in advance :)**\n",
      "\n",
      "**Context:** Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "Multivariate, Time-Series\n",
      "\n",
      "**Associated Tasks:**\n",
      "Regression, Clustering\n",
      "\n",
      "**Data Set Information:**\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). \n",
      "Notes: \n",
      "1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3. \n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "\n",
      "**Attribute Information:**\n",
      "1.date: Date in format dd/mm/yyyy \n",
      "\n",
      "2.time: time in format hh:mm:ss \n",
      "\n",
      "3.global_active_power: household global minute-averaged active power (in kilowatt) \n",
      "\n",
      "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt) \n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt) \n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere) \n",
      "\n",
      "7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
      "\n",
      "8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n",
      "\n",
      "9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: electric-power-consumption-data-set\n",
      "name 'detect' is not defined Language is not detected: Household Electric Power Consumption\n",
      "name 'detect' is not defined Language is not detected: time series analysis- regression / clustering\n",
      "name 'detect' is not defined Language is not detected: I need help to analyze this data set **with R code**, if someone can help me I'd appreciate a lot and **I'd send some money** for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file. \n",
      "**Thanks in advance :)**\n",
      "\n",
      "**Context:** Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "Multivariate, Time-Series\n",
      "\n",
      "**Associated Tasks:**\n",
      "Regression, Clustering\n",
      "\n",
      "**Data Set Information:**\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). \n",
      "Notes: \n",
      "1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3. \n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "\n",
      "**Attribute Information:**\n",
      "1.date: Date in format dd/mm/yyyy \n",
      "\n",
      "2.time: time in format hh:mm:ss \n",
      "\n",
      "3.global_active_power: household global minute-averaged active power (in kilowatt) \n",
      "\n",
      "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt) \n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt) \n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere) \n",
      "\n",
      "7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
      "\n",
      "8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n",
      "\n",
      "9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: ktsprabhu/imdb-top-4070-explore-the-cinema-data\n",
      "name 'detect' is not defined Language is not detected: imdb-top-4070-explore-the-cinema-data\n",
      "name 'detect' is not defined Language is not detected: ktsprabhu\n",
      "name 'detect' is not defined Language is not detected: IMDb Top 4070: Explore the Cinema Data\n",
      "name 'detect' is not defined Language is not detected: Python - Exploratory Data Analysis\n",
      "name 'detect' is not defined Language is not detected: **Description**:\n",
      "Dive into the world of exceptional cinema with our meticulously curated dataset, \"IMDb's Gems Unveiled.\" This dataset is a result of an extensive data collection effort based on two critical criteria: IMDb ratings exceeding 7 and a substantial number of votes, surpassing 10,000. The outcome? A treasure trove of 4070 movies meticulously selected from IMDb's vast repository.\n",
      "\n",
      "What sets this dataset apart is its richness and diversity. With more than 20 data points meticulously gathered for each movie, this collection offers a comprehensive insight into each cinematic masterpiece. Our data collection process leveraged the power of Selenium and Pandas modules, ensuring accuracy and reliability.\n",
      "\n",
      "Cleaning this vast dataset was a meticulous task, combining both Excel and Python for optimum precision. Analysis is powered by Pandas, Matplotlib, and NLTK, enabling to uncover hidden patterns, trends, and themes within the realm of cinema.\n",
      "\n",
      "**Note**: The data is collected as of April 2023.\n",
      "Future versions of this analysis include Movie recommendation system\n",
      "Please do connect for any queries,\n",
      "All Love, No Hate.\n",
      "name 'detect' is not defined Language is not detected: imdb-top-4070-explore-the-cinema-data\n",
      "name 'detect' is not defined Language is not detected: ktsprabhu\n",
      "name 'detect' is not defined Language is not detected: IMDb Top 4070: Explore the Cinema Data\n",
      "name 'detect' is not defined Language is not detected: Python - Exploratory Data Analysis\n",
      "name 'detect' is not defined Language is not detected: **Description**:\n",
      "Dive into the world of exceptional cinema with our meticulously curated dataset, \"IMDb's Gems Unveiled.\" This dataset is a result of an extensive data collection effort based on two critical criteria: IMDb ratings exceeding 7 and a substantial number of votes, surpassing 10,000. The outcome? A treasure trove of 4070 movies meticulously selected from IMDb's vast repository.\n",
      "\n",
      "What sets this dataset apart is its richness and diversity. With more than 20 data points meticulously gathered for each movie, this collection offers a comprehensive insight into each cinematic masterpiece. Our data collection process leveraged the power of Selenium and Pandas modules, ensuring accuracy and reliability.\n",
      "\n",
      "Cleaning this vast dataset was a meticulous task, combining both Excel and Python for optimum precision. Analysis is powered by Pandas, Matplotlib, and NLTK, enabling to uncover hidden patterns, trends, and themes within the realm of cinema.\n",
      "\n",
      "**Note**: The data is collected as of April 2023.\n",
      "Future versions of this analysis include Movie recommendation system\n",
      "Please do connect for any queries,\n",
      "All Love, No Hate.\n",
      "name 'detect' is not defined Language is not detected: chriszhengao/cma-best-track-data\n",
      "name 'detect' is not defined Language is not detected: cma-best-track-data\n",
      "name 'detect' is not defined Language is not detected: chriszhengao\n",
      "name 'detect' is not defined Language is not detected: CMA Tropical Cyclone Best Track Data\n",
      "name 'detect' is not defined Language is not detected: Tropical cyclones data of north-west Pacific every six hours since 1949.\n",
      "name 'detect' is not defined Language is not detected: # Description   \n",
      "The current version of the CMA Tropical Cyclone Optimal Path dataset provides the position and intensity of tropical cyclones in the waters of the Northwest Pacific Ocean (including the South China Sea, north of the equator and west of 180°E) for each 6-hour period since 1949, which are placed in separate text files according to the year, and will be added year by year in the future.   \n",
      "   \n",
      "Starting from 2017, for typhoons making landfall in China, the frequency of the best track time is encoded to once every 3 hours during the 24-hour period before their landfall.   \n",
      "   \n",
      "Starting from 2018, for typhoons making landfall in China, the frequency of the best track time is encoded to once every three hours during the 24-hour period before landfall and during the period of land activities in China.   \n",
      "   \n",
      "# File Format   \n",
      "CHYYYYBST.txt --   \n",
      "   \n",
      "CH: taken from English CHINA, indicating that this dataset is compiled by China;    \n",
      "   \n",
      "YYYY: is the year, expressed as four digits;   \n",
      "   \n",
      "BST: taken from English BEST TRACK, indicating that this dataset is the best path dataset.   \n",
      "   \n",
      "# Document content format   \n",
      "## 1. Header Lines   \n",
      "![Header](https://tcdata.typhoon.org.cn/images/best-track-data-format-pic_1.png)   \n",
      "| Field | Length | Description |\n",
      "|-------|--------|-------------|\n",
      "| AAAAA | 5      | Classification flag; '66666' indicates the best track data. |\n",
      "| BBBB  | 4      | International number; the last two digits of the year + two-digit serial number. |\n",
      "| CCC   | 3      | Number of rows in the track data record. |\n",
      "| DDDD  | 4      | Serial number of tropical cyclones, including tropical depressions. |\n",
      "| EEEE  | 4      | China's identification number for tropical cyclones. |\n",
      "| F     | 1      | Tropical cyclone termination record: 0 for dissipation, 1 for moving out of the responsibility area of the Western Pacific Typhoon Committee, 2 for merging, 3 for quasi-stationary. |\n",
      "| G     | 1      | Hourly interval between each row of the path; before 2017, it was 6 hours, starting from 2017, individual cases with a 3-hour encryption record are marked as 3, and others remain 6. |\n",
      "| H...H | 20     | English name of the tropical cyclone; \"(-1)n\" is added after the name to indicate the secondary center and its serial number. |\n",
      "| I...I | 8      | Date on which the dataset is formed. |\n",
      "   \n",
      "## 2. Data Lines   \n",
      "![Data Lines](https://tcdata.typhoon.org.cn/images/best-track-data-format-pic_2.png)   \n",
      "| Field          | Description |\n",
      "|----------------|-------------|\n",
      "| YYYYMMDDHH     | Date and time in UTC: YYYY year, MM month, DD day, HH hour. |\n",
      "| I              | Intensity marker based on the average wind speed within 2 minutes around the exact time point. Refer to the National Standard \"Tropical Cyclone Grades\" (GB/T 19201-2006): <br> 0 - Weaker than Tropical Depression (TD), or intensity unknown. <br> 1 - Tropical Depression (TD, 10.8-17.1 m/s). <br> 2 - Tropical Storm (TS, 17.2-24.4 m/s). <br> 3 - Severe Tropical Storm (STS, 24.5-32.6 m/s). <br> 4 - Typhoon (TY, 32.7-41.4 m/s). <br> 5 - Severe Typhoon (STY, 41.5-50.9 m/s). <br> 6 - Super Typhoon (SuperTY, ≥51.0 m/s). <br> 9 - Extratropical transition, the first digit indicates the completion of the transition. |\n",
      "| LAT            | Latitude (0.1°N). |\n",
      "| LONG           | Longitude (0.1°E). |\n",
      "| PRES           | Central minimum pressure (hPa). |\n",
      "| WND            | 2-minute average maximum sustained wind speed near the center (MSW, m/s). WND=9 indicates MSW &lt; 10 m/s, WND=0 indicates missing data. |\n",
      "| OWD            | 2-minute average wind speed (m/s) with two cases: <br> (a) For tropical cyclones making landfall in China, it represents the wind speed of coastal strong winds. <br> (b) When a tropical cyclone is in the South China Sea, it represents the maximum wind speed within a range of 300-500 km from the center. |   \n",
      "\n",
      "# Citation   \n",
      "Ying, M., W. Zhang, H. Yu, X. Lu, J. Feng, Y. Fan, Y. Zhu, and D. Chen, 2014: An overview of the China Meteorological Administration tropical cyclone database. J. Atmos. Oceanic Technol., 31, 287-301. doi: 10.1175/JTECH-D-12-00119.1\n",
      "Lu, X. Q., H. Yu, M. Ying, B. K. Zhao, S. Zhang, L. M. Lin, L. N. Bai, and R. J. Wan, 2021: Western North Pacific tropical cyclone database created by the China Meteorological Administration. Adv. Atmos. Sci., 38(4), 690−699. doi: 10.1007/s00376-020-0211-7   \n",
      "name 'detect' is not defined Language is not detected: cma-best-track-data\n",
      "name 'detect' is not defined Language is not detected: chriszhengao\n",
      "name 'detect' is not defined Language is not detected: CMA Tropical Cyclone Best Track Data\n",
      "name 'detect' is not defined Language is not detected: Tropical cyclones data of north-west Pacific every six hours since 1949.\n",
      "name 'detect' is not defined Language is not detected: # Description   \n",
      "The current version of the CMA Tropical Cyclone Optimal Path dataset provides the position and intensity of tropical cyclones in the waters of the Northwest Pacific Ocean (including the South China Sea, north of the equator and west of 180°E) for each 6-hour period since 1949, which are placed in separate text files according to the year, and will be added year by year in the future.   \n",
      "   \n",
      "Starting from 2017, for typhoons making landfall in China, the frequency of the best track time is encoded to once every 3 hours during the 24-hour period before their landfall.   \n",
      "   \n",
      "Starting from 2018, for typhoons making landfall in China, the frequency of the best track time is encoded to once every three hours during the 24-hour period before landfall and during the period of land activities in China.   \n",
      "   \n",
      "# File Format   \n",
      "CHYYYYBST.txt --   \n",
      "   \n",
      "CH: taken from English CHINA, indicating that this dataset is compiled by China;    \n",
      "   \n",
      "YYYY: is the year, expressed as four digits;   \n",
      "   \n",
      "BST: taken from English BEST TRACK, indicating that this dataset is the best path dataset.   \n",
      "   \n",
      "# Document content format   \n",
      "## 1. Header Lines   \n",
      "![Header](https://tcdata.typhoon.org.cn/images/best-track-data-format-pic_1.png)   \n",
      "| Field | Length | Description |\n",
      "|-------|--------|-------------|\n",
      "| AAAAA | 5      | Classification flag; '66666' indicates the best track data. |\n",
      "| BBBB  | 4      | International number; the last two digits of the year + two-digit serial number. |\n",
      "| CCC   | 3      | Number of rows in the track data record. |\n",
      "| DDDD  | 4      | Serial number of tropical cyclones, including tropical depressions. |\n",
      "| EEEE  | 4      | China's identification number for tropical cyclones. |\n",
      "| F     | 1      | Tropical cyclone termination record: 0 for dissipation, 1 for moving out of the responsibility area of the Western Pacific Typhoon Committee, 2 for merging, 3 for quasi-stationary. |\n",
      "| G     | 1      | Hourly interval between each row of the path; before 2017, it was 6 hours, starting from 2017, individual cases with a 3-hour encryption record are marked as 3, and others remain 6. |\n",
      "| H...H | 20     | English name of the tropical cyclone; \"(-1)n\" is added after the name to indicate the secondary center and its serial number. |\n",
      "| I...I | 8      | Date on which the dataset is formed. |\n",
      "   \n",
      "## 2. Data Lines   \n",
      "![Data Lines](https://tcdata.typhoon.org.cn/images/best-track-data-format-pic_2.png)   \n",
      "| Field          | Description |\n",
      "|----------------|-------------|\n",
      "| YYYYMMDDHH     | Date and time in UTC: YYYY year, MM month, DD day, HH hour. |\n",
      "| I              | Intensity marker based on the average wind speed within 2 minutes around the exact time point. Refer to the National Standard \"Tropical Cyclone Grades\" (GB/T 19201-2006): <br> 0 - Weaker than Tropical Depression (TD), or intensity unknown. <br> 1 - Tropical Depression (TD, 10.8-17.1 m/s). <br> 2 - Tropical Storm (TS, 17.2-24.4 m/s). <br> 3 - Severe Tropical Storm (STS, 24.5-32.6 m/s). <br> 4 - Typhoon (TY, 32.7-41.4 m/s). <br> 5 - Severe Typhoon (STY, 41.5-50.9 m/s). <br> 6 - Super Typhoon (SuperTY, ≥51.0 m/s). <br> 9 - Extratropical transition, the first digit indicates the completion of the transition. |\n",
      "| LAT            | Latitude (0.1°N). |\n",
      "| LONG           | Longitude (0.1°E). |\n",
      "| PRES           | Central minimum pressure (hPa). |\n",
      "| WND            | 2-minute average maximum sustained wind speed near the center (MSW, m/s). WND=9 indicates MSW &lt; 10 m/s, WND=0 indicates missing data. |\n",
      "| OWD            | 2-minute average wind speed (m/s) with two cases: <br> (a) For tropical cyclones making landfall in China, it represents the wind speed of coastal strong winds. <br> (b) When a tropical cyclone is in the South China Sea, it represents the maximum wind speed within a range of 300-500 km from the center. |   \n",
      "\n",
      "# Citation   \n",
      "Ying, M., W. Zhang, H. Yu, X. Lu, J. Feng, Y. Fan, Y. Zhu, and D. Chen, 2014: An overview of the China Meteorological Administration tropical cyclone database. J. Atmos. Oceanic Technol., 31, 287-301. doi: 10.1175/JTECH-D-12-00119.1\n",
      "Lu, X. Q., H. Yu, M. Ying, B. K. Zhao, S. Zhang, L. M. Lin, L. N. Bai, and R. J. Wan, 2021: Western North Pacific tropical cyclone database created by the China Meteorological Administration. Adv. Atmos. Sci., 38(4), 690−699. doi: 10.1007/s00376-020-0211-7   \n",
      "name 'detect' is not defined Language is not detected: nikhil1e9/goodreads-books\n",
      "name 'detect' is not defined Language is not detected: goodreads-books\n",
      "name 'detect' is not defined Language is not detected: nikhil1e9\n",
      "name 'detect' is not defined Language is not detected: Goodreads Popular and Trending Books\n",
      "name 'detect' is not defined Language is not detected: Explore the collection of Trending and Popular books from Goodreads\n",
      "name 'detect' is not defined Language is not detected: ## Motivation\n",
      "&gt; Being a bibliophile and a book lover, I decided to create this dataset for all the book enthusiasts out there who are looking to unravel patterns and connections in their favorite books and explore some of the most interesting and finest books ever written.\n",
      "\n",
      "## Description\n",
      "&gt; This dataset has been scraped from [**Goodreads**](https://www.goodreads.com/) and contains information on **popular** and **trending** books listed on Goodreads spanning 125 years. The features of the dataset are described below:\n",
      "\n",
      "## Features\n",
      "This file contains some general information about a book such as its *title, author, score* etc.\n",
      "&gt; - **Title**: Title of the book\n",
      "- **Author**: Author of the book\n",
      "- **Score**: Average user rating/score of the book\n",
      "- **Ratings**: Total number of ratings given to the book by users\n",
      "- **Shelvings**: Total number of users having the book saved on their Goodreads account\n",
      "- **Published**: The year in which the book was published\n",
      "- **Description**: Description of the book \n",
      "- **Image**: Image URL of the book\n",
      "\n",
      "## Use cases\n",
      "This dataset presents numerous opportunities for exploration and analysis, offering valuable insights into the world of literature, reader preferences, and the art of storytelling. Some of the potential use cases include:\n",
      "&gt; - **Recommendation Systems**: The dataset can be employed to develop recommendation algorithms that match readers with books tailored to their preferences based on user ratings.\n",
      "- **Context Analysis**: Perform text analysis on book descriptions to understand the emerging patterns in trending books and how it contributes to their success.\n",
      "- **Author Insights**: Study author patterns and trends to understand how certain authors consistently produce beloved and influential works.\n",
      "\n",
      "Please, provide an upvote👍if the dataset was useful for your task. It would be much appreciated😄\n",
      "name 'detect' is not defined Language is not detected: goodreads-books\n",
      "name 'detect' is not defined Language is not detected: nikhil1e9\n",
      "name 'detect' is not defined Language is not detected: Goodreads Popular and Trending Books\n",
      "name 'detect' is not defined Language is not detected: Explore the collection of Trending and Popular books from Goodreads\n",
      "name 'detect' is not defined Language is not detected: ## Motivation\n",
      "&gt; Being a bibliophile and a book lover, I decided to create this dataset for all the book enthusiasts out there who are looking to unravel patterns and connections in their favorite books and explore some of the most interesting and finest books ever written.\n",
      "\n",
      "## Description\n",
      "&gt; This dataset has been scraped from [**Goodreads**](https://www.goodreads.com/) and contains information on **popular** and **trending** books listed on Goodreads spanning 125 years. The features of the dataset are described below:\n",
      "\n",
      "## Features\n",
      "This file contains some general information about a book such as its *title, author, score* etc.\n",
      "&gt; - **Title**: Title of the book\n",
      "- **Author**: Author of the book\n",
      "- **Score**: Average user rating/score of the book\n",
      "- **Ratings**: Total number of ratings given to the book by users\n",
      "- **Shelvings**: Total number of users having the book saved on their Goodreads account\n",
      "- **Published**: The year in which the book was published\n",
      "- **Description**: Description of the book \n",
      "- **Image**: Image URL of the book\n",
      "\n",
      "## Use cases\n",
      "This dataset presents numerous opportunities for exploration and analysis, offering valuable insights into the world of literature, reader preferences, and the art of storytelling. Some of the potential use cases include:\n",
      "&gt; - **Recommendation Systems**: The dataset can be employed to develop recommendation algorithms that match readers with books tailored to their preferences based on user ratings.\n",
      "- **Context Analysis**: Perform text analysis on book descriptions to understand the emerging patterns in trending books and how it contributes to their success.\n",
      "- **Author Insights**: Study author patterns and trends to understand how certain authors consistently produce beloved and influential works.\n",
      "\n",
      "Please, provide an upvote👍if the dataset was useful for your task. It would be much appreciated😄\n",
      "name 'detect' is not defined Language is not detected: deovcs/amazon-dataset\n",
      "name 'detect' is not defined Language is not detected: amazon-dataset\n",
      "name 'detect' is not defined Language is not detected: deovcs\n",
      "name 'detect' is not defined Language is not detected: Amazon dataset\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics, CDs, Clothing, Music, Products, Movies, Beauty, Home, Games\n",
      "name 'detect' is not defined Language is not detected: ## Datasets\n",
      "The related codes are available on my Github [TsingZ0](https://github.com/TsingZ0/TLSAN) and the related paper is [TLSAN: Time-aware Long- and Short-term Attention Network for Next-item Recommendation](https://www.sciencedirect.com/science/article/abs/pii/S0925231221002605). The full-text is also available here: https://www.researchgate.net/publication/349912702_TLSAN_Time-aware_Long-_and_Short-term_Attention_Network_for_Next-item_Recommendation.\n",
      "\n",
      "Amazon exposes the official datasets (http://jmcauley.ucsd.edu/data/amazon/) which have filtered out users and items with less than 5 reviews and removed a large amount of invalid data. Because of above advantages, these datasets are widely utilized by researchers. We also chose Amazon's dataset for experiments. In our experiments, only users, items, interactions, and category information are utilized. We do the preprocessing in the following two steps:\n",
      "1. Remove the users whose interactions less than 10 and the items which interactions less than 8 to ensure the effectiveness of each user and item.\n",
      "2. Select the users with more than 4 sessions, and select up to 90 behavior records for the remaining users. This step guarantees the existence of long- and short-term behavior records and all behavior records occurred within recent three months.\n",
      "\n",
      "### Statistics (after preprocessing)\n",
      "| Datasets | users | items | categories | samples | avg.<br>items/cate | avg.<br>behaviors/item | avg.<br>behaviors/user |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Electronics | 39991 | 22048 | 673 | 561100 | 32.8 | 25.4 | 14.0|\n",
      "| CDs-Vinyl | 24179 | 27602 | 310 | 470087 | 89.0 | 17.0 | 19.4|\n",
      "| Clothing-Shoes | 2010 | 1723 | 226 | 13157 | 7.6 | 7.6 | 6.5|\n",
      "| Digital-Music | 1659 | 1583 | 53 | 28852 | 29.9 | 18.2 | 17.4|\n",
      "| Office-Products | 1720 | 901 | 170 | 29387 | 5.3 | 32.6 | 17.0|\n",
      "| Movies-TV | 35896 | 28589 | 15 | 752676 | 1905.9 | 20.9 | 26.3|\n",
      "| Beauty | 3783 | 2658 | 179 | 54225 | 14.8 | 20.4 | 14.3|\n",
      "| Home-Kitchen | 11567 | 7722 | 683 | 143088 | 11.3 | 12.3 | 18.5|\n",
      "| Video-Games | 5436 | 4295 | 58 | 83748 | 74.1 | 19.5 | 15.4|\n",
      "| Toys-and-Games | 2677 | 2474 | 221 | 37515 | 11.2 | 15.2 | 14.0|\n",
      "name 'detect' is not defined Language is not detected: amazon-dataset\n",
      "name 'detect' is not defined Language is not detected: deovcs\n",
      "name 'detect' is not defined Language is not detected: Amazon dataset\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics, CDs, Clothing, Music, Products, Movies, Beauty, Home, Games\n",
      "name 'detect' is not defined Language is not detected: ## Datasets\n",
      "The related codes are available on my Github [TsingZ0](https://github.com/TsingZ0/TLSAN) and the related paper is [TLSAN: Time-aware Long- and Short-term Attention Network for Next-item Recommendation](https://www.sciencedirect.com/science/article/abs/pii/S0925231221002605). The full-text is also available here: https://www.researchgate.net/publication/349912702_TLSAN_Time-aware_Long-_and_Short-term_Attention_Network_for_Next-item_Recommendation.\n",
      "\n",
      "Amazon exposes the official datasets (http://jmcauley.ucsd.edu/data/amazon/) which have filtered out users and items with less than 5 reviews and removed a large amount of invalid data. Because of above advantages, these datasets are widely utilized by researchers. We also chose Amazon's dataset for experiments. In our experiments, only users, items, interactions, and category information are utilized. We do the preprocessing in the following two steps:\n",
      "1. Remove the users whose interactions less than 10 and the items which interactions less than 8 to ensure the effectiveness of each user and item.\n",
      "2. Select the users with more than 4 sessions, and select up to 90 behavior records for the remaining users. This step guarantees the existence of long- and short-term behavior records and all behavior records occurred within recent three months.\n",
      "\n",
      "### Statistics (after preprocessing)\n",
      "| Datasets | users | items | categories | samples | avg.<br>items/cate | avg.<br>behaviors/item | avg.<br>behaviors/user |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Electronics | 39991 | 22048 | 673 | 561100 | 32.8 | 25.4 | 14.0|\n",
      "| CDs-Vinyl | 24179 | 27602 | 310 | 470087 | 89.0 | 17.0 | 19.4|\n",
      "| Clothing-Shoes | 2010 | 1723 | 226 | 13157 | 7.6 | 7.6 | 6.5|\n",
      "| Digital-Music | 1659 | 1583 | 53 | 28852 | 29.9 | 18.2 | 17.4|\n",
      "| Office-Products | 1720 | 901 | 170 | 29387 | 5.3 | 32.6 | 17.0|\n",
      "| Movies-TV | 35896 | 28589 | 15 | 752676 | 1905.9 | 20.9 | 26.3|\n",
      "| Beauty | 3783 | 2658 | 179 | 54225 | 14.8 | 20.4 | 14.3|\n",
      "| Home-Kitchen | 11567 | 7722 | 683 | 143088 | 11.3 | 12.3 | 18.5|\n",
      "| Video-Games | 5436 | 4295 | 58 | 83748 | 74.1 | 19.5 | 15.4|\n",
      "| Toys-and-Games | 2677 | 2474 | 221 | 37515 | 11.2 | 15.2 | 14.0|\n",
      "name 'detect' is not defined Language is not detected: nitishabharathi/stack-overflow-developer-survery-20112015\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survery-20112015\n",
      "name 'detect' is not defined Language is not detected: nitishabharathi\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survery 2011-2015\n",
      "name 'detect' is not defined Language is not detected: Individual responses on the Developer Survey 2011 - 2015\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Stack Overflow’s annual Developer Survey is the largest and most comprehensive survey of people who code around the world. Each year, Stack Overflow field a survey covering everything from developers’ favourite technologies to their job preferences.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Individual responses on the Developer Survey fielded by Stack Overflow 2011 - 2015\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Massive thanks to [Stack Overflow ](https://insights.stackoverflow.com/survey)\n",
      "\n",
      "### 2016-2019 Data\n",
      "\n",
      "2016 - https://www.kaggle.com/jonmhong/stackoverflow2016\n",
      "2017 - https://www.kaggle.com/stackoverflow/so-survey-2017\n",
      "2018 - https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey\n",
      "2019 - https://www.kaggle.com/manishachakraborty/stack-overflow-annual-developer-survey-2019\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survery-20112015\n",
      "name 'detect' is not defined Language is not detected: nitishabharathi\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survery 2011-2015\n",
      "name 'detect' is not defined Language is not detected: Individual responses on the Developer Survey 2011 - 2015\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Stack Overflow’s annual Developer Survey is the largest and most comprehensive survey of people who code around the world. Each year, Stack Overflow field a survey covering everything from developers’ favourite technologies to their job preferences.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Individual responses on the Developer Survey fielded by Stack Overflow 2011 - 2015\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Massive thanks to [Stack Overflow ](https://insights.stackoverflow.com/survey)\n",
      "\n",
      "### 2016-2019 Data\n",
      "\n",
      "2016 - https://www.kaggle.com/jonmhong/stackoverflow2016\n",
      "2017 - https://www.kaggle.com/stackoverflow/so-survey-2017\n",
      "2018 - https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey\n",
      "2019 - https://www.kaggle.com/manishachakraborty/stack-overflow-annual-developer-survey-2019\n",
      "name 'detect' is not defined Language is not detected: jirakst/bookcrossing\n",
      "name 'detect' is not defined Language is not detected: bookcrossing\n",
      "name 'detect' is not defined Language is not detected: jirakst\n",
      "name 'detect' is not defined Language is not detected: BookCrossing\n",
      "name 'detect' is not defined Language is not detected: Book rating dataset to build a recommendation engine\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "BookCrossing is the act of releasing your books \"into the wild\" for a stranger to find, or via \"controlled release\" to another BookCrossing member, and tracking where they go via journal entries from around the world. Our community of 1,951,800 passionate, generous book-lovers is changing the world and touching lives, one traveling book at a time.\n",
      "name 'detect' is not defined Language is not detected: bookcrossing\n",
      "name 'detect' is not defined Language is not detected: jirakst\n",
      "name 'detect' is not defined Language is not detected: BookCrossing\n",
      "name 'detect' is not defined Language is not detected: Book rating dataset to build a recommendation engine\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "BookCrossing is the act of releasing your books \"into the wild\" for a stranger to find, or via \"controlled release\" to another BookCrossing member, and tracking where they go via journal entries from around the world. Our community of 1,951,800 passionate, generous book-lovers is changing the world and touching lives, one traveling book at a time.\n",
      "name 'detect' is not defined Language is not detected: toubali/nonlinear-regression\n",
      "name 'detect' is not defined Language is not detected: nonlinear-regression\n",
      "name 'detect' is not defined Language is not detected: toubali\n",
      "name 'detect' is not defined Language is not detected: non-linear regression\n",
      "name 'detect' is not defined Language is not detected: nonlinear-regression\n",
      "name 'detect' is not defined Language is not detected: toubali\n",
      "name 'detect' is not defined Language is not detected: non-linear regression\n",
      "name 'detect' is not defined Language is not detected: kalcal/simple-linear-regression\n",
      "name 'detect' is not defined Language is not detected: simple-linear-regression\n",
      "name 'detect' is not defined Language is not detected: kalcal\n",
      "name 'detect' is not defined Language is not detected: simple linear regression\n",
      "name 'detect' is not defined Language is not detected: simple-linear-regression\n",
      "name 'detect' is not defined Language is not detected: kalcal\n",
      "name 'detect' is not defined Language is not detected: simple linear regression\n",
      "name 'detect' is not defined Language is not detected: safrin03/predictive-analytics-for-customer-churn-dataset\n",
      "name 'detect' is not defined Language is not detected: predictive-analytics-for-customer-churn-dataset\n",
      "name 'detect' is not defined Language is not detected: safrin03\n",
      "name 'detect' is not defined Language is not detected: Predictive Analytics for Customer Churn: Dataset\n",
      "name 'detect' is not defined Language is not detected: Analyzing Customer Behavior to Predict Churn: A Subscription Service Case Study\n",
      "name 'detect' is not defined Language is not detected: **Context :**\n",
      "This dataset is part of a data science project focused on customer churn prediction for a subscription-based service. Customer churn, the rate at which customers cancel their subscriptions, is a vital metric for businesses offering subscription services. Predictive analytics techniques are employed to anticipate which customers are likely to churn, enabling companies to take proactive measures for customer retention.\n",
      "\n",
      "**Content :**\n",
      "This dataset contains anonymized information about customer subscriptions and their interaction with the service. The data includes various features such as subscription type, payment method, viewing preferences, customer support interactions, and other relevant attributes. It consists of three files such as \"test.csv\", \"train.csv\", \"data_descriptions.csv\". \n",
      "\n",
      "**Columns :**\n",
      "\n",
      "**CustomerID:** Unique identifier for each customer\n",
      "\n",
      "**SubscriptionType:** Type of subscription plan chosen by the customer (e.g., Basic, Premium, Deluxe)\n",
      "\n",
      "**PaymentMethod:** Method used for payment (e.g., Credit Card, Electronic Check, PayPal)\n",
      "\n",
      "**PaperlessBilling:** Whether the customer uses paperless billing (Yes/No)\n",
      "\n",
      "**ContentType:** Type of content accessed by the customer (e.g., Movies, TV Shows, Documentaries)\n",
      "\n",
      "**MultiDeviceAccess:** Whether the customer has access on multiple devices (Yes/No)\n",
      "\n",
      "**DeviceRegistered:** Device registered by the customer (e.g., Smartphone, Smart TV, Laptop)\n",
      "\n",
      "**GenrePreference:** Genre preference of the customer (e.g., Action, Drama, Comedy)\n",
      "\n",
      "**Gender:** Gender of the customer (Male/Female)\n",
      "\n",
      "**ParentalControl:** Whether parental control is enabled (Yes/No)\n",
      "\n",
      "**SubtitlesEnabled:** Whether subtitles are enabled (Yes/No)\n",
      "\n",
      "**AccountAge:** Age of the customer's subscription account (in months)\n",
      "\n",
      "**MonthlyCharges:** Monthly subscription charges\n",
      "\n",
      "**TotalCharges:** Total charges incurred by the customer\n",
      "\n",
      "**ViewingHoursPerWeek:** Average number of viewing hours per week\n",
      "\n",
      "**SupportTicketsPerMonth:** Number of customer support tickets raised per month\n",
      "\n",
      "**AverageViewingDuration:** Average duration of each viewing session\n",
      "\n",
      "**ContentDownloadsPerMonth:** Number of content downloads per month\n",
      "\n",
      "**UserRating:** Customer satisfaction rating (1 to 5)\n",
      "\n",
      "**WatchlistSize:** Size of the customer's content watchlist\n",
      "\n",
      "**Acknowledgments :**\n",
      "The dataset used in this project is obtained from Data Science Challenge on Coursera and is used for educational and research purposes. Any resemblance to real persons or entities is purely coincidental.\n",
      "name 'detect' is not defined Language is not detected: predictive-analytics-for-customer-churn-dataset\n",
      "name 'detect' is not defined Language is not detected: safrin03\n",
      "name 'detect' is not defined Language is not detected: Predictive Analytics for Customer Churn: Dataset\n",
      "name 'detect' is not defined Language is not detected: Analyzing Customer Behavior to Predict Churn: A Subscription Service Case Study\n",
      "name 'detect' is not defined Language is not detected: **Context :**\n",
      "This dataset is part of a data science project focused on customer churn prediction for a subscription-based service. Customer churn, the rate at which customers cancel their subscriptions, is a vital metric for businesses offering subscription services. Predictive analytics techniques are employed to anticipate which customers are likely to churn, enabling companies to take proactive measures for customer retention.\n",
      "\n",
      "**Content :**\n",
      "This dataset contains anonymized information about customer subscriptions and their interaction with the service. The data includes various features such as subscription type, payment method, viewing preferences, customer support interactions, and other relevant attributes. It consists of three files such as \"test.csv\", \"train.csv\", \"data_descriptions.csv\". \n",
      "\n",
      "**Columns :**\n",
      "\n",
      "**CustomerID:** Unique identifier for each customer\n",
      "\n",
      "**SubscriptionType:** Type of subscription plan chosen by the customer (e.g., Basic, Premium, Deluxe)\n",
      "\n",
      "**PaymentMethod:** Method used for payment (e.g., Credit Card, Electronic Check, PayPal)\n",
      "\n",
      "**PaperlessBilling:** Whether the customer uses paperless billing (Yes/No)\n",
      "\n",
      "**ContentType:** Type of content accessed by the customer (e.g., Movies, TV Shows, Documentaries)\n",
      "\n",
      "**MultiDeviceAccess:** Whether the customer has access on multiple devices (Yes/No)\n",
      "\n",
      "**DeviceRegistered:** Device registered by the customer (e.g., Smartphone, Smart TV, Laptop)\n",
      "\n",
      "**GenrePreference:** Genre preference of the customer (e.g., Action, Drama, Comedy)\n",
      "\n",
      "**Gender:** Gender of the customer (Male/Female)\n",
      "\n",
      "**ParentalControl:** Whether parental control is enabled (Yes/No)\n",
      "\n",
      "**SubtitlesEnabled:** Whether subtitles are enabled (Yes/No)\n",
      "\n",
      "**AccountAge:** Age of the customer's subscription account (in months)\n",
      "\n",
      "**MonthlyCharges:** Monthly subscription charges\n",
      "\n",
      "**TotalCharges:** Total charges incurred by the customer\n",
      "\n",
      "**ViewingHoursPerWeek:** Average number of viewing hours per week\n",
      "\n",
      "**SupportTicketsPerMonth:** Number of customer support tickets raised per month\n",
      "\n",
      "**AverageViewingDuration:** Average duration of each viewing session\n",
      "\n",
      "**ContentDownloadsPerMonth:** Number of content downloads per month\n",
      "\n",
      "**UserRating:** Customer satisfaction rating (1 to 5)\n",
      "\n",
      "**WatchlistSize:** Size of the customer's content watchlist\n",
      "\n",
      "**Acknowledgments :**\n",
      "The dataset used in this project is obtained from Data Science Challenge on Coursera and is used for educational and research purposes. Any resemblance to real persons or entities is purely coincidental.\n",
      "name 'detect' is not defined Language is not detected: arpikr/uci-drug\n",
      "name 'detect' is not defined Language is not detected: uci-drug\n",
      "name 'detect' is not defined Language is not detected: arpikr\n",
      "name 'detect' is not defined Language is not detected: UCI_Drug\n",
      "name 'detect' is not defined Language is not detected: Multiple Research Problem\n",
      "name 'detect' is not defined Language is not detected: **Data Set Information:**\n",
      "The dataset provides patient reviews on specific drugs along with related conditions and a 10-star patient rating reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. The intention was to study\n",
      "\n",
      "(1) sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,\n",
      "(2) the transferability of models among domains, i.e. conditions, and\n",
      "(3) the transferability of models among different data sources (see 'Drug Review Dataset (Druglib.com)').\n",
      "\n",
      "The data is split into a train (75%) a test (25%) partition (see publication) and stored in two .tsv (tab-separated-values) files, respectively.\n",
      "\n",
      "\n",
      "Machine learning has permeated nearly all fields and disciplines of study. One hot topic is using natural language processing and sentiment analysis to identify, extract, and make use of subjective information. The UCI ML Drug Review dataset provides patient reviews on specific drugs along with related conditions and a 10-star patient rating system reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. This data was published in a study on sentiment analysis of drug experience over multiple facets, ex. sentiments learned on specific aspects such as effectiveness and side effects (see the acknowledgments section to learn more).\n",
      "\n",
      "The sky's the limit here in terms of what your team can do! Teams are free to add supplementary datasets in conjunction with the drug review dataset in their Kernel. Discussion is highly encouraged within the forum and Slack so everyone can learn from their peers.\n",
      "\n",
      "Important notes:\n",
      "\n",
      "When using this dataset, you agree that you\n",
      "1) only use the data for research purposes\n",
      "2) don't use the data for any commercial purposes\n",
      "3) don't distribute the data to anyone else\n",
      "4) cite us: https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: uci-drug\n",
      "name 'detect' is not defined Language is not detected: arpikr\n",
      "name 'detect' is not defined Language is not detected: UCI_Drug\n",
      "name 'detect' is not defined Language is not detected: Multiple Research Problem\n",
      "name 'detect' is not defined Language is not detected: **Data Set Information:**\n",
      "The dataset provides patient reviews on specific drugs along with related conditions and a 10-star patient rating reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. The intention was to study\n",
      "\n",
      "(1) sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,\n",
      "(2) the transferability of models among domains, i.e. conditions, and\n",
      "(3) the transferability of models among different data sources (see 'Drug Review Dataset (Druglib.com)').\n",
      "\n",
      "The data is split into a train (75%) a test (25%) partition (see publication) and stored in two .tsv (tab-separated-values) files, respectively.\n",
      "\n",
      "\n",
      "Machine learning has permeated nearly all fields and disciplines of study. One hot topic is using natural language processing and sentiment analysis to identify, extract, and make use of subjective information. The UCI ML Drug Review dataset provides patient reviews on specific drugs along with related conditions and a 10-star patient rating system reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. This data was published in a study on sentiment analysis of drug experience over multiple facets, ex. sentiments learned on specific aspects such as effectiveness and side effects (see the acknowledgments section to learn more).\n",
      "\n",
      "The sky's the limit here in terms of what your team can do! Teams are free to add supplementary datasets in conjunction with the drug review dataset in their Kernel. Discussion is highly encouraged within the forum and Slack so everyone can learn from their peers.\n",
      "\n",
      "Important notes:\n",
      "\n",
      "When using this dataset, you agree that you\n",
      "1) only use the data for research purposes\n",
      "2) don't use the data for any commercial purposes\n",
      "3) don't distribute the data to anyone else\n",
      "4) cite us: https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sjleshrac/airlines-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: airlines-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: sjleshrac\n",
      "name 'detect' is not defined Language is not detected: Airlines Customer satisfaction\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction with various other factors\n",
      "name 'detect' is not defined Language is not detected: This data given by an airline organization. The actual name of the company is not given due to various purposes that's why the name Invistico airlines.\n",
      "\n",
      "The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. \n",
      "\n",
      "The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values. \n",
      "\n",
      "Also the airlines need to know on which aspect of the services offered by them have to be emphasized more to generate more satisfied customers.\n",
      "name 'detect' is not defined Language is not detected: airlines-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: sjleshrac\n",
      "name 'detect' is not defined Language is not detected: Airlines Customer satisfaction\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction with various other factors\n",
      "name 'detect' is not defined Language is not detected: This data given by an airline organization. The actual name of the company is not given due to various purposes that's why the name Invistico airlines.\n",
      "\n",
      "The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. \n",
      "\n",
      "The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values. \n",
      "\n",
      "Also the airlines need to know on which aspect of the services offered by them have to be emphasized more to generate more satisfied customers.\n",
      "name 'detect' is not defined Language is not detected: frobert/handdrawn-shapes-hds-dataset\n",
      "name 'detect' is not defined Language is not detected: handdrawn-shapes-hds-dataset\n",
      "name 'detect' is not defined Language is not detected: frobert\n",
      "name 'detect' is not defined Language is not detected: Hand-drawn Shapes (HDS) Dataset\n",
      "name 'detect' is not defined Language is not detected: Images like Rectangles, Ellipses and Triangles with vertices.\n",
      "name 'detect' is not defined Language is not detected: I have created this Dataset for my app **[Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586)**.\n",
      "\n",
      "On GitHub: https://github.com/frobertpixto/hand-drawn-shapes-dataset\n",
      "\n",
      "See the complete DataSheet (as described in https://arxiv.org/pdf/1803.09010.pdf) for the HDS Dataset [here](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/Datasheet_for_Datasets.HDS.pdf).\n",
      "\n",
      "## The Images \n",
      "One shape per image. Drawings exist for 4 shapes:\n",
      "- Rectangle\n",
      "- Ellipse\n",
      "- Triangle\n",
      "- Other\n",
      "\n",
      "![Image examples](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/readme_images/train_images.png?raw=true)\n",
      "\n",
      "The Dataset contains images (70px x 70px x 1 gray channel) distributed as:\n",
      "\n",
      "| Total | Other |Rectangle | Ellipse | Triangle |\n",
      "| :---------------:|---------------:|---------------:|---------------:|---------------: |\n",
      "| **27292** images  |  7287  |  6956  | 6454 | 6595 |\n",
      "\n",
      "The shapes have been size-normalized and centered in a fixed-size image.\n",
      "\n",
      "\n",
      "## Vertices\n",
      "\n",
      "![Vertices for ellipses](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/processing/find_vertices/readme_images/vertices_ell.png?raw=true)\n",
      "\n",
      "\n",
      "Quick Geometry refresher:\n",
      "- Vertices in shapes are the points where two or more line segments or edges meet (like a corner for a rectangle). \n",
      "- Vertices of an ellipse are the 4 corner points at which the ellipse takes the maximum turn. Technically, an ellipse will have 2 vertices and 2 covertices. We will call them all vertices here.\n",
      "- The singular of vertices is vertex.\n",
      "\n",
      "Coordinates of vertices are interesting as they are much **more precise** than just a surrounding box used in [Object detection](https://en.wikipedia.org/wiki/Object_detection).  \n",
      "Vertices allow us to determine the **angle** of the shape and it **exact size**.   \n",
      "\n",
      "### Labelling of vertices\n",
      "Labelling was done by me using a tool I created in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586).\n",
      "For each image, the tool also generated a csv file with 1 line per vertex.\n",
      "Each Vertex has:\n",
      "- a x coordinate between 0 and 1\n",
      "- a y coordinate between 0 and 1\n",
      "\n",
      "Where:\n",
      "- (0,0) is the top left corner of the image\n",
      "- (1,1) is the bottom right corner of the image\n",
      "\n",
      "Note that the vertices are in no particular order. I sort them clockwise in the Extract-Transform-Load (ETL) processing.\n",
      "\n",
      "### Example of a .csv file content for vertices of a rectangle\n",
      "```\n",
      "0.14,0.28\n",
      "0.87,0.29\n",
      "0.86,0.67\n",
      "0.14,0.67\n",
      "```\n",
      "\n",
      "### Usefulness of vertices\n",
      "Aside from drawing shapes on images like in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586), another real-life example could be to determine the direction of a car (rectangle) or a ship (ellipse) in a direct overhead view. \n",
      "\n",
      "\n",
      "## Visualization and processing\n",
      "I have a few kernels that will allow you to see:\n",
      "- the samples in the Extract-Transform-Load (ETL) phase.\n",
      "- a complete example of processing (after the ETL).\n",
      "\n",
      "### Notebooks - Classification - Shape\n",
      "- ETL and Classification: [hds-shapes-etl-and-classify](https://www.kaggle.com/frobert/hds-shapes-etl-and-classify) \n",
      "### Notebooks - Regression - Determine position of vertices\n",
      "| Step | Rectangle |  Ellipse | Triangle |\n",
      "| :---------------:|---------------:|---------------:|---------------: |\n",
      "| ETL  | [hds-rectangle-1-etl](https://www.kaggle.com/code/frobert/hds-rectangle-1-etl)  | [hds-ellipse-1-etl](https://www.kaggle.com/code/frobert/hds-ellipse-1-etl) | [hds-triangle-1-etl](https://www.kaggle.com/code/frobert/hds-triangle-1-etl) |\n",
      "| Regression  | [hds-rectangle-2-regression](https://www.kaggle.com/code/frobert/hds-rectangle-2-regression) | [hds-ellipse-2-regression](https://www.kaggle.com/code/frobert/hds-ellipse-2-regression) | [hds-triangle-2-regression](https://www.kaggle.com/code/frobert/hds-triangle-2-regression) |\n",
      "\n",
      "## Direct augmentation of the data. \n",
      "  - 3 variations were generated per image\n",
      "    1. Normal\n",
      "    2. 1.5 to 3.0 wider\n",
      "    3. 1.5 to 3.0 narrower\n",
      "  - One advantage is that I realized that:\n",
      "    - People tend to make equilibrated shapes (Circle, Square, Equilateral triangle). \n",
      "    - Most elongated images were interesting and sometime presented a different challenge than the original. \n",
      "  - This processing was not done for type Other.\n",
      "  - I validated them all manually (or we could say visually) and removed the generated images that were not interesting.\n",
      "  - This is different than the Augmentation done during Training (like horizontal and vertical flips, rotations) because:\n",
      "    - It applies to all images including Validation set and Test set.\n",
      "    - Being generated before being drawn provided images of a better quality.\n",
      "\n",
      "I then used these images to train models that are used in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586) Auto-Shapes feature.\n",
      "\n",
      "## People who drew the images\n",
      "Images were mostly generated by asking people I knew to draw Ellipses, Rectangles and Triangles in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586).  \n",
      "People were aged from 7 to 87. I asked them to draw the way they wanted, slow or fast. In particular, I did not ask to draw nice shape. It was the **intent** while drawing that interested me.\n",
      "\n",
      "## Shape of type Other\n",
      "The first ML models had no **Other** shape. I was initially happy with the excellent accuracy when the drawing was actually a rectangle, triangle or ellipse. But when testing internally with [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586), drawings of shapes like stars, hearts or crosses would always be recognized as one of Ellipse, Rectangle or Triangle. It is not a great user experience.\n",
      "So the shape of type Other solves that.\n",
      "\n",
      "---\n",
      "Francois Robert\n",
      "name 'detect' is not defined Language is not detected: handdrawn-shapes-hds-dataset\n",
      "name 'detect' is not defined Language is not detected: frobert\n",
      "name 'detect' is not defined Language is not detected: Hand-drawn Shapes (HDS) Dataset\n",
      "name 'detect' is not defined Language is not detected: Images like Rectangles, Ellipses and Triangles with vertices.\n",
      "name 'detect' is not defined Language is not detected: I have created this Dataset for my app **[Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586)**.\n",
      "\n",
      "On GitHub: https://github.com/frobertpixto/hand-drawn-shapes-dataset\n",
      "\n",
      "See the complete DataSheet (as described in https://arxiv.org/pdf/1803.09010.pdf) for the HDS Dataset [here](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/Datasheet_for_Datasets.HDS.pdf).\n",
      "\n",
      "## The Images \n",
      "One shape per image. Drawings exist for 4 shapes:\n",
      "- Rectangle\n",
      "- Ellipse\n",
      "- Triangle\n",
      "- Other\n",
      "\n",
      "![Image examples](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/readme_images/train_images.png?raw=true)\n",
      "\n",
      "The Dataset contains images (70px x 70px x 1 gray channel) distributed as:\n",
      "\n",
      "| Total | Other |Rectangle | Ellipse | Triangle |\n",
      "| :---------------:|---------------:|---------------:|---------------:|---------------: |\n",
      "| **27292** images  |  7287  |  6956  | 6454 | 6595 |\n",
      "\n",
      "The shapes have been size-normalized and centered in a fixed-size image.\n",
      "\n",
      "\n",
      "## Vertices\n",
      "\n",
      "![Vertices for ellipses](https://github.com/frobertpixto/hand-drawn-shapes-dataset/blob/main/processing/find_vertices/readme_images/vertices_ell.png?raw=true)\n",
      "\n",
      "\n",
      "Quick Geometry refresher:\n",
      "- Vertices in shapes are the points where two or more line segments or edges meet (like a corner for a rectangle). \n",
      "- Vertices of an ellipse are the 4 corner points at which the ellipse takes the maximum turn. Technically, an ellipse will have 2 vertices and 2 covertices. We will call them all vertices here.\n",
      "- The singular of vertices is vertex.\n",
      "\n",
      "Coordinates of vertices are interesting as they are much **more precise** than just a surrounding box used in [Object detection](https://en.wikipedia.org/wiki/Object_detection).  \n",
      "Vertices allow us to determine the **angle** of the shape and it **exact size**.   \n",
      "\n",
      "### Labelling of vertices\n",
      "Labelling was done by me using a tool I created in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586).\n",
      "For each image, the tool also generated a csv file with 1 line per vertex.\n",
      "Each Vertex has:\n",
      "- a x coordinate between 0 and 1\n",
      "- a y coordinate between 0 and 1\n",
      "\n",
      "Where:\n",
      "- (0,0) is the top left corner of the image\n",
      "- (1,1) is the bottom right corner of the image\n",
      "\n",
      "Note that the vertices are in no particular order. I sort them clockwise in the Extract-Transform-Load (ETL) processing.\n",
      "\n",
      "### Example of a .csv file content for vertices of a rectangle\n",
      "```\n",
      "0.14,0.28\n",
      "0.87,0.29\n",
      "0.86,0.67\n",
      "0.14,0.67\n",
      "```\n",
      "\n",
      "### Usefulness of vertices\n",
      "Aside from drawing shapes on images like in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586), another real-life example could be to determine the direction of a car (rectangle) or a ship (ellipse) in a direct overhead view. \n",
      "\n",
      "\n",
      "## Visualization and processing\n",
      "I have a few kernels that will allow you to see:\n",
      "- the samples in the Extract-Transform-Load (ETL) phase.\n",
      "- a complete example of processing (after the ETL).\n",
      "\n",
      "### Notebooks - Classification - Shape\n",
      "- ETL and Classification: [hds-shapes-etl-and-classify](https://www.kaggle.com/frobert/hds-shapes-etl-and-classify) \n",
      "### Notebooks - Regression - Determine position of vertices\n",
      "| Step | Rectangle |  Ellipse | Triangle |\n",
      "| :---------------:|---------------:|---------------:|---------------: |\n",
      "| ETL  | [hds-rectangle-1-etl](https://www.kaggle.com/code/frobert/hds-rectangle-1-etl)  | [hds-ellipse-1-etl](https://www.kaggle.com/code/frobert/hds-ellipse-1-etl) | [hds-triangle-1-etl](https://www.kaggle.com/code/frobert/hds-triangle-1-etl) |\n",
      "| Regression  | [hds-rectangle-2-regression](https://www.kaggle.com/code/frobert/hds-rectangle-2-regression) | [hds-ellipse-2-regression](https://www.kaggle.com/code/frobert/hds-ellipse-2-regression) | [hds-triangle-2-regression](https://www.kaggle.com/code/frobert/hds-triangle-2-regression) |\n",
      "\n",
      "## Direct augmentation of the data. \n",
      "  - 3 variations were generated per image\n",
      "    1. Normal\n",
      "    2. 1.5 to 3.0 wider\n",
      "    3. 1.5 to 3.0 narrower\n",
      "  - One advantage is that I realized that:\n",
      "    - People tend to make equilibrated shapes (Circle, Square, Equilateral triangle). \n",
      "    - Most elongated images were interesting and sometime presented a different challenge than the original. \n",
      "  - This processing was not done for type Other.\n",
      "  - I validated them all manually (or we could say visually) and removed the generated images that were not interesting.\n",
      "  - This is different than the Augmentation done during Training (like horizontal and vertical flips, rotations) because:\n",
      "    - It applies to all images including Validation set and Test set.\n",
      "    - Being generated before being drawn provided images of a better quality.\n",
      "\n",
      "I then used these images to train models that are used in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586) Auto-Shapes feature.\n",
      "\n",
      "## People who drew the images\n",
      "Images were mostly generated by asking people I knew to draw Ellipses, Rectangles and Triangles in [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586).  \n",
      "People were aged from 7 to 87. I asked them to draw the way they wanted, slow or fast. In particular, I did not ask to draw nice shape. It was the **intent** while drawing that interested me.\n",
      "\n",
      "## Shape of type Other\n",
      "The first ML models had no **Other** shape. I was initially happy with the excellent accuracy when the drawing was actually a rectangle, triangle or ellipse. But when testing internally with [Mix on Pix](https://apps.apple.com/us/app/mix-on-pix-text-on-photos/id633281586), drawings of shapes like stars, hearts or crosses would always be recognized as one of Ellipse, Rectangle or Triangle. It is not a great user experience.\n",
      "So the shape of type Other solves that.\n",
      "\n",
      "---\n",
      "Francois Robert\n",
      "name 'detect' is not defined Language is not detected: kalilurrahman/python-data-visualization-essentials-guide\n",
      "name 'detect' is not defined Language is not detected: python-data-visualization-essentials-guide\n",
      "name 'detect' is not defined Language is not detected: kalilurrahman\n",
      "name 'detect' is not defined Language is not detected: Python Data Visualization Essentials Guide\n",
      "name 'detect' is not defined Language is not detected: For Exercises in Data Visualization\n",
      "name 'detect' is not defined Language is not detected: \n",
      "\n",
      "#context \n",
      "The information collated for Python Data Visualization Essentials Guide - Book\n",
      "\n",
      "#sources \n",
      "Generally available in the public internet\n",
      "\n",
      "#inspiration\n",
      "All the great data scientists, statisticians, programmers and enthusiasts\n",
      "\n",
      "#availability\n",
      "Available in a github page https://github.com/kalilurrahman/dataset \n",
      "\n",
      "name 'detect' is not defined Language is not detected: python-data-visualization-essentials-guide\n",
      "name 'detect' is not defined Language is not detected: kalilurrahman\n",
      "name 'detect' is not defined Language is not detected: Python Data Visualization Essentials Guide\n",
      "name 'detect' is not defined Language is not detected: For Exercises in Data Visualization\n",
      "name 'detect' is not defined Language is not detected: \n",
      "\n",
      "#context \n",
      "The information collated for Python Data Visualization Essentials Guide - Book\n",
      "\n",
      "#sources \n",
      "Generally available in the public internet\n",
      "\n",
      "#inspiration\n",
      "All the great data scientists, statisticians, programmers and enthusiasts\n",
      "\n",
      "#availability\n",
      "Available in a github page https://github.com/kalilurrahman/dataset \n",
      "\n",
      "name 'detect' is not defined Language is not detected: noiruuuu/anime-recommendations-database-vol2\n",
      "name 'detect' is not defined Language is not detected: anime-recommendations-database-vol2\n",
      "name 'detect' is not defined Language is not detected: noiruuuu\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendations Database vol.2\n",
      "name 'detect' is not defined Language is not detected: Recommendation data from 108,000 users at myanimelist.net\n",
      "name 'detect' is not defined Language is not detected: \n",
      "### Context\n",
      "This data set contains information on user preference data from 108,024 users on 15,221 animes. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.\n",
      "This data set includes animes up to 2020 winter.\n",
      "108,024 users are targeted at any anime fan around the world between the ages of 14 to 34.\n",
      "\n",
      "\n",
      "### Content\n",
      "**Anime.csv**\n",
      "- anime_id - myanimelist.net's unique id identifying an anime.\n",
      "- title - full title name of anime.\n",
      "- genres - comma separated list of genres for this anime.\n",
      "- media - movie, TV, OVA, etc.\n",
      "- episodes - how many episodes in this show. (1 if movie or ova).\n",
      "- rating - average rating out of 10 for this anime.\n",
      "- members - number of community members that are in this anime's \"group\".\n",
      "- start_date - when this anime started.\n",
      "- season - what season this anime started.\n",
      "- source - manga, light_novel, original, etc.\n",
      "\n",
      "\n",
      "**Rating.csv**\n",
      "- user_id - non identifiable randomly generated user id.\n",
      "- anime_id - the anime that this user has rated.\n",
      "- rating - rating out of 10 this user has assigned (0 if the user watched it but didn't assign a rating).\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to myanimelist.net API for providing anime data and user ratings,\n",
      "and thanks to CooperUnion(https://www.kaggle.com/CooperUnion/anime-recommendations-database)\n",
      "\n",
      "### Inspiration\n",
      "Building a better anime recommendation system based only on user viewing history.\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: anime-recommendations-database-vol2\n",
      "name 'detect' is not defined Language is not detected: noiruuuu\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendations Database vol.2\n",
      "name 'detect' is not defined Language is not detected: Recommendation data from 108,000 users at myanimelist.net\n",
      "name 'detect' is not defined Language is not detected: \n",
      "### Context\n",
      "This data set contains information on user preference data from 108,024 users on 15,221 animes. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.\n",
      "This data set includes animes up to 2020 winter.\n",
      "108,024 users are targeted at any anime fan around the world between the ages of 14 to 34.\n",
      "\n",
      "\n",
      "### Content\n",
      "**Anime.csv**\n",
      "- anime_id - myanimelist.net's unique id identifying an anime.\n",
      "- title - full title name of anime.\n",
      "- genres - comma separated list of genres for this anime.\n",
      "- media - movie, TV, OVA, etc.\n",
      "- episodes - how many episodes in this show. (1 if movie or ova).\n",
      "- rating - average rating out of 10 for this anime.\n",
      "- members - number of community members that are in this anime's \"group\".\n",
      "- start_date - when this anime started.\n",
      "- season - what season this anime started.\n",
      "- source - manga, light_novel, original, etc.\n",
      "\n",
      "\n",
      "**Rating.csv**\n",
      "- user_id - non identifiable randomly generated user id.\n",
      "- anime_id - the anime that this user has rated.\n",
      "- rating - rating out of 10 this user has assigned (0 if the user watched it but didn't assign a rating).\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to myanimelist.net API for providing anime data and user ratings,\n",
      "and thanks to CooperUnion(https://www.kaggle.com/CooperUnion/anime-recommendations-database)\n",
      "\n",
      "### Inspiration\n",
      "Building a better anime recommendation system based only on user viewing history.\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: mypapit/mangomassnet552-dataset\n",
      "name 'detect' is not defined Language is not detected: mangomassnet552-dataset\n",
      "name 'detect' is not defined Language is not detected: mypapit\n",
      "name 'detect' is not defined Language is not detected: MangoMassNet-552 Dataset\n",
      "name 'detect' is not defined Language is not detected: Mango Fruit Mass Estimation Dataset (Harumanis)\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "The dataset was created because there was no freely available Mango image dataset which includes mango images, its grade and mass/weight.\n",
      "\n",
      "This dataset contains 552 images of Harumanis Mango (clone number MA 128) collected from Fruit Collection Center, FAMA Perlis, Malaysia. The images in the dataset is resize according to A4 paper ratio of 8:10. All mango samples are taken on top of blank A4 paper, as the paper is used as visual cue for mass estimation.\n",
      "\n",
      "You can refer to the paper found in IEEEXplore titled \"[Mango Mass Estimation from RGB Image with Convolutional Neural Network](https://ieeexplore.ieee.org/abstract/document/9918807)\" for more information.\n",
      "\n",
      "#Citation\n",
      "If you use the dataset, please cite this dataset and the paper:\n",
      "\n",
      "M. H. Bin Ismail, M. N. Wagimin and T. R. Razak, \"Estimating Mango Mass from RGB Image with Convolutional Neural Network,\" 2022 3rd International Conference on Artificial Intelligence and Data Sciences (AiDAS), IPOH, Malaysia, 2022, pp. 105-110, doi: 10.1109/AiDAS56890.2022.9918807.\n",
      "\n",
      "Dataset\n",
      "Mohammad Hafiz bin Ismail, & Mohd Nazuan Wagimin. (2022). <i>MangoMassNet-552 Dataset</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/3987156\n",
      "\n",
      "# Inspiration\n",
      "- Mass estimation of mangoes / Regression problems\n",
      "- Automated mango grading\n",
      "- Image classification of mangoes\n",
      "- Image classification of Mango variety (cv Harumanis)\n",
      "\n",
      "# License and Copyright\n",
      "Copyright 2022 (c) Mohammad Hafiz bin Ismail, Mohd Nazuan Wagimin \n",
      "- This dataset is licensed under Attribution-ShareAlike 4.0 International (CC BY-SA 4.0).\n",
      "- You can use the dataset in your work, research, study, etc. Provided that you cite/attribute the dataset properly\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: mangomassnet552-dataset\n",
      "name 'detect' is not defined Language is not detected: mypapit\n",
      "name 'detect' is not defined Language is not detected: MangoMassNet-552 Dataset\n",
      "name 'detect' is not defined Language is not detected: Mango Fruit Mass Estimation Dataset (Harumanis)\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "The dataset was created because there was no freely available Mango image dataset which includes mango images, its grade and mass/weight.\n",
      "\n",
      "This dataset contains 552 images of Harumanis Mango (clone number MA 128) collected from Fruit Collection Center, FAMA Perlis, Malaysia. The images in the dataset is resize according to A4 paper ratio of 8:10. All mango samples are taken on top of blank A4 paper, as the paper is used as visual cue for mass estimation.\n",
      "\n",
      "You can refer to the paper found in IEEEXplore titled \"[Mango Mass Estimation from RGB Image with Convolutional Neural Network](https://ieeexplore.ieee.org/abstract/document/9918807)\" for more information.\n",
      "\n",
      "#Citation\n",
      "If you use the dataset, please cite this dataset and the paper:\n",
      "\n",
      "M. H. Bin Ismail, M. N. Wagimin and T. R. Razak, \"Estimating Mango Mass from RGB Image with Convolutional Neural Network,\" 2022 3rd International Conference on Artificial Intelligence and Data Sciences (AiDAS), IPOH, Malaysia, 2022, pp. 105-110, doi: 10.1109/AiDAS56890.2022.9918807.\n",
      "\n",
      "Dataset\n",
      "Mohammad Hafiz bin Ismail, & Mohd Nazuan Wagimin. (2022). <i>MangoMassNet-552 Dataset</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/3987156\n",
      "\n",
      "# Inspiration\n",
      "- Mass estimation of mangoes / Regression problems\n",
      "- Automated mango grading\n",
      "- Image classification of mangoes\n",
      "- Image classification of Mango variety (cv Harumanis)\n",
      "\n",
      "# License and Copyright\n",
      "Copyright 2022 (c) Mohammad Hafiz bin Ismail, Mohd Nazuan Wagimin \n",
      "- This dataset is licensed under Attribution-ShareAlike 4.0 International (CC BY-SA 4.0).\n",
      "- You can use the dataset in your work, research, study, etc. Provided that you cite/attribute the dataset properly\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: lubaroli/notmnist\n",
      "name 'detect' is not defined Language is not detected: notmnist\n",
      "name 'detect' is not defined Language is not detected: lubaroli\n",
      "name 'detect' is not defined Language is not detected: notMNIST dataset\n",
      "name 'detect' is not defined Language is not detected: Used in Udacity's Deep Learning MOOC\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset was created by Yaroslav Bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J.\n",
      "\n",
      "### Content\n",
      "\n",
      "A set of training and test images of letters from A to J on various typefaces. The images size is 28x28 pixels.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset can be found on Tensorflow github page as well as on the blog from Yaroslav, here.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This is a pretty good dataset to train classifiers! According to Yaroslav:\n",
      "\n",
      ">> Judging by the examples, one would expect this to be a harder task\n",
      ">> than MNIST. This seems to be the case -- logistic regression on top of\n",
      ">> stacked auto-encoder with fine-tuning gets about 89% accuracy whereas\n",
      ">> same approach gives got 98% on MNIST. Dataset consists of small\n",
      ">> hand-cleaned part, about 19k instances, and large uncleaned dataset,\n",
      ">> 500k instances. Two parts have approximately 0.5% and 6.5% label error\n",
      ">> rate. I got this by looking through glyphs and counting how often my\n",
      ">> guess of the letter didn't match it's unicode value in the font file.\n",
      "\n",
      "Enjoy!\n",
      "name 'detect' is not defined Language is not detected: notmnist\n",
      "name 'detect' is not defined Language is not detected: lubaroli\n",
      "name 'detect' is not defined Language is not detected: notMNIST dataset\n",
      "name 'detect' is not defined Language is not detected: Used in Udacity's Deep Learning MOOC\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset was created by Yaroslav Bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J.\n",
      "\n",
      "### Content\n",
      "\n",
      "A set of training and test images of letters from A to J on various typefaces. The images size is 28x28 pixels.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset can be found on Tensorflow github page as well as on the blog from Yaroslav, here.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This is a pretty good dataset to train classifiers! According to Yaroslav:\n",
      "\n",
      ">> Judging by the examples, one would expect this to be a harder task\n",
      ">> than MNIST. This seems to be the case -- logistic regression on top of\n",
      ">> stacked auto-encoder with fine-tuning gets about 89% accuracy whereas\n",
      ">> same approach gives got 98% on MNIST. Dataset consists of small\n",
      ">> hand-cleaned part, about 19k instances, and large uncleaned dataset,\n",
      ">> 500k instances. Two parts have approximately 0.5% and 6.5% label error\n",
      ">> rate. I got this by looking through glyphs and counting how often my\n",
      ">> guess of the letter didn't match it's unicode value in the font file.\n",
      "\n",
      "Enjoy!\n",
      "name 'detect' is not defined Language is not detected: jeffheaton/tabular-feature-engineering-dataset\n",
      "name 'detect' is not defined Language is not detected: tabular-feature-engineering-dataset\n",
      "name 'detect' is not defined Language is not detected: jeffheaton\n",
      "name 'detect' is not defined Language is not detected: Tabular Feature Engineering Dataset\n",
      "name 'detect' is not defined Language is not detected: What equations are most difficult for various model types to synthesize?\n",
      "name 'detect' is not defined Language is not detected: Machine learning models, such as neural networks, decision trees, random forests, and gradient boosting machines, accept a feature vector, and provide a prediction.  These models learn in a supervised fashion where we provide feature vectors with the expected output.  It is common practice to engineer new features from the provided feature set.  Such engineered features will either augment or replace portions of the existing feature vector.  These engineered features are essentially calculated fields based on the values of the other features.  \n",
      "\n",
      "Engineering such features is primarily a manual, time-consuming task.  Additionally, each type of model will respond differently to different kinds of engineered features.  This paper reports empirical research to demonstrate what kinds of engineered features are best suited to various machine learning model types.  We provide this recommendation by generating several datasets that we designed to benefit from a particular type of engineered feature.  The experiment demonstrates to what degree the machine learning model can synthesize the needed feature on its own.  If a model can synthesize a planned feature, it is not necessary to provide that feature.  The research demonstrated that the studied models do indeed perform differently with various types of engineered features. \n",
      "\n",
      "We generated this dataset for the following paper:\n",
      "\n",
      "Heaton, J. (2016, April). [An Empirical Analysis of Feature Engineering for Predictive Modeling](https://arxiv.org/abs/1701.07852). In *SoutheastCon 2016* (pp. 1-6). IEEE.\n",
      "\n",
      "### Included Features\n",
      "The dataset is made up of several files that contain the following features:\n",
      "* **BMI** (bmi)\n",
      "$$ \\frac{m}{h^2} $$\n",
      "* **Counts** (counts)\n",
      "$$ [0, 1, 1, 0, 0, 1] = 3 $$\n",
      "* **Standard Deviation** (div)\n",
      "$$ SD = \\sqrt{\\frac{\\sum{|x-\\hat{x}|^2}}{n}} $$\n",
      "* **Difference** (diff)\n",
      "$$ a-b $$\n",
      "* **Distance** (dist)\n",
      "$$ \\sqrt{(a - b)^2 + (c - d)^2} $$\n",
      "* **Log** (log)\n",
      "$$ \\log{a} $$\n",
      "* **Max** (max)\n",
      "$$ \\max{(a,b,c,d,...)} $$\n",
      "* **Polynomial** (poly)\n",
      "$$ 1+5x+8x^2 $$\n",
      "* **Powers** (pow)\n",
      "$$ x^y $$\n",
      "* **Quadratic/Roots** (quad)\n",
      "$$ \\left| \\frac{-b+\\sqrt{b^2-4ac}}{2a}- \\frac{-b-\\sqrt{b^2-4ac}}{2a} \\right| $$\n",
      "* **Ratio of Differences** (r_diff)\n",
      "$$ \\frac{a-b}{c-d} $$\n",
      "* **Ratio of Polynomial** (r_poly)\n",
      "$$ y=\\frac{1}{5x+8x^2} $$\n",
      "* **Ratio** (ratio)\n",
      "$$ \\frac{a}{b} $$\n",
      "* **Product Power Ratio** (rel)\n",
      "$$ \\frac{ab}{c^2} $$\n",
      "* **Square Root** (sqrt)\n",
      "$$ \\sqrt{a} $$\n",
      "* **Sum** (sum)\n",
      "$$ a+b $$\n",
      "name 'detect' is not defined Language is not detected: tabular-feature-engineering-dataset\n",
      "name 'detect' is not defined Language is not detected: jeffheaton\n",
      "name 'detect' is not defined Language is not detected: Tabular Feature Engineering Dataset\n",
      "name 'detect' is not defined Language is not detected: What equations are most difficult for various model types to synthesize?\n",
      "name 'detect' is not defined Language is not detected: Machine learning models, such as neural networks, decision trees, random forests, and gradient boosting machines, accept a feature vector, and provide a prediction.  These models learn in a supervised fashion where we provide feature vectors with the expected output.  It is common practice to engineer new features from the provided feature set.  Such engineered features will either augment or replace portions of the existing feature vector.  These engineered features are essentially calculated fields based on the values of the other features.  \n",
      "\n",
      "Engineering such features is primarily a manual, time-consuming task.  Additionally, each type of model will respond differently to different kinds of engineered features.  This paper reports empirical research to demonstrate what kinds of engineered features are best suited to various machine learning model types.  We provide this recommendation by generating several datasets that we designed to benefit from a particular type of engineered feature.  The experiment demonstrates to what degree the machine learning model can synthesize the needed feature on its own.  If a model can synthesize a planned feature, it is not necessary to provide that feature.  The research demonstrated that the studied models do indeed perform differently with various types of engineered features. \n",
      "\n",
      "We generated this dataset for the following paper:\n",
      "\n",
      "Heaton, J. (2016, April). [An Empirical Analysis of Feature Engineering for Predictive Modeling](https://arxiv.org/abs/1701.07852). In *SoutheastCon 2016* (pp. 1-6). IEEE.\n",
      "\n",
      "### Included Features\n",
      "The dataset is made up of several files that contain the following features:\n",
      "* **BMI** (bmi)\n",
      "$$ \\frac{m}{h^2} $$\n",
      "* **Counts** (counts)\n",
      "$$ [0, 1, 1, 0, 0, 1] = 3 $$\n",
      "* **Standard Deviation** (div)\n",
      "$$ SD = \\sqrt{\\frac{\\sum{|x-\\hat{x}|^2}}{n}} $$\n",
      "* **Difference** (diff)\n",
      "$$ a-b $$\n",
      "* **Distance** (dist)\n",
      "$$ \\sqrt{(a - b)^2 + (c - d)^2} $$\n",
      "* **Log** (log)\n",
      "$$ \\log{a} $$\n",
      "* **Max** (max)\n",
      "$$ \\max{(a,b,c,d,...)} $$\n",
      "* **Polynomial** (poly)\n",
      "$$ 1+5x+8x^2 $$\n",
      "* **Powers** (pow)\n",
      "$$ x^y $$\n",
      "* **Quadratic/Roots** (quad)\n",
      "$$ \\left| \\frac{-b+\\sqrt{b^2-4ac}}{2a}- \\frac{-b-\\sqrt{b^2-4ac}}{2a} \\right| $$\n",
      "* **Ratio of Differences** (r_diff)\n",
      "$$ \\frac{a-b}{c-d} $$\n",
      "* **Ratio of Polynomial** (r_poly)\n",
      "$$ y=\\frac{1}{5x+8x^2} $$\n",
      "* **Ratio** (ratio)\n",
      "$$ \\frac{a}{b} $$\n",
      "* **Product Power Ratio** (rel)\n",
      "$$ \\frac{ab}{c^2} $$\n",
      "* **Square Root** (sqrt)\n",
      "$$ \\sqrt{a} $$\n",
      "* **Sum** (sum)\n",
      "$$ a+b $$\n",
      "name 'detect' is not defined Language is not detected: jainpooja/topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: jainpooja\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles 2.0\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya: Text Classification Data Set (Hacklive 3)\n",
      "name 'detect' is not defined Language is not detected: **Topic Modeling for Research Articles 2.0**\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "**Dataset Column description**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3486681%2F03486d5878c9443cce843065f8123113%2FCapture.PNG?generation=1602917005279232&alt=media)\n",
      "\n",
      "**Tags.csv**\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence, Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: jainpooja\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles 2.0\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya: Text Classification Data Set (Hacklive 3)\n",
      "name 'detect' is not defined Language is not detected: **Topic Modeling for Research Articles 2.0**\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "**Dataset Column description**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3486681%2F03486d5878c9443cce843065f8123113%2FCapture.PNG?generation=1602917005279232&alt=media)\n",
      "\n",
      "**Tags.csv**\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence, Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      "name 'detect' is not defined Language is not detected: vmb2021/bellabeat-capstone\n",
      "name 'detect' is not defined Language is not detected: bellabeat-capstone\n",
      "name 'detect' is not defined Language is not detected: vmb2021\n",
      "name 'detect' is not defined Language is not detected: Bellabeat - Capstone\n",
      "name 'detect' is not defined Language is not detected: Case Study 2 based on FitBit Fitness Tracker Data\n",
      "name 'detect' is not defined Language is not detected: Business task\n",
      "Provide a high-level recommendation to help guide Bellabeat’s marketing strategy to unlock new growth opportunities.\n",
      "\n",
      "Key stakeholders\n",
      "Urška Sršen, cofounder and Chief Creative Officer of Bellabeat\n",
      "Sando Mur, Mathematician and Bellabeat’s cofounder\n",
      "\n",
      "Data sources used\n",
      "https://www.kaggle.com/arashnic/fitbit \n",
      "FitBit Fitness Tracker Data (CC0: Public Domain, dataset made available through Mobius)\n",
      "\n",
      "Documentation of any cleaning or manipulation of data\n",
      "\n",
      "\n",
      "RStudio Cloud is the best tool for this project due to data size.\n",
      "Packages used\n",
      "\n",
      "install.packages(\"lubridate\")\n",
      "library(lubridate)\n",
      "\n",
      "library(ggplot2)\n",
      " \n",
      " install.packages(\"dplyr\")  \n",
      " library(dplyr)\n",
      "\n",
      "\n",
      "\n",
      "### Context\n",
      "\n",
      "There's a story behind every dataset and here's your opportunity to share yours.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: bellabeat-capstone\n",
      "name 'detect' is not defined Language is not detected: vmb2021\n",
      "name 'detect' is not defined Language is not detected: Bellabeat - Capstone\n",
      "name 'detect' is not defined Language is not detected: Case Study 2 based on FitBit Fitness Tracker Data\n",
      "name 'detect' is not defined Language is not detected: Business task\n",
      "Provide a high-level recommendation to help guide Bellabeat’s marketing strategy to unlock new growth opportunities.\n",
      "\n",
      "Key stakeholders\n",
      "Urška Sršen, cofounder and Chief Creative Officer of Bellabeat\n",
      "Sando Mur, Mathematician and Bellabeat’s cofounder\n",
      "\n",
      "Data sources used\n",
      "https://www.kaggle.com/arashnic/fitbit \n",
      "FitBit Fitness Tracker Data (CC0: Public Domain, dataset made available through Mobius)\n",
      "\n",
      "Documentation of any cleaning or manipulation of data\n",
      "\n",
      "\n",
      "RStudio Cloud is the best tool for this project due to data size.\n",
      "Packages used\n",
      "\n",
      "install.packages(\"lubridate\")\n",
      "library(lubridate)\n",
      "\n",
      "library(ggplot2)\n",
      " \n",
      " install.packages(\"dplyr\")  \n",
      " library(dplyr)\n",
      "\n",
      "\n",
      "\n",
      "### Context\n",
      "\n",
      "There's a story behind every dataset and here's your opportunity to share yours.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: iabhishekbhardwaj/fraud-detection\n",
      "name 'detect' is not defined Language is not detected: fraud-detection\n",
      "name 'detect' is not defined Language is not detected: iabhishekbhardwaj\n",
      "name 'detect' is not defined Language is not detected: fraud detection\n",
      "name 'detect' is not defined Language is not detected: banking dection and financial \n",
      "name 'detect' is not defined Language is not detected: fraud-detection\n",
      "name 'detect' is not defined Language is not detected: iabhishekbhardwaj\n",
      "name 'detect' is not defined Language is not detected: fraud detection\n",
      "name 'detect' is not defined Language is not detected: banking dection and financial \n",
      "name 'detect' is not defined Language is not detected: fatihoge/myanimelist-top-12k-manga\n",
      "name 'detect' is not defined Language is not detected: myanimelist-top-12k-manga\n",
      "name 'detect' is not defined Language is not detected: fatihoge\n",
      "name 'detect' is not defined Language is not detected: myanimelist top 15k manga\n",
      "name 'detect' is not defined Language is not detected: This dataset is taken from myanimelists top manga section\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset is taken from myanimelist by using the myanimelist api https://myanimelist.net/apiconfig/references/api/v2.\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset have all the information that is in myanimelist manga details except for the private information to user\n",
      "\n",
      "### Missing Values\n",
      "\n",
      "Missing values are specified as empty strings and NaN values.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I created this dataset to make a manga recommendation system.\n",
      "name 'detect' is not defined Language is not detected: myanimelist-top-12k-manga\n",
      "name 'detect' is not defined Language is not detected: fatihoge\n",
      "name 'detect' is not defined Language is not detected: myanimelist top 15k manga\n",
      "name 'detect' is not defined Language is not detected: This dataset is taken from myanimelists top manga section\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset is taken from myanimelist by using the myanimelist api https://myanimelist.net/apiconfig/references/api/v2.\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset have all the information that is in myanimelist manga details except for the private information to user\n",
      "\n",
      "### Missing Values\n",
      "\n",
      "Missing values are specified as empty strings and NaN values.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I created this dataset to make a manga recommendation system.\n",
      "name 'detect' is not defined Language is not detected: yashdharme36/airfare-ml-predicting-flight-fares\n",
      "name 'detect' is not defined Language is not detected: airfare-ml-predicting-flight-fares\n",
      "name 'detect' is not defined Language is not detected: yashdharme36\n",
      "name 'detect' is not defined Language is not detected: Airfare ML : Predicting Flight Fares\n",
      "name 'detect' is not defined Language is not detected: Flight data for informed ticket purchases : scraped from EaseMyTrip with Python\n",
      "name 'detect' is not defined Language is not detected: **Context:**\n",
      "This dataset contains flight fare data that was collected from the EaseMyTrip website using web scraping techniques. The data was collected with the goal of providing users with information that could help them make informed decisions about when and where to purchase flight tickets. By analyzing patterns in flight fares over time, users can identify the best times to book tickets and potentially save money.\n",
      "\n",
      "\n",
      "**Sources:**\n",
      "1. Data collected using Python script with Beautiful Soup and Selenium libraries.\n",
      "2. Script collected data on various flight details such as Date of booking, Date of travel, Airline and class, Departure time and source, Arrival time and destination, Duration, Total stops, Price.\n",
      "3. The scraping process was designed to collect data for flights departing from a specific set of airports (Top 7 busiest airports in India).\n",
      "Note that the Departure Time feature also includes the Source airport, and the Arrival Time feature also includes the Destination airport. Which is later extracted in Cleaned_dataset. Also both cleaned and scraped datasets have provided so that one can use dataset as per their requirement and convenience.\n",
      "\n",
      "\n",
      "**Inspiration:**\n",
      "1. Dataset created to provide users with valuable resource for analyzing flight fares in India.\n",
      "2. Detailed information on flight fares over time can be used to develop more accurate pricing models and inform users about best times to book tickets.\n",
      "3. Data can also be used to study trends and patterns in the travel industry through air can act as a valuable resource for researchers and analysts.\n",
      "\n",
      "\n",
      "**Limitations:**\n",
      "1. This dataset only covers flights departing from specific airports and limited to a certain time period.\n",
      "2. To perform time series analysis one have gather data for at least top 10 busiest airports for 365 days.\n",
      "3. This does not cover variations in aviation fuel prices as this is the one of influencing factor for deciding fare, hence the same dataset might not be useful for next year, but I will try to update it twice in an year.\n",
      "4. Also demand and supply for the particular flight seat is not available in the dataset as this data is not publicly available on any flight booking web site.\n",
      "\n",
      "\n",
      "**Scope of Improvement:**\n",
      "1. The dataset could be enhanced by including additional features such as current aviation fuel prices and the distance between the source and destination in terms of longitude and latitude.\n",
      "2. The data could also be expanded to include more airlines and more airports, providing a more comprehensive view of the flight market.\n",
      "3. Additionally, it may be helpful to include data on flight cancellations, delays, and other factors that can impact the price and availability of flights.\n",
      "4. Finally, while the current dataset provides information on flight prices, it does not include information on the quality of the flight experience, such as legroom, in-flight amenities, and customer reviews. Including this type of data could provide a more complete picture of the flight market and help travelers make more informed decisions.\n",
      "name 'detect' is not defined Language is not detected: airfare-ml-predicting-flight-fares\n",
      "name 'detect' is not defined Language is not detected: yashdharme36\n",
      "name 'detect' is not defined Language is not detected: Airfare ML : Predicting Flight Fares\n",
      "name 'detect' is not defined Language is not detected: Flight data for informed ticket purchases : scraped from EaseMyTrip with Python\n",
      "name 'detect' is not defined Language is not detected: **Context:**\n",
      "This dataset contains flight fare data that was collected from the EaseMyTrip website using web scraping techniques. The data was collected with the goal of providing users with information that could help them make informed decisions about when and where to purchase flight tickets. By analyzing patterns in flight fares over time, users can identify the best times to book tickets and potentially save money.\n",
      "\n",
      "\n",
      "**Sources:**\n",
      "1. Data collected using Python script with Beautiful Soup and Selenium libraries.\n",
      "2. Script collected data on various flight details such as Date of booking, Date of travel, Airline and class, Departure time and source, Arrival time and destination, Duration, Total stops, Price.\n",
      "3. The scraping process was designed to collect data for flights departing from a specific set of airports (Top 7 busiest airports in India).\n",
      "Note that the Departure Time feature also includes the Source airport, and the Arrival Time feature also includes the Destination airport. Which is later extracted in Cleaned_dataset. Also both cleaned and scraped datasets have provided so that one can use dataset as per their requirement and convenience.\n",
      "\n",
      "\n",
      "**Inspiration:**\n",
      "1. Dataset created to provide users with valuable resource for analyzing flight fares in India.\n",
      "2. Detailed information on flight fares over time can be used to develop more accurate pricing models and inform users about best times to book tickets.\n",
      "3. Data can also be used to study trends and patterns in the travel industry through air can act as a valuable resource for researchers and analysts.\n",
      "\n",
      "\n",
      "**Limitations:**\n",
      "1. This dataset only covers flights departing from specific airports and limited to a certain time period.\n",
      "2. To perform time series analysis one have gather data for at least top 10 busiest airports for 365 days.\n",
      "3. This does not cover variations in aviation fuel prices as this is the one of influencing factor for deciding fare, hence the same dataset might not be useful for next year, but I will try to update it twice in an year.\n",
      "4. Also demand and supply for the particular flight seat is not available in the dataset as this data is not publicly available on any flight booking web site.\n",
      "\n",
      "\n",
      "**Scope of Improvement:**\n",
      "1. The dataset could be enhanced by including additional features such as current aviation fuel prices and the distance between the source and destination in terms of longitude and latitude.\n",
      "2. The data could also be expanded to include more airlines and more airports, providing a more comprehensive view of the flight market.\n",
      "3. Additionally, it may be helpful to include data on flight cancellations, delays, and other factors that can impact the price and availability of flights.\n",
      "4. Finally, while the current dataset provides information on flight prices, it does not include information on the quality of the flight experience, such as legroom, in-flight amenities, and customer reviews. Including this type of data could provide a more complete picture of the flight market and help travelers make more informed decisions.\n",
      "name 'detect' is not defined Language is not detected: rmisra/politifact-fact-check-dataset\n",
      "name 'detect' is not defined Language is not detected: politifact-fact-check-dataset\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: Politifact Fact Check Dataset\n",
      "name 'detect' is not defined Language is not detected: High-quality dataset with 21k fact check statements between 2008 to 2022\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "We present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https://www.politifact.com/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n",
      "\n",
      "Cover photo credits: https://blog.condati.com/5-claims-retail-marketing-analytics-fact-check\n",
      "\n",
      "### Content\n",
      "Each record consists of 8 attributes:\n",
      "\n",
      "* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n",
      "* ```statement_originator```: the person who made the statement being fact checked\n",
      "* ```statement```:  statement being fact checked\n",
      "* ```statement_date```:  the date when statement being fact checked was made\n",
      "* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n",
      "* ```factchecker```: name of the person who fact checked the claim\n",
      "* ```factcheck_date```:  date when the fact checked article was published\n",
      "* ```factcheck_analysis_link```:  link to the fact checked analysis article\n",
      "\n",
      "The cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https://gist.github.com/rishabhmisra/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh and Jigyasa Grover. \"Do Not ‘Fake It Till You Make It’! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n",
      "2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140/RG.2.2.29923.22566 (2022).\n",
      "```\n",
      "\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@incollection{misra2022not,\n",
      "  title={Do Not ‘Fake It Till You Make It’! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n",
      "  author={Misra, Rishabh and Grover, Jigyasa},\n",
      "  booktitle={Deep Learning for Social Media Data Analytics},\n",
      "  pages={213--235},\n",
      "  year={2022},\n",
      "  publisher={Springer}\n",
      "}\n",
      "@dataset{misra2022politifact,\n",
      "  author = {Misra, Rishabh},\n",
      "  year = {2022},\n",
      "  month = {09},\n",
      "  pages = {},\n",
      "  title = {Politifact Fact Check Dataset},\n",
      "  doi = {10.13140/RG.2.2.29923.22566}\n",
      "}\n",
      "```\n",
      "\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was collected from [PolitiFact](https://www.politifact.com/). \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Can you categorize facts as true or false?  \n",
      "\n",
      "* Does the sources of false facts have a temporal pattern?\n",
      "\n",
      "* Is there a linguistic pattern in false facts?\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
      "\n",
      "* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: politifact-fact-check-dataset\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: Politifact Fact Check Dataset\n",
      "name 'detect' is not defined Language is not detected: High-quality dataset with 21k fact check statements between 2008 to 2022\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "We present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https://www.politifact.com/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n",
      "\n",
      "Cover photo credits: https://blog.condati.com/5-claims-retail-marketing-analytics-fact-check\n",
      "\n",
      "### Content\n",
      "Each record consists of 8 attributes:\n",
      "\n",
      "* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n",
      "* ```statement_originator```: the person who made the statement being fact checked\n",
      "* ```statement```:  statement being fact checked\n",
      "* ```statement_date```:  the date when statement being fact checked was made\n",
      "* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n",
      "* ```factchecker```: name of the person who fact checked the claim\n",
      "* ```factcheck_date```:  date when the fact checked article was published\n",
      "* ```factcheck_analysis_link```:  link to the fact checked analysis article\n",
      "\n",
      "The cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https://gist.github.com/rishabhmisra/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh and Jigyasa Grover. \"Do Not ‘Fake It Till You Make It’! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n",
      "2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140/RG.2.2.29923.22566 (2022).\n",
      "```\n",
      "\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@incollection{misra2022not,\n",
      "  title={Do Not ‘Fake It Till You Make It’! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n",
      "  author={Misra, Rishabh and Grover, Jigyasa},\n",
      "  booktitle={Deep Learning for Social Media Data Analytics},\n",
      "  pages={213--235},\n",
      "  year={2022},\n",
      "  publisher={Springer}\n",
      "}\n",
      "@dataset{misra2022politifact,\n",
      "  author = {Misra, Rishabh},\n",
      "  year = {2022},\n",
      "  month = {09},\n",
      "  pages = {},\n",
      "  title = {Politifact Fact Check Dataset},\n",
      "  doi = {10.13140/RG.2.2.29923.22566}\n",
      "}\n",
      "```\n",
      "\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was collected from [PolitiFact](https://www.politifact.com/). \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Can you categorize facts as true or false?  \n",
      "\n",
      "* Does the sources of false facts have a temporal pattern?\n",
      "\n",
      "* Is there a linguistic pattern in false facts?\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
      "\n",
      "* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: rmisra/news-category-dataset\n",
      "name 'detect' is not defined Language is not detected: news-category-dataset\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: News Category Dataset\n",
      "name 'detect' is not defined Language is not detected: Identify the type of news based on headlines and short descriptions\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "** **Please cite the dataset using the BibTex provided in one of the following sections if you are using it in your research, thank you!** **\n",
      "\n",
      "This dataset contains around 210k news headlines from 2012 to 2022 from [HuffPost](https://www.huffingtonpost.com/). This is one of the biggest news datasets and can serve as a benchmark for a variety of computational linguistic tasks. HuffPost stopped maintaining an extensive archive of news articles sometime after this dataset was first collected in 2018, so it is not possible to collect such a dataset in the present day. Due to changes in the website, there are about 200k headlines between 2012 and May 2018 and 10k headlines between May 2018 and 2022.\n",
      "\n",
      "### Content\n",
      "Each record in the dataset consists of the following attributes:\n",
      "- category: category in which the article was published.\n",
      "- headline: the headline of the news article.\n",
      "- authors: list of authors who contributed to the article.\n",
      "- link: link to the original news article. \n",
      "- short_description: Abstract of the news article.\n",
      "- date: publication date of the article.\n",
      "\n",
      "There are a total of 42 news categories in the dataset. The top-15 categories and corresponding article counts are as follows:\n",
      "\n",
      "* ```POLITICS```: ```35602```\n",
      "\n",
      "* ```WELLNESS```: ```17945```\n",
      "\n",
      "* ```ENTERTAINMENT```: ```17362```\n",
      "\n",
      "* ```TRAVEL```: ```9900```\n",
      "\n",
      "* ```STYLE & BEAUTY```: ```9814```\n",
      "\n",
      "* ```PARENTING```: ```8791```\n",
      "\n",
      "* ```HEALTHY LIVING```: ```6694```\n",
      "\n",
      "* ```QUEER VOICES```: ```6347```\n",
      "\n",
      "* ```FOOD & DRINK```: ```6340```\n",
      "\n",
      "* ```BUSINESS```: ```5992```\n",
      "\n",
      "* ```COMEDY```: ```5400```\n",
      "\n",
      "* ```SPORTS```: ```5077```\n",
      "\n",
      "* ```BLACK VOICES```: ```4583```\n",
      "\n",
      "* ```HOME & LIVING```: ```4320```\n",
      "\n",
      "* ```PARENTS```: ```3955```\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).\n",
      "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
      "```\n",
      "\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@article{misra2022news,\n",
      "  title={News Category Dataset},\n",
      "  author={Misra, Rishabh},\n",
      "  journal={arXiv preprint arXiv:2209.11429},\n",
      "  year={2022}\n",
      "}\n",
      "@book{misra2021sculpting,\n",
      "  author = {Misra, Rishabh and Grover, Jigyasa},\n",
      "  year = {2021},\n",
      "  month = {01},\n",
      "  pages = {},\n",
      "  title = {Sculpting Data for ML: The first act of Machine Learning},\n",
      "  isbn = {9798585463570}\n",
      "}\n",
      "```\n",
      "\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was collected from [HuffPost](https://www.huffingtonpost.com/). \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Can you categorize news articles based on their headlines and short descriptions?  \n",
      "\n",
      "* Do news articles from different categories have different writing styles?\n",
      "\n",
      "* A classifier trained on this dataset could be used on a free text to identify the type of language being used.\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "* [Politifact Fact Check Dataset](https://www.kaggle.com/datasets/rmisra/politifact-fact-check-dataset)\n",
      "name 'detect' is not defined Language is not detected: news-category-dataset\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: News Category Dataset\n",
      "name 'detect' is not defined Language is not detected: Identify the type of news based on headlines and short descriptions\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "** **Please cite the dataset using the BibTex provided in one of the following sections if you are using it in your research, thank you!** **\n",
      "\n",
      "This dataset contains around 210k news headlines from 2012 to 2022 from [HuffPost](https://www.huffingtonpost.com/). This is one of the biggest news datasets and can serve as a benchmark for a variety of computational linguistic tasks. HuffPost stopped maintaining an extensive archive of news articles sometime after this dataset was first collected in 2018, so it is not possible to collect such a dataset in the present day. Due to changes in the website, there are about 200k headlines between 2012 and May 2018 and 10k headlines between May 2018 and 2022.\n",
      "\n",
      "### Content\n",
      "Each record in the dataset consists of the following attributes:\n",
      "- category: category in which the article was published.\n",
      "- headline: the headline of the news article.\n",
      "- authors: list of authors who contributed to the article.\n",
      "- link: link to the original news article. \n",
      "- short_description: Abstract of the news article.\n",
      "- date: publication date of the article.\n",
      "\n",
      "There are a total of 42 news categories in the dataset. The top-15 categories and corresponding article counts are as follows:\n",
      "\n",
      "* ```POLITICS```: ```35602```\n",
      "\n",
      "* ```WELLNESS```: ```17945```\n",
      "\n",
      "* ```ENTERTAINMENT```: ```17362```\n",
      "\n",
      "* ```TRAVEL```: ```9900```\n",
      "\n",
      "* ```STYLE & BEAUTY```: ```9814```\n",
      "\n",
      "* ```PARENTING```: ```8791```\n",
      "\n",
      "* ```HEALTHY LIVING```: ```6694```\n",
      "\n",
      "* ```QUEER VOICES```: ```6347```\n",
      "\n",
      "* ```FOOD & DRINK```: ```6340```\n",
      "\n",
      "* ```BUSINESS```: ```5992```\n",
      "\n",
      "* ```COMEDY```: ```5400```\n",
      "\n",
      "* ```SPORTS```: ```5077```\n",
      "\n",
      "* ```BLACK VOICES```: ```4583```\n",
      "\n",
      "* ```HOME & LIVING```: ```4320```\n",
      "\n",
      "* ```PARENTS```: ```3955```\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).\n",
      "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
      "```\n",
      "\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@article{misra2022news,\n",
      "  title={News Category Dataset},\n",
      "  author={Misra, Rishabh},\n",
      "  journal={arXiv preprint arXiv:2209.11429},\n",
      "  year={2022}\n",
      "}\n",
      "@book{misra2021sculpting,\n",
      "  author = {Misra, Rishabh and Grover, Jigyasa},\n",
      "  year = {2021},\n",
      "  month = {01},\n",
      "  pages = {},\n",
      "  title = {Sculpting Data for ML: The first act of Machine Learning},\n",
      "  isbn = {9798585463570}\n",
      "}\n",
      "```\n",
      "\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was collected from [HuffPost](https://www.huffingtonpost.com/). \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Can you categorize news articles based on their headlines and short descriptions?  \n",
      "\n",
      "* Do news articles from different categories have different writing styles?\n",
      "\n",
      "* A classifier trained on this dataset could be used on a free text to identify the type of language being used.\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "* [Politifact Fact Check Dataset](https://www.kaggle.com/datasets/rmisra/politifact-fact-check-dataset)\n",
      "name 'detect' is not defined Language is not detected: rmisra/news-headlines-dataset-for-sarcasm-detection\n",
      "name 'detect' is not defined Language is not detected: news-headlines-dataset-for-sarcasm-detection\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: News Headlines Dataset For Sarcasm Detection\n",
      "name 'detect' is not defined Language is not detected: High quality dataset for the task of Sarcasm and Fake News Detection\n",
      "name 'detect' is not defined Language is not detected: ###Context\n",
      "** **Please cite the dataset using the BibTex provided in one of the following sections if you are using it in your research, thank you!** **\n",
      "\n",
      "Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\n",
      "\n",
      "To overcome the limitations related to noise in Twitter datasets, this **News Headlines dataset for Sarcasm Detection** is collected from two news website. [*TheOnion*](https://www.theonion.com/) aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from [*HuffPost*](https://www.huffingtonpost.com/).\n",
      "\n",
      "This new dataset has following advantages over the existing Twitter datasets:\n",
      "\n",
      "* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n",
      "\n",
      "* Furthermore, since the sole purpose of *TheOnion* is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n",
      "\n",
      "* Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.\n",
      "\n",
      "### Content\n",
      "Each record consists of three attributes:\n",
      "\n",
      "* ```is_sarcastic```: 1 if the record is sarcastic otherwise 0\n",
      "\n",
      "* ```headline```: the headline of the news article\n",
      "\n",
      "* ```article_link```: link to the original news article. Useful in collecting supplementary data\n",
      "\n",
      "General statistics of data, instructions on how to read the data in python, and basic exploratory analysis could be found at [this GitHub repo](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection). A hybrid NN architecture trained on this dataset can be found at [this GitHub repo](https://github.com/rishabhmisra/Sarcasm-Detection-using-NN).\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
      "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
      "```\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@article{misra2023Sarcasm,\n",
      "  title = {Sarcasm Detection using News Headlines Dataset},\n",
      "  journal = {AI Open},\n",
      "  volume = {4},\n",
      "  pages = {13-18},\n",
      "  year = {2023},\n",
      "  issn = {2666-6510},\n",
      "  doi = {https://doi.org/10.1016/j.aiopen.2023.01.001},\n",
      "  url = {https://www.sciencedirect.com/science/article/pii/S2666651023000013},\n",
      "  author = {Rishabh Misra and Prahal Arora},\n",
      "}\n",
      "@book{misra2021sculpting,\n",
      "  author = {Misra, Rishabh and Grover, Jigyasa},\n",
      "  year = {2021},\n",
      "  month = {01},\n",
      "  pages = {},\n",
      "  title = {Sculpting Data for ML: The first act of Machine Learning},\n",
      "  isbn = {9798585463570}\n",
      "}\n",
      "```\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Can you identify sarcastic sentences? Can you distinguish between fake news and legitimate news?\n",
      "\n",
      "### Reading the data\n",
      "Following code snippet could be used to read the data:\n",
      "\n",
      "```\n",
      "import json\n",
      "\n",
      "def parse_data(file):\n",
      "    for l in open(file,'r'):\n",
      "        yield json.loads(l)\n",
      "\n",
      "data = list(parse_data('./Sarcasm_Headlines_Dataset.json'))\n",
      "```\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "* [Politifact Fact Check Dataset](https://www.kaggle.com/datasets/rmisra/politifact-fact-check-dataset)\n",
      "name 'detect' is not defined Language is not detected: news-headlines-dataset-for-sarcasm-detection\n",
      "name 'detect' is not defined Language is not detected: rmisra\n",
      "name 'detect' is not defined Language is not detected: News Headlines Dataset For Sarcasm Detection\n",
      "name 'detect' is not defined Language is not detected: High quality dataset for the task of Sarcasm and Fake News Detection\n",
      "name 'detect' is not defined Language is not detected: ###Context\n",
      "** **Please cite the dataset using the BibTex provided in one of the following sections if you are using it in your research, thank you!** **\n",
      "\n",
      "Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\n",
      "\n",
      "To overcome the limitations related to noise in Twitter datasets, this **News Headlines dataset for Sarcasm Detection** is collected from two news website. [*TheOnion*](https://www.theonion.com/) aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from [*HuffPost*](https://www.huffingtonpost.com/).\n",
      "\n",
      "This new dataset has following advantages over the existing Twitter datasets:\n",
      "\n",
      "* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n",
      "\n",
      "* Furthermore, since the sole purpose of *TheOnion* is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n",
      "\n",
      "* Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.\n",
      "\n",
      "### Content\n",
      "Each record consists of three attributes:\n",
      "\n",
      "* ```is_sarcastic```: 1 if the record is sarcastic otherwise 0\n",
      "\n",
      "* ```headline```: the headline of the news article\n",
      "\n",
      "* ```article_link```: link to the original news article. Useful in collecting supplementary data\n",
      "\n",
      "General statistics of data, instructions on how to read the data in python, and basic exploratory analysis could be found at [this GitHub repo](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection). A hybrid NN architecture trained on this dataset can be found at [this GitHub repo](https://github.com/rishabhmisra/Sarcasm-Detection-using-NN).\n",
      "\n",
      "### Citation\n",
      "\n",
      "If you're using this dataset for your work, please cite the following articles:\n",
      "\n",
      "Citation in text format:\n",
      "```\n",
      "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
      "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
      "```\n",
      "Citation in BibTex format:\n",
      "```\n",
      "@article{misra2023Sarcasm,\n",
      "  title = {Sarcasm Detection using News Headlines Dataset},\n",
      "  journal = {AI Open},\n",
      "  volume = {4},\n",
      "  pages = {13-18},\n",
      "  year = {2023},\n",
      "  issn = {2666-6510},\n",
      "  doi = {https://doi.org/10.1016/j.aiopen.2023.01.001},\n",
      "  url = {https://www.sciencedirect.com/science/article/pii/S2666651023000013},\n",
      "  author = {Rishabh Misra and Prahal Arora},\n",
      "}\n",
      "@book{misra2021sculpting,\n",
      "  author = {Misra, Rishabh and Grover, Jigyasa},\n",
      "  year = {2021},\n",
      "  month = {01},\n",
      "  pages = {},\n",
      "  title = {Sculpting Data for ML: The first act of Machine Learning},\n",
      "  isbn = {9798585463570}\n",
      "}\n",
      "```\n",
      "Please link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Can you identify sarcastic sentences? Can you distinguish between fake news and legitimate news?\n",
      "\n",
      "### Reading the data\n",
      "Following code snippet could be used to read the data:\n",
      "\n",
      "```\n",
      "import json\n",
      "\n",
      "def parse_data(file):\n",
      "    for l in open(file,'r'):\n",
      "        yield json.loads(l)\n",
      "\n",
      "data = list(parse_data('./Sarcasm_Headlines_Dataset.json'))\n",
      "```\n",
      "\n",
      "### Want to contribute your own datasets?\n",
      "\n",
      "If you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n",
      "\n",
      "### Other datasets\n",
      "Please also checkout the following datasets collected by me:\n",
      "\n",
      "* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n",
      "\n",
      "* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n",
      "\n",
      "* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
      "\n",
      "* [Politifact Fact Check Dataset](https://www.kaggle.com/datasets/rmisra/politifact-fact-check-dataset)\n",
      "name 'detect' is not defined Language is not detected: sidharthareddy/a-contentbased-video-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: a-contentbased-video-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sidharthareddy\n",
      "name 'detect' is not defined Language is not detected: A content-based video recommendation system&nbsp;\n",
      "name 'detect' is not defined Language is not detected: a-contentbased-video-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sidharthareddy\n",
      "name 'detect' is not defined Language is not detected: A content-based video recommendation system&nbsp;\n",
      "name 'detect' is not defined Language is not detected: micheldc55/anual-salary-reports-survey\n",
      "name 'detect' is not defined Language is not detected: anual-salary-reports-survey\n",
      "name 'detect' is not defined Language is not detected: micheldc55\n",
      "name 'detect' is not defined Language is not detected: Anual Salary reports survey\n",
      "name 'detect' is not defined Language is not detected: Simulated salaries for Data Analysis\n",
      "name 'detect' is not defined Language is not detected: Simulated salaries to run correlations between salaries and ages. Used for a class that I taught this year, though it would be interesting to see what predictions you can generate based on the data available! If you have any interesting ideas, please do not hesitate to contact me!\n",
      "\n",
      "The file was created using a \";\" separator, so be sure to use that if you are using pandas for reading the file!\n",
      "name 'detect' is not defined Language is not detected: anual-salary-reports-survey\n",
      "name 'detect' is not defined Language is not detected: micheldc55\n",
      "name 'detect' is not defined Language is not detected: Anual Salary reports survey\n",
      "name 'detect' is not defined Language is not detected: Simulated salaries for Data Analysis\n",
      "name 'detect' is not defined Language is not detected: Simulated salaries to run correlations between salaries and ages. Used for a class that I taught this year, though it would be interesting to see what predictions you can generate based on the data available! If you have any interesting ideas, please do not hesitate to contact me!\n",
      "\n",
      "The file was created using a \";\" separator, so be sure to use that if you are using pandas for reading the file!\n",
      "name 'detect' is not defined Language is not detected: aarzookuhar/hotel-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: hotel-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: aarzookuhar\n",
      "name 'detect' is not defined Language is not detected: Hotel Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: hotel-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: aarzookuhar\n",
      "name 'detect' is not defined Language is not detected: Hotel Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: mukund23/hackerearth-machine-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: hackerearth-machine-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: mukund23\n",
      "name 'detect' is not defined Language is not detected: Hackerearth Machine Learning Challenge\n",
      "name 'detect' is not defined Language is not detected: Predict the Power(Kw/h) produced by Windmills\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "It is the year 2021 and we are at the verge of a massive climatic change. With global warming at its peak and fossil fuels inching towards its extinction, it is the need of the hour to step up and take responsibility for our planet. Developing countries all over the world are making a shift towards a cleaner energy source and are looking at ways to expand their global energy source power. \n",
      "\n",
      "Switching to renewable energy sources is a great way to reduce dependency on imported fuels and increase cost efficiency. It is time we move towards a low-carbon future by embracing solar, hydro, geothermal energy and so on, to protect mother nature.\n",
      "\n",
      "An efficient energy source that has been gaining popularity around the world is wind turbines. Wind turbines generate power by capturing the kinetic energy of the wind. Factors such as temperature, wind direction, turbine status, weather, blade length, and so on influence the amount of power generated.\n",
      "\n",
      "### Task\n",
      "\n",
      "You are appointed by an environmentalist for their Non Government Organization as a climate warrior who comes to the rescue. Your task is to build a sophisticated Machine Learning model that predicts the power that is generated (in KW/h) based on the various features provided in the dataset.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: hackerearth-machine-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: mukund23\n",
      "name 'detect' is not defined Language is not detected: Hackerearth Machine Learning Challenge\n",
      "name 'detect' is not defined Language is not detected: Predict the Power(Kw/h) produced by Windmills\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "It is the year 2021 and we are at the verge of a massive climatic change. With global warming at its peak and fossil fuels inching towards its extinction, it is the need of the hour to step up and take responsibility for our planet. Developing countries all over the world are making a shift towards a cleaner energy source and are looking at ways to expand their global energy source power. \n",
      "\n",
      "Switching to renewable energy sources is a great way to reduce dependency on imported fuels and increase cost efficiency. It is time we move towards a low-carbon future by embracing solar, hydro, geothermal energy and so on, to protect mother nature.\n",
      "\n",
      "An efficient energy source that has been gaining popularity around the world is wind turbines. Wind turbines generate power by capturing the kinetic energy of the wind. Factors such as temperature, wind direction, turbine status, weather, blade length, and so on influence the amount of power generated.\n",
      "\n",
      "### Task\n",
      "\n",
      "You are appointed by an environmentalist for their Non Government Organization as a climate warrior who comes to the rescue. Your task is to build a sophisticated Machine Learning model that predicts the power that is generated (in KW/h) based on the various features provided in the dataset.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: akhilups/insurance-product-purchase-prediction\n",
      "name 'detect' is not defined Language is not detected: insurance-product-purchase-prediction\n",
      "name 'detect' is not defined Language is not detected: akhilups\n",
      "name 'detect' is not defined Language is not detected: Insurance Product Purchase Prediction\n",
      "name 'detect' is not defined Language is not detected: Prediction based on shopping history\n",
      "name 'detect' is not defined Language is not detected: The training data contain transaction history for customers that ended up purchasing a policy. For each customer_ID, you are given their quote history. In the training set you have the entire quote history, the last row of which contains the coverage options they purchased. \n",
      "\n",
      "What is a customer?\n",
      "\n",
      "Each customer has many shopping points, where a shopping point is defined by a customer with certain characteristics viewing a product and its associated cost at a particular time.\n",
      "•\tSome customer characteristics may change over time (e.g. as the customer changes or provides new information), and the cost depends on both the product and the customer characteristics.\n",
      "•\tA customer may represent a collection of people, as policies can cover more than one person.\n",
      "•\tA customer may purchase a product that was not viewed!\n",
      "Product Options\n",
      "Each product has 7 customizable options selected by customers, each with 2, 3, or 4 ordinal values possible:\n",
      " \n",
      "A product is simply a vector with length 7 whose values are chosen from each of the options listed above. The cost of a product is a function of both the product options and customer characteristics.\n",
      "\n",
      "Variable Descriptions\n",
      "\n",
      "customer_ID - A unique identifier for the customer\n",
      "shopping_pt - Unique identifier for the shopping point of a given customer\n",
      "record_type - 0=shopping point, 1=purchase point\n",
      "day - Day of the week (0-6, 0=Monday)\n",
      "time - Time of day (HH:MM)\n",
      "state - State where shopping point occurred\n",
      "location - Location ID where shopping point occurred\n",
      "group_size - How many people will be covered under the policy (1, 2, 3 or 4)\n",
      "homeowner - Whether the customer owns a home or not (0=no, 1=yes)\n",
      "car_age - Age of the customer’s car\n",
      "car_value - How valuable was the customer’s car when new\n",
      "risk_factor - An ordinal assessment of how risky the customer is (1, 2, 3, 4)\n",
      "age_oldest - Age of the oldest person in customer's group\n",
      "age_youngest - Age of the youngest person in customer’s group\n",
      "married_couple - Does the customer group contain a married couple (0=no, 1=yes)\n",
      "C_previous - What the customer formerly had or currently has for product option C (0=nothing, 1, 2, 3,4)\n",
      "duration_previous -  how long (in years) the customer was covered by their previous issuer\n",
      "A,B,C,D,E,F,G - the coverage options\n",
      "cost - cost of the quoted coverage options\n",
      "\n",
      "name 'detect' is not defined Language is not detected: insurance-product-purchase-prediction\n",
      "name 'detect' is not defined Language is not detected: akhilups\n",
      "name 'detect' is not defined Language is not detected: Insurance Product Purchase Prediction\n",
      "name 'detect' is not defined Language is not detected: Prediction based on shopping history\n",
      "name 'detect' is not defined Language is not detected: The training data contain transaction history for customers that ended up purchasing a policy. For each customer_ID, you are given their quote history. In the training set you have the entire quote history, the last row of which contains the coverage options they purchased. \n",
      "\n",
      "What is a customer?\n",
      "\n",
      "Each customer has many shopping points, where a shopping point is defined by a customer with certain characteristics viewing a product and its associated cost at a particular time.\n",
      "•\tSome customer characteristics may change over time (e.g. as the customer changes or provides new information), and the cost depends on both the product and the customer characteristics.\n",
      "•\tA customer may represent a collection of people, as policies can cover more than one person.\n",
      "•\tA customer may purchase a product that was not viewed!\n",
      "Product Options\n",
      "Each product has 7 customizable options selected by customers, each with 2, 3, or 4 ordinal values possible:\n",
      " \n",
      "A product is simply a vector with length 7 whose values are chosen from each of the options listed above. The cost of a product is a function of both the product options and customer characteristics.\n",
      "\n",
      "Variable Descriptions\n",
      "\n",
      "customer_ID - A unique identifier for the customer\n",
      "shopping_pt - Unique identifier for the shopping point of a given customer\n",
      "record_type - 0=shopping point, 1=purchase point\n",
      "day - Day of the week (0-6, 0=Monday)\n",
      "time - Time of day (HH:MM)\n",
      "state - State where shopping point occurred\n",
      "location - Location ID where shopping point occurred\n",
      "group_size - How many people will be covered under the policy (1, 2, 3 or 4)\n",
      "homeowner - Whether the customer owns a home or not (0=no, 1=yes)\n",
      "car_age - Age of the customer’s car\n",
      "car_value - How valuable was the customer’s car when new\n",
      "risk_factor - An ordinal assessment of how risky the customer is (1, 2, 3, 4)\n",
      "age_oldest - Age of the oldest person in customer's group\n",
      "age_youngest - Age of the youngest person in customer’s group\n",
      "married_couple - Does the customer group contain a married couple (0=no, 1=yes)\n",
      "C_previous - What the customer formerly had or currently has for product option C (0=nothing, 1, 2, 3,4)\n",
      "duration_previous -  how long (in years) the customer was covered by their previous issuer\n",
      "A,B,C,D,E,F,G - the coverage options\n",
      "cost - cost of the quoted coverage options\n",
      "\n",
      "name 'detect' is not defined Language is not detected: prognosticshse/preventive-to-predicitve-maintenance\n",
      "name 'detect' is not defined Language is not detected: preventive-to-predicitve-maintenance\n",
      "name 'detect' is not defined Language is not detected: prognosticshse\n",
      "name 'detect' is not defined Language is not detected: Preventive to Predictive Maintenance\n",
      "name 'detect' is not defined Language is not detected: Extensive data set for machine learning on remaining useful life prognosis (PHM)\n",
      "name 'detect' is not defined Language is not detected: **Context:**\n",
      "This data set originates from a practice-relevant degradation process, which is representative for Prognostics and Health Management (PHM) applications. The observed degradation process is the clogging of filters when separating of solid particles from gas. A test bench is used for this purpose, which performs automated life testing of filter media by loading them. For testing, dust complying with ISO standard 12103-1 and with a known particle size distribution is employed. The employed filter media is made of randomly oriented non-woven fibre material. \n",
      "Further data sets are generated for various practice-relevant data situations which do not correspond to the ideal conditions of full data coverage. These data sets are uploaded to Kaggle by the user \"Prognostics @ HSE\" in a continuous process. In order to avoid the carryover between two data sets, a different configuration of the filter tests is used for each uploaded practice-relevant data situation, for example by selecting a different filter media.\n",
      " \n",
      "**Detailed specification:**\n",
      "For more information about the general operation and the components used, see the provided description file *Preventive to Predicitve Maintenance dataset.pdf*\n",
      "\n",
      "**Given data situation:**\n",
      "The data set *Preventive to Predicitve Maintenance* is about the transition of a preventive maintenance strategy to a predictive maintenance strategy of a replaceable part, in this case a filter. To aid the realisation of predictive maintenance, life cycles have already been recorded from the application studied. However, the preventive maintenance in place so far causes them to be replaced after a fixed period of time, regardless of the condition of the degrading part. As a result, the end of life is not known for most records and thus they are right-censored. The so given training data are recorded runs of the filter up to a periodic replacement interval. \n",
      "When specifying the interval length for preventive maintenance, a trade-off has to be made between wasted life and the frequency of unplanned downtimes that occur, when having a particularly short life. The interval here is chosen so that, on average, failure is observed at the shortest 10% of the filter lives in the training data. The other lives are censored. The filter failure occurs when the differential pressure across the filter exceeds 600 Pa. The maintenance interval length depends on the amount of dust fed in per time, which is constant within a test run. For example, at twice the dust feed, the maintenance interval is half as long. The same relationship therefore applies to the respective censoring time, which scales inversely proportional with the particle feed. The variations between lifetimes are therefore primarily based on the type of dust, the flow rate and manufacturing tolerances. The filter medium CC 600 G was used exclusively for these measurement samples, which are included in this data set.\n",
      "\n",
      "**Task:**\n",
      "The objective of the data set is to precisely predict the remaining useful life (RUL) of the filter for the given test data, so a transition to predictive maintenance is made possible. For this purpose, the dataset contains training and test data, consisting both of 50 life tests respectively. The test data contains randomly right-censored run-to-failure measurements and the respective RUL as a ground truth to the prediction task. The main challenge is how to make the most use of the right-censored life data within the training data.\n",
      "Due to the detailed description of the setup and the various physical filter models described in literature, it is possible to support the actual data-driven models by integrating physical knowledge respectively models in the sense of theory-guided data science or informed machine learning (various names are common).\n",
      "\n",
      "**Acknowledgement:**\n",
      "Thanks go to Marc Hönig (Scientific Employee), Marcel Braig (Scientific Employee) and Christopher Rein (Research Assistant) for contributing to the recording of these life tests. \n",
      "\n",
      "**Data set Creator:**\n",
      "Hochschule Esslingen - University of Applied Sciences\n",
      "Research Department Reliability Engineering and Prognostics and Health Management\n",
      "Robert-Bosch-Straße 1\n",
      "73037 Göppingen\n",
      "Germany\n",
      "\n",
      "**Dataset Citation:**\n",
      "Hagmeyer, S., Mauthe, F., & Zeiler, P. (2021). Creation of Publicly Available Data Sets for Prognostics and Diagnostics Addressing Data Scenarios Relevant to Industrial Applications. *International Journal of Prognostics and Health Management*, Volume 12, Issue 2, DOI: 10.36001/ijphm.2021.v12i2.3087\n",
      "name 'detect' is not defined Language is not detected: preventive-to-predicitve-maintenance\n",
      "name 'detect' is not defined Language is not detected: prognosticshse\n",
      "name 'detect' is not defined Language is not detected: Preventive to Predictive Maintenance\n",
      "name 'detect' is not defined Language is not detected: Extensive data set for machine learning on remaining useful life prognosis (PHM)\n",
      "name 'detect' is not defined Language is not detected: **Context:**\n",
      "This data set originates from a practice-relevant degradation process, which is representative for Prognostics and Health Management (PHM) applications. The observed degradation process is the clogging of filters when separating of solid particles from gas. A test bench is used for this purpose, which performs automated life testing of filter media by loading them. For testing, dust complying with ISO standard 12103-1 and with a known particle size distribution is employed. The employed filter media is made of randomly oriented non-woven fibre material. \n",
      "Further data sets are generated for various practice-relevant data situations which do not correspond to the ideal conditions of full data coverage. These data sets are uploaded to Kaggle by the user \"Prognostics @ HSE\" in a continuous process. In order to avoid the carryover between two data sets, a different configuration of the filter tests is used for each uploaded practice-relevant data situation, for example by selecting a different filter media.\n",
      " \n",
      "**Detailed specification:**\n",
      "For more information about the general operation and the components used, see the provided description file *Preventive to Predicitve Maintenance dataset.pdf*\n",
      "\n",
      "**Given data situation:**\n",
      "The data set *Preventive to Predicitve Maintenance* is about the transition of a preventive maintenance strategy to a predictive maintenance strategy of a replaceable part, in this case a filter. To aid the realisation of predictive maintenance, life cycles have already been recorded from the application studied. However, the preventive maintenance in place so far causes them to be replaced after a fixed period of time, regardless of the condition of the degrading part. As a result, the end of life is not known for most records and thus they are right-censored. The so given training data are recorded runs of the filter up to a periodic replacement interval. \n",
      "When specifying the interval length for preventive maintenance, a trade-off has to be made between wasted life and the frequency of unplanned downtimes that occur, when having a particularly short life. The interval here is chosen so that, on average, failure is observed at the shortest 10% of the filter lives in the training data. The other lives are censored. The filter failure occurs when the differential pressure across the filter exceeds 600 Pa. The maintenance interval length depends on the amount of dust fed in per time, which is constant within a test run. For example, at twice the dust feed, the maintenance interval is half as long. The same relationship therefore applies to the respective censoring time, which scales inversely proportional with the particle feed. The variations between lifetimes are therefore primarily based on the type of dust, the flow rate and manufacturing tolerances. The filter medium CC 600 G was used exclusively for these measurement samples, which are included in this data set.\n",
      "\n",
      "**Task:**\n",
      "The objective of the data set is to precisely predict the remaining useful life (RUL) of the filter for the given test data, so a transition to predictive maintenance is made possible. For this purpose, the dataset contains training and test data, consisting both of 50 life tests respectively. The test data contains randomly right-censored run-to-failure measurements and the respective RUL as a ground truth to the prediction task. The main challenge is how to make the most use of the right-censored life data within the training data.\n",
      "Due to the detailed description of the setup and the various physical filter models described in literature, it is possible to support the actual data-driven models by integrating physical knowledge respectively models in the sense of theory-guided data science or informed machine learning (various names are common).\n",
      "\n",
      "**Acknowledgement:**\n",
      "Thanks go to Marc Hönig (Scientific Employee), Marcel Braig (Scientific Employee) and Christopher Rein (Research Assistant) for contributing to the recording of these life tests. \n",
      "\n",
      "**Data set Creator:**\n",
      "Hochschule Esslingen - University of Applied Sciences\n",
      "Research Department Reliability Engineering and Prognostics and Health Management\n",
      "Robert-Bosch-Straße 1\n",
      "73037 Göppingen\n",
      "Germany\n",
      "\n",
      "**Dataset Citation:**\n",
      "Hagmeyer, S., Mauthe, F., & Zeiler, P. (2021). Creation of Publicly Available Data Sets for Prognostics and Diagnostics Addressing Data Scenarios Relevant to Industrial Applications. *International Journal of Prognostics and Health Management*, Volume 12, Issue 2, DOI: 10.36001/ijphm.2021.v12i2.3087\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10/amazon-book-dataset30000-books-with-30-category\n",
      "name 'detect' is not defined Language is not detected: amazon-book-dataset30000-books-with-30-category\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10\n",
      "name 'detect' is not defined Language is not detected: Amazon Book Dataset:30000+ books with 30+ category\n",
      "name 'detect' is not defined Language is not detected: Enhance Projects: 30K+ Books, 30+ Categories Fuel your Recommendation System.\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F17934690%2F9bd907d66e84734e234f80d170fda39b%2Famazon%20book.jpg?generation=1702711304028317&alt=media)\n",
      "\n",
      "\n",
      "**Context:**\n",
      "Introducing Amazon Book Explorer: A meticulously crafted dataset featuring 30,000+ books spanning 30+ diverse categories. Your ultimate companion for insightful analysis and groundbreaking projects on Kaggle.\n",
      "\n",
      "**Inspirations for Amazon Book Dataset:**\n",
      "\n",
      "1. **Personalized Recommendations:** Develop algorithms for tailored book suggestions.\n",
      "2. **Genre Trends Analysis:** Uncover insights into the popularity and dynamics of various literary genres.\n",
      "3. **Educational Insights:** Facilitate data-driven study guides, enhancing learning experiences.\n",
      "4. **Market Intelligence:** Guide publishers and sellers with trends on what readers crave.\n",
      "5. **Innovation in ML:** Fuel projects in machine learning, particularly in recommendation systems.\n",
      "6. **Narrative Analytics:** Explore the storytelling landscape, discerning patterns and preferences.\n",
      "7. **Dynamic Price Analysis:** Investigate pricing strategies correlated with book popularity.\n",
      "8. **Genre-Based Exploration:** Tailor exploration to specific categories for nuanced insights.\n",
      "9. **Collaborative Filtering:** Pioneer collaborative recommendation systems for enriched results.\n",
      "10. **Text Mining:** Extract valuable information from book names and authors for deeper analysis.\n",
      "\n",
      "Unleash the power of Amazon Book Explorer for a myriad of impactful and creative endeavors!\n",
      "\n",
      "#### **Books Categories** \n",
      "\n",
      "1. Action & Adventure\n",
      "2. Arts, Film & Photography\n",
      "3. Biographies, Diaries & True Accounts\n",
      "4. Business & Economics\n",
      "5. Children's Books\n",
      "6. Comics & Graphic Novels\n",
      "7. Computers & Internet\n",
      "8. Crafts, Hobbies & Home\n",
      "9. Crime, Thriller & Mystery\n",
      "10. Engineering\n",
      "11. Exam Preparation\n",
      "12. Health, Family & Personal Development\n",
      "13. Health, Fitness & Nutrition\n",
      "14. Humour\n",
      "15. Historical Fiction\n",
      "16. History\n",
      "17. Language, Linguistics & Writing\n",
      "18. Law\n",
      "19. Literature & Fiction\n",
      "20. Medicine & Health Sciences\n",
      "21. Politics\n",
      "22. Reference\n",
      "23. Religion & Spirituality\n",
      "24. Romance\n",
      "25. School Books\n",
      "26. Science & Mathematics\n",
      "27. Science Fiction & Fantasy\n",
      "28. Sciences, Technology & Medicine\n",
      "29. Society & Social Sciences\n",
      "30. Teen & Young Adult\n",
      "31. Sports\n",
      "32. Textbooks & Study Guides\n",
      "33. Travel & Tourism\n",
      "\n",
      "\n",
      "**Data Collection methodology**\n",
      "\n",
      "1.**Scraping Adventure Begins:**\n",
      "Utilized Beautiful Soup in Python on Google Colab for Amazon book website.\n",
      "\n",
      "2.**Diverse Categories Unveiled:**\n",
      "Scoped each category, extracting rich data like book names, authors, and more.\n",
      "\n",
      "3.**Data Harvesting in Batches:**\n",
      "Executed scraping iteratively, ensuring all categories contribute to the dataset.\n",
      "\n",
      "4.**Merge and Unify:**\n",
      "Consolidated individual category datasets into a harmonious single CSV format.\n",
      "\n",
      "5.**CSV Creation for Clarity:**\n",
      "Organized data into a CSV file for seamless accessibility and future analysis.\n",
      "\n",
      "6.**Verification and Cleaning:**\n",
      "Ensured dataset integrity by validating and cleaning entries for accuracy.\n",
      "\n",
      "7.**Ready for Exploration:**\n",
      "Completed the process, presenting a comprehensive Amazon Books dataset.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: amazon-book-dataset30000-books-with-30-category\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10\n",
      "name 'detect' is not defined Language is not detected: Amazon Book Dataset:30000+ books with 30+ category\n",
      "name 'detect' is not defined Language is not detected: Enhance Projects: 30K+ Books, 30+ Categories Fuel your Recommendation System.\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F17934690%2F9bd907d66e84734e234f80d170fda39b%2Famazon%20book.jpg?generation=1702711304028317&alt=media)\n",
      "\n",
      "\n",
      "**Context:**\n",
      "Introducing Amazon Book Explorer: A meticulously crafted dataset featuring 30,000+ books spanning 30+ diverse categories. Your ultimate companion for insightful analysis and groundbreaking projects on Kaggle.\n",
      "\n",
      "**Inspirations for Amazon Book Dataset:**\n",
      "\n",
      "1. **Personalized Recommendations:** Develop algorithms for tailored book suggestions.\n",
      "2. **Genre Trends Analysis:** Uncover insights into the popularity and dynamics of various literary genres.\n",
      "3. **Educational Insights:** Facilitate data-driven study guides, enhancing learning experiences.\n",
      "4. **Market Intelligence:** Guide publishers and sellers with trends on what readers crave.\n",
      "5. **Innovation in ML:** Fuel projects in machine learning, particularly in recommendation systems.\n",
      "6. **Narrative Analytics:** Explore the storytelling landscape, discerning patterns and preferences.\n",
      "7. **Dynamic Price Analysis:** Investigate pricing strategies correlated with book popularity.\n",
      "8. **Genre-Based Exploration:** Tailor exploration to specific categories for nuanced insights.\n",
      "9. **Collaborative Filtering:** Pioneer collaborative recommendation systems for enriched results.\n",
      "10. **Text Mining:** Extract valuable information from book names and authors for deeper analysis.\n",
      "\n",
      "Unleash the power of Amazon Book Explorer for a myriad of impactful and creative endeavors!\n",
      "\n",
      "#### **Books Categories** \n",
      "\n",
      "1. Action & Adventure\n",
      "2. Arts, Film & Photography\n",
      "3. Biographies, Diaries & True Accounts\n",
      "4. Business & Economics\n",
      "5. Children's Books\n",
      "6. Comics & Graphic Novels\n",
      "7. Computers & Internet\n",
      "8. Crafts, Hobbies & Home\n",
      "9. Crime, Thriller & Mystery\n",
      "10. Engineering\n",
      "11. Exam Preparation\n",
      "12. Health, Family & Personal Development\n",
      "13. Health, Fitness & Nutrition\n",
      "14. Humour\n",
      "15. Historical Fiction\n",
      "16. History\n",
      "17. Language, Linguistics & Writing\n",
      "18. Law\n",
      "19. Literature & Fiction\n",
      "20. Medicine & Health Sciences\n",
      "21. Politics\n",
      "22. Reference\n",
      "23. Religion & Spirituality\n",
      "24. Romance\n",
      "25. School Books\n",
      "26. Science & Mathematics\n",
      "27. Science Fiction & Fantasy\n",
      "28. Sciences, Technology & Medicine\n",
      "29. Society & Social Sciences\n",
      "30. Teen & Young Adult\n",
      "31. Sports\n",
      "32. Textbooks & Study Guides\n",
      "33. Travel & Tourism\n",
      "\n",
      "\n",
      "**Data Collection methodology**\n",
      "\n",
      "1.**Scraping Adventure Begins:**\n",
      "Utilized Beautiful Soup in Python on Google Colab for Amazon book website.\n",
      "\n",
      "2.**Diverse Categories Unveiled:**\n",
      "Scoped each category, extracting rich data like book names, authors, and more.\n",
      "\n",
      "3.**Data Harvesting in Batches:**\n",
      "Executed scraping iteratively, ensuring all categories contribute to the dataset.\n",
      "\n",
      "4.**Merge and Unify:**\n",
      "Consolidated individual category datasets into a harmonious single CSV format.\n",
      "\n",
      "5.**CSV Creation for Clarity:**\n",
      "Organized data into a CSV file for seamless accessibility and future analysis.\n",
      "\n",
      "6.**Verification and Cleaning:**\n",
      "Ensured dataset integrity by validating and cleaning entries for accuracy.\n",
      "\n",
      "7.**Ready for Exploration:**\n",
      "Completed the process, presenting a comprehensive Amazon Books dataset.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10/all-top-rated-imdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: all-top-rated-imdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10\n",
      "name 'detect' is not defined Language is not detected: All 8000+ Top Rated IMDB Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Enhance Your Recommendations System: IMDB's Best for Precise Movie Suggestions!\n",
      "name 'detect' is not defined Language is not detected: Context:\n",
      "\n",
      "I made this dataset for \"Unlock cinematic gems with a dataset featuring IMDB's top-rated movies, ensuring precise and exceptional movie recommendations for an unparalleled viewing&nbsp;experience.\"\n",
      "\n",
      "source:\n",
      "The dataset was collected from The Movie Database (TMDB) using a valid API key.\n",
      "The CSV data was scrape https://api.themoviedb.org/3/movie/top_rated/ by ensuring proper authorization to access  their database .\n",
      "\n",
      "The raw data obtained from API responses was processed to extract relevant information. This may include parsing JSON responses, handling pagination, and cleaning the data to ensure consistency.\n",
      "\n",
      "Inspiration:\n",
      "The inspiration behind making this dataset is that you can build a recommendation system for your project and you can also do EDA on this dataset and make your mini project.\n",
      "name 'detect' is not defined Language is not detected: all-top-rated-imdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: rajkumardubey10\n",
      "name 'detect' is not defined Language is not detected: All 8000+ Top Rated IMDB Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Enhance Your Recommendations System: IMDB's Best for Precise Movie Suggestions!\n",
      "name 'detect' is not defined Language is not detected: Context:\n",
      "\n",
      "I made this dataset for \"Unlock cinematic gems with a dataset featuring IMDB's top-rated movies, ensuring precise and exceptional movie recommendations for an unparalleled viewing&nbsp;experience.\"\n",
      "\n",
      "source:\n",
      "The dataset was collected from The Movie Database (TMDB) using a valid API key.\n",
      "The CSV data was scrape https://api.themoviedb.org/3/movie/top_rated/ by ensuring proper authorization to access  their database .\n",
      "\n",
      "The raw data obtained from API responses was processed to extract relevant information. This may include parsing JSON responses, handling pagination, and cleaning the data to ensure consistency.\n",
      "\n",
      "Inspiration:\n",
      "The inspiration behind making this dataset is that you can build a recommendation system for your project and you can also do EDA on this dataset and make your mini project.\n",
      "name 'detect' is not defined Language is not detected: /national-accounts\n",
      "name 'detect' is not defined Language is not detected: national-accounts\n",
      "name 'detect' is not defined Language is not detected: National Accounts\n",
      "name 'detect' is not defined Language is not detected: Global GDP & Government Expenditures since 1946\n",
      "name 'detect' is not defined Language is not detected: The Economic Statistics Branch of the United Nations Statistics Division (UNSD) maintains and annually updates the National Accounts Official Country Data database. This work is carried out in accordance with the recommendation of the Statistical Commission at its first session that the Statistics Division of the United Nations should publish regularly the most recent available data on national accounts for as many countries and areas as possible. The database contains detailed official national accounts statistics in national currencies as provided by the National Statistical Offices. \n",
      "\n",
      "Data are available for most of the countries or areas of the world and form a valuable source of information on their economies. The database contains data as far back as 1946, up to the year t-1, with data for most countries available from the 1970s. The database covers not only national accounts main aggregates such as gross domestic product, national income, saving, value added by industry and household and government consumption expenditure and its relationships; but also detailed statistics for institutional sectors (including the rest of the world), comprising the production account, the generation of income account, the allocation of primary income account, the secondary distribution of income account, the use of disposable income account, the capital account and the financial account, if they are compiled by countries. \n",
      "\n",
      "The statistics for each country or area are presented according to the uniform table headings and classifications as recommended in the United Nations System of National Accounts 1993 (1993 SNA). A summary of the 1993 SNA conceptual framework, classifications and definitions are included in the yearly publication “National Accounts Statistics, Main Aggregates and Detailed Tables”.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was kindly published by the United Nation on the UNData site. You can find [the original dataset here](http://data.un.org/Explorer.aspx).\n",
      "\n",
      "### License\n",
      "[Per the UNData terms of use](http://data.un.org/Host.aspx?Content=UNdataUse): all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that [UNdata](http://data.un.org/Explorer.aspx) is cited as the reference. \n",
      "name 'detect' is not defined Language is not detected: national-accounts\n",
      "name 'detect' is not defined Language is not detected: National Accounts\n",
      "name 'detect' is not defined Language is not detected: Global GDP & Government Expenditures since 1946\n",
      "name 'detect' is not defined Language is not detected: The Economic Statistics Branch of the United Nations Statistics Division (UNSD) maintains and annually updates the National Accounts Official Country Data database. This work is carried out in accordance with the recommendation of the Statistical Commission at its first session that the Statistics Division of the United Nations should publish regularly the most recent available data on national accounts for as many countries and areas as possible. The database contains detailed official national accounts statistics in national currencies as provided by the National Statistical Offices. \n",
      "\n",
      "Data are available for most of the countries or areas of the world and form a valuable source of information on their economies. The database contains data as far back as 1946, up to the year t-1, with data for most countries available from the 1970s. The database covers not only national accounts main aggregates such as gross domestic product, national income, saving, value added by industry and household and government consumption expenditure and its relationships; but also detailed statistics for institutional sectors (including the rest of the world), comprising the production account, the generation of income account, the allocation of primary income account, the secondary distribution of income account, the use of disposable income account, the capital account and the financial account, if they are compiled by countries. \n",
      "\n",
      "The statistics for each country or area are presented according to the uniform table headings and classifications as recommended in the United Nations System of National Accounts 1993 (1993 SNA). A summary of the 1993 SNA conceptual framework, classifications and definitions are included in the yearly publication “National Accounts Statistics, Main Aggregates and Detailed Tables”.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset was kindly published by the United Nation on the UNData site. You can find [the original dataset here](http://data.un.org/Explorer.aspx).\n",
      "\n",
      "### License\n",
      "[Per the UNData terms of use](http://data.un.org/Host.aspx?Content=UNdataUse): all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that [UNdata](http://data.un.org/Explorer.aspx) is cited as the reference. \n",
      "name 'detect' is not defined Language is not detected: balakrishcodes/others\n",
      "name 'detect' is not defined Language is not detected: others\n",
      "name 'detect' is not defined Language is not detected: balakrishcodes\n",
      "name 'detect' is not defined Language is not detected: Collection of Classification & Regression Datasets\n",
      "name 'detect' is not defined Language is not detected: This dataset encompasses my collection of classification and regression\n",
      "name 'detect' is not defined Language is not detected: This dataset encompasses my personal collection of classification and regression.\n",
      "\n",
      "File list -\n",
      "\n",
      "1. Customer.csv\n",
      "2. House_Price.csv\n",
      "3. Movie_classification.csv\n",
      "4. Movie_regression.xls\n",
      "5. canada_per_capita.csv\n",
      "6. carprices.csv\n",
      "7. daily-min-temperatures.csv\n",
      "8. daily-total-female-births-CA.csv\n",
      "9. hiring.csv\n",
      "10.house-votes-84.csv\n",
      "11. insurance_data.csv\n",
      "12. nih_labels.csv\n",
      "13. salaries.csv\n",
      "14. shampoo.csv\n",
      "15. us-airlines-monthly-aircraft-miles-flown.csv\n",
      "16. xrayfull.csv\n",
      "name 'detect' is not defined Language is not detected: others\n",
      "name 'detect' is not defined Language is not detected: balakrishcodes\n",
      "name 'detect' is not defined Language is not detected: Collection of Classification & Regression Datasets\n",
      "name 'detect' is not defined Language is not detected: This dataset encompasses my collection of classification and regression\n",
      "name 'detect' is not defined Language is not detected: This dataset encompasses my personal collection of classification and regression.\n",
      "\n",
      "File list -\n",
      "\n",
      "1. Customer.csv\n",
      "2. House_Price.csv\n",
      "3. Movie_classification.csv\n",
      "4. Movie_regression.xls\n",
      "5. canada_per_capita.csv\n",
      "6. carprices.csv\n",
      "7. daily-min-temperatures.csv\n",
      "8. daily-total-female-births-CA.csv\n",
      "9. hiring.csv\n",
      "10.house-votes-84.csv\n",
      "11. insurance_data.csv\n",
      "12. nih_labels.csv\n",
      "13. salaries.csv\n",
      "14. shampoo.csv\n",
      "15. us-airlines-monthly-aircraft-miles-flown.csv\n",
      "16. xrayfull.csv\n",
      "name 'detect' is not defined Language is not detected: quentinmcteer/indiegogo-crowdfunding-data\n",
      "name 'detect' is not defined Language is not detected: indiegogo-crowdfunding-data\n",
      "name 'detect' is not defined Language is not detected: quentinmcteer\n",
      "name 'detect' is not defined Language is not detected: Indiegogo Crowdfunding Campaigns\n",
      "name 'detect' is not defined Language is not detected: Compilation of roughly 22,000 Indiegogo campaigns scraped by Web Robots.io\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This is a mostly clean dataset that includes 22,000 Indiegogo crowdfunding campaigns between 2011-2020. Note that it is not a complete compilation of all Indiegogo campaigns during this time frame, just a sample. Using the original data, I created features by month, category, and country/geography. Additionally, I added a 'state' column that indicates whether or not the campaign was fully funded (i.e. was successful in achieving its goal). Finally, there are many other characteristic columns that qualify that type of campaign, including text data describing each Indiegogo project.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This csv was created using publicly available data housed under WebRobots.io. Web Robots is an IT firm based in Lithuania that is working on next-generation web crawling technologies. The Indiegogo data posted here is a cleaned-up version of an early 2021 Indiegogo web scraping project that the company put together. You can find a link to their original JSON files here: https://webrobots.io/indiegogo-dataset/\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to the amazing Redditors that pointed me to Web Robots, the organization is doing great work at the forefront of data collection and web scraping. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Would love to see some cool EDA that breaks down the data by category, geography, and time (year, month, day). Also, it wouldn't be Kaggle data if you couldn't build a model. I would enjoy seeing some attempts to predict the likelihood of campaign success, similar to what is done with the popular Kickstarter dataset on this platform. Finally, I would like to see some NLP analysis done using the text data that describes each Indiegogo campaign in the dataset.\n",
      "name 'detect' is not defined Language is not detected: indiegogo-crowdfunding-data\n",
      "name 'detect' is not defined Language is not detected: quentinmcteer\n",
      "name 'detect' is not defined Language is not detected: Indiegogo Crowdfunding Campaigns\n",
      "name 'detect' is not defined Language is not detected: Compilation of roughly 22,000 Indiegogo campaigns scraped by Web Robots.io\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This is a mostly clean dataset that includes 22,000 Indiegogo crowdfunding campaigns between 2011-2020. Note that it is not a complete compilation of all Indiegogo campaigns during this time frame, just a sample. Using the original data, I created features by month, category, and country/geography. Additionally, I added a 'state' column that indicates whether or not the campaign was fully funded (i.e. was successful in achieving its goal). Finally, there are many other characteristic columns that qualify that type of campaign, including text data describing each Indiegogo project.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This csv was created using publicly available data housed under WebRobots.io. Web Robots is an IT firm based in Lithuania that is working on next-generation web crawling technologies. The Indiegogo data posted here is a cleaned-up version of an early 2021 Indiegogo web scraping project that the company put together. You can find a link to their original JSON files here: https://webrobots.io/indiegogo-dataset/\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to the amazing Redditors that pointed me to Web Robots, the organization is doing great work at the forefront of data collection and web scraping. \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Would love to see some cool EDA that breaks down the data by category, geography, and time (year, month, day). Also, it wouldn't be Kaggle data if you couldn't build a model. I would enjoy seeing some attempts to predict the likelihood of campaign success, similar to what is done with the popular Kickstarter dataset on this platform. Finally, I would like to see some NLP analysis done using the text data that describes each Indiegogo campaign in the dataset.\n",
      "name 'detect' is not defined Language is not detected: akouaorsot/nietzsches-bibliography\n",
      "name 'detect' is not defined Language is not detected: nietzsches-bibliography\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Friedrich W. Nietzsche Bibliography\n",
      "name 'detect' is not defined Language is not detected: A text compilation of the most notable works of Friedrich W. Nietzsche \n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers, this is my first dataset, and I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in the pessimism of which a key thinker was Nietzsche. This dataset is a CSV file that contains the corpus of each of his most famous books from *Beyond Good and Evil* to *Thus Spoke Zarathustra*. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book-title,publishing_date, text, text_clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, Hello Fellow Kagglers, this is my first dataset, and I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in the pessimism of which a key thinker was Nietzsche. This dataset is a CSV file that contains the corpus of each of his most famous books from *Beyond Good and Evil* to *Thus Spoke Zarathustra*. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book-title,publishing_date, text, text_clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "1. An exploratory analysis on term frequency \n",
      "2. A word cloud of Nietzsche's ideas\n",
      "3. A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: nietzsches-bibliography\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Friedrich W. Nietzsche Bibliography\n",
      "name 'detect' is not defined Language is not detected: A text compilation of the most notable works of Friedrich W. Nietzsche \n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers, this is my first dataset, and I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in the pessimism of which a key thinker was Nietzsche. This dataset is a CSV file that contains the corpus of each of his most famous books from *Beyond Good and Evil* to *Thus Spoke Zarathustra*. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book-title,publishing_date, text, text_clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, Hello Fellow Kagglers, this is my first dataset, and I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in the pessimism of which a key thinker was Nietzsche. This dataset is a CSV file that contains the corpus of each of his most famous books from *Beyond Good and Evil* to *Thus Spoke Zarathustra*. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book-title,publishing_date, text, text_clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "1. An exploratory analysis on term frequency \n",
      "2. A word cloud of Nietzsche's ideas\n",
      "3. A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: akouaorsot/political-though-work-corpus\n",
      "name 'detect' is not defined Language is not detected: political-though-work-corpus\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Political & Philosophical Thought Bibliography\n",
      "name 'detect' is not defined Language is not detected: Over 13.6m Words and 1.8k Works from Human Thought for Modeling and Analysis\n",
      "name 'detect' is not defined Language is not detected: Please see the Discussion for updates and to voice your concerns and suggestions!\n",
      "\n",
      "In the vein to keep bringing good philosophical works, this dataset offers a different corpus focused on political thought. It contains the corpus of both pre-modern and modern times. Given the constant evolution of such a burgeoning field, I will try to keep the list updated on a weekly or bi-weekly basis adding more classic works or recent gems I found. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite. After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book title, publishing date, authors, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "* An exploratory analysis on term frequency\n",
      "* A word cloud of a specific author's ideas or the general themes among all authors\n",
      "* A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: political-though-work-corpus\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Political & Philosophical Thought Bibliography\n",
      "name 'detect' is not defined Language is not detected: Over 13.6m Words and 1.8k Works from Human Thought for Modeling and Analysis\n",
      "name 'detect' is not defined Language is not detected: Please see the Discussion for updates and to voice your concerns and suggestions!\n",
      "\n",
      "In the vein to keep bringing good philosophical works, this dataset offers a different corpus focused on political thought. It contains the corpus of both pre-modern and modern times. Given the constant evolution of such a burgeoning field, I will try to keep the list updated on a weekly or bi-weekly basis adding more classic works or recent gems I found. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite. After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book title, publishing date, authors, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "* An exploratory analysis on term frequency\n",
      "* A word cloud of a specific author's ideas or the general themes among all authors\n",
      "* A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: akouaorsot/schopenhauer-work-corpus\n",
      "name 'detect' is not defined Language is not detected: schopenhauer-work-corpus\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Arthur Schopenhauer Bibliography\n",
      "name 'detect' is not defined Language is not detected: Text Corpus from one of the most influential pessimist thinker.\n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers, I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in pessimism as a body of work, there is a key thinker like Schopenhauer. This dataset is a CSV file that contains the corpus of each of his most famous books from The World as Will (3 volumes) to his later works.. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book_title, publishing date, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "1- An exploratory analysis on term frequency\n",
      "2- A word cloud of Schopenhauer's ideas\n",
      "3- A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: schopenhauer-work-corpus\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Arthur Schopenhauer Bibliography\n",
      "name 'detect' is not defined Language is not detected: Text Corpus from one of the most influential pessimist thinker.\n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers, I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in pessimism as a body of work, there is a key thinker like Schopenhauer. This dataset is a CSV file that contains the corpus of each of his most famous books from The World as Will (3 volumes) to his later works.. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book_title, publishing date, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "1- An exploratory analysis on term frequency\n",
      "2- A word cloud of Schopenhauer's ideas\n",
      "3- A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: akouaorsot/empiricisms-thinkers\n",
      "name 'detect' is not defined Language is not detected: empiricisms-thinkers\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Empiricist Thinkers\n",
      "name 'detect' is not defined Language is not detected: Key works of prominent empiricist thinkers\n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers,  I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in empiricism as a school of thought. This dataset is a CSV file that contains the corpus of three key thinkers: John Locke, David Hume, and Georges Berkley. I do intend on adding more texts, especially from Kant, Hobbes, and the likes. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book title, publishing date, authors, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "1. An exploratory analysis on term frequency\n",
      "2. A word cloud of a specific author's ideas or the general themes among all authors\n",
      "3. A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: empiricisms-thinkers\n",
      "name 'detect' is not defined Language is not detected: akouaorsot\n",
      "name 'detect' is not defined Language is not detected: Empiricist Thinkers\n",
      "name 'detect' is not defined Language is not detected: Key works of prominent empiricist thinkers\n",
      "name 'detect' is not defined Language is not detected: Hello Fellow Kagglers,  I wanted to bring something at the intersection of what I like the most: philosophy and data. In that regard, this is for data and philosophy enthusiasts, more particularly interested in empiricism as a school of thought. This dataset is a CSV file that contains the corpus of three key thinkers: John Locke, David Hume, and Georges Berkley. I do intend on adding more texts, especially from Kant, Hobbes, and the likes. Though the initial intent was to have a Natural Language Processing task, it is yours to explore and be creative as the possibilities in data are infinite.\n",
      "\n",
      "After web scrapping the original texts, I created some functions to clean and tokenized them. So, you will find an auto-increment column and the four other columns as follows: book title, publishing date, authors, text, text clean.\n",
      "\n",
      "And above all, it is thanks to Project Gutenberg, a phenomenal platform for all book lovers and generally knowledge avid people, that I could obtain those texts at no cost. So, please support them in their continuous effort in making knowledge accessible: https://www.gutenberg.org/\n",
      "\n",
      "In the following bullet points, I would propose possible exploration routes but do not feel constrained to go above and beyond:\n",
      "\n",
      "1. An exploratory analysis on term frequency\n",
      "2. A word cloud of a specific author's ideas or the general themes among all authors\n",
      "3. A Recommendation system for someone wanting to read those books with an evolving string of ideas\n",
      "name 'detect' is not defined Language is not detected: satrapankti/amazon-beauty-product-recommendation\n",
      "name 'detect' is not defined Language is not detected: amazon-beauty-product-recommendation\n",
      "name 'detect' is not defined Language is not detected: satrapankti\n",
      "name 'detect' is not defined Language is not detected: Amazon Beauty Products\n",
      "name 'detect' is not defined Language is not detected: 2+ Million records Amazon Beauty Products Dataset along with URL.\n",
      "name 'detect' is not defined Language is not detected: Amazon relies heavily on a **Recommendation Engine** that reviews customer ratings and purchase history to recommend items and improve sales.\n",
      "This is a dataset related to over **2 Million+** customer reviews and ratings of Beauty related products sold on their website.\n",
      "•\tThe unique **UserId** (Customer Identification)\n",
      "•\tThe **ProductId** ASIN (Amazon's unique product identification code for each product)\n",
      "•\t**ProductType** (22 Unique Category of Products)\n",
      "•\t**Ratings** (ranging from 1-5 based on customer satisfaction) \n",
      "•\tThe **Timestamp** of the rating (in UNIX time)\n",
      "•\t**URL** link to the product directly on Amazon\n",
      "name 'detect' is not defined Language is not detected: amazon-beauty-product-recommendation\n",
      "name 'detect' is not defined Language is not detected: satrapankti\n",
      "name 'detect' is not defined Language is not detected: Amazon Beauty Products\n",
      "name 'detect' is not defined Language is not detected: 2+ Million records Amazon Beauty Products Dataset along with URL.\n",
      "name 'detect' is not defined Language is not detected: Amazon relies heavily on a **Recommendation Engine** that reviews customer ratings and purchase history to recommend items and improve sales.\n",
      "This is a dataset related to over **2 Million+** customer reviews and ratings of Beauty related products sold on their website.\n",
      "•\tThe unique **UserId** (Customer Identification)\n",
      "•\tThe **ProductId** ASIN (Amazon's unique product identification code for each product)\n",
      "•\t**ProductType** (22 Unique Category of Products)\n",
      "•\t**Ratings** (ranging from 1-5 based on customer satisfaction) \n",
      "•\tThe **Timestamp** of the rating (in UNIX time)\n",
      "•\t**URL** link to the product directly on Amazon\n",
      "name 'detect' is not defined Language is not detected: rohitpant/serendipites-article-dataset\n",
      "name 'detect' is not defined Language is not detected: serendipites-article-dataset\n",
      "name 'detect' is not defined Language is not detected: rohitpant\n",
      "name 'detect' is not defined Language is not detected: Serendipite's article dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset to practice recommendation algorithms\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset containing ratings by users for different articles.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "First dataset has columns with data on user , article and the ratings.\n",
      "Second dataset contains the attributes of the article, website name, article topic and a column with the complete article.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: serendipites-article-dataset\n",
      "name 'detect' is not defined Language is not detected: rohitpant\n",
      "name 'detect' is not defined Language is not detected: Serendipite's article dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset to practice recommendation algorithms\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset containing ratings by users for different articles.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "First dataset has columns with data on user , article and the ratings.\n",
      "Second dataset contains the attributes of the article, website name, article topic and a column with the complete article.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: gaurav896/weather-in-australia\n",
      "name 'detect' is not defined Language is not detected: weather-in-australia\n",
      "name 'detect' is not defined Language is not detected: gaurav896\n",
      "name 'detect' is not defined Language is not detected: Weather in Australia\n",
      "name 'detect' is not defined Language is not detected: Predict Rainfall in Australia - latest data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I was working on a project related to COVID-19 and Weather in Australia. Hence thought of uploading it as it may benefits others.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains daily weather observations from numerous Australian weather stations.\n",
      "\n",
      "The target variable RainTomorrow means: Did it rain the next day? Yes or No.\n",
      "\n",
      "You can also this data as binary classification model or regression model.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Observations were drawn from numerous weather stations. The daily observations are available from http://www.bom.gov.au/climate/data. Copyright Commonwealth of Australia 2010, Bureau of Meteorology.\n",
      "\n",
      "Definitions adapted from http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Inspiration came from Rattle Package as mentioned in the link.\n",
      "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
      "name 'detect' is not defined Language is not detected: weather-in-australia\n",
      "name 'detect' is not defined Language is not detected: gaurav896\n",
      "name 'detect' is not defined Language is not detected: Weather in Australia\n",
      "name 'detect' is not defined Language is not detected: Predict Rainfall in Australia - latest data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I was working on a project related to COVID-19 and Weather in Australia. Hence thought of uploading it as it may benefits others.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains daily weather observations from numerous Australian weather stations.\n",
      "\n",
      "The target variable RainTomorrow means: Did it rain the next day? Yes or No.\n",
      "\n",
      "You can also this data as binary classification model or regression model.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Observations were drawn from numerous weather stations. The daily observations are available from http://www.bom.gov.au/climate/data. Copyright Commonwealth of Australia 2010, Bureau of Meteorology.\n",
      "\n",
      "Definitions adapted from http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Inspiration came from Rattle Package as mentioned in the link.\n",
      "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
      "name 'detect' is not defined Language is not detected: jcraggy/baseball\n",
      "name 'detect' is not defined Language is not detected: baseball\n",
      "name 'detect' is not defined Language is not detected: jcraggy\n",
      "name 'detect' is not defined Language is not detected: Baseball\n",
      "name 'detect' is not defined Language is not detected: MLB Predict Home Runs\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Try to find the best predictors indicative of a home run, reported as log loss \n",
      "\n",
      "Log Loss quantifies the accuracy of a classifier by penalizing false classifications. \n",
      "Minimizing the Log Loss is equivalent to maximizing the accuracy of the classifier. \n",
      "\n",
      "### Content\n",
      "\n",
      "Data set adapted from SLICE Competition Season 01 Episode 09   https://www.kaggle.com/c/sliced-s01e09-playoffs-1\n",
      "\n",
      "These are only 2 hour competitions so time is limited. Here we can use the data set and take more time for analysis.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Adapted largely from David Robinson on YouTube. His modeling techniques are greatly appreciated in traversing the tidyverse()\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Try to find the best predictors indicative of a home run, reported as log loss\n",
      "\n",
      "Numeric predictors, categorical predictors, hybrid, how low can we go?\n",
      "\n",
      "\n",
      "# File descriptions\n",
      "* train.csv - the training set (from 2020)\n",
      "* test.csv - the test set (from 2021)\n",
      "* sample_submission.csv - a sample submission file in the correct format\n",
      "* park_dimensions.csv - various park details and dimensions (OPTIONAL)\n",
      "\n",
      "# Data dictionary\n",
      "## train.csv\n",
      "* bip_id: unique identifier of ball in play\n",
      "* game_date: date of game (YYYY-MM-DD)\n",
      "* home_team: home team abbreviation\n",
      "* away_team: away team abbreviation\n",
      "* batter_team: batter's team abbreviation\n",
      "* batter_name: batter's name\n",
      "* pitcher_name: pitcher's name\n",
      "* batter_id: batter's unique identifier\n",
      "* pitcher_id: pitcher's unique identifier\n",
      "* is_batter_lefty: binary encoding of left-handed batters\n",
      "* is_pitcher_lefty: binary encoding of left-handed pitchers\n",
      "* bb_type: batted ball type classification\n",
      "* bearing: horizontal direction classification of ball leaving the bat (i.e. 'left' ball is traveling to the left side of the field)\n",
      "* pitch_name: name of pitch type thrown\n",
      "* park: unique identifier of park venue\n",
      "* inning: inning number within game\n",
      "* outs_when_up: current number of outs\n",
      "* balls: current number of balls\n",
      "* strikes: current number of strikes\n",
      "* plate_x: ball position left(-) or right(+) of center plate (feet)\n",
      "* plate_z: ball position above home plate (feet)\n",
      "* pitch_mph: speed of pitched ball (miles per hour)\n",
      "* launch_speed: speed of ball leaving the bat (miles per hour)\n",
      "* launch_angle: vertical angle of ball leaving the bat (degrees relative to horizontal)\n",
      "* is_home_run: binary encoding of home runs\n",
      "\n",
      "## test.csv\n",
      "same as train.csv but without the target variable is_home_run\n",
      "\n",
      "## park_dimensions.csv\n",
      "* park: unique identifier of park venue\n",
      "* NAME: park name\n",
      "* Cover: designation of stadiums with retractable roof or fixed dome\n",
      "* LF_Dim: distance to left field wall (feet)\n",
      "* CF_Dim: distance to center field wall (feet)\n",
      "* RF_Dim: distance to right field wall (feet)\n",
      "* LF_W: height of left field wall (feet)\n",
      "* CF_W: height of center field wall (feet)\n",
      "* RF_W: height of right field wall (feet)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: baseball\n",
      "name 'detect' is not defined Language is not detected: jcraggy\n",
      "name 'detect' is not defined Language is not detected: Baseball\n",
      "name 'detect' is not defined Language is not detected: MLB Predict Home Runs\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Try to find the best predictors indicative of a home run, reported as log loss \n",
      "\n",
      "Log Loss quantifies the accuracy of a classifier by penalizing false classifications. \n",
      "Minimizing the Log Loss is equivalent to maximizing the accuracy of the classifier. \n",
      "\n",
      "### Content\n",
      "\n",
      "Data set adapted from SLICE Competition Season 01 Episode 09   https://www.kaggle.com/c/sliced-s01e09-playoffs-1\n",
      "\n",
      "These are only 2 hour competitions so time is limited. Here we can use the data set and take more time for analysis.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Adapted largely from David Robinson on YouTube. His modeling techniques are greatly appreciated in traversing the tidyverse()\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Try to find the best predictors indicative of a home run, reported as log loss\n",
      "\n",
      "Numeric predictors, categorical predictors, hybrid, how low can we go?\n",
      "\n",
      "\n",
      "# File descriptions\n",
      "* train.csv - the training set (from 2020)\n",
      "* test.csv - the test set (from 2021)\n",
      "* sample_submission.csv - a sample submission file in the correct format\n",
      "* park_dimensions.csv - various park details and dimensions (OPTIONAL)\n",
      "\n",
      "# Data dictionary\n",
      "## train.csv\n",
      "* bip_id: unique identifier of ball in play\n",
      "* game_date: date of game (YYYY-MM-DD)\n",
      "* home_team: home team abbreviation\n",
      "* away_team: away team abbreviation\n",
      "* batter_team: batter's team abbreviation\n",
      "* batter_name: batter's name\n",
      "* pitcher_name: pitcher's name\n",
      "* batter_id: batter's unique identifier\n",
      "* pitcher_id: pitcher's unique identifier\n",
      "* is_batter_lefty: binary encoding of left-handed batters\n",
      "* is_pitcher_lefty: binary encoding of left-handed pitchers\n",
      "* bb_type: batted ball type classification\n",
      "* bearing: horizontal direction classification of ball leaving the bat (i.e. 'left' ball is traveling to the left side of the field)\n",
      "* pitch_name: name of pitch type thrown\n",
      "* park: unique identifier of park venue\n",
      "* inning: inning number within game\n",
      "* outs_when_up: current number of outs\n",
      "* balls: current number of balls\n",
      "* strikes: current number of strikes\n",
      "* plate_x: ball position left(-) or right(+) of center plate (feet)\n",
      "* plate_z: ball position above home plate (feet)\n",
      "* pitch_mph: speed of pitched ball (miles per hour)\n",
      "* launch_speed: speed of ball leaving the bat (miles per hour)\n",
      "* launch_angle: vertical angle of ball leaving the bat (degrees relative to horizontal)\n",
      "* is_home_run: binary encoding of home runs\n",
      "\n",
      "## test.csv\n",
      "same as train.csv but without the target variable is_home_run\n",
      "\n",
      "## park_dimensions.csv\n",
      "* park: unique identifier of park venue\n",
      "* NAME: park name\n",
      "* Cover: designation of stadiums with retractable roof or fixed dome\n",
      "* LF_Dim: distance to left field wall (feet)\n",
      "* CF_Dim: distance to center field wall (feet)\n",
      "* RF_Dim: distance to right field wall (feet)\n",
      "* LF_W: height of left field wall (feet)\n",
      "* CF_W: height of center field wall (feet)\n",
      "* RF_W: height of right field wall (feet)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aslanahmedov/self-driving-carbehavioural-cloning\n",
      "name 'detect' is not defined Language is not detected: self-driving-carbehavioural-cloning\n",
      "name 'detect' is not defined Language is not detected: aslanahmedov\n",
      "name 'detect' is not defined Language is not detected: Self Driving Car\n",
      "name 'detect' is not defined Language is not detected: Behavioural Cloning Complete Guide\n",
      "name 'detect' is not defined Language is not detected: ![ezgif com-gif-maker](https://user-images.githubusercontent.com/91852182/147305077-8b86ec92-ed26-43ca-860c-5812fea9b1d8.gif)\n",
      "\n",
      "# SELF-DRIVING CAR USING UDACITY’S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE\n",
      "\n",
      "\n",
      "## Table of Contents\n",
      "### Introduction\n",
      "\n",
      "- Problem Definition\n",
      "- Solution Approach\n",
      "- Technologies Used\n",
      "- Convolutional Neural Networks (CNN)\n",
      "- Time-Distributed Layers\n",
      "\n",
      "### Udacity Simulator and Dataset\n",
      "### The Training Process\n",
      "### Augmentation and image pre-processing\n",
      "### Experimental configurations\n",
      "### Network architectures\n",
      "### Results\n",
      "\n",
      "- Value loss or Accuracy\n",
      "- Why We Use ELU Over RELU\n",
      "\n",
      "\n",
      "### The Connection Part\n",
      "### Files\n",
      "### Overview\n",
      "### References\n",
      "\n",
      "## Introduction \n",
      "Self-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity’s Car Simulator environment. Udacity has released the simulator as an open source software and  enthusiasts have hosted a competition (challenge) to teach a car how to drive using only camera images and deep learning. Driving a car in an autonomous manner requires learning to control steering angle, throttle and brakes. Behavioral cloning technique is used to mimic human driving behavior in the training mode on the track. That means a dataset is generated in the simulator by user driven car in training mode, and the deep neural network model then drives the car in autonomous mode. Ultimately, the car was able to run on Track 1 generalizing well. The project aims at reaching the same accuracy on real time data in the future.![6](https://user-images.githubusercontent.com/91852182/147298831-225740f9-6903-4570-8336-0c9f16676456.png)\n",
      "\n",
      "\n",
      "### Problem Definition\n",
      "\n",
      "Udacity released an open source simulator for self-driving cars to depict a real-time environment. The challenge is to mimic the driving behavior of a human on the simulator with the help of a model trained by deep neural networks. The concept is called Behavioral Cloning, to mimic how a human drives. The simulator contains two tracks and two modes, namely, training mode and autonomous mode. The dataset is generated from the simulator by the user, driving the car in training mode. This dataset is also known as the “good” driving data. This is followed by testing on the track, seeing how the deep learning model performs after being trained by that user data.\n",
      "\n",
      "### Solution Approach\n",
      " ![1](https://user-images.githubusercontent.com/91852182/147298261-4d57a5c1-1fda-4654-9741-2f284e6d0479.png)\n",
      " \n",
      " The problem is solved in the following steps: \n",
      " \n",
      "- The simulator can be used to collect data by driving the car in the training mode using a joystick or keyboard, providing the so called “good-driving” behavior input data in form of a driving_log (.csv file) and a set of images. The simulator acts as a server and pipes these images and data log to the Python client. \n",
      "- The client (Python program) is the machine learning model built using Deep Neural Networks. These models are developed on Keras (a high-level API over Tensorflow). Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets as the second step. Detailed description of CNN models experimented and used can be referred to in the chapter on network architectures. \n",
      "- Once the model is trained, it provides steering angles and throttle to drive in an autonomous mode to the server (simulator). \n",
      "- These modules, or inputs, are piped back to the server and are used to drive the car autonomously in the simulator and keep it from falling off the track.\n",
      "\n",
      "### Technologies Used \n",
      "\n",
      "Technologies that are used in the implementation of this project and the motivation behind using these are described in this section.\n",
      " \n",
      "TensorFlow: This an open-source library for dataflow programming. It is widely used for machine learning applications. It is also used as both a math library and for large computation. For this project Keras, a high-level API that uses TensorFlow as the backend is used. Keras facilitate in building the models easily as it more user friendly. \n",
      "\n",
      "Different libraries are available in Python that helps in machine learning projects. Several of those libraries have improved the performance of this project. Few of them are mentioned in this section. First, “Numpy” that provides with high-level math function collection to support multi-dimensional metrices and arrays. This is used for faster computations over the weights (gradients) in neural networks. Second, “scikit-learn” is a machine learning library for Python which features different algorithms and Machine Learning function packages. Another one is OpenCV (Open Source Computer Vision Library) which is designed for computational efficiency with focus on real-time applications. In this project, OpenCV is used for image preprocessing and augmentation techniques. \n",
      "\n",
      "The project makes use of Conda Environment which is an open source distribution for Python which simplifies package management and deployment. It is best for large scale data processing. The machine on which this project was built, is a personal computer. \n",
      "\n",
      "### Convolutional Neural Networks (CNN)\n",
      "\n",
      "CNN is a type of feed-forward neural network computing system that can be used to learn from input data. Learning is accomplished by determining a set of weights or filter values that allow the network to model the behavior according to the training data. The desired output and the output generated by CNN initialized with random weights will be different. This difference (generated error) is backpropagated through the layers of CNN to adjust the weights of the neurons, which in turn reduces the error and allows us produce output closer to the desired one. \n",
      "\n",
      "CNN is good at capturing hierarchical and spatial data from images. It utilizes filters that look at regions of an input image with a defined window size and map it to some output. It then slides the window by some defined stride to other regions, covering the whole image. Each convolution filter layer thus captures the properties of this input image hierarchically in a series of subsequent layers, capturing the details like lines in image, then shapes, then whole objects in later layers. CNN can be a good fit to feed the images of a dataset and classify them into their respective classes. \n",
      "\n",
      "### Time-Distributed Layers\n",
      "\n",
      "Another type of layers sometimes used in deep learning networks is a Time- distributed layer. Time-Distributed layers are provided in Keras as wrapper layers. Every temporal slice of an input is applied with this wrapper layer. The requirement for input is that to be at least three-dimensional, first index can be considered as temporal dimension. These Time-Distributed can be applied to a dense layer to each of the timesteps, independently or even used with Convolutional Layers. The way they can be written is also simple in Keras as shown in Figure 1 and Figure 2.\n",
      "\n",
      "![2](https://user-images.githubusercontent.com/91852182/147298483-4f37a092-7e71-4ce6-9274-9a133d138a4c.png)\n",
      "\n",
      "Fig. 1: TimeDistributed Dense layer\n",
      "\n",
      "![3](https://user-images.githubusercontent.com/91852182/147298501-6459d968-a279-4140-9be3-2d3ea826d9f6.png)\n",
      "\n",
      "Fig. 2: TimeDistributed Convolution layer\n",
      "\n",
      "## Udacity Simulator and Dataset \n",
      "\n",
      "We will first download the [simulator](https://github.com/udacity/self-driving-car-sim) to start our behavioural training process. Udacity has built a simulator for self-driving cars and made it open source for the enthusiasts, so they can work on something close to a real-time environment. It is built on Unity, the video game development platform. The simulator consists of a configurable resolution and controls setting and is very user friendly. The graphics and input configurations can be changed according to user preference and machine configuration as shown in Figure 3. The user pushes the “Play!” button to enter the simulator user interface. You can enter the Controls tab to explore the keyboard controls, quite similar to a racing game which can be seen in Figure 4. \n",
      "\n",
      "![ 4](https://user-images.githubusercontent.com/91852182/147298708-de15ebc5-2482-42f8-b2a2-8d3c59fceff4.png)\n",
      "\n",
      "Fig. 3: Configuration screen                                                                    \n",
      "\n",
      "![5](https://user-images.githubusercontent.com/91852182/147298712-944e2c2d-e01d-459b-8a7d-3c5471bea179.png)\n",
      "\n",
      "Fig. 4: Controls Configuration\n",
      "\n",
      "The first actual screen of the simulator can be seen in Figure 5 and its components are discussed below. The simulator involves two tracks. One of them can be considered as simple and another one as complex that can be evident in the screenshots attached in Figure 6 and Figure 7. The word “simple” here just means that it has fewer curvy tracks and is easier to drive on, refer Figure 6. The “complex” track has steep elevations, sharp turns, shadowed environment, and is tough to drive on, even by a user doing it manually. Please refer Figure 6. There are two modes for driving the car in the simulator: (1) Training mode and (2) Autonomous mode. The training mode gives you the option of recording your run and capturing the training dataset. The small red sign at the top right of the screen in the Figure 6 and 7 depicts the car is being driven in training mode. The autonomous mode can be used to test the models to see if it can drive on the track without human intervention. Also, if you try to press the controls to get the car back on track, it will immediately notify you that it shifted to manual controls. The mode screenshot can be as seen in Figure 8. Once we have mastered how the car driven controls in simulator using keyboard keys, then we get started with record button to collect data. We will save the data from it in a specified folder as you can see below.\n",
      "\n",
      "![6](https://user-images.githubusercontent.com/91852182/147298837-17eecb80-0a3f-4edb-a5f3-050a318f66e0.png)\n",
      "\n",
      "<img alt=\"7\" src=\"https://user-images.githubusercontent.com/91852182/147298975-e05dc738-2fb7-4dca-a28d-9756285d94cc.png\">\n",
      "\n",
      "The simulator’s feature to create your own dataset of images makes it easy to work on the problem. Some reasons why this feature is useful are as follows: \n",
      "\n",
      "- The simulator has built the driving features in such a way that it simulates that there are three cameras on the car. The three cameras are in the center, right and left on the front of the car, which captures continuously when we record in the training mode. \n",
      "- The stream of images is captured, and we can set the location on the disk for saving the data after pushing the record button. The image set are labelled in a sophisticated manner with a prefix of center, left, or right indicating from which camera the image has been captured. \n",
      "- Along with the image dataset, it also generates a datalog.csv file. This file contains the image paths with corresponding steering angle, throttle, brakes, and speed of the car at that instance. \n",
      "\n",
      "A few images from the dataset are shown below .\n",
      "\n",
      "<img alt=\"8\" src=\"https://user-images.githubusercontent.com/91852182/147299066-17bb3db0-5f1c-44ff-ab7e-41786c243d4f.png\">\n",
      "\n",
      "# Pic wich i have in folder IMG only for visualization your data will be more than 5000 pic.\n",
      "\n",
      "A sample of driving_log.csv file is shown in Figure 9.\n",
      "\n",
      "* *Column 1, 2, 3:*  contains paths to the dataset images of center, right and left respectively Column 4: contains the steering angle \n",
      "* *Column 4:*  value as 0 depicts straight, positive value is right turn and negative value is left turn. \n",
      "* *Column 5:*  contains the throttle or acceleration at that instance \n",
      "* *Column 6:*  contains the brakes or deceleration at that instance \n",
      "* *Column 7:*  contains the speed of the vehicle \n",
      "\n",
      "![9](https://user-images.githubusercontent.com/91852182/147299491-77d853c4-604b-42ef-8db4-2469a2993e00.png)\n",
      "\n",
      "Fig 9 . driving_log.csv\n",
      "\n",
      "## The Training Process\n",
      "\n",
      "For the process of getting the self driving car working, we have to upload the images that we recorded using the simulator. First of all, we will open [GitHub Desktop.](https://desktop.github.com/)If we do not have an account, we will create a new one. With that, we will create a new repository.\n",
      "\n",
      "![10](https://user-images.githubusercontent.com/91852182/147299586-a4afa457-d6ed-4383-a8d9-adf6fbcded47.png)\n",
      "\n",
      "We will be using [Google Colab](https://colab.research.google.com/) for doing the training process or [Kaggle](https://www.kaggle.com/).\n",
      "We will open a new python3 notebook and get started. Next, we will git clone the repo.\n",
      "\n",
      "```!git clone https://github.com/Asikpalysik/Self-Driving-Car.git```\n",
      " \n",
      "We will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.\n",
      " \n",
      "```\n",
      "import os\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "import keras\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imgaug import augmenters as iaa\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import ntpath\n",
      "import random\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "```\n",
      "We wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format\n",
      "\n",
      "```\n",
      "dir = \"/Users/asik/Desktop/SelfDrivingCar\"\n",
      "columns = [\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"reverse\", \"speed\"]\n",
      "data = pd.read_csv(os.path.join(dir, \"driving_log.csv\"), names=columns)\n",
      "pd.set_option(\"display.max_colwidth\", -1)\n",
      "data.head()\n",
      "```\n",
      "\n",
      "![11](https://user-images.githubusercontent.com/91852182/147300673-09faac02-c98a-4139-9151-b5da96b74593.png)\n",
      "\n",
      "As this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.\n",
      "\n",
      "```\n",
      "def pathleaf(path):\n",
      "    head, tail = ntpath.split(path)\n",
      "    return tail\n",
      "    \n",
      "data[\"center\"] = data[\"center\"].apply(pathleaf)\n",
      "data[\"left\"] = data[\"left\"].apply(pathleaf)\n",
      "data[\"right\"] = data[\"right\"].apply(pathleaf)\n",
      "data.head()\n",
      "```\n",
      "\n",
      "![12](https://user-images.githubusercontent.com/91852182/147300702-81a81f27-41f8-49a6-843b-67838b1bcb84.png)\n",
      "\n",
      "We will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame ‘steering’, we will divide it to the number of bins. We keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.\n",
      "\n",
      "```\n",
      "num_bins = 25\n",
      "samples_per_bin = 400\n",
      "hist, bins = np.histogram(data[\"steering\"], num_bins)\n",
      "print(bins)\n",
      "```\n",
      "\n",
      "![13](https://user-images.githubusercontent.com/91852182/147300730-c543c7cd-9ced-4019-9aa8-555e44b123cc.png)\n",
      "\n",
      "Plot on it\n",
      "\n",
      "```\n",
      "center = (bins[:-1] + bins[1:]) * 0.5\n",
      "plt.bar(center, hist, width=0.05)\n",
      "plt.plot(\n",
      "    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n",
      "    (samples_per_bin, samples_per_bin),\n",
      ")\n",
      "```\n",
      "\n",
      "<img alt=\"14\" src=\"https://user-images.githubusercontent.com/91852182/147301498-cb9aac07-837e-4f66-8825-36608843dab4.png\">\n",
      "\n",
      "```\n",
      "print(\"Total Data:\", len(data))\n",
      "```\n",
      "-&gt;&gt;Total Data: 1795\n",
      "\n",
      "\n",
      "We wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.\n",
      "\n",
      "```\n",
      "remove_list = []\n",
      "for j in range(num_bins):\n",
      "    list_ = []\n",
      "    for i in range(len(data[\"steering\"])):\n",
      "        if data[\"steering\"][i] &gt;= bins[j] and data[\"steering\"][i] &lt;= bins[j + 1]:\n",
      "            list_.append(i)\n",
      "    list_ = shuffle(list_)\n",
      "    list_ = list_[samples_per_bin:]\n",
      "    remove_list.extend(list_)\n",
      "print(\"Removed:\", len(remove_list))\n",
      "```\n",
      "-&gt;&gt; Removed: 945\n",
      "\n",
      "```\n",
      "data.drop(data.index[remove_list], inplace=True)\n",
      "print(\"Remaining:\", len(data))\n",
      "-&gt;&gt;Remaining: 850\n",
      "```\n",
      "\n",
      "Plot on it\n",
      "\n",
      "```\n",
      "hist, _ = np.histogram(data[\"steering\"], (num_bins))\n",
      "plt.bar(center, hist, width=0.05)\n",
      "plt.plot(\n",
      "    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n",
      "    (samples_per_bin, samples_per_bin),\n",
      ")\n",
      "```\n",
      "\n",
      "<img alt=\"15\" src=\"https://user-images.githubusercontent.com/91852182/147301598-185b5e70-17f8-41db-81d3-50cfc5db3902.png\">\n",
      "\n",
      "```\n",
      "print(data.iloc[1])\n",
      "```\n",
      "\n",
      "![16](https://user-images.githubusercontent.com/91852182/147301621-fcfb3f40-607f-4216-af9e-d82967e24bc9.png)\n",
      "\n",
      "We will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index, we will use cut data for now.\n",
      "\n",
      "```\n",
      "def load_img_steering(datadir, df):\n",
      "    image_path = []\n",
      "    steering = []\n",
      "    for i in range(len(data)):\n",
      "        indexed_data = data.iloc[i]\n",
      "        center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n",
      "        image_path.append(os.path.join(datadir, center.strip()))\n",
      "        steering.append(float(indexed_data[3]))\n",
      "        image_path.append(os.path.join(datadir, left.strip()))\n",
      "        steering.append(float(indexed_data[3]) + 0.15)\n",
      "        image_path.append(os.path.join(datadir, right.strip()))\n",
      "        steering.append(float(indexed_data[3]) - 0.15)\n",
      "    image_paths = np.asarray(image_path)\n",
      "    steerings = np.asarray(steering)\n",
      "    return image_paths, steerings\n",
      "```\n",
      "\n",
      "We will be splitting the image path as well as storing arrays accordingly.\n",
      "\n",
      "```\n",
      "image_paths, steerings = load_img_steering(dir + \"/IMG\", data)\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(\n",
      "    image_paths, steerings, test_size=0.2, random_state=6\n",
      ")\n",
      "print(\"Training Samples: {}\\nValid Samples: {}\".format(len(X_train), len(X_valid)))\n",
      "```\n",
      "-&gt;&gt;Training Samples: 2040 \n",
      "\n",
      "-&gt;&gt;Valid Samples: 510\n",
      "\n",
      "We will have the histograms now.\n",
      "\n",
      "```\n",
      "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
      "axes[0].hist(y_train, bins=num_bins, width=0.05, color=\"blue\")\n",
      "axes[0].set_title(\"Training set\")\n",
      "axes[1].hist(y_valid, bins=num_bins, width=0.05, color=\"red\")\n",
      "axes[1].set_title(\"Validation set\")\n",
      "```\n",
      "\n",
      "<img alt=\"17\" src=\"https://user-images.githubusercontent.com/91852182/147301799-0e3ca6a5-6f4e-41a9-b2c5-7bf5b1dad400.png\">\n",
      "\n",
      "## Augmentation and image pre-processing\n",
      "The biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.\n",
      "\n",
      "- Zoom \n",
      "\n",
      "\n",
      "The images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in  below.\n",
      "\n",
      "<img alt=\"18\" src=\"https://user-images.githubusercontent.com/91852182/147301992-fdc9d029-3a33-4f2f-8dc2-222067a07a0a.png\">\n",
      "\n",
      "```\n",
      "def zoom(image):\n",
      "    zoom = iaa.Affine(scale=(1, 1.3))\n",
      "    image = zoom.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "zoomed_image = zoom(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(zoomed_image)\n",
      "axs[1].set_title(\"Zoomed Image\")\n",
      "```\n",
      "\n",
      "- Flip (horizontal) \n",
      "\n",
      "The image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below. \n",
      "\n",
      "<img alt=\"19\" src=\"https://user-images.githubusercontent.com/91852182/147302076-3bd4311e-79b2-4750-84a7-c1061ffcdae3.png\">\n",
      "\n",
      "```\n",
      "def random_flip(image, steering_angle):\n",
      "    image = cv2.flip(image, 1)\n",
      "    steering_angle = -steering_angle\n",
      "    return image, steering_angle\n",
      "\n",
      "random_index = random.randint(0, 1000)\n",
      "image = image_paths[random_index]\n",
      "steering_angle = steerings[random_index]\n",
      "\n",
      "original_image = mpimg.imread(image)\n",
      "flipped_image, flipped_steering_angle = random_flip(original_image, steering_angle)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image - \" + \"Steering Angle:\" + str(steering_angle))\n",
      "\n",
      "axs[1].imshow(flipped_image)\n",
      "axs[1].set_title(\"Flipped Image - \" + \"Steering Angle:\" + str(flipped_steering_angle))\n",
      "```\n",
      "\n",
      "- Shift (horizontal/vertical)\n",
      "\n",
      "\n",
      "The image is shifted by a small amount, it is vertical shift and horizontal shift as below.\n",
      "\n",
      "\n",
      "<img alt=\"20\" src=\"https://user-images.githubusercontent.com/91852182/147302200-34bae3a8-2236-42d7-819d-ea694a382a02.png\">\n",
      "\n",
      "```\n",
      "def pan(image):\n",
      "    pan = iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
      "    image = pan.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "panned_image = pan(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(panned_image)\n",
      "axs[1].set_title(\"Panned Image\")\n",
      "```\n",
      "\n",
      "- Brightness \n",
      "\n",
      "\n",
      "To generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. \n",
      "\n",
      "<img alt=\"21\" src=\"https://user-images.githubusercontent.com/91852182/147302277-defccf7a-43f4-459c-a0db-536f01b77110.png\">\n",
      "\n",
      "```\n",
      "def random_brightness(image):\n",
      "    brightness = iaa.Multiply((0.2, 1.2))\n",
      "    image = brightness.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "brightness_altered_image = random_brightness(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(brightness_altered_image)\n",
      "axs[1].set_title(\"Brightness altered image \")\n",
      "```\n",
      "\n",
      "To have a look what we have at this moment\n",
      "\n",
      "```\n",
      "def random_augment(image, steering_angle):\n",
      "    image = mpimg.imread(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = pan(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = zoom(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = random_brightness(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image, steering_angle = random_flip(image, steering_angle)\n",
      "    return image, steering_angle\n",
      "\n",
      "ncol = 2\n",
      "nrow = 10\n",
      "\n",
      "fig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))\n",
      "fig.tight_layout()\n",
      "\n",
      "for i in range(10):\n",
      "    randnum = random.randint(0, len(image_paths) - 1)\n",
      "    random_image = image_paths[randnum]\n",
      "    random_steering = steerings[randnum]\n",
      "    original_image = mpimg.imread(random_image)\n",
      "    augmented_image, steering = random_augment(random_image, random_steering)\n",
      "    axs[i][0].imshow(original_image)\n",
      "    axs[i][0].set_title(\"Original Image\")\n",
      "\n",
      "    axs[i][1].imshow(augmented_image)\n",
      "    axs[i][1].set_title(\"Augmented Image\")\n",
      "```\n",
      "\n",
      "![23](https://user-images.githubusercontent.com/91852182/147302403-33b0de7e-5cab-46b1-a16a-77321cec9cbe.png)\n",
      "![24](https://user-images.githubusercontent.com/91852182/147302408-aadfd20d-6cc0-43cf-9da7-39e1ada3fe9a.png)\n",
      "\n",
      "\n",
      "I continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values.\n",
      "\n",
      "```\n",
      "def img_preprocess(img):\n",
      "    ## Crop image to remove unnecessary features\n",
      "    img = img[60:135, :, :]\n",
      "    ## Change to YUV image\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
      "    ## Gaussian blur\n",
      "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
      "    ## Decrease size for easier processing\n",
      "    img = cv2.resize(img, (200, 66))\n",
      "    ## Normalize values\n",
      "    img = img / 255\n",
      "    return img\n",
      "```\n",
      "\n",
      "To compare and visualize I plotted the original and the pre-processed image.\n",
      "\n",
      "```\n",
      "image = image_paths[100]\n",
      "original_image = mpimg.imread(image)\n",
      "preprocessed_image = img_preprocess(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(preprocessed_image)\n",
      "axs[1].set_title(\"Preprocessed Image\")\n",
      "```\n",
      "\n",
      "<img alt=\"25\" src=\"https://user-images.githubusercontent.com/91852182/147302472-ac64e7d7-1ec5-403a-82ff-046258584145.png\">\n",
      "\n",
      "```\n",
      "def batch_generator(image_paths, steering_ang, batch_size, istraining):\n",
      "    while True:\n",
      "        batch_img = []\n",
      "        batch_steering = []\n",
      "\n",
      "        for i in range(batch_size):\n",
      "            random_index = random.randint(0, len(image_paths) - 1)\n",
      "\n",
      "            if istraining:\n",
      "                im, steering = random_augment(\n",
      "                    image_paths[random_index], steering_ang[random_index]\n",
      "                )\n",
      "\n",
      "            else:\n",
      "                im = mpimg.imread(image_paths[random_index])\n",
      "                steering = steering_ang[random_index]\n",
      "\n",
      "            im = img_preprocess(im)\n",
      "            batch_img.append(im)\n",
      "            batch_steering.append(steering)\n",
      "\n",
      "        yield (np.asarray(batch_img), np.asarray(batch_steering))\n",
      "```\n",
      "\n",
      "So far so good. Next, I converted all the images into numpy array.\n",
      "\n",
      "```\n",
      "x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))\n",
      "x_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(x_train_gen[0])\n",
      "axs[0].set_title(\"Training Image\")\n",
      "\n",
      "axs[1].imshow(x_valid_gen[0])\n",
      "axs[1].set_title(\"Validation Image\")\n",
      "```\n",
      "\n",
      "<img alt=\"26\" src=\"https://user-images.githubusercontent.com/91852182/147302514-54b40af4-baea-491e-ba7b-73e6d752872e.png\">\n",
      "\n",
      "## Experimental configurations\n",
      "\n",
      "Configurations used to set up the models for training the Python Client to provide the Neural Network outputs that drive the car on the simulator. The tweaking of parameters and rigorous experiments were tried to reach the best combination. Though each of the models had their unique behaviors and differed in their performance with each tweak, the following combination of configuration can be considered as the optimal: \n",
      "\n",
      "- The sequential models built on Keras with deep neural network layers are used to train the data. \n",
      "- Models are only trained using the dataset from Track 1. \n",
      "- 80% of the dataset is used for training, 20% is used for testing. \n",
      "- Epochs = 10, i.e. number of iterations or passes through the complete dataset. Experimented with larger number of epochs also, but the model tried to “overfit”. In other words, the model learns the details in the training data too well, while impacting the performance on new dataset. \n",
      "- Batch-size = 100, i.e. number of image samples propagated through the network, like a subset of data as complete dataset is too big to be passed all at once.\n",
      "- Learning rate = 0.0001, i.e. how the coefficients of the weights or gradients change in the network. \n",
      "\n",
      "There are different combinations of Convolution layer, Time-Distributed layer, MaxPooling layer, Flatten, Dropout, Dense and so on, that can be used to implement the Neural Network models. \n",
      "\n",
      "## Network architectures\n",
      "\n",
      "The design of the network is based on the NVIDIA model, which has been used by NVIDIA for the end-to-end self driving test. As such, it is well suited for the project.It is a deep convolution network which works well with supervised image classification / regression problems. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction.\n",
      "\n",
      "I've added the following adjustments to the model.\n",
      "\n",
      "- I used Lambda layer to normalized input images to avoid saturation and make gradients work better.\n",
      "- I've added an additional dropout layer to avoid overfitting after the convolution layers.\n",
      "- I've also included ELU for activation function for every layer except for the output layer to introduce non-linearity.\n",
      "\n",
      "In the end, the model looks like as follows:\n",
      "\n",
      "- Image normalization\n",
      "- Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
      "- Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
      "- Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
      "- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
      "- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
      "- Drop out (0.5)\n",
      "- Fully connected: neurons: 100, activation: ELU\n",
      "- Fully connected: neurons: 50, activation: ELU\n",
      "- Fully connected: neurons: 10, activation: ELU\n",
      "- Fully connected: neurons: 1 (output)\n",
      "\n",
      "![27](https://user-images.githubusercontent.com/91852182/147302681-661ef73d-8cb1-452f-bbcd-323052972189.png)\n",
      "\n",
      "As per the NVIDIA model, the convolution layers are meant to handle feature engineering and the fully connected layer for predicting the steering angle. However, as stated in the NVIDIA document, it is not clear where to draw such a clear distinction. Overall, the model is very functional to clone the given steering behavior. The below is a model structure output from the Keras which gives more details on the shapes and the number of parameters.\n",
      "\n",
      "We will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used. We are dealing with images that have (200,66) dimensions. Our current datset has 5386 images to train with but MNSIT has around 60,000 images to train with. Our behavioural cloning code has simply has to return appropriate steering angle which is a regression type example. For these things, we need a more advanced model which is provided by nvdia and known as nvdia model.\n",
      "\n",
      "For defining the model architecture, we need to define the model object. Normalization state can be skipped as we have already normalized it. We will add the convolution layer. As compared to the model, we will organize accordingly. The Nvdia model uses 24 filters in the layer along with a kernel of size 5,5. We will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images. Horizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time. As this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is “elu”.\n",
      "\n",
      "Revisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation ‘elu’.\n",
      "\n",
      "According to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.\n",
      "\n",
      "Next we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.\n",
      "\n",
      "Our last convolution layer outputs an array shape of (1,18) by 64.\n",
      "\n",
      "We end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to “0” during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data -&gt;y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.\n",
      "\n",
      "\n",
      "```\n",
      "def NvidiaModel():\n",
      "  model = Sequential()\n",
      "  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=\"elu\"))\n",
      "  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=\"elu\"))\n",
      "  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=\"elu\")) \n",
      "  model.add(Convolution2D(64,(3,3),activation=\"elu\"))   \n",
      "  model.add(Convolution2D(64,(3,3),activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Flatten())\n",
      "  model.add(Dense(100,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(50,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(10,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(1))\n",
      "  model.compile(optimizer=Adam(lr=1e-3),loss=\"mse\")\n",
      "  return model\n",
      "```\n",
      "\n",
      "```\n",
      "model = NvidiaModel()\n",
      "print(model.summary())\n",
      "```\n",
      "\n",
      "![28](https://user-images.githubusercontent.com/91852182/147302750-f96a9310-e1d7-475d-bb55-d02c457d2139.png)\n",
      "\n",
      "## Results\n",
      "\n",
      "The following results were observed for described architectures. I had to come up with two different performance metrics. \n",
      "- Value loss or Accuracy (computed during training phase) \n",
      "- Generalization on Track 1 (drive performance)\n",
      "\n",
      "### Value loss or Accuracy \n",
      "\n",
      "The first evaluation parameter considered here is “Loss” over each epoch of the training run. To calculate value loss over each epoch, Keras provides “val_loss”, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.\n",
      "\n",
      "```\n",
      "history = model.fit_generator(\n",
      "    batch_generator(X_train, y_train, 100, 1),\n",
      "    steps_per_epoch=300,\n",
      "    epochs=10,\n",
      "    validation_data=batch_generator(X_valid, y_valid, 100, 0),\n",
      "    validation_steps=200,\n",
      "    verbose=1,\n",
      "    shuffle=1,\n",
      ")\n",
      "```\n",
      "\n",
      "![29](https://user-images.githubusercontent.com/91852182/147302882-5ad7e328-a18e-498c-8596-c6cb31573582.png)\n",
      "\n",
      "### Why We Use ELU Over RELU\n",
      "\n",
      "We can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.\n",
      "\n",
      "```\n",
      "plt.plot(history.history[\"loss\"])\n",
      "plt.plot(history.history[\"val_loss\"])\n",
      "plt.legend([\"training\", \"validation\"])\n",
      "plt.title(\"Loss\")\n",
      "plt.xlabel(\"Epoch\")\n",
      "```\n",
      "\n",
      "<img alt=\"30\" src=\"https://user-images.githubusercontent.com/91852182/147302963-abb40cb3-dd32-4991-8db0-f56e81fee23f.png\">\n",
      "\n",
      "Save model\n",
      "\n",
      "```\n",
      "model.save('model.h5')\n",
      "```\n",
      "\n",
      "## The Connection Part\n",
      "\n",
      "This step is required to run the model in the simulated car. For implementing web service using python, we need to install flask. We will use Anaconda environment. Flask is a python micro framework that is used to build the web app. We will use Visual Studio Code.\n",
      "\n",
      "![31](https://user-images.githubusercontent.com/91852182/147303011-116b5dbd-9892-490e-bc6f-b7644556243a.png)\n",
      "\n",
      "We will open the folder where we kept the saved .h5 file, then again open a file but before that, we will install some dependencies. We will also create an anaoconda environment too for doing our work. Click Create Python 3.8.12. with my experience I need to mention that i had some problems with environments, the was a lot of conflicts so I will advise you just to follow my steps exactly the same.\n",
      "\n",
      "![32](https://user-images.githubusercontent.com/91852182/147303041-3d9a12bb-e42a-4439-8e6d-549122a6e121.png)\n",
      "\n",
      "After we created new env on Python 3.8.12 moving to VScode and opening it under SDC environment. So basically, we now will work with special environment on special Python version. Terminal need to be as well under same.\n",
      "\n",
      "![34](https://user-images.githubusercontent.com/91852182/147303077-0761ce1c-b713-4237-97d3-d4e272e0adcf.png)\n",
      "\n",
      "![33](https://user-images.githubusercontent.com/91852182/147303069-3fa65901-85ae-49f0-a11c-bb10025936e5.png)\n",
      "\n",
      "I will create a list conda ```list -e &gt; requirements.txt``` requirements.txt for you to know what exactly I used there.  It important to use **keras 2.4.3** and be careful with **python-engineio=3.13.0** and lastly most important **python-socketio=4.6.1**(follow requirements.txt). As well you will see drive.py file without this code will not work you can simple copy paste it I will not go deep about this file. \n",
      "\n",
      "Now when all your requirements are installed, we are ready to run magic code in terminal. In your folder you will see something like I show below.\n",
      "\n",
      "![35](https://user-images.githubusercontent.com/91852182/147303161-90c4b09a-a180-4c34-b695-b16a7388525e.png)\n",
      "\n",
      "Just type python ```drive.py model.h5``` in your Terminal.  wait few few second open Udacity Simulator and choose option Autonomous mode. That it, you will see something as I show below.\n",
      "\n",
      "![36](https://user-images.githubusercontent.com/91852182/147303194-84f45ca9-c61a-4c3d-b054-ec731e2fd79f.png)\n",
      "\n",
      "## Files\n",
      "\n",
      "The project contains the following files:\n",
      "- *SelfDrivingCar.py* (script used to create the model and train the model)\n",
      "- *drive.py* (script to drive the car - feel free to modify this file)\n",
      "- *driving_log.csv* (csv file from simulator - data)\n",
      "- *IMG* (folder) (folder with images from simulator - data)\n",
      "- *model.h5* (a trained Keras model)\n",
      "- *requirements.txt* (important requirements for project)\n",
      "- *SelfDrivingCar.ipynb* (full explanation in notebook)\n",
      "- *SelfDrivingCar.pdf* (full explanation in PDF)\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this project, we use deep neural networks and convolutional neural networks to clone driving behavior. The model is trained, validated and tested using Keras. The model outputs a steering angle to an autonomous vehicle. The autonomous vehicle is provided as a simulator. Image data and steering angles are used to train a neural network and drive the simulation car autonomously around the track.\n",
      "This project started with training the models and tweaking parameters to get the best performance on the track \n",
      "The use of CNN for getting the spatial features and RNN for the temporal features in the image dataset makes this combination a great fit for building fast and lesser computation required neural networks. Substituting recurrent layers for pooling layers might reduce the loss of information and would be worth exploring in the future projects. \n",
      "It is interesting to find the use of combinations of real world dataset and simulator data to train these models. Then I can get the true nature of how a model can be trained in the simulator and generalized to the real world or vice versa. There are many experimental implementations carried out in the field of self-driving cars and this project contributes towards a significant part of it. \n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "GitHub - [https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning](https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning)\n",
      "\n",
      "GitHub - [https://github.com/SakshayMahna/P4-BehavioralCloning](https://github.com/SakshayMahna/P4-BehavioralCloning)\n",
      "\n",
      "Deep Learning for Self-Driving Cars - [https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2](https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2)\n",
      "\n",
      "GitHub - [https://github.com/llSourcell/How_to_simulate_a_self_driving_car](https://github.com/llSourcell/How_to_simulate_a_self_driving_car)\n",
      "\n",
      "GitHub - [https://github.com/naokishibuya/car-behavioral-cloning](https://github.com/naokishibuya/car-behavioral-cloning)\n",
      "\n",
      "End to End Learning for Self-Driving Cars - [https://arxiv.org/pdf/1604.07316v1.pdf](https://arxiv.org/pdf/1604.07316v1.pdf)\n",
      "\n",
      "Aditya Babhulkar - [https://scholarworks.calstate.edu/downloads/fx719m76s](https://scholarworks.calstate.edu/downloads/fx719m76s)\n",
      "\n",
      "GitHub - [https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: self-driving-carbehavioural-cloning\n",
      "name 'detect' is not defined Language is not detected: aslanahmedov\n",
      "name 'detect' is not defined Language is not detected: Self Driving Car\n",
      "name 'detect' is not defined Language is not detected: Behavioural Cloning Complete Guide\n",
      "name 'detect' is not defined Language is not detected: ![ezgif com-gif-maker](https://user-images.githubusercontent.com/91852182/147305077-8b86ec92-ed26-43ca-860c-5812fea9b1d8.gif)\n",
      "\n",
      "# SELF-DRIVING CAR USING UDACITY’S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE\n",
      "\n",
      "\n",
      "## Table of Contents\n",
      "### Introduction\n",
      "\n",
      "- Problem Definition\n",
      "- Solution Approach\n",
      "- Technologies Used\n",
      "- Convolutional Neural Networks (CNN)\n",
      "- Time-Distributed Layers\n",
      "\n",
      "### Udacity Simulator and Dataset\n",
      "### The Training Process\n",
      "### Augmentation and image pre-processing\n",
      "### Experimental configurations\n",
      "### Network architectures\n",
      "### Results\n",
      "\n",
      "- Value loss or Accuracy\n",
      "- Why We Use ELU Over RELU\n",
      "\n",
      "\n",
      "### The Connection Part\n",
      "### Files\n",
      "### Overview\n",
      "### References\n",
      "\n",
      "## Introduction \n",
      "Self-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity’s Car Simulator environment. Udacity has released the simulator as an open source software and  enthusiasts have hosted a competition (challenge) to teach a car how to drive using only camera images and deep learning. Driving a car in an autonomous manner requires learning to control steering angle, throttle and brakes. Behavioral cloning technique is used to mimic human driving behavior in the training mode on the track. That means a dataset is generated in the simulator by user driven car in training mode, and the deep neural network model then drives the car in autonomous mode. Ultimately, the car was able to run on Track 1 generalizing well. The project aims at reaching the same accuracy on real time data in the future.![6](https://user-images.githubusercontent.com/91852182/147298831-225740f9-6903-4570-8336-0c9f16676456.png)\n",
      "\n",
      "\n",
      "### Problem Definition\n",
      "\n",
      "Udacity released an open source simulator for self-driving cars to depict a real-time environment. The challenge is to mimic the driving behavior of a human on the simulator with the help of a model trained by deep neural networks. The concept is called Behavioral Cloning, to mimic how a human drives. The simulator contains two tracks and two modes, namely, training mode and autonomous mode. The dataset is generated from the simulator by the user, driving the car in training mode. This dataset is also known as the “good” driving data. This is followed by testing on the track, seeing how the deep learning model performs after being trained by that user data.\n",
      "\n",
      "### Solution Approach\n",
      " ![1](https://user-images.githubusercontent.com/91852182/147298261-4d57a5c1-1fda-4654-9741-2f284e6d0479.png)\n",
      " \n",
      " The problem is solved in the following steps: \n",
      " \n",
      "- The simulator can be used to collect data by driving the car in the training mode using a joystick or keyboard, providing the so called “good-driving” behavior input data in form of a driving_log (.csv file) and a set of images. The simulator acts as a server and pipes these images and data log to the Python client. \n",
      "- The client (Python program) is the machine learning model built using Deep Neural Networks. These models are developed on Keras (a high-level API over Tensorflow). Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets as the second step. Detailed description of CNN models experimented and used can be referred to in the chapter on network architectures. \n",
      "- Once the model is trained, it provides steering angles and throttle to drive in an autonomous mode to the server (simulator). \n",
      "- These modules, or inputs, are piped back to the server and are used to drive the car autonomously in the simulator and keep it from falling off the track.\n",
      "\n",
      "### Technologies Used \n",
      "\n",
      "Technologies that are used in the implementation of this project and the motivation behind using these are described in this section.\n",
      " \n",
      "TensorFlow: This an open-source library for dataflow programming. It is widely used for machine learning applications. It is also used as both a math library and for large computation. For this project Keras, a high-level API that uses TensorFlow as the backend is used. Keras facilitate in building the models easily as it more user friendly. \n",
      "\n",
      "Different libraries are available in Python that helps in machine learning projects. Several of those libraries have improved the performance of this project. Few of them are mentioned in this section. First, “Numpy” that provides with high-level math function collection to support multi-dimensional metrices and arrays. This is used for faster computations over the weights (gradients) in neural networks. Second, “scikit-learn” is a machine learning library for Python which features different algorithms and Machine Learning function packages. Another one is OpenCV (Open Source Computer Vision Library) which is designed for computational efficiency with focus on real-time applications. In this project, OpenCV is used for image preprocessing and augmentation techniques. \n",
      "\n",
      "The project makes use of Conda Environment which is an open source distribution for Python which simplifies package management and deployment. It is best for large scale data processing. The machine on which this project was built, is a personal computer. \n",
      "\n",
      "### Convolutional Neural Networks (CNN)\n",
      "\n",
      "CNN is a type of feed-forward neural network computing system that can be used to learn from input data. Learning is accomplished by determining a set of weights or filter values that allow the network to model the behavior according to the training data. The desired output and the output generated by CNN initialized with random weights will be different. This difference (generated error) is backpropagated through the layers of CNN to adjust the weights of the neurons, which in turn reduces the error and allows us produce output closer to the desired one. \n",
      "\n",
      "CNN is good at capturing hierarchical and spatial data from images. It utilizes filters that look at regions of an input image with a defined window size and map it to some output. It then slides the window by some defined stride to other regions, covering the whole image. Each convolution filter layer thus captures the properties of this input image hierarchically in a series of subsequent layers, capturing the details like lines in image, then shapes, then whole objects in later layers. CNN can be a good fit to feed the images of a dataset and classify them into their respective classes. \n",
      "\n",
      "### Time-Distributed Layers\n",
      "\n",
      "Another type of layers sometimes used in deep learning networks is a Time- distributed layer. Time-Distributed layers are provided in Keras as wrapper layers. Every temporal slice of an input is applied with this wrapper layer. The requirement for input is that to be at least three-dimensional, first index can be considered as temporal dimension. These Time-Distributed can be applied to a dense layer to each of the timesteps, independently or even used with Convolutional Layers. The way they can be written is also simple in Keras as shown in Figure 1 and Figure 2.\n",
      "\n",
      "![2](https://user-images.githubusercontent.com/91852182/147298483-4f37a092-7e71-4ce6-9274-9a133d138a4c.png)\n",
      "\n",
      "Fig. 1: TimeDistributed Dense layer\n",
      "\n",
      "![3](https://user-images.githubusercontent.com/91852182/147298501-6459d968-a279-4140-9be3-2d3ea826d9f6.png)\n",
      "\n",
      "Fig. 2: TimeDistributed Convolution layer\n",
      "\n",
      "## Udacity Simulator and Dataset \n",
      "\n",
      "We will first download the [simulator](https://github.com/udacity/self-driving-car-sim) to start our behavioural training process. Udacity has built a simulator for self-driving cars and made it open source for the enthusiasts, so they can work on something close to a real-time environment. It is built on Unity, the video game development platform. The simulator consists of a configurable resolution and controls setting and is very user friendly. The graphics and input configurations can be changed according to user preference and machine configuration as shown in Figure 3. The user pushes the “Play!” button to enter the simulator user interface. You can enter the Controls tab to explore the keyboard controls, quite similar to a racing game which can be seen in Figure 4. \n",
      "\n",
      "![ 4](https://user-images.githubusercontent.com/91852182/147298708-de15ebc5-2482-42f8-b2a2-8d3c59fceff4.png)\n",
      "\n",
      "Fig. 3: Configuration screen                                                                    \n",
      "\n",
      "![5](https://user-images.githubusercontent.com/91852182/147298712-944e2c2d-e01d-459b-8a7d-3c5471bea179.png)\n",
      "\n",
      "Fig. 4: Controls Configuration\n",
      "\n",
      "The first actual screen of the simulator can be seen in Figure 5 and its components are discussed below. The simulator involves two tracks. One of them can be considered as simple and another one as complex that can be evident in the screenshots attached in Figure 6 and Figure 7. The word “simple” here just means that it has fewer curvy tracks and is easier to drive on, refer Figure 6. The “complex” track has steep elevations, sharp turns, shadowed environment, and is tough to drive on, even by a user doing it manually. Please refer Figure 6. There are two modes for driving the car in the simulator: (1) Training mode and (2) Autonomous mode. The training mode gives you the option of recording your run and capturing the training dataset. The small red sign at the top right of the screen in the Figure 6 and 7 depicts the car is being driven in training mode. The autonomous mode can be used to test the models to see if it can drive on the track without human intervention. Also, if you try to press the controls to get the car back on track, it will immediately notify you that it shifted to manual controls. The mode screenshot can be as seen in Figure 8. Once we have mastered how the car driven controls in simulator using keyboard keys, then we get started with record button to collect data. We will save the data from it in a specified folder as you can see below.\n",
      "\n",
      "![6](https://user-images.githubusercontent.com/91852182/147298837-17eecb80-0a3f-4edb-a5f3-050a318f66e0.png)\n",
      "\n",
      "<img alt=\"7\" src=\"https://user-images.githubusercontent.com/91852182/147298975-e05dc738-2fb7-4dca-a28d-9756285d94cc.png\">\n",
      "\n",
      "The simulator’s feature to create your own dataset of images makes it easy to work on the problem. Some reasons why this feature is useful are as follows: \n",
      "\n",
      "- The simulator has built the driving features in such a way that it simulates that there are three cameras on the car. The three cameras are in the center, right and left on the front of the car, which captures continuously when we record in the training mode. \n",
      "- The stream of images is captured, and we can set the location on the disk for saving the data after pushing the record button. The image set are labelled in a sophisticated manner with a prefix of center, left, or right indicating from which camera the image has been captured. \n",
      "- Along with the image dataset, it also generates a datalog.csv file. This file contains the image paths with corresponding steering angle, throttle, brakes, and speed of the car at that instance. \n",
      "\n",
      "A few images from the dataset are shown below .\n",
      "\n",
      "<img alt=\"8\" src=\"https://user-images.githubusercontent.com/91852182/147299066-17bb3db0-5f1c-44ff-ab7e-41786c243d4f.png\">\n",
      "\n",
      "# Pic wich i have in folder IMG only for visualization your data will be more than 5000 pic.\n",
      "\n",
      "A sample of driving_log.csv file is shown in Figure 9.\n",
      "\n",
      "* *Column 1, 2, 3:*  contains paths to the dataset images of center, right and left respectively Column 4: contains the steering angle \n",
      "* *Column 4:*  value as 0 depicts straight, positive value is right turn and negative value is left turn. \n",
      "* *Column 5:*  contains the throttle or acceleration at that instance \n",
      "* *Column 6:*  contains the brakes or deceleration at that instance \n",
      "* *Column 7:*  contains the speed of the vehicle \n",
      "\n",
      "![9](https://user-images.githubusercontent.com/91852182/147299491-77d853c4-604b-42ef-8db4-2469a2993e00.png)\n",
      "\n",
      "Fig 9 . driving_log.csv\n",
      "\n",
      "## The Training Process\n",
      "\n",
      "For the process of getting the self driving car working, we have to upload the images that we recorded using the simulator. First of all, we will open [GitHub Desktop.](https://desktop.github.com/)If we do not have an account, we will create a new one. With that, we will create a new repository.\n",
      "\n",
      "![10](https://user-images.githubusercontent.com/91852182/147299586-a4afa457-d6ed-4383-a8d9-adf6fbcded47.png)\n",
      "\n",
      "We will be using [Google Colab](https://colab.research.google.com/) for doing the training process or [Kaggle](https://www.kaggle.com/).\n",
      "We will open a new python3 notebook and get started. Next, we will git clone the repo.\n",
      "\n",
      "```!git clone https://github.com/Asikpalysik/Self-Driving-Car.git```\n",
      " \n",
      "We will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.\n",
      " \n",
      "```\n",
      "import os\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "import keras\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imgaug import augmenters as iaa\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import ntpath\n",
      "import random\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "```\n",
      "We wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format\n",
      "\n",
      "```\n",
      "dir = \"/Users/asik/Desktop/SelfDrivingCar\"\n",
      "columns = [\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"reverse\", \"speed\"]\n",
      "data = pd.read_csv(os.path.join(dir, \"driving_log.csv\"), names=columns)\n",
      "pd.set_option(\"display.max_colwidth\", -1)\n",
      "data.head()\n",
      "```\n",
      "\n",
      "![11](https://user-images.githubusercontent.com/91852182/147300673-09faac02-c98a-4139-9151-b5da96b74593.png)\n",
      "\n",
      "As this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.\n",
      "\n",
      "```\n",
      "def pathleaf(path):\n",
      "    head, tail = ntpath.split(path)\n",
      "    return tail\n",
      "    \n",
      "data[\"center\"] = data[\"center\"].apply(pathleaf)\n",
      "data[\"left\"] = data[\"left\"].apply(pathleaf)\n",
      "data[\"right\"] = data[\"right\"].apply(pathleaf)\n",
      "data.head()\n",
      "```\n",
      "\n",
      "![12](https://user-images.githubusercontent.com/91852182/147300702-81a81f27-41f8-49a6-843b-67838b1bcb84.png)\n",
      "\n",
      "We will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame ‘steering’, we will divide it to the number of bins. We keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.\n",
      "\n",
      "```\n",
      "num_bins = 25\n",
      "samples_per_bin = 400\n",
      "hist, bins = np.histogram(data[\"steering\"], num_bins)\n",
      "print(bins)\n",
      "```\n",
      "\n",
      "![13](https://user-images.githubusercontent.com/91852182/147300730-c543c7cd-9ced-4019-9aa8-555e44b123cc.png)\n",
      "\n",
      "Plot on it\n",
      "\n",
      "```\n",
      "center = (bins[:-1] + bins[1:]) * 0.5\n",
      "plt.bar(center, hist, width=0.05)\n",
      "plt.plot(\n",
      "    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n",
      "    (samples_per_bin, samples_per_bin),\n",
      ")\n",
      "```\n",
      "\n",
      "<img alt=\"14\" src=\"https://user-images.githubusercontent.com/91852182/147301498-cb9aac07-837e-4f66-8825-36608843dab4.png\">\n",
      "\n",
      "```\n",
      "print(\"Total Data:\", len(data))\n",
      "```\n",
      "-&gt;&gt;Total Data: 1795\n",
      "\n",
      "\n",
      "We wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.\n",
      "\n",
      "```\n",
      "remove_list = []\n",
      "for j in range(num_bins):\n",
      "    list_ = []\n",
      "    for i in range(len(data[\"steering\"])):\n",
      "        if data[\"steering\"][i] &gt;= bins[j] and data[\"steering\"][i] &lt;= bins[j + 1]:\n",
      "            list_.append(i)\n",
      "    list_ = shuffle(list_)\n",
      "    list_ = list_[samples_per_bin:]\n",
      "    remove_list.extend(list_)\n",
      "print(\"Removed:\", len(remove_list))\n",
      "```\n",
      "-&gt;&gt; Removed: 945\n",
      "\n",
      "```\n",
      "data.drop(data.index[remove_list], inplace=True)\n",
      "print(\"Remaining:\", len(data))\n",
      "-&gt;&gt;Remaining: 850\n",
      "```\n",
      "\n",
      "Plot on it\n",
      "\n",
      "```\n",
      "hist, _ = np.histogram(data[\"steering\"], (num_bins))\n",
      "plt.bar(center, hist, width=0.05)\n",
      "plt.plot(\n",
      "    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n",
      "    (samples_per_bin, samples_per_bin),\n",
      ")\n",
      "```\n",
      "\n",
      "<img alt=\"15\" src=\"https://user-images.githubusercontent.com/91852182/147301598-185b5e70-17f8-41db-81d3-50cfc5db3902.png\">\n",
      "\n",
      "```\n",
      "print(data.iloc[1])\n",
      "```\n",
      "\n",
      "![16](https://user-images.githubusercontent.com/91852182/147301621-fcfb3f40-607f-4216-af9e-d82967e24bc9.png)\n",
      "\n",
      "We will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index, we will use cut data for now.\n",
      "\n",
      "```\n",
      "def load_img_steering(datadir, df):\n",
      "    image_path = []\n",
      "    steering = []\n",
      "    for i in range(len(data)):\n",
      "        indexed_data = data.iloc[i]\n",
      "        center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n",
      "        image_path.append(os.path.join(datadir, center.strip()))\n",
      "        steering.append(float(indexed_data[3]))\n",
      "        image_path.append(os.path.join(datadir, left.strip()))\n",
      "        steering.append(float(indexed_data[3]) + 0.15)\n",
      "        image_path.append(os.path.join(datadir, right.strip()))\n",
      "        steering.append(float(indexed_data[3]) - 0.15)\n",
      "    image_paths = np.asarray(image_path)\n",
      "    steerings = np.asarray(steering)\n",
      "    return image_paths, steerings\n",
      "```\n",
      "\n",
      "We will be splitting the image path as well as storing arrays accordingly.\n",
      "\n",
      "```\n",
      "image_paths, steerings = load_img_steering(dir + \"/IMG\", data)\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(\n",
      "    image_paths, steerings, test_size=0.2, random_state=6\n",
      ")\n",
      "print(\"Training Samples: {}\\nValid Samples: {}\".format(len(X_train), len(X_valid)))\n",
      "```\n",
      "-&gt;&gt;Training Samples: 2040 \n",
      "\n",
      "-&gt;&gt;Valid Samples: 510\n",
      "\n",
      "We will have the histograms now.\n",
      "\n",
      "```\n",
      "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
      "axes[0].hist(y_train, bins=num_bins, width=0.05, color=\"blue\")\n",
      "axes[0].set_title(\"Training set\")\n",
      "axes[1].hist(y_valid, bins=num_bins, width=0.05, color=\"red\")\n",
      "axes[1].set_title(\"Validation set\")\n",
      "```\n",
      "\n",
      "<img alt=\"17\" src=\"https://user-images.githubusercontent.com/91852182/147301799-0e3ca6a5-6f4e-41a9-b2c5-7bf5b1dad400.png\">\n",
      "\n",
      "## Augmentation and image pre-processing\n",
      "The biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.\n",
      "\n",
      "- Zoom \n",
      "\n",
      "\n",
      "The images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in  below.\n",
      "\n",
      "<img alt=\"18\" src=\"https://user-images.githubusercontent.com/91852182/147301992-fdc9d029-3a33-4f2f-8dc2-222067a07a0a.png\">\n",
      "\n",
      "```\n",
      "def zoom(image):\n",
      "    zoom = iaa.Affine(scale=(1, 1.3))\n",
      "    image = zoom.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "zoomed_image = zoom(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(zoomed_image)\n",
      "axs[1].set_title(\"Zoomed Image\")\n",
      "```\n",
      "\n",
      "- Flip (horizontal) \n",
      "\n",
      "The image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below. \n",
      "\n",
      "<img alt=\"19\" src=\"https://user-images.githubusercontent.com/91852182/147302076-3bd4311e-79b2-4750-84a7-c1061ffcdae3.png\">\n",
      "\n",
      "```\n",
      "def random_flip(image, steering_angle):\n",
      "    image = cv2.flip(image, 1)\n",
      "    steering_angle = -steering_angle\n",
      "    return image, steering_angle\n",
      "\n",
      "random_index = random.randint(0, 1000)\n",
      "image = image_paths[random_index]\n",
      "steering_angle = steerings[random_index]\n",
      "\n",
      "original_image = mpimg.imread(image)\n",
      "flipped_image, flipped_steering_angle = random_flip(original_image, steering_angle)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image - \" + \"Steering Angle:\" + str(steering_angle))\n",
      "\n",
      "axs[1].imshow(flipped_image)\n",
      "axs[1].set_title(\"Flipped Image - \" + \"Steering Angle:\" + str(flipped_steering_angle))\n",
      "```\n",
      "\n",
      "- Shift (horizontal/vertical)\n",
      "\n",
      "\n",
      "The image is shifted by a small amount, it is vertical shift and horizontal shift as below.\n",
      "\n",
      "\n",
      "<img alt=\"20\" src=\"https://user-images.githubusercontent.com/91852182/147302200-34bae3a8-2236-42d7-819d-ea694a382a02.png\">\n",
      "\n",
      "```\n",
      "def pan(image):\n",
      "    pan = iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
      "    image = pan.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "panned_image = pan(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(panned_image)\n",
      "axs[1].set_title(\"Panned Image\")\n",
      "```\n",
      "\n",
      "- Brightness \n",
      "\n",
      "\n",
      "To generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. \n",
      "\n",
      "<img alt=\"21\" src=\"https://user-images.githubusercontent.com/91852182/147302277-defccf7a-43f4-459c-a0db-536f01b77110.png\">\n",
      "\n",
      "```\n",
      "def random_brightness(image):\n",
      "    brightness = iaa.Multiply((0.2, 1.2))\n",
      "    image = brightness.augment_image(image)\n",
      "    return image\n",
      "\n",
      "image = image_paths[random.randint(0, 1000)]\n",
      "original_image = mpimg.imread(image)\n",
      "brightness_altered_image = random_brightness(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(brightness_altered_image)\n",
      "axs[1].set_title(\"Brightness altered image \")\n",
      "```\n",
      "\n",
      "To have a look what we have at this moment\n",
      "\n",
      "```\n",
      "def random_augment(image, steering_angle):\n",
      "    image = mpimg.imread(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = pan(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = zoom(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image = random_brightness(image)\n",
      "    if np.random.rand() &lt; 0.5:\n",
      "        image, steering_angle = random_flip(image, steering_angle)\n",
      "    return image, steering_angle\n",
      "\n",
      "ncol = 2\n",
      "nrow = 10\n",
      "\n",
      "fig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))\n",
      "fig.tight_layout()\n",
      "\n",
      "for i in range(10):\n",
      "    randnum = random.randint(0, len(image_paths) - 1)\n",
      "    random_image = image_paths[randnum]\n",
      "    random_steering = steerings[randnum]\n",
      "    original_image = mpimg.imread(random_image)\n",
      "    augmented_image, steering = random_augment(random_image, random_steering)\n",
      "    axs[i][0].imshow(original_image)\n",
      "    axs[i][0].set_title(\"Original Image\")\n",
      "\n",
      "    axs[i][1].imshow(augmented_image)\n",
      "    axs[i][1].set_title(\"Augmented Image\")\n",
      "```\n",
      "\n",
      "![23](https://user-images.githubusercontent.com/91852182/147302403-33b0de7e-5cab-46b1-a16a-77321cec9cbe.png)\n",
      "![24](https://user-images.githubusercontent.com/91852182/147302408-aadfd20d-6cc0-43cf-9da7-39e1ada3fe9a.png)\n",
      "\n",
      "\n",
      "I continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values.\n",
      "\n",
      "```\n",
      "def img_preprocess(img):\n",
      "    ## Crop image to remove unnecessary features\n",
      "    img = img[60:135, :, :]\n",
      "    ## Change to YUV image\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
      "    ## Gaussian blur\n",
      "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
      "    ## Decrease size for easier processing\n",
      "    img = cv2.resize(img, (200, 66))\n",
      "    ## Normalize values\n",
      "    img = img / 255\n",
      "    return img\n",
      "```\n",
      "\n",
      "To compare and visualize I plotted the original and the pre-processed image.\n",
      "\n",
      "```\n",
      "image = image_paths[100]\n",
      "original_image = mpimg.imread(image)\n",
      "preprocessed_image = img_preprocess(original_image)\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(original_image)\n",
      "axs[0].set_title(\"Original Image\")\n",
      "\n",
      "axs[1].imshow(preprocessed_image)\n",
      "axs[1].set_title(\"Preprocessed Image\")\n",
      "```\n",
      "\n",
      "<img alt=\"25\" src=\"https://user-images.githubusercontent.com/91852182/147302472-ac64e7d7-1ec5-403a-82ff-046258584145.png\">\n",
      "\n",
      "```\n",
      "def batch_generator(image_paths, steering_ang, batch_size, istraining):\n",
      "    while True:\n",
      "        batch_img = []\n",
      "        batch_steering = []\n",
      "\n",
      "        for i in range(batch_size):\n",
      "            random_index = random.randint(0, len(image_paths) - 1)\n",
      "\n",
      "            if istraining:\n",
      "                im, steering = random_augment(\n",
      "                    image_paths[random_index], steering_ang[random_index]\n",
      "                )\n",
      "\n",
      "            else:\n",
      "                im = mpimg.imread(image_paths[random_index])\n",
      "                steering = steering_ang[random_index]\n",
      "\n",
      "            im = img_preprocess(im)\n",
      "            batch_img.append(im)\n",
      "            batch_steering.append(steering)\n",
      "\n",
      "        yield (np.asarray(batch_img), np.asarray(batch_steering))\n",
      "```\n",
      "\n",
      "So far so good. Next, I converted all the images into numpy array.\n",
      "\n",
      "```\n",
      "x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))\n",
      "x_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
      "fig.tight_layout()\n",
      "\n",
      "axs[0].imshow(x_train_gen[0])\n",
      "axs[0].set_title(\"Training Image\")\n",
      "\n",
      "axs[1].imshow(x_valid_gen[0])\n",
      "axs[1].set_title(\"Validation Image\")\n",
      "```\n",
      "\n",
      "<img alt=\"26\" src=\"https://user-images.githubusercontent.com/91852182/147302514-54b40af4-baea-491e-ba7b-73e6d752872e.png\">\n",
      "\n",
      "## Experimental configurations\n",
      "\n",
      "Configurations used to set up the models for training the Python Client to provide the Neural Network outputs that drive the car on the simulator. The tweaking of parameters and rigorous experiments were tried to reach the best combination. Though each of the models had their unique behaviors and differed in their performance with each tweak, the following combination of configuration can be considered as the optimal: \n",
      "\n",
      "- The sequential models built on Keras with deep neural network layers are used to train the data. \n",
      "- Models are only trained using the dataset from Track 1. \n",
      "- 80% of the dataset is used for training, 20% is used for testing. \n",
      "- Epochs = 10, i.e. number of iterations or passes through the complete dataset. Experimented with larger number of epochs also, but the model tried to “overfit”. In other words, the model learns the details in the training data too well, while impacting the performance on new dataset. \n",
      "- Batch-size = 100, i.e. number of image samples propagated through the network, like a subset of data as complete dataset is too big to be passed all at once.\n",
      "- Learning rate = 0.0001, i.e. how the coefficients of the weights or gradients change in the network. \n",
      "\n",
      "There are different combinations of Convolution layer, Time-Distributed layer, MaxPooling layer, Flatten, Dropout, Dense and so on, that can be used to implement the Neural Network models. \n",
      "\n",
      "## Network architectures\n",
      "\n",
      "The design of the network is based on the NVIDIA model, which has been used by NVIDIA for the end-to-end self driving test. As such, it is well suited for the project.It is a deep convolution network which works well with supervised image classification / regression problems. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction.\n",
      "\n",
      "I've added the following adjustments to the model.\n",
      "\n",
      "- I used Lambda layer to normalized input images to avoid saturation and make gradients work better.\n",
      "- I've added an additional dropout layer to avoid overfitting after the convolution layers.\n",
      "- I've also included ELU for activation function for every layer except for the output layer to introduce non-linearity.\n",
      "\n",
      "In the end, the model looks like as follows:\n",
      "\n",
      "- Image normalization\n",
      "- Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
      "- Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
      "- Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
      "- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
      "- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
      "- Drop out (0.5)\n",
      "- Fully connected: neurons: 100, activation: ELU\n",
      "- Fully connected: neurons: 50, activation: ELU\n",
      "- Fully connected: neurons: 10, activation: ELU\n",
      "- Fully connected: neurons: 1 (output)\n",
      "\n",
      "![27](https://user-images.githubusercontent.com/91852182/147302681-661ef73d-8cb1-452f-bbcd-323052972189.png)\n",
      "\n",
      "As per the NVIDIA model, the convolution layers are meant to handle feature engineering and the fully connected layer for predicting the steering angle. However, as stated in the NVIDIA document, it is not clear where to draw such a clear distinction. Overall, the model is very functional to clone the given steering behavior. The below is a model structure output from the Keras which gives more details on the shapes and the number of parameters.\n",
      "\n",
      "We will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used. We are dealing with images that have (200,66) dimensions. Our current datset has 5386 images to train with but MNSIT has around 60,000 images to train with. Our behavioural cloning code has simply has to return appropriate steering angle which is a regression type example. For these things, we need a more advanced model which is provided by nvdia and known as nvdia model.\n",
      "\n",
      "For defining the model architecture, we need to define the model object. Normalization state can be skipped as we have already normalized it. We will add the convolution layer. As compared to the model, we will organize accordingly. The Nvdia model uses 24 filters in the layer along with a kernel of size 5,5. We will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images. Horizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time. As this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is “elu”.\n",
      "\n",
      "Revisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation ‘elu’.\n",
      "\n",
      "According to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.\n",
      "\n",
      "Next we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.\n",
      "\n",
      "Our last convolution layer outputs an array shape of (1,18) by 64.\n",
      "\n",
      "We end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to “0” during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data -&gt;y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.\n",
      "\n",
      "\n",
      "```\n",
      "def NvidiaModel():\n",
      "  model = Sequential()\n",
      "  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=\"elu\"))\n",
      "  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=\"elu\"))\n",
      "  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=\"elu\")) \n",
      "  model.add(Convolution2D(64,(3,3),activation=\"elu\"))   \n",
      "  model.add(Convolution2D(64,(3,3),activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Flatten())\n",
      "  model.add(Dense(100,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(50,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(10,activation=\"elu\"))\n",
      "  model.add(Dropout(0.5))\n",
      "  model.add(Dense(1))\n",
      "  model.compile(optimizer=Adam(lr=1e-3),loss=\"mse\")\n",
      "  return model\n",
      "```\n",
      "\n",
      "```\n",
      "model = NvidiaModel()\n",
      "print(model.summary())\n",
      "```\n",
      "\n",
      "![28](https://user-images.githubusercontent.com/91852182/147302750-f96a9310-e1d7-475d-bb55-d02c457d2139.png)\n",
      "\n",
      "## Results\n",
      "\n",
      "The following results were observed for described architectures. I had to come up with two different performance metrics. \n",
      "- Value loss or Accuracy (computed during training phase) \n",
      "- Generalization on Track 1 (drive performance)\n",
      "\n",
      "### Value loss or Accuracy \n",
      "\n",
      "The first evaluation parameter considered here is “Loss” over each epoch of the training run. To calculate value loss over each epoch, Keras provides “val_loss”, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.\n",
      "\n",
      "```\n",
      "history = model.fit_generator(\n",
      "    batch_generator(X_train, y_train, 100, 1),\n",
      "    steps_per_epoch=300,\n",
      "    epochs=10,\n",
      "    validation_data=batch_generator(X_valid, y_valid, 100, 0),\n",
      "    validation_steps=200,\n",
      "    verbose=1,\n",
      "    shuffle=1,\n",
      ")\n",
      "```\n",
      "\n",
      "![29](https://user-images.githubusercontent.com/91852182/147302882-5ad7e328-a18e-498c-8596-c6cb31573582.png)\n",
      "\n",
      "### Why We Use ELU Over RELU\n",
      "\n",
      "We can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.\n",
      "\n",
      "```\n",
      "plt.plot(history.history[\"loss\"])\n",
      "plt.plot(history.history[\"val_loss\"])\n",
      "plt.legend([\"training\", \"validation\"])\n",
      "plt.title(\"Loss\")\n",
      "plt.xlabel(\"Epoch\")\n",
      "```\n",
      "\n",
      "<img alt=\"30\" src=\"https://user-images.githubusercontent.com/91852182/147302963-abb40cb3-dd32-4991-8db0-f56e81fee23f.png\">\n",
      "\n",
      "Save model\n",
      "\n",
      "```\n",
      "model.save('model.h5')\n",
      "```\n",
      "\n",
      "## The Connection Part\n",
      "\n",
      "This step is required to run the model in the simulated car. For implementing web service using python, we need to install flask. We will use Anaconda environment. Flask is a python micro framework that is used to build the web app. We will use Visual Studio Code.\n",
      "\n",
      "![31](https://user-images.githubusercontent.com/91852182/147303011-116b5dbd-9892-490e-bc6f-b7644556243a.png)\n",
      "\n",
      "We will open the folder where we kept the saved .h5 file, then again open a file but before that, we will install some dependencies. We will also create an anaoconda environment too for doing our work. Click Create Python 3.8.12. with my experience I need to mention that i had some problems with environments, the was a lot of conflicts so I will advise you just to follow my steps exactly the same.\n",
      "\n",
      "![32](https://user-images.githubusercontent.com/91852182/147303041-3d9a12bb-e42a-4439-8e6d-549122a6e121.png)\n",
      "\n",
      "After we created new env on Python 3.8.12 moving to VScode and opening it under SDC environment. So basically, we now will work with special environment on special Python version. Terminal need to be as well under same.\n",
      "\n",
      "![34](https://user-images.githubusercontent.com/91852182/147303077-0761ce1c-b713-4237-97d3-d4e272e0adcf.png)\n",
      "\n",
      "![33](https://user-images.githubusercontent.com/91852182/147303069-3fa65901-85ae-49f0-a11c-bb10025936e5.png)\n",
      "\n",
      "I will create a list conda ```list -e &gt; requirements.txt``` requirements.txt for you to know what exactly I used there.  It important to use **keras 2.4.3** and be careful with **python-engineio=3.13.0** and lastly most important **python-socketio=4.6.1**(follow requirements.txt). As well you will see drive.py file without this code will not work you can simple copy paste it I will not go deep about this file. \n",
      "\n",
      "Now when all your requirements are installed, we are ready to run magic code in terminal. In your folder you will see something like I show below.\n",
      "\n",
      "![35](https://user-images.githubusercontent.com/91852182/147303161-90c4b09a-a180-4c34-b695-b16a7388525e.png)\n",
      "\n",
      "Just type python ```drive.py model.h5``` in your Terminal.  wait few few second open Udacity Simulator and choose option Autonomous mode. That it, you will see something as I show below.\n",
      "\n",
      "![36](https://user-images.githubusercontent.com/91852182/147303194-84f45ca9-c61a-4c3d-b054-ec731e2fd79f.png)\n",
      "\n",
      "## Files\n",
      "\n",
      "The project contains the following files:\n",
      "- *SelfDrivingCar.py* (script used to create the model and train the model)\n",
      "- *drive.py* (script to drive the car - feel free to modify this file)\n",
      "- *driving_log.csv* (csv file from simulator - data)\n",
      "- *IMG* (folder) (folder with images from simulator - data)\n",
      "- *model.h5* (a trained Keras model)\n",
      "- *requirements.txt* (important requirements for project)\n",
      "- *SelfDrivingCar.ipynb* (full explanation in notebook)\n",
      "- *SelfDrivingCar.pdf* (full explanation in PDF)\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this project, we use deep neural networks and convolutional neural networks to clone driving behavior. The model is trained, validated and tested using Keras. The model outputs a steering angle to an autonomous vehicle. The autonomous vehicle is provided as a simulator. Image data and steering angles are used to train a neural network and drive the simulation car autonomously around the track.\n",
      "This project started with training the models and tweaking parameters to get the best performance on the track \n",
      "The use of CNN for getting the spatial features and RNN for the temporal features in the image dataset makes this combination a great fit for building fast and lesser computation required neural networks. Substituting recurrent layers for pooling layers might reduce the loss of information and would be worth exploring in the future projects. \n",
      "It is interesting to find the use of combinations of real world dataset and simulator data to train these models. Then I can get the true nature of how a model can be trained in the simulator and generalized to the real world or vice versa. There are many experimental implementations carried out in the field of self-driving cars and this project contributes towards a significant part of it. \n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "GitHub - [https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning](https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning)\n",
      "\n",
      "GitHub - [https://github.com/SakshayMahna/P4-BehavioralCloning](https://github.com/SakshayMahna/P4-BehavioralCloning)\n",
      "\n",
      "Deep Learning for Self-Driving Cars - [https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2](https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2)\n",
      "\n",
      "GitHub - [https://github.com/llSourcell/How_to_simulate_a_self_driving_car](https://github.com/llSourcell/How_to_simulate_a_self_driving_car)\n",
      "\n",
      "GitHub - [https://github.com/naokishibuya/car-behavioral-cloning](https://github.com/naokishibuya/car-behavioral-cloning)\n",
      "\n",
      "End to End Learning for Self-Driving Cars - [https://arxiv.org/pdf/1604.07316v1.pdf](https://arxiv.org/pdf/1604.07316v1.pdf)\n",
      "\n",
      "Aditya Babhulkar - [https://scholarworks.calstate.edu/downloads/fx719m76s](https://scholarworks.calstate.edu/downloads/fx719m76s)\n",
      "\n",
      "GitHub - [https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: jarvissharma/recommendation\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: jarvissharma\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: jarvissharma\n",
      "name 'detect' is not defined Language is not detected: recommendation\n",
      "name 'detect' is not defined Language is not detected: trajput508/coursera-data\n",
      "name 'detect' is not defined Language is not detected: coursera-data\n",
      "name 'detect' is not defined Language is not detected: trajput508\n",
      "name 'detect' is not defined Language is not detected: Coursera Data\n",
      "name 'detect' is not defined Language is not detected: The course contains Course name, university name, course type,course duration, course language, course rating, course level, instructors name, course details.\n",
      "\n",
      "The data is collected using web-scraping Coursera website. The Coursera course directory helped a lot in finding the course names  and their respective links.\n",
      "\n",
      "This data was collected to make a recommendation system that recommends courses based on similar courses viewed by the user \n",
      "name 'detect' is not defined Language is not detected: coursera-data\n",
      "name 'detect' is not defined Language is not detected: trajput508\n",
      "name 'detect' is not defined Language is not detected: Coursera Data\n",
      "name 'detect' is not defined Language is not detected: The course contains Course name, university name, course type,course duration, course language, course rating, course level, instructors name, course details.\n",
      "\n",
      "The data is collected using web-scraping Coursera website. The Coursera course directory helped a lot in finding the course names  and their respective links.\n",
      "\n",
      "This data was collected to make a recommendation system that recommends courses based on similar courses viewed by the user \n",
      "name 'detect' is not defined Language is not detected: pratiksikdar/british-airways-reviews\n",
      "name 'detect' is not defined Language is not detected: british-airways-reviews\n",
      "name 'detect' is not defined Language is not detected: pratiksikdar\n",
      "name 'detect' is not defined Language is not detected: British Airways Reviews\n",
      "name 'detect' is not defined Language is not detected: \n",
      "🌟 **Introducing the Ultimate United Airlines Seat Reviews Dataset on Kaggle!** 🌟\n",
      "\n",
      "Unlock a treasure trove of valuable insights with our newly uploaded dataset featuring a whopping 3486 real reviews sourced directly from the esteemed aviation critique site, AirlineQuality.com. This meticulously curated dataset promises to be a goldmine for data enthusiasts, aviation aficionados, and anyone seeking a deep dive into the passenger experience aboard United Airlines.\n",
      "\n",
      "🔍 **Key Dataset Attributes**:\n",
      "\n",
      "**Seat_Type**: Dive into the nitty-gritty details of seat types, offering a comprehensive overview of the diverse seating options available on United Airlines flights.\n",
      "\n",
      "**Recommend_to_Others**: Uncover the sentiments of passengers as they share their verdict on whether they would recommend their chosen seat to fellow travelers.\n",
      "\n",
      "**Review_Heading**: Gain quick insights into the essence of each review with succinct review headings, providing a snapshot of passenger sentiments.\n",
      "\n",
      "**Review_Content**: Immerse yourself in the raw, unfiltered narratives of passengers, experiencing the highs and lows of United Airlines through detailed review content.\n",
      "\n",
      "⚙️ **Enhancements for Precision:**\n",
      "\n",
      "**Verification Column**: Elevate your analysis with the newly introduced 'Verification' column, meticulously crafted to discern whether each trip has been validated or not. This added layer of information empowers you to distinguish verified experiences from unverified ones, ensuring a more nuanced understanding of the dataset.\n",
      "💡 **Unlock Business Intelligence**:\n",
      "\n",
      "Whether you're an analyst, researcher, or a curious traveler, this dataset opens up a world of possibilities. Unearth patterns, trends, and hidden gems within the rich tapestry of passenger experiences, shaping a comprehensive narrative about United Airlines' seat quality.\n",
      "\n",
      "📊 **Why Explore This Dataset?**\n",
      "\n",
      "**Real-world Insights**: The reviews are sourced directly from passengers, providing authentic and unfiltered perspectives on seat quality.\n",
      "\n",
      "**Analytical Opportunities**: Leverage the dataset to conduct sentiment analysis, trend spotting, and recommendation modeling, offering invaluable insights for businesses and travelers alike.\n",
      "\n",
      "**Decision Support:** Businesses can use this dataset to inform seat improvement strategies, while travelers can make informed decisions based on peer reviews.\n",
      "\n",
      "Embark on a journey of data exploration with the United Airlines Seat Reviews Dataset on Kaggle—where every row unveils a unique story, and every column is a gateway to informed decision-making. The skies are no longer the limit; your data-driven adventure begins here! ✈️📈\n",
      "name 'detect' is not defined Language is not detected: british-airways-reviews\n",
      "name 'detect' is not defined Language is not detected: pratiksikdar\n",
      "name 'detect' is not defined Language is not detected: British Airways Reviews\n",
      "name 'detect' is not defined Language is not detected: \n",
      "🌟 **Introducing the Ultimate United Airlines Seat Reviews Dataset on Kaggle!** 🌟\n",
      "\n",
      "Unlock a treasure trove of valuable insights with our newly uploaded dataset featuring a whopping 3486 real reviews sourced directly from the esteemed aviation critique site, AirlineQuality.com. This meticulously curated dataset promises to be a goldmine for data enthusiasts, aviation aficionados, and anyone seeking a deep dive into the passenger experience aboard United Airlines.\n",
      "\n",
      "🔍 **Key Dataset Attributes**:\n",
      "\n",
      "**Seat_Type**: Dive into the nitty-gritty details of seat types, offering a comprehensive overview of the diverse seating options available on United Airlines flights.\n",
      "\n",
      "**Recommend_to_Others**: Uncover the sentiments of passengers as they share their verdict on whether they would recommend their chosen seat to fellow travelers.\n",
      "\n",
      "**Review_Heading**: Gain quick insights into the essence of each review with succinct review headings, providing a snapshot of passenger sentiments.\n",
      "\n",
      "**Review_Content**: Immerse yourself in the raw, unfiltered narratives of passengers, experiencing the highs and lows of United Airlines through detailed review content.\n",
      "\n",
      "⚙️ **Enhancements for Precision:**\n",
      "\n",
      "**Verification Column**: Elevate your analysis with the newly introduced 'Verification' column, meticulously crafted to discern whether each trip has been validated or not. This added layer of information empowers you to distinguish verified experiences from unverified ones, ensuring a more nuanced understanding of the dataset.\n",
      "💡 **Unlock Business Intelligence**:\n",
      "\n",
      "Whether you're an analyst, researcher, or a curious traveler, this dataset opens up a world of possibilities. Unearth patterns, trends, and hidden gems within the rich tapestry of passenger experiences, shaping a comprehensive narrative about United Airlines' seat quality.\n",
      "\n",
      "📊 **Why Explore This Dataset?**\n",
      "\n",
      "**Real-world Insights**: The reviews are sourced directly from passengers, providing authentic and unfiltered perspectives on seat quality.\n",
      "\n",
      "**Analytical Opportunities**: Leverage the dataset to conduct sentiment analysis, trend spotting, and recommendation modeling, offering invaluable insights for businesses and travelers alike.\n",
      "\n",
      "**Decision Support:** Businesses can use this dataset to inform seat improvement strategies, while travelers can make informed decisions based on peer reviews.\n",
      "\n",
      "Embark on a journey of data exploration with the United Airlines Seat Reviews Dataset on Kaggle—where every row unveils a unique story, and every column is a gateway to informed decision-making. The skies are no longer the limit; your data-driven adventure begins here! ✈️📈\n",
      "name 'detect' is not defined Language is not detected: cassandrapratt/childrens-book-covers-with-captions\n",
      "name 'detect' is not defined Language is not detected: childrens-book-covers-with-captions\n",
      "name 'detect' is not defined Language is not detected: cassandrapratt\n",
      "name 'detect' is not defined Language is not detected: Children's Book Covers with Captions\n",
      "name 'detect' is not defined Language is not detected: A Dataset for Image Captioning models training\n",
      "name 'detect' is not defined Language is not detected: With a collection of 1710 images manually annotated and an additional 190 images reserved for testing, this dataset serves as a resource for **training and evaluating Computer Vision models**.\n",
      "\n",
      "**Potential Applications:**\n",
      "- Children's Book Recognition: Train models to recognize and categorize children's book covers, enabling applications in library cataloging, recommendation systems, and more.\n",
      "- AI Storytelling: Explore the possibilities of AI-driven storytelling by leveraging the rich annotations to generate descriptive narratives based on cover art.\n",
      "- Educational Tools: Enhance educational tools and platforms by incorporating image recognition capabilities specifically designed for children's literature.\n",
      "name 'detect' is not defined Language is not detected: childrens-book-covers-with-captions\n",
      "name 'detect' is not defined Language is not detected: cassandrapratt\n",
      "name 'detect' is not defined Language is not detected: Children's Book Covers with Captions\n",
      "name 'detect' is not defined Language is not detected: A Dataset for Image Captioning models training\n",
      "name 'detect' is not defined Language is not detected: With a collection of 1710 images manually annotated and an additional 190 images reserved for testing, this dataset serves as a resource for **training and evaluating Computer Vision models**.\n",
      "\n",
      "**Potential Applications:**\n",
      "- Children's Book Recognition: Train models to recognize and categorize children's book covers, enabling applications in library cataloging, recommendation systems, and more.\n",
      "- AI Storytelling: Explore the possibilities of AI-driven storytelling by leveraging the rich annotations to generate descriptive narratives based on cover art.\n",
      "- Educational Tools: Enhance educational tools and platforms by incorporating image recognition capabilities specifically designed for children's literature.\n",
      "name 'detect' is not defined Language is not detected: iliassekkaf/computerparts\n",
      "name 'detect' is not defined Language is not detected: computerparts\n",
      "name 'detect' is not defined Language is not detected: iliassekkaf\n",
      "name 'detect' is not defined Language is not detected: Computer Parts (CPUs and GPUs)\n",
      "name 'detect' is not defined Language is not detected: How did computer specifications and performance evolve over time?\n",
      "name 'detect' is not defined Language is not detected: ### Contents\n",
      "This dataset contains detailed specifications, release dates, and release prices of computer parts.\n",
      "\n",
      "The dataset contains two CSV files: `gpus.csv` for Graphics Processing Units (GPUs), and `cpus.csv` for Central Processing Units (CPUs). Each table has its own list of unique entries, but the list of features includes: clock speeds, maximum temperatures, display resolutions, power draws, number of threads, release dates, release prices, die size, virtualization support, and many other similar fields. For more specific column-level metadata refer to the [Column Metadata](https://www.kaggle.com/iliassekkaf/computerparts/data).\n",
      "\n",
      "Looking for inspiration? Try starting by reading [\"Using regression to predict the GPUs of the future\"](https://www.kaggle.com/skalskip/using-regression-to-predict-gpus-of-the-future).\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* How did performance over price ratio evolve over time? \n",
      "* How about general computing power?  \n",
      "* Are there any manufacturers that are known for some specific range of performance &amp; price? \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data given here belongs mainly to Intel, Game-Debate, and the companies involved in producing the part. I do not own the data I uploaded it solely for informative purposes, under their original license.\n",
      "\n",
      "\n",
      "  [1]: http://www.futureelectronics.com/en/Microprocessors/embedded-processors.aspx\n",
      "name 'detect' is not defined Language is not detected: computerparts\n",
      "name 'detect' is not defined Language is not detected: iliassekkaf\n",
      "name 'detect' is not defined Language is not detected: Computer Parts (CPUs and GPUs)\n",
      "name 'detect' is not defined Language is not detected: How did computer specifications and performance evolve over time?\n",
      "name 'detect' is not defined Language is not detected: ### Contents\n",
      "This dataset contains detailed specifications, release dates, and release prices of computer parts.\n",
      "\n",
      "The dataset contains two CSV files: `gpus.csv` for Graphics Processing Units (GPUs), and `cpus.csv` for Central Processing Units (CPUs). Each table has its own list of unique entries, but the list of features includes: clock speeds, maximum temperatures, display resolutions, power draws, number of threads, release dates, release prices, die size, virtualization support, and many other similar fields. For more specific column-level metadata refer to the [Column Metadata](https://www.kaggle.com/iliassekkaf/computerparts/data).\n",
      "\n",
      "Looking for inspiration? Try starting by reading [\"Using regression to predict the GPUs of the future\"](https://www.kaggle.com/skalskip/using-regression-to-predict-gpus-of-the-future).\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* How did performance over price ratio evolve over time? \n",
      "* How about general computing power?  \n",
      "* Are there any manufacturers that are known for some specific range of performance &amp; price? \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data given here belongs mainly to Intel, Game-Debate, and the companies involved in producing the part. I do not own the data I uploaded it solely for informative purposes, under their original license.\n",
      "\n",
      "\n",
      "  [1]: http://www.futureelectronics.com/en/Microprocessors/embedded-processors.aspx\n",
      "name 'detect' is not defined Language is not detected: prajitdatta/data-stories-of-us-airlines\n",
      "name 'detect' is not defined Language is not detected: data-stories-of-us-airlines\n",
      "name 'detect' is not defined Language is not detected: prajitdatta\n",
      "name 'detect' is not defined Language is not detected: Data Stories of US Airlines, 1987-2008\n",
      "name 'detect' is not defined Language is not detected: Fight arrival and departure details for all commercial flights\n",
      "name 'detect' is not defined Language is not detected: **About the Data**\n",
      "------------------\n",
      "\n",
      "The data used in this project is real and is based on the collection of over 20 years. The total\n",
      "number of record in this dataset is roughly around 120 million rows and the size of the data\n",
      "is approximately 12GB. The data consists of flight arrival and departure details for all\n",
      "commercial flights within the USA, from October 1987 to April 2008. This is a large dataset.\n",
      "There are around 29 attributes.\n",
      "\n",
      "**How to get the data?**\n",
      "The data originally comes from [http://stat-computing.org/dataexpo/2009/the-data.html][1]\n",
      "\n",
      "You can download the data for each year by clicking the appropriate link in the above\n",
      "website (Remember the size is going to be more than 12GB).\n",
      "\n",
      "**(i) Problem Statement**\n",
      "(a)    Check the skewness of Distance travelled by airlines.\n",
      "(b)\tCalculate the mean, median and quantiles of the distance travelled by US Airlines (US). \n",
      "(c)\tCheck the standard deviation of distance travelled by American Airlines (AA).\n",
      "(d)\tDraw a boxplot of  UniqueCarrier with Distance.\n",
      "(e)\tDraw the direction of relationship between ArrDelay and DepDelay by drawing a scatterplot.\n",
      "\n",
      "\n",
      "**(ii) Problem Statement**\n",
      "\n",
      "\n",
      "(a)\tWhat is the probability that a flight which is landing/taking off is “WN” Airlines (marginal probability)\n",
      "(b)\tWhat is the probability that a flight which is landing/taking off is either “WN” or “AA” Airlines (disjoint events)\n",
      "(c)\tWhat is the joint probability that a flight is both “WN” and travels less than 600 miles (joint probability)\n",
      "(d)\tWhat is the conditional probability that the flight travels less than 2500 miles given that the flight is “AA” Airlines (conditional probability)\n",
      "(e)\tWhat is the joint probability of a flight getting cancelled and is supposed to travel less than 2500 miles given that the flight is “AA” Airlines\t\t\t                                     (joint + conditional probability)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**(iii) Problem Statement**\n",
      "\n",
      "\n",
      "(a)\tSuppose arrival delays of flights belonging to “AA” are normally distributed with mean 15 minutes and standard deviation 3 minutes. If the “AA” plans to announce a scheme where it will give 50% cash back if their flights are delayed by 20 minutes, how much percentage of the trips “AA” is supposed to loose this money. (Hint: pnorm)\n",
      "(b)\tAssume that 65% of flights are diverted due to bad weather through the Weather System. What is the probability that in a random sample of 10 flights, 6 are diverted through the Weather System. (Hint: dbinorm)\n",
      "(c)\tDo linear regression between the Arrival Delay and Departure Delay of the flights.\n",
      "(d)\tFind out the confidence interval of the fitted linear regression line.\n",
      "(e)\tPerform a multiple linear regression between the Arrival Delay along with the Departure Delay and Distance travelled by flights.\n",
      "\n",
      "\n",
      "  [1]: http://stat-computing.org/dataexpo/2009/the-data.html\n",
      "name 'detect' is not defined Language is not detected: data-stories-of-us-airlines\n",
      "name 'detect' is not defined Language is not detected: prajitdatta\n",
      "name 'detect' is not defined Language is not detected: Data Stories of US Airlines, 1987-2008\n",
      "name 'detect' is not defined Language is not detected: Fight arrival and departure details for all commercial flights\n",
      "name 'detect' is not defined Language is not detected: **About the Data**\n",
      "------------------\n",
      "\n",
      "The data used in this project is real and is based on the collection of over 20 years. The total\n",
      "number of record in this dataset is roughly around 120 million rows and the size of the data\n",
      "is approximately 12GB. The data consists of flight arrival and departure details for all\n",
      "commercial flights within the USA, from October 1987 to April 2008. This is a large dataset.\n",
      "There are around 29 attributes.\n",
      "\n",
      "**How to get the data?**\n",
      "The data originally comes from [http://stat-computing.org/dataexpo/2009/the-data.html][1]\n",
      "\n",
      "You can download the data for each year by clicking the appropriate link in the above\n",
      "website (Remember the size is going to be more than 12GB).\n",
      "\n",
      "**(i) Problem Statement**\n",
      "(a)    Check the skewness of Distance travelled by airlines.\n",
      "(b)\tCalculate the mean, median and quantiles of the distance travelled by US Airlines (US). \n",
      "(c)\tCheck the standard deviation of distance travelled by American Airlines (AA).\n",
      "(d)\tDraw a boxplot of  UniqueCarrier with Distance.\n",
      "(e)\tDraw the direction of relationship between ArrDelay and DepDelay by drawing a scatterplot.\n",
      "\n",
      "\n",
      "**(ii) Problem Statement**\n",
      "\n",
      "\n",
      "(a)\tWhat is the probability that a flight which is landing/taking off is “WN” Airlines (marginal probability)\n",
      "(b)\tWhat is the probability that a flight which is landing/taking off is either “WN” or “AA” Airlines (disjoint events)\n",
      "(c)\tWhat is the joint probability that a flight is both “WN” and travels less than 600 miles (joint probability)\n",
      "(d)\tWhat is the conditional probability that the flight travels less than 2500 miles given that the flight is “AA” Airlines (conditional probability)\n",
      "(e)\tWhat is the joint probability of a flight getting cancelled and is supposed to travel less than 2500 miles given that the flight is “AA” Airlines\t\t\t                                     (joint + conditional probability)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**(iii) Problem Statement**\n",
      "\n",
      "\n",
      "(a)\tSuppose arrival delays of flights belonging to “AA” are normally distributed with mean 15 minutes and standard deviation 3 minutes. If the “AA” plans to announce a scheme where it will give 50% cash back if their flights are delayed by 20 minutes, how much percentage of the trips “AA” is supposed to loose this money. (Hint: pnorm)\n",
      "(b)\tAssume that 65% of flights are diverted due to bad weather through the Weather System. What is the probability that in a random sample of 10 flights, 6 are diverted through the Weather System. (Hint: dbinorm)\n",
      "(c)\tDo linear regression between the Arrival Delay and Departure Delay of the flights.\n",
      "(d)\tFind out the confidence interval of the fitted linear regression line.\n",
      "(e)\tPerform a multiple linear regression between the Arrival Delay along with the Departure Delay and Distance travelled by flights.\n",
      "\n",
      "\n",
      "  [1]: http://stat-computing.org/dataexpo/2009/the-data.html\n",
      "name 'detect' is not defined Language is not detected: deepikaarikesavan/movie-recommendation-ml\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-ml\n",
      "name 'detect' is not defined Language is not detected: deepikaarikesavan\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation ML\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-ml\n",
      "name 'detect' is not defined Language is not detected: deepikaarikesavan\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation ML\n",
      "name 'detect' is not defined Language is not detected: eyimofeapinnick/nigeria-rent-prices-2022\n",
      "name 'detect' is not defined Language is not detected: nigeria-rent-prices-2022\n",
      "name 'detect' is not defined Language is not detected: eyimofeapinnick\n",
      "name 'detect' is not defined Language is not detected: Nigeria Rent Prices (2022)\n",
      "name 'detect' is not defined Language is not detected: Dataset consisting of rent prices of residential properties in Nigeria\n",
      "name 'detect' is not defined Language is not detected: The dataset for this project was obtained from a Nigerian real estate company called PropertyPro NG (https://www.propertypro.ng/). The extracted dataset contained information such as the rent of residential properties e.g. homes, apartments across different cities in  Nigeria in 2022.\n",
      "\n",
      "The price of this properties are in the Nigerian Naira (NGN)\n",
      "\n",
      "Using web scraping technology through Python and BeautifulSoup4, the data was scraped from their listing page of properties in Nigeria. \n",
      "\n",
      "The goal is to predict the sales price of a property. This would be a good challenge for practicing your feature engineering skills like:\n",
      "- Extracting the type of property from the Title/More Info e.g. Duplex, Semi-Duplex, Apartment, Flat\n",
      "-  Extracting the State, City, Neighborhood or Estate of the property from the Location\n",
      "- Converting the strings in the Bathroom, Toilet or Bedroom columns into numbers\n",
      "- Converting the Price column into a numerical object and extracting the Price Period e.g per year, per sqm from the same column\n",
      "\n",
      "\n",
      "|NAME OF COLUMN| DESCRIPTION OF COLUMN|DATA OBJECT|\n",
      "| --- | --- | --- |\n",
      "|Title| Title of the property e.g. Brand New 5 Bedroom Semi-Detached Duplex |String|\n",
      "|More Info| More Information about the property e.g. 4 BEDROOM HOUSE FOR RENT |String|\n",
      "|Location|Location of the property e.g. Hrc Estate Beside Vgc Estate, Vgc Lekki Lagos|String|\n",
      "|Serviced|Whether the property has been serviced e.g. sockets have been installed or other renovations.|Integer|\n",
      "|Newly Built|Whether the property was newly built|Integer|\n",
      "|Furnished|Whether the property has been furnished|Integer|\n",
      "|Bedrooms|Number of bedrooms on the property|String|\n",
      "|Bathrooms|Number of bathrooms on the property|String|\n",
      "|Toilets|Number of toilets on the property|String|\n",
      "|Price|The price of the property|String **(Target)**|\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nigeria-rent-prices-2022\n",
      "name 'detect' is not defined Language is not detected: eyimofeapinnick\n",
      "name 'detect' is not defined Language is not detected: Nigeria Rent Prices (2022)\n",
      "name 'detect' is not defined Language is not detected: Dataset consisting of rent prices of residential properties in Nigeria\n",
      "name 'detect' is not defined Language is not detected: The dataset for this project was obtained from a Nigerian real estate company called PropertyPro NG (https://www.propertypro.ng/). The extracted dataset contained information such as the rent of residential properties e.g. homes, apartments across different cities in  Nigeria in 2022.\n",
      "\n",
      "The price of this properties are in the Nigerian Naira (NGN)\n",
      "\n",
      "Using web scraping technology through Python and BeautifulSoup4, the data was scraped from their listing page of properties in Nigeria. \n",
      "\n",
      "The goal is to predict the sales price of a property. This would be a good challenge for practicing your feature engineering skills like:\n",
      "- Extracting the type of property from the Title/More Info e.g. Duplex, Semi-Duplex, Apartment, Flat\n",
      "-  Extracting the State, City, Neighborhood or Estate of the property from the Location\n",
      "- Converting the strings in the Bathroom, Toilet or Bedroom columns into numbers\n",
      "- Converting the Price column into a numerical object and extracting the Price Period e.g per year, per sqm from the same column\n",
      "\n",
      "\n",
      "|NAME OF COLUMN| DESCRIPTION OF COLUMN|DATA OBJECT|\n",
      "| --- | --- | --- |\n",
      "|Title| Title of the property e.g. Brand New 5 Bedroom Semi-Detached Duplex |String|\n",
      "|More Info| More Information about the property e.g. 4 BEDROOM HOUSE FOR RENT |String|\n",
      "|Location|Location of the property e.g. Hrc Estate Beside Vgc Estate, Vgc Lekki Lagos|String|\n",
      "|Serviced|Whether the property has been serviced e.g. sockets have been installed or other renovations.|Integer|\n",
      "|Newly Built|Whether the property was newly built|Integer|\n",
      "|Furnished|Whether the property has been furnished|Integer|\n",
      "|Bedrooms|Number of bedrooms on the property|String|\n",
      "|Bathrooms|Number of bathrooms on the property|String|\n",
      "|Toilets|Number of toilets on the property|String|\n",
      "|Price|The price of the property|String **(Target)**|\n",
      "\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal/anime-list-for-recommendation-system-june-2021\n",
      "name 'detect' is not defined Language is not detected: anime-list-for-recommendation-system-june-2021\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal\n",
      "name 'detect' is not defined Language is not detected: Anime List for Recommendation System (June 2021)\n",
      "name 'detect' is not defined Language is not detected: List of 18,000 anime for content based recommendation. Updated each season.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset contains the list and metadata of all published anime (released/unreleased) currently about 18,000 on the MyAnimeList site.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "anime_list.csv\n",
      "\n",
      "- mal_id            : Unique anime identifier.\n",
      "- title                : Title of the anime.\n",
      "- synopsis        :  Synopsis of the anime, 1139 null values were found...initialized to \"-\".\n",
      "- background   : Could not scrape this field successfully and thus is totally empty..will be fixed in next update.\n",
      "- aired               : Range of time anime was airing. example format: \"Sep 30, 2004 to Sep 29, 2005\".\n",
      "- airing               : If it is currently airing or not. 0 if anime has ended 1 if it is ongoing\n",
      "- duration          : Duration of each episode.\n",
      "- episodes         : Number of episodes in the given anime. 485 null values found, initialized to -1.\n",
      "- type                 : Type of anime ['TV', 'Movie', 'OVA', 'Special', 'ONA', 'Music', 'Unknown']\n",
      "- favorites          : Number of people who favorited this anime.\n",
      "- members         : Number of members.\n",
      "- rank                 : Ranking as per June '21. 1760 Null values found, initialized to -1.\n",
      "- popularity        : Popularity Rankings as per June '21.\n",
      "- score               : Average score given by the users.    5296 Null values found.\n",
      "- scored_by       : Number of people to score the anime. 5296 Null values found.\n",
      "- rating               : Rating given to the anime.['PG - Children', 'PG-13 - Teens 13 or older', 'G - All Ages', 'R+ - Mild Nudity', 'R - 17+','Rx - Hentai']\n",
      "- premiered        : Year and season of premiere. 13296 Null values found.\n",
      "- genres              : Genre the anime belongs to.\n",
      "- related              : Related anime.(String format, will be converted to dictionaries in next update)\n",
      "- status               : Status of anime if airing currently or not.\n",
      "- licensors          : Names of Licensor. (String format separated by \",\")\n",
      "- producers        : Names of Producer. (String format separated by \",)\n",
      "- studios             : Names of studio.   (String format separated by \",\")\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is collected from MyAnimeList.com\n",
      "Anime ID collected by web scrapping through the anime ranking pages to get mal_id followed by single page search for each collected ID.\n",
      "\n",
      "\n",
      "### Fixes on Next update (Summer 2021):\n",
      "\n",
      "-The issue of \"background\" will be fixed.\n",
      "-\"related\" will be in dictionary format for easy processing.\n",
      "-Will upload separate filtered_anime_list.csv without NSFW hentai listings for people who needs a SFW list.\n",
      "\n",
      "\n",
      "### Ending Notes\n",
      "\n",
      "This dataset was made as to store list of all anime and its metadata published on the site. If requested, Reviews and Score per user can be added to the dataset for collaborative filtering approach.\n",
      "This was my first web scraping experience for recommendation project I am working on, please upvote if this was of help, Thank You.\n",
      "name 'detect' is not defined Language is not detected: anime-list-for-recommendation-system-june-2021\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal\n",
      "name 'detect' is not defined Language is not detected: Anime List for Recommendation System (June 2021)\n",
      "name 'detect' is not defined Language is not detected: List of 18,000 anime for content based recommendation. Updated each season.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset contains the list and metadata of all published anime (released/unreleased) currently about 18,000 on the MyAnimeList site.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "anime_list.csv\n",
      "\n",
      "- mal_id            : Unique anime identifier.\n",
      "- title                : Title of the anime.\n",
      "- synopsis        :  Synopsis of the anime, 1139 null values were found...initialized to \"-\".\n",
      "- background   : Could not scrape this field successfully and thus is totally empty..will be fixed in next update.\n",
      "- aired               : Range of time anime was airing. example format: \"Sep 30, 2004 to Sep 29, 2005\".\n",
      "- airing               : If it is currently airing or not. 0 if anime has ended 1 if it is ongoing\n",
      "- duration          : Duration of each episode.\n",
      "- episodes         : Number of episodes in the given anime. 485 null values found, initialized to -1.\n",
      "- type                 : Type of anime ['TV', 'Movie', 'OVA', 'Special', 'ONA', 'Music', 'Unknown']\n",
      "- favorites          : Number of people who favorited this anime.\n",
      "- members         : Number of members.\n",
      "- rank                 : Ranking as per June '21. 1760 Null values found, initialized to -1.\n",
      "- popularity        : Popularity Rankings as per June '21.\n",
      "- score               : Average score given by the users.    5296 Null values found.\n",
      "- scored_by       : Number of people to score the anime. 5296 Null values found.\n",
      "- rating               : Rating given to the anime.['PG - Children', 'PG-13 - Teens 13 or older', 'G - All Ages', 'R+ - Mild Nudity', 'R - 17+','Rx - Hentai']\n",
      "- premiered        : Year and season of premiere. 13296 Null values found.\n",
      "- genres              : Genre the anime belongs to.\n",
      "- related              : Related anime.(String format, will be converted to dictionaries in next update)\n",
      "- status               : Status of anime if airing currently or not.\n",
      "- licensors          : Names of Licensor. (String format separated by \",\")\n",
      "- producers        : Names of Producer. (String format separated by \",)\n",
      "- studios             : Names of studio.   (String format separated by \",\")\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is collected from MyAnimeList.com\n",
      "Anime ID collected by web scrapping through the anime ranking pages to get mal_id followed by single page search for each collected ID.\n",
      "\n",
      "\n",
      "### Fixes on Next update (Summer 2021):\n",
      "\n",
      "-The issue of \"background\" will be fixed.\n",
      "-\"related\" will be in dictionary format for easy processing.\n",
      "-Will upload separate filtered_anime_list.csv without NSFW hentai listings for people who needs a SFW list.\n",
      "\n",
      "\n",
      "### Ending Notes\n",
      "\n",
      "This dataset was made as to store list of all anime and its metadata published on the site. If requested, Reviews and Score per user can be added to the dataset for collaborative filtering approach.\n",
      "This was my first web scraping experience for recommendation project I am working on, please upvote if this was of help, Thank You.\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal/netflix-tv-shows-and-movie-list\n",
      "name 'detect' is not defined Language is not detected: netflix-tv-shows-and-movie-list\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal\n",
      "name 'detect' is not defined Language is not detected: Netflix Movie and TV Shows (June 2021)\n",
      "name 'detect' is not defined Language is not detected: Data of 7000 Netflix Movies and TV shows for Recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset contains the list and metadata of all TV Shows and Movies available on Netflix currently about 7000 taken from the IMDB website.\n",
      "Upvote if you liked it.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "netflix_list.csv\n",
      "\n",
      "- imdb_id           : Unique show identifier.\n",
      "- title                  : Title of the show.\n",
      "- popular_rank   : Ranking as given by IMDB when filtered by popularity.\n",
      "- certificate        : Contains the age certifications received by the show. Many null values.\n",
      "- startYear          : When the show was first broadcasted.\n",
      "- endYear           : Year of show ending\n",
      "- episodes         : Number of episodes in the show. 1 for movies.\n",
      "- type                 : Movie or Series\n",
      "- orign_country  : Country of origin of the show\n",
      "- language          : Language of the show.\n",
      "- plot                   : Synopsis of the show.\n",
      "- summary          : Summary of the story of the show.\n",
      "- rating               : Average rating given to the show.\n",
      "- numVotes        : Number of votes received by the show.\n",
      "- genres             : Genre the show belongs to.\n",
      "- isAdult             : 1  If adult content present. 0 if not.\n",
      "- cast                 : Main cast of the show in list format.\n",
      "- image_url        : Link to poster image.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is collected from IMDB website\n",
      "Data collected by web scrapping through the shows ranking pages with filtered to show Netflix related content(16000+ entries) and noting down the imdb_id, followed by single page search for each collected ID and unique title name.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: netflix-tv-shows-and-movie-list\n",
      "name 'detect' is not defined Language is not detected: snehaanbhawal\n",
      "name 'detect' is not defined Language is not detected: Netflix Movie and TV Shows (June 2021)\n",
      "name 'detect' is not defined Language is not detected: Data of 7000 Netflix Movies and TV shows for Recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Dataset contains the list and metadata of all TV Shows and Movies available on Netflix currently about 7000 taken from the IMDB website.\n",
      "Upvote if you liked it.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "netflix_list.csv\n",
      "\n",
      "- imdb_id           : Unique show identifier.\n",
      "- title                  : Title of the show.\n",
      "- popular_rank   : Ranking as given by IMDB when filtered by popularity.\n",
      "- certificate        : Contains the age certifications received by the show. Many null values.\n",
      "- startYear          : When the show was first broadcasted.\n",
      "- endYear           : Year of show ending\n",
      "- episodes         : Number of episodes in the show. 1 for movies.\n",
      "- type                 : Movie or Series\n",
      "- orign_country  : Country of origin of the show\n",
      "- language          : Language of the show.\n",
      "- plot                   : Synopsis of the show.\n",
      "- summary          : Summary of the story of the show.\n",
      "- rating               : Average rating given to the show.\n",
      "- numVotes        : Number of votes received by the show.\n",
      "- genres             : Genre the show belongs to.\n",
      "- isAdult             : 1  If adult content present. 0 if not.\n",
      "- cast                 : Main cast of the show in list format.\n",
      "- image_url        : Link to poster image.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This is collected from IMDB website\n",
      "Data collected by web scrapping through the shows ranking pages with filtered to show Netflix related content(16000+ entries) and noting down the imdb_id, followed by single page search for each collected ID and unique title name.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aleexharris/bitcoin-network-on-chain-blockchain-data\n",
      "name 'detect' is not defined Language is not detected: bitcoin-network-on-chain-blockchain-data\n",
      "name 'detect' is not defined Language is not detected: aleexharris\n",
      "name 'detect' is not defined Language is not detected: Bitcoin Network On-Chain Blockchain Data\n",
      "name 'detect' is not defined Language is not detected: incl. Hashrate, Difficulty, Miner Revenues, Block Size, Tx Fees and more\n",
      "name 'detect' is not defined Language is not detected: **About This Dataset**\n",
      "\n",
      "This dataset aims to collect and collate information about activity on the Bitcoin blockchain, with a focus directed at on-chain data which describes the state of the network at any given time. Some examples of these metrics include:\n",
      "- Total Supply\n",
      "- Hashrate\n",
      "- Difficulty\n",
      "- Block Size\n",
      "- Transaction Rate\n",
      "- Mempool Size\n",
      "- Miner Revenue\n",
      "- Fear and Greed Index\n",
      "- Net Unrealized Profit/Loss (NUPL)\n",
      "- Coin Days Destroyed (CDD)\n",
      "\n",
      "Descriptions of all column headers are included as part of the .csv files in this dataset and are also viewable below.\n",
      "\n",
      "The current iteration of the dataset leverages the blockchain.com API to provide both daily and half-hourly data about the state of the chain, as well as daily metrics provided by lookintobitcoin.com. Future iterations of this dataset expect to utilise other sources of information as well. Please see the linked Kaggle Notebook under the Provenance section of this page for more information.\n",
      "\n",
      "Current sources of information include:\n",
      "- Blockchain.com\n",
      "- LookIntoBitcoin.com\n",
      "\n",
      "Please recommend other sources of information that should also be added to this dataset.\n",
      "\n",
      "Note: The 1st Bitcoin block was mined on 3rd January 2009 by Satoshi, but the second was not mined until January 9th, 6 days later, hence the period of null data for many metrics between 3rd-9th January 2009.\n",
      "\n",
      "**Disclaimer**\n",
      "\n",
      "This is information – not financial advice or recommendation. The content and materials featured or linked to in this dataset are for your information and education only and are not attended to address your particular personal requirements.\n",
      "\n",
      "The information does not constitute financial advice or recommendation and should not be considered as such. The owner of this dataset is not regulated by the Financial Conduct Authority (FCA) is not a financial advisor and is therefore not authorised to offer financial advice.\n",
      "\n",
      "Do your own research and seek independent advice when required. Always do your own research and seek independent financial advice when required. Any arrangement made between you and any third party named or linked to from the site is at your sole risk and responsibility. The owner of this dataset and its associated curators assume no liability for your actions.\n",
      "\n",
      "Investing carries risk. The value of investments and any income derived from them can fall as well as rise and you may not get back the original amount you invested.\n",
      "name 'detect' is not defined Language is not detected: bitcoin-network-on-chain-blockchain-data\n",
      "name 'detect' is not defined Language is not detected: aleexharris\n",
      "name 'detect' is not defined Language is not detected: Bitcoin Network On-Chain Blockchain Data\n",
      "name 'detect' is not defined Language is not detected: incl. Hashrate, Difficulty, Miner Revenues, Block Size, Tx Fees and more\n",
      "name 'detect' is not defined Language is not detected: **About This Dataset**\n",
      "\n",
      "This dataset aims to collect and collate information about activity on the Bitcoin blockchain, with a focus directed at on-chain data which describes the state of the network at any given time. Some examples of these metrics include:\n",
      "- Total Supply\n",
      "- Hashrate\n",
      "- Difficulty\n",
      "- Block Size\n",
      "- Transaction Rate\n",
      "- Mempool Size\n",
      "- Miner Revenue\n",
      "- Fear and Greed Index\n",
      "- Net Unrealized Profit/Loss (NUPL)\n",
      "- Coin Days Destroyed (CDD)\n",
      "\n",
      "Descriptions of all column headers are included as part of the .csv files in this dataset and are also viewable below.\n",
      "\n",
      "The current iteration of the dataset leverages the blockchain.com API to provide both daily and half-hourly data about the state of the chain, as well as daily metrics provided by lookintobitcoin.com. Future iterations of this dataset expect to utilise other sources of information as well. Please see the linked Kaggle Notebook under the Provenance section of this page for more information.\n",
      "\n",
      "Current sources of information include:\n",
      "- Blockchain.com\n",
      "- LookIntoBitcoin.com\n",
      "\n",
      "Please recommend other sources of information that should also be added to this dataset.\n",
      "\n",
      "Note: The 1st Bitcoin block was mined on 3rd January 2009 by Satoshi, but the second was not mined until January 9th, 6 days later, hence the period of null data for many metrics between 3rd-9th January 2009.\n",
      "\n",
      "**Disclaimer**\n",
      "\n",
      "This is information – not financial advice or recommendation. The content and materials featured or linked to in this dataset are for your information and education only and are not attended to address your particular personal requirements.\n",
      "\n",
      "The information does not constitute financial advice or recommendation and should not be considered as such. The owner of this dataset is not regulated by the Financial Conduct Authority (FCA) is not a financial advisor and is therefore not authorised to offer financial advice.\n",
      "\n",
      "Do your own research and seek independent advice when required. Always do your own research and seek independent financial advice when required. Any arrangement made between you and any third party named or linked to from the site is at your sole risk and responsibility. The owner of this dataset and its associated curators assume no liability for your actions.\n",
      "\n",
      "Investing carries risk. The value of investments and any income derived from them can fall as well as rise and you may not get back the original amount you invested.\n",
      "name 'detect' is not defined Language is not detected: amirhoseinsedaghati/the-weather-of-187-countries-in-2020\n",
      "name 'detect' is not defined Language is not detected: the-weather-of-187-countries-in-2020\n",
      "name 'detect' is not defined Language is not detected: amirhoseinsedaghati\n",
      "name 'detect' is not defined Language is not detected: The Weather of 187 Countries in 2020\n",
      "name 'detect' is not defined Language is not detected: The weather of 187 Countries/Regions collected by weather stations\n",
      "name 'detect' is not defined Language is not detected: ###Version 1:\n",
      "\n",
      "I collected this version of the dataset to perform A/B Testing on data related to the Coronavirus. All of the data was obtained from [The National Oceanic and Atmospheric Administration](https://www.ncei.noaa.gov/) (NOAA), ensuring its authenticity. The dataset contains 1,392,575 instances and 23 features and uses **metric** units for measurement, named `the weather of 187 countries in 2020.csv`.\n",
      "\n",
      "The weather observations were collected by stations located in 187 countries/regions, spanning from January 22, 2020 to July 27, 2020.\n",
      "\n",
      "When weather data is not available for a country or region, such as Guatemala, I use the nearest country or region with a weather station, as advised by the NOAA support team. However, this approach can lead to duplicated instances in the dataset, totaling about 320,000 instances. These instances are not truly duplicated, as each belongs to a country or region without its own weather station. In these cases, neighboring countries or regions, such as Mexico, report the weather situation for the affected regions to NOAA. While the weather situations may not be exactly the same in different regions, we can consider them to be the same to avoid potential errors, which may arise due to NOAA's grouping of regions.\n",
      "\n",
      "###Version 2:\n",
      "\n",
      "I've uploaded a new version of this dataset that has 1,026,119 instances and 23 features and uses **metric** units for measurement, named `the weather of 155 countries in 2020.csv`. This version has no duplicated instances. So, You can choose one of these to work with. And please feel free to give me feedback, or ask me questions in the discussion tab of the dataset.\n",
      "\n",
      "###The Feature Explanations:\n",
      "\n",
      "- STATION: The station identifier\n",
      "- Country/Region: The country or region where the station is located\n",
      "- DATE: The date of the weather observation in YYYY-MM-DD format\n",
      "- Year: The year of the weather observation\n",
      "- Month: The month of the weather observation\n",
      "- Day: The day of the weather observation\n",
      "- PRCP: The amount of precipitation (rain or snow)\n",
      "- SNWD: The depth of snow on the ground\n",
      "- TAVG: The average temperature\n",
      "- TMAX: The maximum temperature\n",
      "- TMIN: The minimum temperature\n",
      "- SNOW: The amount of snowfall\n",
      "- LATITUDE: The latitude of the station's location\n",
      "- LONGITUDE: The longitude of the station's location\n",
      "- ELEVATION: The elevation of the station's location\n",
      "- PRCP_ATTRIBUTES: Additional attributes for the precipitation data\n",
      "- TAVG_ATTRIBUTES: Additional attributes for the average temperature data\n",
      "- TMAX_ATTRIBUTES:  Additional attributes for the maximum temperature data\n",
      "- TMIN_ATTRIBUTES: Additional attributes for the minimum temperature data\n",
      "- DAPR: The number of days since the last precipitation of any amount\n",
      "- MDPR: The multiday precipitation total\n",
      "- WESD: The water equivalent of the snow depth on the ground\n",
      "- SNWD_ATTRIBUTES: Additional attributes for the snow depth data\n",
      "name 'detect' is not defined Language is not detected: the-weather-of-187-countries-in-2020\n",
      "name 'detect' is not defined Language is not detected: amirhoseinsedaghati\n",
      "name 'detect' is not defined Language is not detected: The Weather of 187 Countries in 2020\n",
      "name 'detect' is not defined Language is not detected: The weather of 187 Countries/Regions collected by weather stations\n",
      "name 'detect' is not defined Language is not detected: ###Version 1:\n",
      "\n",
      "I collected this version of the dataset to perform A/B Testing on data related to the Coronavirus. All of the data was obtained from [The National Oceanic and Atmospheric Administration](https://www.ncei.noaa.gov/) (NOAA), ensuring its authenticity. The dataset contains 1,392,575 instances and 23 features and uses **metric** units for measurement, named `the weather of 187 countries in 2020.csv`.\n",
      "\n",
      "The weather observations were collected by stations located in 187 countries/regions, spanning from January 22, 2020 to July 27, 2020.\n",
      "\n",
      "When weather data is not available for a country or region, such as Guatemala, I use the nearest country or region with a weather station, as advised by the NOAA support team. However, this approach can lead to duplicated instances in the dataset, totaling about 320,000 instances. These instances are not truly duplicated, as each belongs to a country or region without its own weather station. In these cases, neighboring countries or regions, such as Mexico, report the weather situation for the affected regions to NOAA. While the weather situations may not be exactly the same in different regions, we can consider them to be the same to avoid potential errors, which may arise due to NOAA's grouping of regions.\n",
      "\n",
      "###Version 2:\n",
      "\n",
      "I've uploaded a new version of this dataset that has 1,026,119 instances and 23 features and uses **metric** units for measurement, named `the weather of 155 countries in 2020.csv`. This version has no duplicated instances. So, You can choose one of these to work with. And please feel free to give me feedback, or ask me questions in the discussion tab of the dataset.\n",
      "\n",
      "###The Feature Explanations:\n",
      "\n",
      "- STATION: The station identifier\n",
      "- Country/Region: The country or region where the station is located\n",
      "- DATE: The date of the weather observation in YYYY-MM-DD format\n",
      "- Year: The year of the weather observation\n",
      "- Month: The month of the weather observation\n",
      "- Day: The day of the weather observation\n",
      "- PRCP: The amount of precipitation (rain or snow)\n",
      "- SNWD: The depth of snow on the ground\n",
      "- TAVG: The average temperature\n",
      "- TMAX: The maximum temperature\n",
      "- TMIN: The minimum temperature\n",
      "- SNOW: The amount of snowfall\n",
      "- LATITUDE: The latitude of the station's location\n",
      "- LONGITUDE: The longitude of the station's location\n",
      "- ELEVATION: The elevation of the station's location\n",
      "- PRCP_ATTRIBUTES: Additional attributes for the precipitation data\n",
      "- TAVG_ATTRIBUTES: Additional attributes for the average temperature data\n",
      "- TMAX_ATTRIBUTES:  Additional attributes for the maximum temperature data\n",
      "- TMIN_ATTRIBUTES: Additional attributes for the minimum temperature data\n",
      "- DAPR: The number of days since the last precipitation of any amount\n",
      "- MDPR: The multiday precipitation total\n",
      "- WESD: The water equivalent of the snow depth on the ground\n",
      "- SNWD_ATTRIBUTES: Additional attributes for the snow depth data\n",
      "name 'detect' is not defined Language is not detected: anashamoutni/optical-care-reimbursements-in-france-20092021\n",
      "name 'detect' is not defined Language is not detected: optical-care-reimbursements-in-france-20092021\n",
      "name 'detect' is not defined Language is not detected: anashamoutni\n",
      "name 'detect' is not defined Language is not detected: Optical Care reimbursements in France (2009-2021)\n",
      "name 'detect' is not defined Language is not detected: All Health Insurance Plans included\n",
      "name 'detect' is not defined Language is not detected: The dataset available for download on this page is part of the monthly Open Damir series - a complete database on optical health insurance expenditures (data from all health insurance schemes in France from 2009 to 2021, monthly data updated every year).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: optical-care-reimbursements-in-france-20092021\n",
      "name 'detect' is not defined Language is not detected: anashamoutni\n",
      "name 'detect' is not defined Language is not detected: Optical Care reimbursements in France (2009-2021)\n",
      "name 'detect' is not defined Language is not detected: All Health Insurance Plans included\n",
      "name 'detect' is not defined Language is not detected: The dataset available for download on this page is part of the monthly Open Damir series - a complete database on optical health insurance expenditures (data from all health insurance schemes in France from 2009 to 2021, monthly data updated every year).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nayanack/netflix\n",
      "name 'detect' is not defined Language is not detected: netflix\n",
      "name 'detect' is not defined Language is not detected: nayanack\n",
      "name 'detect' is not defined Language is not detected: Netflix Chronicles: Exploring Movies and TV Shows \n",
      "name 'detect' is not defined Language is not detected: Unlocking Netflix: A Comprehensive Database of Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12038776%2Fdbabda1e8f2d39e88b030173303b2724%2FNetflix.jpg?generation=1713257307281984&alt=media)\n",
      "#  Dataset Overview\n",
      "&gt;Netflix is one of the most popular media and video streaming platforms. They have over 10000 movies or tv shows available on their platform, as of mid-2021, they have over 222M Subscribers globally. This tabular dataset consists of listings of all the movies and tv shows available on Netflix, along with details such as - cast, directors, ratings, release year, duration, etc.\n",
      "\n",
      "## Key details about the dataset:\n",
      "1. Number of Entries: The dataset contains 8807 entries.\n",
      "2. Columns: There are 12 columns in total.\n",
      "3. Column Details:\n",
      "       * show_id: Unique identifier for each show.\n",
      "       * type: Indicates whether the entry is a movie or a TV show.\n",
      "       * title: The title of the movie or TV show.\n",
      "       * director: The director(s) of the movie or TV show.\n",
      "       * cast: The cast members of the movie or TV show.\n",
      "       * country: The country or countries where the movie or TV show was produced.\n",
      "       * date_added: The date when the movie or TV show was added to Netflix.\n",
      "       * release_year: The year when the movie or TV show was released.\n",
      "       * rating: The rating assigned to the movie or TV show.\n",
      "       * duration: The duration of the movie or TV show.\n",
      "       * listed_in: The categories or genres that the movie or TV show belongs to.\n",
      "\n",
      "# Usage \n",
      "This dataset can be used for various analytical purposes such as exploring trends in Netflix content, analyzing user preferences, building recommendation systems, and more.\n",
      "name 'detect' is not defined Language is not detected: netflix\n",
      "name 'detect' is not defined Language is not detected: nayanack\n",
      "name 'detect' is not defined Language is not detected: Netflix Chronicles: Exploring Movies and TV Shows \n",
      "name 'detect' is not defined Language is not detected: Unlocking Netflix: A Comprehensive Database of Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12038776%2Fdbabda1e8f2d39e88b030173303b2724%2FNetflix.jpg?generation=1713257307281984&alt=media)\n",
      "#  Dataset Overview\n",
      "&gt;Netflix is one of the most popular media and video streaming platforms. They have over 10000 movies or tv shows available on their platform, as of mid-2021, they have over 222M Subscribers globally. This tabular dataset consists of listings of all the movies and tv shows available on Netflix, along with details such as - cast, directors, ratings, release year, duration, etc.\n",
      "\n",
      "## Key details about the dataset:\n",
      "1. Number of Entries: The dataset contains 8807 entries.\n",
      "2. Columns: There are 12 columns in total.\n",
      "3. Column Details:\n",
      "       * show_id: Unique identifier for each show.\n",
      "       * type: Indicates whether the entry is a movie or a TV show.\n",
      "       * title: The title of the movie or TV show.\n",
      "       * director: The director(s) of the movie or TV show.\n",
      "       * cast: The cast members of the movie or TV show.\n",
      "       * country: The country or countries where the movie or TV show was produced.\n",
      "       * date_added: The date when the movie or TV show was added to Netflix.\n",
      "       * release_year: The year when the movie or TV show was released.\n",
      "       * rating: The rating assigned to the movie or TV show.\n",
      "       * duration: The duration of the movie or TV show.\n",
      "       * listed_in: The categories or genres that the movie or TV show belongs to.\n",
      "\n",
      "# Usage \n",
      "This dataset can be used for various analytical purposes such as exploring trends in Netflix content, analyzing user preferences, building recommendation systems, and more.\n",
      "name 'detect' is not defined Language is not detected: yashkmd/top-rated-movies\n",
      "name 'detect' is not defined Language is not detected: top-rated-movies\n",
      "name 'detect' is not defined Language is not detected: yashkmd\n",
      "name 'detect' is not defined Language is not detected: TMDB 5000 Movie Dataset\n",
      "name 'detect' is not defined Language is not detected: Top rated movies from The Movies Database\n",
      "name 'detect' is not defined Language is not detected: The \"Top Rated Movies Dataset\" is a comprehensive collection of data that showcases a list of highly acclaimed and popular movies from various genres, spanning across multiple decades. This dataset provides a valuable resource for movie enthusiasts, researchers, and data analysts interested in exploring and analyzing the characteristics and trends of top-rated films.\n",
      "\n",
      "Features:\n",
      "\n",
      "adult: This feature indicates whether the movie contains adult content or not. It is represented as a binary value (0 for non-adult, 1 for adult).\n",
      "genre_ids: This feature consists of unique identifiers assigned to each movie genre. Multiple genres can be associated with a single movie, and they are represented as a list of genre IDs.\n",
      "original_language: This feature denotes the original language in which the movie was produced. It is represented as a two-letter language code (e.g., \"en\" for English, \"fr\" for French).\n",
      "original_title: This feature represents the original title of the movie in its original language.\n",
      "overview: This feature provides a brief summary or overview of the movie's plot or storyline.\n",
      "popularity: This feature quantifies the popularity of the movie among the audience. It is a numerical value that reflects the movie's relative popularity compared to other movies in the dataset.\n",
      "release_date: This feature indicates the date when the movie was released. It is represented as a date in the format \"YYYY-MM-DD\".\n",
      "title: This feature denotes the title of the movie in the language it is commonly known or marketed as.\n",
      "vote_average: This feature represents the average rating or score given to the movie by the audience. It is a numerical value ranging from 0 to 10.\n",
      "vote_count: This feature indicates the number of votes or ratings received by the movie. It reflects the level of engagement and popularity among the audience.\n",
      "\n",
      "Potential Uses:\n",
      "The \"Top Rated Movies Dataset\" can be utilized for a variety of purposes, including:\n",
      "\n",
      "Movie Analysis: Researchers can explore trends and patterns in the genres, ratings, and box office performance of highly regarded movies.\n",
      "Recommender Systems: The dataset can be used as a basis for developing movie recommendation algorithms or systems that suggest top-rated movies to users.\n",
      "Filmmaking Insights: Filmmakers can gain insights into successful movie genres, directors, and actors that have consistently received critical acclaim.\n",
      "Data Visualization: Data analysts and enthusiasts can create compelling visualizations to depict the relationships between various movie attributes.\n",
      "Comparison Studies: The dataset allows for comparisons between different rating systems, such as IMDb and Metascore, to determine any disparities or correlations.\n",
      "Note:\n",
      "The \"Top Rated Movies Dataset\" does not claim to represent an exhaustive list of all top-rated movies but rather aims to provide a diverse and representative sample of highly regarded films from different eras and genres. The dataset may have been curated based on various sources, including public ratings, critical reception, and box office success.\n",
      "name 'detect' is not defined Language is not detected: top-rated-movies\n",
      "name 'detect' is not defined Language is not detected: yashkmd\n",
      "name 'detect' is not defined Language is not detected: TMDB 5000 Movie Dataset\n",
      "name 'detect' is not defined Language is not detected: Top rated movies from The Movies Database\n",
      "name 'detect' is not defined Language is not detected: The \"Top Rated Movies Dataset\" is a comprehensive collection of data that showcases a list of highly acclaimed and popular movies from various genres, spanning across multiple decades. This dataset provides a valuable resource for movie enthusiasts, researchers, and data analysts interested in exploring and analyzing the characteristics and trends of top-rated films.\n",
      "\n",
      "Features:\n",
      "\n",
      "adult: This feature indicates whether the movie contains adult content or not. It is represented as a binary value (0 for non-adult, 1 for adult).\n",
      "genre_ids: This feature consists of unique identifiers assigned to each movie genre. Multiple genres can be associated with a single movie, and they are represented as a list of genre IDs.\n",
      "original_language: This feature denotes the original language in which the movie was produced. It is represented as a two-letter language code (e.g., \"en\" for English, \"fr\" for French).\n",
      "original_title: This feature represents the original title of the movie in its original language.\n",
      "overview: This feature provides a brief summary or overview of the movie's plot or storyline.\n",
      "popularity: This feature quantifies the popularity of the movie among the audience. It is a numerical value that reflects the movie's relative popularity compared to other movies in the dataset.\n",
      "release_date: This feature indicates the date when the movie was released. It is represented as a date in the format \"YYYY-MM-DD\".\n",
      "title: This feature denotes the title of the movie in the language it is commonly known or marketed as.\n",
      "vote_average: This feature represents the average rating or score given to the movie by the audience. It is a numerical value ranging from 0 to 10.\n",
      "vote_count: This feature indicates the number of votes or ratings received by the movie. It reflects the level of engagement and popularity among the audience.\n",
      "\n",
      "Potential Uses:\n",
      "The \"Top Rated Movies Dataset\" can be utilized for a variety of purposes, including:\n",
      "\n",
      "Movie Analysis: Researchers can explore trends and patterns in the genres, ratings, and box office performance of highly regarded movies.\n",
      "Recommender Systems: The dataset can be used as a basis for developing movie recommendation algorithms or systems that suggest top-rated movies to users.\n",
      "Filmmaking Insights: Filmmakers can gain insights into successful movie genres, directors, and actors that have consistently received critical acclaim.\n",
      "Data Visualization: Data analysts and enthusiasts can create compelling visualizations to depict the relationships between various movie attributes.\n",
      "Comparison Studies: The dataset allows for comparisons between different rating systems, such as IMDb and Metascore, to determine any disparities or correlations.\n",
      "Note:\n",
      "The \"Top Rated Movies Dataset\" does not claim to represent an exhaustive list of all top-rated movies but rather aims to provide a diverse and representative sample of highly regarded films from different eras and genres. The dataset may have been curated based on various sources, including public ratings, critical reception, and box office success.\n",
      "name 'detect' is not defined Language is not detected: muhammedabdulazeem/petitions-from-changeorg\n",
      "name 'detect' is not defined Language is not detected: petitions-from-changeorg\n",
      "name 'detect' is not defined Language is not detected: muhammedabdulazeem\n",
      "name 'detect' is not defined Language is not detected: Petitions from change.org\n",
      "name 'detect' is not defined Language is not detected: More than 24k+ petitions from change.org\n",
      "name 'detect' is not defined Language is not detected: ### Story\n",
      "\n",
      "Everyday there are many petitions put up by the people on the internet for some cause. [change.org](http://change.org/) is one such organization which helps to do so. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains nearly 40+ columns. You can explore the dataset by creating a notebook instance or by downloading the dataset. Data was scraped from change.org website using python library [scrapy](https://scrapy.org/).\n",
      "\n",
      "\n",
      "### Task creation\n",
      "\n",
      "You can create a task if you want to preform any kind of data analysis activity or use machine-learning models to predict any particular attribute. They are both classification and regression task that can be performed with this dataset such as victory for any petition can be taken as classification task and number of signs for that particular petition can be taken as a task of regression.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: petitions-from-changeorg\n",
      "name 'detect' is not defined Language is not detected: muhammedabdulazeem\n",
      "name 'detect' is not defined Language is not detected: Petitions from change.org\n",
      "name 'detect' is not defined Language is not detected: More than 24k+ petitions from change.org\n",
      "name 'detect' is not defined Language is not detected: ### Story\n",
      "\n",
      "Everyday there are many petitions put up by the people on the internet for some cause. [change.org](http://change.org/) is one such organization which helps to do so. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains nearly 40+ columns. You can explore the dataset by creating a notebook instance or by downloading the dataset. Data was scraped from change.org website using python library [scrapy](https://scrapy.org/).\n",
      "\n",
      "\n",
      "### Task creation\n",
      "\n",
      "You can create a task if you want to preform any kind of data analysis activity or use machine-learning models to predict any particular attribute. They are both classification and regression task that can be performed with this dataset such as victory for any petition can be taken as classification task and number of signs for that particular petition can be taken as a task of regression.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: asharalikamil/regression-technique-eda\n",
      "name 'detect' is not defined Language is not detected: regression-technique-eda\n",
      "name 'detect' is not defined Language is not detected: asharalikamil\n",
      "name 'detect' is not defined Language is not detected: Regression Technique EDA\n",
      "name 'detect' is not defined Language is not detected: regression-technique-eda\n",
      "name 'detect' is not defined Language is not detected: asharalikamil\n",
      "name 'detect' is not defined Language is not detected: Regression Technique EDA\n",
      "name 'detect' is not defined Language is not detected: kritikseth/achieved-frames-per-second-fps-in-video-games\n",
      "name 'detect' is not defined Language is not detected: achieved-frames-per-second-fps-in-video-games\n",
      "name 'detect' is not defined Language is not detected: kritikseth\n",
      "name 'detect' is not defined Language is not detected: Achieved Frames per Second (FPS) in Video Games\n",
      "name 'detect' is not defined Language is not detected: This dataset contains measurement of FPS for video games executed on computers.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains FPS measurement of video games executed on computers. Each row of the dataset describes the outcome of FPS measurement (outcome is attribute FPS) for a video game executed on a computer. A computer is characterized by the CPU and the GPU. For both the name is resolved to technical specifications (features starting with Cpu and Gpu). The technical specification of CPU and GPU are technical specification that describe the factory state of the respective component. The game is characterized by the name, the displayed resolution, and the quality setting that was adjusted during the measurement (features starting with Game). In the following there is a short descriptions of the data sources and a description for each feature in the dataset.\n",
      "name 'detect' is not defined Language is not detected: achieved-frames-per-second-fps-in-video-games\n",
      "name 'detect' is not defined Language is not detected: kritikseth\n",
      "name 'detect' is not defined Language is not detected: Achieved Frames per Second (FPS) in Video Games\n",
      "name 'detect' is not defined Language is not detected: This dataset contains measurement of FPS for video games executed on computers.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains FPS measurement of video games executed on computers. Each row of the dataset describes the outcome of FPS measurement (outcome is attribute FPS) for a video game executed on a computer. A computer is characterized by the CPU and the GPU. For both the name is resolved to technical specifications (features starting with Cpu and Gpu). The technical specification of CPU and GPU are technical specification that describe the factory state of the respective component. The game is characterized by the name, the displayed resolution, and the quality setting that was adjusted during the measurement (features starting with Game). In the following there is a short descriptions of the data sources and a description for each feature in the dataset.\n",
      "name 'detect' is not defined Language is not detected: ociule/crop-data-challenge-2018-cland\n",
      "name 'detect' is not defined Language is not detected: crop-data-challenge-2018-cland\n",
      "name 'detect' is not defined Language is not detected: ociule\n",
      "name 'detect' is not defined Language is not detected: Crop Data Challenge 2018 http://cland.lsce.ipsl.fr\n",
      "name 'detect' is not defined Language is not detected: FORECASTING CROP YIELDS FROM DATA, MODELS, AND EXPERT KNOWLEDGE\n",
      "name 'detect' is not defined Language is not detected: \n",
      "\n",
      "http://cland.lsce.ipsl.fr/index.php/workshops/forecasting-crop-yields/33-data-challenge-in-french\n",
      "\n",
      "Some files have an extra index column, you can easily see this in the summary statistics Kaggle provides. year_harvest should go to max 53, but instead it goes to the max=number of rows.\n",
      "\n",
      "So use read_table:\n",
      "\n",
      "    df_train = pd.read_table('../input/TrainingDataSet_Maize.csv', index_col=0)\n",
      "    df_test_to_submit = pd.read_table('../input/TestDataSet_Maize_blind.csv')\n",
      "name 'detect' is not defined Language is not detected: crop-data-challenge-2018-cland\n",
      "name 'detect' is not defined Language is not detected: ociule\n",
      "name 'detect' is not defined Language is not detected: Crop Data Challenge 2018 http://cland.lsce.ipsl.fr\n",
      "name 'detect' is not defined Language is not detected: FORECASTING CROP YIELDS FROM DATA, MODELS, AND EXPERT KNOWLEDGE\n",
      "name 'detect' is not defined Language is not detected: \n",
      "\n",
      "http://cland.lsce.ipsl.fr/index.php/workshops/forecasting-crop-yields/33-data-challenge-in-french\n",
      "\n",
      "Some files have an extra index column, you can easily see this in the summary statistics Kaggle provides. year_harvest should go to max 53, but instead it goes to the max=number of rows.\n",
      "\n",
      "So use read_table:\n",
      "\n",
      "    df_train = pd.read_table('../input/TrainingDataSet_Maize.csv', index_col=0)\n",
      "    df_test_to_submit = pd.read_table('../input/TestDataSet_Maize_blind.csv')\n",
      "name 'detect' is not defined Language is not detected: hemanthsai7/loandefault\n",
      "name 'detect' is not defined Language is not detected: loandefault\n",
      "name 'detect' is not defined Language is not detected: hemanthsai7\n",
      "name 'detect' is not defined Language is not detected: Loan Default prediction dataset\n",
      "name 'detect' is not defined Language is not detected: Predict who are the loan defaulters\n",
      "name 'detect' is not defined Language is not detected: Banks run into losses when customers don't pay their loans on time. Because of this, every year, banks have losses in crores, and this also impacts the country's economic growth to a large extent. In this hackathon, we look at various attributes such as funded amount, location, loan, balance, etc., to predict whether a person will be a loan defaulter.\n",
      "\n",
      "To solve this problem, MachineHack has created a training dataset of 67,463 rows and 35 columns and a testing dataset of 28,913 rows and 34 columns. The hackathon demands a few pre-requisite skills like big datasets, underfitting vs overfitting, and the ability to optimize “log_loss” to generalize well on unseen data.\n",
      "name 'detect' is not defined Language is not detected: loandefault\n",
      "name 'detect' is not defined Language is not detected: hemanthsai7\n",
      "name 'detect' is not defined Language is not detected: Loan Default prediction dataset\n",
      "name 'detect' is not defined Language is not detected: Predict who are the loan defaulters\n",
      "name 'detect' is not defined Language is not detected: Banks run into losses when customers don't pay their loans on time. Because of this, every year, banks have losses in crores, and this also impacts the country's economic growth to a large extent. In this hackathon, we look at various attributes such as funded amount, location, loan, balance, etc., to predict whether a person will be a loan defaulter.\n",
      "\n",
      "To solve this problem, MachineHack has created a training dataset of 67,463 rows and 35 columns and a testing dataset of 28,913 rows and 34 columns. The hackathon demands a few pre-requisite skills like big datasets, underfitting vs overfitting, and the ability to optimize “log_loss” to generalize well on unseen data.\n",
      "name 'detect' is not defined Language is not detected: venkatkumar001/30dayml-blending-datadifferent-encoding-methods\n",
      "name 'detect' is not defined Language is not detected: 30dayml-blending-datadifferent-encoding-methods\n",
      "name 'detect' is not defined Language is not detected: venkatkumar001\n",
      "name 'detect' is not defined Language is not detected: 30DayMl_Blending data(Different Encoding methods)\n",
      "name 'detect' is not defined Language is not detected: 30DayML Datas| Add blending method\n",
      "name 'detect' is not defined Language is not detected: **30Days ML**\n",
      "I am trying to Blending and stacking regression method apply for 30 days ML dataset \n",
      "30day dataset apply feature encoding methods like ordinal,one-hot, standard apply to predict output efficiently\n",
      "\n",
      "Thank you\n",
      "name 'detect' is not defined Language is not detected: 30dayml-blending-datadifferent-encoding-methods\n",
      "name 'detect' is not defined Language is not detected: venkatkumar001\n",
      "name 'detect' is not defined Language is not detected: 30DayMl_Blending data(Different Encoding methods)\n",
      "name 'detect' is not defined Language is not detected: 30DayML Datas| Add blending method\n",
      "name 'detect' is not defined Language is not detected: **30Days ML**\n",
      "I am trying to Blending and stacking regression method apply for 30 days ML dataset \n",
      "30day dataset apply feature encoding methods like ordinal,one-hot, standard apply to predict output efficiently\n",
      "\n",
      "Thank you\n",
      "name 'detect' is not defined Language is not detected: utkarshx27/default-of-credit-card-clients-dataset\n",
      "name 'detect' is not defined Language is not detected: default-of-credit-card-clients-dataset\n",
      "name 'detect' is not defined Language is not detected: utkarshx27\n",
      "name 'detect' is not defined Language is not detected: Predict Credit Card Defaulters\n",
      "name 'detect' is not defined Language is not detected: Predict Credit Card Defaulters\n",
      "name 'detect' is not defined Language is not detected: ```\n",
      "This research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\n",
      "```\n",
      "\n",
      "| Column | Description |\n",
      "|--------|-------------|\n",
      "| X1     | Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. |\n",
      "| X2     | Gender (1 = male; 2 = female). |\n",
      "| X3     | Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). |\n",
      "| X4     | Marital status (1 = married; 2 = single; 3 = others). |\n",
      "| X5     | Age (year). |\n",
      "| X6-X11 | History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; ...; X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. |\n",
      "| X12-X17 | Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; ...; X17 = amount of bill statement in April, 2005. |\n",
      "| X18-X23 | Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; ...; X23 = amount paid in April, 2005. |\n",
      "| Y      | Target Column - binary variable, default payment (Yes = 1, No = 0). |\n",
      "\n",
      "\n",
      "Relevant Papers:\n",
      "```\n",
      "Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
      "```\n",
      "name 'detect' is not defined Language is not detected: default-of-credit-card-clients-dataset\n",
      "name 'detect' is not defined Language is not detected: utkarshx27\n",
      "name 'detect' is not defined Language is not detected: Predict Credit Card Defaulters\n",
      "name 'detect' is not defined Language is not detected: Predict Credit Card Defaulters\n",
      "name 'detect' is not defined Language is not detected: ```\n",
      "This research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\n",
      "```\n",
      "\n",
      "| Column | Description |\n",
      "|--------|-------------|\n",
      "| X1     | Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. |\n",
      "| X2     | Gender (1 = male; 2 = female). |\n",
      "| X3     | Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). |\n",
      "| X4     | Marital status (1 = married; 2 = single; 3 = others). |\n",
      "| X5     | Age (year). |\n",
      "| X6-X11 | History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; ...; X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. |\n",
      "| X12-X17 | Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; ...; X17 = amount of bill statement in April, 2005. |\n",
      "| X18-X23 | Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; ...; X23 = amount paid in April, 2005. |\n",
      "| Y      | Target Column - binary variable, default payment (Yes = 1, No = 0). |\n",
      "\n",
      "\n",
      "Relevant Papers:\n",
      "```\n",
      "Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
      "```\n",
      "name 'detect' is not defined Language is not detected: dev7halo/wine-information\n",
      "name 'detect' is not defined Language is not detected: wine-information\n",
      "name 'detect' is not defined Language is not detected: dev7halo\n",
      "name 'detect' is not defined Language is not detected: Wine Information\n",
      "name 'detect' is not defined Language is not detected: Wine Information with nation, varieties, flavor, price, etc\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I don't like wine very much. **I love Soju**\n",
      "However, while developing a recommendation algorithm, I was looking for useful data, and I chose wine-related data. Since the data was collected on the Korean website, the data standards are in line with Korea.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This datasets 2 .csv File.\n",
      "* wine_info.csv contains 31 columns and 21605 rows of wine information.\n",
      "* cleansingWine.csv based on Wine base notebook .\n",
      "\n",
      "Data informations.\n",
      "1) id - wine ID\n",
      "2) name - wine Name\n",
      "3) producer - \n",
      "4) nation - Country of origin\n",
      "5) local1 - region1\n",
      "6) local2 - region2\n",
      "7) local3 - region3\n",
      "8) local4 - region4\n",
      "9) ~ 20) varieties1 ~ varieties12 - wine varieties\n",
      "21)  type - wine type\n",
      "22) use - wine use\n",
      "23) abv - Alcohol by volume\n",
      "24) degree - Drinking temperature\n",
      "25) sweet - Sweet rank (score between 1 and 5)\n",
      "26) acidity - Acidity rank (score between 1 and 5)\n",
      "27) body - Body rank (score between 1 and 5)\n",
      "28) tannin - Tannin rank (score between 1 and 5)\n",
      "29) price - wine price that based on WON **NOT dollars!!** (₩, WON)\n",
      "30) year - Production year\n",
      "31) ml - wine Volume (milliliter)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Data was collected on May 24th and preprocessed until May 28th.\n",
      "\n",
      "Since the data was collected on the Korean website, some countries and regions data includes Korean and English, and the price is in Korean won, not dollars.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I think that this dataset offers some great opportunities for recommedation models. \n",
      "My overall goal is to create a model that can identify the variety, winery, and location of a wine based on a description. \n",
      "Pleas upload your creative model and idea !\n",
      "name 'detect' is not defined Language is not detected: wine-information\n",
      "name 'detect' is not defined Language is not detected: dev7halo\n",
      "name 'detect' is not defined Language is not detected: Wine Information\n",
      "name 'detect' is not defined Language is not detected: Wine Information with nation, varieties, flavor, price, etc\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I don't like wine very much. **I love Soju**\n",
      "However, while developing a recommendation algorithm, I was looking for useful data, and I chose wine-related data. Since the data was collected on the Korean website, the data standards are in line with Korea.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This datasets 2 .csv File.\n",
      "* wine_info.csv contains 31 columns and 21605 rows of wine information.\n",
      "* cleansingWine.csv based on Wine base notebook .\n",
      "\n",
      "Data informations.\n",
      "1) id - wine ID\n",
      "2) name - wine Name\n",
      "3) producer - \n",
      "4) nation - Country of origin\n",
      "5) local1 - region1\n",
      "6) local2 - region2\n",
      "7) local3 - region3\n",
      "8) local4 - region4\n",
      "9) ~ 20) varieties1 ~ varieties12 - wine varieties\n",
      "21)  type - wine type\n",
      "22) use - wine use\n",
      "23) abv - Alcohol by volume\n",
      "24) degree - Drinking temperature\n",
      "25) sweet - Sweet rank (score between 1 and 5)\n",
      "26) acidity - Acidity rank (score between 1 and 5)\n",
      "27) body - Body rank (score between 1 and 5)\n",
      "28) tannin - Tannin rank (score between 1 and 5)\n",
      "29) price - wine price that based on WON **NOT dollars!!** (₩, WON)\n",
      "30) year - Production year\n",
      "31) ml - wine Volume (milliliter)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Data was collected on May 24th and preprocessed until May 28th.\n",
      "\n",
      "Since the data was collected on the Korean website, some countries and regions data includes Korean and English, and the price is in Korean won, not dollars.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I think that this dataset offers some great opportunities for recommedation models. \n",
      "My overall goal is to create a model that can identify the variety, winery, and location of a wine based on a description. \n",
      "Pleas upload your creative model and idea !\n",
      "name 'detect' is not defined Language is not detected: gauravduttakiit/open-jobs-analyzer-and-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: open-jobs-analyzer-and-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: gauravduttakiit\n",
      "name 'detect' is not defined Language is not detected: Open Jobs Analyzer and Recommendation System\n",
      "name 'detect' is not defined Language is not detected: open-jobs-analyzer-and-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: gauravduttakiit\n",
      "name 'detect' is not defined Language is not detected: Open Jobs Analyzer and Recommendation System\n",
      "name 'detect' is not defined Language is not detected: ruthgn/new-orleans-airbnb-listings-and-reviews\n",
      "name 'detect' is not defined Language is not detected: new-orleans-airbnb-listings-and-reviews\n",
      "name 'detect' is not defined Language is not detected: ruthgn\n",
      "name 'detect' is not defined Language is not detected: New Orleans Airbnb Listings and Reviews\n",
      "name 'detect' is not defined Language is not detected: Airbnb listings in New Orleans, Louisiana\n",
      "name 'detect' is not defined Language is not detected: ### Data Set Information\n",
      "\n",
      "This data set describes the listing activity of Airbnb homestays in New Orleans, Louisiana, as part of the [Inside Airbnb](http://insideairbnb.com/behind.html) initiative. The data set was compiled on November 7, 2021. See the [New Orleans Airbnb data visually here](http://insideairbnb.com/new-orleans/).\n",
      "\n",
      "Some personally identifying information has been removed from the data uploaded here.\n",
      "\n",
      "\n",
      "### Contents\n",
      "\n",
      "The following Airbnb activity is included in this New Orleans data set:\n",
      "\n",
      "Listings, including full descriptions and average review score (`new_orleans_airbnb_listings.csv`)\n",
      "Reviews, including unique id for each reviewer and detailed comments (`reviews.csv`)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Data credit goes to Murray Cox and [Inside Airbnb](http://insideairbnb.com/behind.html). The original source for this particular New Orleans data can be found [here](http://insideairbnb.com/get-the-data.html)--where you can also find information on the different listing ids and their price and availability for different calendar dates (if you're interested in looking at how Airbnb rental listing price fluctuates over time).\n",
      "\n",
      "\n",
      "### Context\n",
      "\n",
      "The data set can be used to answer some interesting questions, such as:\n",
      "- Can you predict how much a short-term rental in New Orleans should charge per night based on it's location and amenities?\n",
      "- Can you describe the vibe of each neighborhood in using listing descriptions?\n",
      "- What are the most common amenities to have among short-term rental listings in New Orleans?\n",
      "- What elements contribute to a popular or highly-rated listing?\n",
      "- Is there any noticeable difference in favorability among different NOLA neighborhood/areas and what could be the reason for it?\n",
      "\n",
      "Furthermore, it's also important to note that Inside Airbnb (provider of dataset) is a mission driven activist project with the objective to provide data that quantifies the impact of short-term rentals on housing and residential communities; and also provides a platform to support advocacy for policies to protect cities from the impacts of short-term rentals. \n",
      "\n",
      "According to travel guides, New Orleans is one of the top ten most-visited cities in the United States. It was severely affected by Hurricane Katrina in August 2005, which flooded more than 80% of the city, killed more than 1,800 people, and displaced thousands of residents, causing a population decline of over 50%. Since Katrina, major redevelopment efforts have led to a rebound in the city's population. Concerns about gentrification, new residents buying property in formerly closely knit communities, and displacement of longtime residents have all been a major discussion topic.\n",
      "\n",
      "Bearing the given context in mind, this data set shared by Inside Airbnb also allows you to ask fundamental questions about Airbnb in any neighbourhood, or across the city as a whole, such as:\n",
      "- How many listings are in my neighbourhood and where are they?\n",
      "- How many houses and apartments are being rented out frequently to tourists and not to long-term residents?\n",
      "- How much are hosts making from renting to tourists (compare that to long-term rentals)?\n",
      "- Which hosts are running a business with multiple listings and where they?\n",
      "\n",
      "The questions (and their answers) get to the core of the debate for many cities around the world, with Airbnb claiming that their hosts only occasionally rent the homes in which they live. In addition, many city or state legislation or ordinances that address residential housing, short term or vacation rentals, and zoning usually make reference to allowed use, including:\n",
      "- how many nights a dwelling is rented per year\n",
      "- minimum nights stay\n",
      "- whether the host is present\n",
      "- how many rooms are being rented in a building\n",
      "- the number of occupants allowed in a rental\n",
      "- whether the listing is licensed\n",
      "\n",
      "(Visit their [site](http://insideairbnb.com/about.html) for more details.)\n",
      "name 'detect' is not defined Language is not detected: new-orleans-airbnb-listings-and-reviews\n",
      "name 'detect' is not defined Language is not detected: ruthgn\n",
      "name 'detect' is not defined Language is not detected: New Orleans Airbnb Listings and Reviews\n",
      "name 'detect' is not defined Language is not detected: Airbnb listings in New Orleans, Louisiana\n",
      "name 'detect' is not defined Language is not detected: ### Data Set Information\n",
      "\n",
      "This data set describes the listing activity of Airbnb homestays in New Orleans, Louisiana, as part of the [Inside Airbnb](http://insideairbnb.com/behind.html) initiative. The data set was compiled on November 7, 2021. See the [New Orleans Airbnb data visually here](http://insideairbnb.com/new-orleans/).\n",
      "\n",
      "Some personally identifying information has been removed from the data uploaded here.\n",
      "\n",
      "\n",
      "### Contents\n",
      "\n",
      "The following Airbnb activity is included in this New Orleans data set:\n",
      "\n",
      "Listings, including full descriptions and average review score (`new_orleans_airbnb_listings.csv`)\n",
      "Reviews, including unique id for each reviewer and detailed comments (`reviews.csv`)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Data credit goes to Murray Cox and [Inside Airbnb](http://insideairbnb.com/behind.html). The original source for this particular New Orleans data can be found [here](http://insideairbnb.com/get-the-data.html)--where you can also find information on the different listing ids and their price and availability for different calendar dates (if you're interested in looking at how Airbnb rental listing price fluctuates over time).\n",
      "\n",
      "\n",
      "### Context\n",
      "\n",
      "The data set can be used to answer some interesting questions, such as:\n",
      "- Can you predict how much a short-term rental in New Orleans should charge per night based on it's location and amenities?\n",
      "- Can you describe the vibe of each neighborhood in using listing descriptions?\n",
      "- What are the most common amenities to have among short-term rental listings in New Orleans?\n",
      "- What elements contribute to a popular or highly-rated listing?\n",
      "- Is there any noticeable difference in favorability among different NOLA neighborhood/areas and what could be the reason for it?\n",
      "\n",
      "Furthermore, it's also important to note that Inside Airbnb (provider of dataset) is a mission driven activist project with the objective to provide data that quantifies the impact of short-term rentals on housing and residential communities; and also provides a platform to support advocacy for policies to protect cities from the impacts of short-term rentals. \n",
      "\n",
      "According to travel guides, New Orleans is one of the top ten most-visited cities in the United States. It was severely affected by Hurricane Katrina in August 2005, which flooded more than 80% of the city, killed more than 1,800 people, and displaced thousands of residents, causing a population decline of over 50%. Since Katrina, major redevelopment efforts have led to a rebound in the city's population. Concerns about gentrification, new residents buying property in formerly closely knit communities, and displacement of longtime residents have all been a major discussion topic.\n",
      "\n",
      "Bearing the given context in mind, this data set shared by Inside Airbnb also allows you to ask fundamental questions about Airbnb in any neighbourhood, or across the city as a whole, such as:\n",
      "- How many listings are in my neighbourhood and where are they?\n",
      "- How many houses and apartments are being rented out frequently to tourists and not to long-term residents?\n",
      "- How much are hosts making from renting to tourists (compare that to long-term rentals)?\n",
      "- Which hosts are running a business with multiple listings and where they?\n",
      "\n",
      "The questions (and their answers) get to the core of the debate for many cities around the world, with Airbnb claiming that their hosts only occasionally rent the homes in which they live. In addition, many city or state legislation or ordinances that address residential housing, short term or vacation rentals, and zoning usually make reference to allowed use, including:\n",
      "- how many nights a dwelling is rented per year\n",
      "- minimum nights stay\n",
      "- whether the host is present\n",
      "- how many rooms are being rented in a building\n",
      "- the number of occupants allowed in a rental\n",
      "- whether the listing is licensed\n",
      "\n",
      "(Visit their [site](http://insideairbnb.com/about.html) for more details.)\n",
      "name 'detect' is not defined Language is not detected: mterzolo/lego-sets\n",
      "name 'detect' is not defined Language is not detected: lego-sets\n",
      "name 'detect' is not defined Language is not detected: mterzolo\n",
      "name 'detect' is not defined Language is not detected: Lego Sets\n",
      "name 'detect' is not defined Language is not detected: Is the Millenium Falcon really worth $800?\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Have you ever wondered what is the most expensive lego set in the world? Or how many pieces does the Taj Mahal set contain? What about, how these features might interact with each other? I wanted to answer some of these questions and more, so I scraped lego set and price information from Lego's website.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains lego sets scraped from lego.com. Each observation is a different lego set and there are features like how many pieces are in the set, how much the set sells for, etc. This dataset contains lego sets from all the different countries they sell online to (except South Korea).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Some other questions this dataset might help answer:\n",
      "\n",
      "What is the average suggested age for lego sets?\n",
      "Do electronics have a significant impact on the price of legos?\n",
      "Are more unique lego sets are sold in the U.S or in Spain?\n",
      "name 'detect' is not defined Language is not detected: lego-sets\n",
      "name 'detect' is not defined Language is not detected: mterzolo\n",
      "name 'detect' is not defined Language is not detected: Lego Sets\n",
      "name 'detect' is not defined Language is not detected: Is the Millenium Falcon really worth $800?\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Have you ever wondered what is the most expensive lego set in the world? Or how many pieces does the Taj Mahal set contain? What about, how these features might interact with each other? I wanted to answer some of these questions and more, so I scraped lego set and price information from Lego's website.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains lego sets scraped from lego.com. Each observation is a different lego set and there are features like how many pieces are in the set, how much the set sells for, etc. This dataset contains lego sets from all the different countries they sell online to (except South Korea).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Some other questions this dataset might help answer:\n",
      "\n",
      "What is the average suggested age for lego sets?\n",
      "Do electronics have a significant impact on the price of legos?\n",
      "Are more unique lego sets are sold in the U.S or in Spain?\n",
      "name 'detect' is not defined Language is not detected: deepaktheanalyst/books-details-dataset\n",
      "name 'detect' is not defined Language is not detected: books-details-dataset\n",
      "name 'detect' is not defined Language is not detected: deepaktheanalyst\n",
      "name 'detect' is not defined Language is not detected: Books_details_dataset\n",
      "name 'detect' is not defined Language is not detected: \"Goodreads Books: Ratings, Reviews, Genres & More\"\n",
      "name 'detect' is not defined Language is not detected: **Dataset Description: Goodreads Book Data**\n",
      "\n",
      "**1. Introduction:**\n",
      "This dataset contains information about various books obtained by web scraping the Goodreads website using the Python libraries `requests`, `Beautiful Soup`, and `multithreading`. The data provides insights into books from different categories, including their titles, authors, ratings, number of ratings, number of reviews, descriptions, and genres.\n",
      "\n",
      "**2. Data Collection:**\n",
      "The data was collected by scraping multiple categories on the Goodreads website. The scraping process involved making HTTP requests to the web server using the `requests` library, parsing the HTML content using `Beautiful Soup`, and implementing `multithreading` to accelerate the data collection process. The dataset encompasses a diverse collection of books from different genres and categories.\n",
      "\n",
      "**3. Data Columns:**\n",
      "The dataset includes the following columns:\n",
      "\n",
      "1. `title`: The title of the book.\n",
      "2. `author`: The author(s) of the book.\n",
      "3. `rating`: The average rating of the book based on user reviews on Goodreads.\n",
      "4. `no_of_ratings`: The total number of ratings the book has received on Goodreads.\n",
      "5. `no_of_reviews`: The total number of reviews the book has received on Goodreads.\n",
      "6. `description`: A brief description or summary of the book.\n",
      "7. `genres`: The genres or categories the book belongs to.\n",
      "\n",
      "**4. Data Format:**\n",
      "The dataset is presented in a tabular format (e.g., CSV, Excel), where each row represents a unique book and its corresponding attributes. The data is organized in a structured manner to enable easy analysis and insights.\n",
      "\n",
      "**5. Data Quality:**\n",
      "Efforts were made to ensure data accuracy during the web scraping process. However, it is essential to note that the data's reliability depends on the quality of information available on the Goodreads website at the time of scraping. Minor inconsistencies or missing data points may be present due to variations in the website's structure or content changes over time.\n",
      "\n",
      "**6. Potential Use Cases:**\n",
      "This dataset can be utilized for various purposes, including but not limited to:\n",
      "- Book recommendation systems based on ratings and genres.\n",
      "- Analyzing popular authors and their books.\n",
      "- Understanding the relationship between the number of ratings, reviews, and the overall rating.\n",
      "- Exploring the distribution of books across different genres.\n",
      "\n",
      "**7. Citation:**\n",
      "If you use this dataset for research or analysis, it is essential to acknowledge the data source by providing a citation to Goodreads, the Python libraries used, and mentioning the use of web scraping techniques.\n",
      "\n",
      "Please note that before sharing or using this dataset, you should comply with Goodreads' terms of service and respect copyright laws regarding the books' content.\n",
      "name 'detect' is not defined Language is not detected: books-details-dataset\n",
      "name 'detect' is not defined Language is not detected: deepaktheanalyst\n",
      "name 'detect' is not defined Language is not detected: Books_details_dataset\n",
      "name 'detect' is not defined Language is not detected: \"Goodreads Books: Ratings, Reviews, Genres & More\"\n",
      "name 'detect' is not defined Language is not detected: **Dataset Description: Goodreads Book Data**\n",
      "\n",
      "**1. Introduction:**\n",
      "This dataset contains information about various books obtained by web scraping the Goodreads website using the Python libraries `requests`, `Beautiful Soup`, and `multithreading`. The data provides insights into books from different categories, including their titles, authors, ratings, number of ratings, number of reviews, descriptions, and genres.\n",
      "\n",
      "**2. Data Collection:**\n",
      "The data was collected by scraping multiple categories on the Goodreads website. The scraping process involved making HTTP requests to the web server using the `requests` library, parsing the HTML content using `Beautiful Soup`, and implementing `multithreading` to accelerate the data collection process. The dataset encompasses a diverse collection of books from different genres and categories.\n",
      "\n",
      "**3. Data Columns:**\n",
      "The dataset includes the following columns:\n",
      "\n",
      "1. `title`: The title of the book.\n",
      "2. `author`: The author(s) of the book.\n",
      "3. `rating`: The average rating of the book based on user reviews on Goodreads.\n",
      "4. `no_of_ratings`: The total number of ratings the book has received on Goodreads.\n",
      "5. `no_of_reviews`: The total number of reviews the book has received on Goodreads.\n",
      "6. `description`: A brief description or summary of the book.\n",
      "7. `genres`: The genres or categories the book belongs to.\n",
      "\n",
      "**4. Data Format:**\n",
      "The dataset is presented in a tabular format (e.g., CSV, Excel), where each row represents a unique book and its corresponding attributes. The data is organized in a structured manner to enable easy analysis and insights.\n",
      "\n",
      "**5. Data Quality:**\n",
      "Efforts were made to ensure data accuracy during the web scraping process. However, it is essential to note that the data's reliability depends on the quality of information available on the Goodreads website at the time of scraping. Minor inconsistencies or missing data points may be present due to variations in the website's structure or content changes over time.\n",
      "\n",
      "**6. Potential Use Cases:**\n",
      "This dataset can be utilized for various purposes, including but not limited to:\n",
      "- Book recommendation systems based on ratings and genres.\n",
      "- Analyzing popular authors and their books.\n",
      "- Understanding the relationship between the number of ratings, reviews, and the overall rating.\n",
      "- Exploring the distribution of books across different genres.\n",
      "\n",
      "**7. Citation:**\n",
      "If you use this dataset for research or analysis, it is essential to acknowledge the data source by providing a citation to Goodreads, the Python libraries used, and mentioning the use of web scraping techniques.\n",
      "\n",
      "Please note that before sharing or using this dataset, you should comply with Goodreads' terms of service and respect copyright laws regarding the books' content.\n",
      "name 'detect' is not defined Language is not detected: macrosynergy/fixed-income-returns-and-macro-trends\n",
      "name 'detect' is not defined Language is not detected: fixed-income-returns-and-macro-trends\n",
      "name 'detect' is not defined Language is not detected: macrosynergy\n",
      "name 'detect' is not defined Language is not detected: JPMaQS Quantamental Indicators\n",
      "name 'detect' is not defined Language is not detected: JPMaQS Quantamental Indicators\n",
      "name 'detect' is not defined Language is not detected: ###Background\n",
      " \n",
      "Macrosynergy, a macroeconomic research and technology company rooted in asset management, has partnered with J.P. Morgan to build **JPMaQS**, the J.P. Morgan Macrosynergy Quantamental System. JPMaQS is a service that makes it easy to use quantitative-fundamental (quantamental) information for algorithmic trading, as well as for the development of discretionary trader support tools. The system allows for quick and cost-efficient backtesting of macro quantamental investment strategies. Full JPMaQS data and content are proprietary and offered to J.P. Morgan’s institutional clients as a premium data and content service; however, Macrosynergy is dedicated to promoting economic knowledge to benefit the broader community. We are open to reviewing any potential study with this commitment in mind as long as the data is not used for trading and investment. Please visit here for details:  https://academy.macrosynergy.com/academia/\n",
      "\n",
      "Macrosynergy developed and maintains a free educational site https://academy.macrosynergy.com/strategies/, where we publish examples of how quantamental indicators can create simple trading strategies. This is an excellent resource for both aspiring and seasoned macro investors. \n",
      "\n",
      "For more information on JPMaQS please visit https://www.jpmorgan.com/insights/global-research/markets/macrosynergy-quantamental-system \n",
      "\n",
      "\n",
      "###The example dataset\n",
      "\n",
      "The example dataset contains a small selection of quantamental indicators. JPMaQS data are usually available in panels, with one type of indicator (category) being available for a range of currency areas. The example notebook contains a few select categories for a subset of developed and emerging markets: AUD (Australian dollar), CAD (Canadian dollar), CHF (Swiss franc), CLP (Chilean peso), COP (Colombian peso), CZK (Czech Republic koruna), EUR (euro), GBP (British pound), HUF (Hungarian forint), IDR (Indonesian rupiah), ILS (Israeli shekel), INR (Indian rupee), JPY (Japanese yen), KRW (Korean won), MXN (Mexican peso), NOK (Norwegian krone), NZD (New Zealand dollar), PLN (Polish zloty), SEK (Swedish krona), TRY (Turkish lira), TWD (Taiwanese dollar), USD (U.S. dollar), ZAR (South African rand).\n",
      "\n",
      "\n",
      "Should users find the data inspiring, have questions or want to explore the full set of 5000+ unique macro quantamental indicators (available for JP Morgan clients), please email kaggle@macrosynergy.com.\n",
      "\n",
      "#Categories\n",
      "\n",
      "##Quantamental Indicators\n",
      "\n",
      "###Intuitive growth trends \n",
      "\n",
      "*Ticker:* **INTRGDP\\_NSA\\_P1M1ML12\\_3MMA / INTRGDPv5Y\\_NSA\\_P1M1ML12\\_3MMA**\n",
      "\n",
      "*Label:* Intuitive real GDP growth: % oya, 3mma / Excess intuitive real GDP growth trend: based on 5 year lookback\n",
      "\n",
      "*Definition:*  Latest estimable GDP growth trend based on actual national accounts and monthly activity data, based on sets of regressions that replicate conventional charting methods in markets (Macrosynergy methodology): % over a year ago, 3-month moving average / Latest estimated \"intuitive\" GDP growth trend, % over a year ago, 3-month moving average minus a long-term median of that country's actual GDP growth rate at that time: based on 5 year lookback of the latter \n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Each day on which a new economic indicator is released a full new vintage of monthly-frequency GDP growth rates (% over a year ago) is being estimated\n",
      "- For the __latest months__ of each vintage, for which national accounts data have not yet been published, GDP growth is estimated based on GLS (generalized least squares) regression. This regression predicts GDP growth for a month based on the relevant growth indicators that have so far been published and the autocorrelation effects of past error terms. This means that the error of that same regression for the latest reported quarter carries over into subsequent months. Errors of linear predictions of GDP growth based on a selected few growth indicators are often autocorrelated due to unobservable trends, typically related to sectors for which no data are available\n",
      "- For the older history of each vintage, for which national accounts have already bveen released, the official quarterly-frequency GDP growth rates are decomposed into monthly-frequency growth rates using OLS regression based on monthly activity indicators.\n",
      "- The candidate monthly-frequency indicators are pre-selected according to markets' popular economic release calendars and further narrowed down based their predictive power prior to the point in time at which the selection must have been done. Thus, there is statistical pre-selection based on prior training data. In summary, this means that the set of explanatory monthly-frequency activity indicators changes overtime according to (i) publication schedule and (ii) past predictive power\n",
      "- Only country-specific activity data are used for forestimating country growth. We would not use, for example, U.S. activity data to estimated Canadian growth. This allows differentiating growth information cleanly across countries, which is a good basis for building good relative growth indicators across countries\n",
      "- Important note: The quantamental series of a 3-month moving average, as presented here, is not the same as the 3-month moving average of a quantamental series. Instead the former is a 3-month moving average of the concurrently available vintage. Since the latest month and the previous months may be estimated based on different monthly-frequency data, depending on publication lag, the quantamental 3-month moving averages contain independent information and may look very different from the smoothed monthly quantamental series\n",
      "- The excess intuitive GDP growth trend subtracts a real-time 5-year or 10-year GDP growth median. This serves as a simplistic but fairly objective estimate for potential GDP growth at the time\n",
      "\n",
      "*Ticker:* **RGDP_SA_P1Q1QL4_20QMA**\n",
      "\n",
      "*Label:* Long-term real GDP growth: 5-year moving average\n",
      "\n",
      "*Definition:* Long-term real GDP growth, % over a year ago, based on trailing lookback window: 5-year (20 quarters) moving average\n",
      "\n",
      "*Notes:* \n",
      "\n",
      "This average uses only quarterly national accounts data of the concurrently available vintage. For some older history where quarterly reports were not available, annual data history has been used\n",
      "\n",
      "###Consumer price inflation trends\n",
      "\n",
      "*Ticker:* **CPIH\\_SA\\_P1M1ML12** \n",
      "\n",
      "*Label:* Standard annual headline consumer price inflation\n",
      "\n",
      "*Definition:* Most widely watched consumer price measure of the currency area, % change of latest release over a year ago\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Typically this inflation rate is based on the main CPI index of a country. In some countries other indices were used in the past, such as the UK (retail price index) and India (a wholesale price index)\n",
      "- The headline consumer price index is not necessarily the one preferred by the central bank, who may focus more on core or net inflation measures. However, it typically does have a great weight in the public discussion on inflation\n",
      "- The underlying data series are monthly with the exception of Australia and New Zealand\n",
      "- Australia and New Zealand only produce quarterly official CPIs. For Australia a monthly CPI inflation rate is estimated based on a monthly TD-MI Inflation Gauge. For New Zealand only the latest quarterly official CPI data are used\n",
      "\n",
      "\n",
      "*Ticker:* **CPIH\\_SJA\\_P3M3ML3AR / \\_P6M6ML6AR** \n",
      "\n",
      "*Label:* Adjusted headline consumer price trend: % 3m/3m ar / % 6m/6m ar\n",
      "\n",
      "*Definition:* Headline consumer price index, seasonally- and jump-adjusted: % of latest 3 months over previous 3 months at annualized rate / % of latest 6 months over previous 6 months at an annualized rate\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Seasonal adjustment factors are sequentially re-estimated as new data are being released. This means a new data vintage is being created at every estimation\n",
      "- Jump adjustment means that two types of outliers are adjusted for. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for such spikes are often holiday shifts between two months, while the causes for jumps are mostly indirect taxes, administered prices or FX turbulences. Second, large one-off jumps in the index are replaced by the local trend. The criteria for adjustment are statistical based on pattern recognition\n",
      "- In most markets shorter-horizon price trends are watched mainly by market economists rather then the broader public\n",
      "- Australia and New Zealand only produce quarterly offical CPIs. For Australia a monthly CPI is estimated based on a monthly TD-MI Inflation Gauge. For New Zealand latest quarterly official CPI data are used\n",
      "\n",
      "\n",
      "\n",
      "*Ticker:* **CPIC_SA_P1M1ML12**\n",
      "\n",
      "*Label:* Annual core consumer price inflation\n",
      "\n",
      "*Definition:* Core consumer price index preferred by the central bank or market, % change of latest release over a year ago\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Unlike headline consumer prices, this index is chosen based on local central bank or market convention rather than popular following.\n",
      "- The basic idea behind core price indices is to exclude volatile or erratic prices that cloud the detection of a trend. Often this means the exclusion of specific food and energy products from the basket\n",
      "- The basis for the adjustment is usually a consumer price index. An exception is the U.S. for which the central bank has preferred focusing on a price index of personal consumer expenditures\n",
      "- Australia and New Zealand only produce quarterly offical CPIs. For Australia a monthly CPI is estimated based on a monthly TD-MI Inflation Gauge (core measure, ex volatile items). For New Zealand latest quarterly official CPI data are used\n",
      "\n",
      "Core CPI here means CPI that excludes volatile price components according to preferred local convention. The list of excluded items by market can be checked on https://markets.jpmorgan.com/#jpmaqs (password protected)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*Ticker:* **CPIC\\_SJA\\_P3M3ML3AR / CPIC\\_SJA\\_P6M6ML6AR**\n",
      "\n",
      "*Label:* Adjusted latest core consumer price trend: % 3m/3m ar / % 6m/6m ar\n",
      "\n",
      "*Definition:* Core consumer price index, seasonally and jump-adjusted: % of latest 3 months over previous 3 months at an annualized rate / % of latest 6 months over previous 6 months at an annualized rate\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Seasonal adjustment factors are sequentially re-estimated as new data are released\n",
      "- Jump adjustment means that two types of outliers are adjusted. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for spikes are often holiday shifts between two months, while the causes for jumps are mostly indirect taxes, administered prices or FX turbulences. Second, large one-off jumps in the index are replaced by the local trend. The criteria for adjustment are statistical and based on pattern recognition\n",
      "\n",
      "###Inflation target\n",
      "\n",
      "*Ticker:* **INFTEFF_NSA / INFTARGET\\_NSA**\n",
      "\n",
      "*Label:* Effective official inflation target (Macrosynergy method), % over a year ago / Estimated extended official target for next year, % over a year ago\n",
      "\n",
      "*Definition:* Estimated offical inflation target for next year, adjusted for past target deviations. / Estimate of the inflation rate targeted by the government or central bank for the next calendar year, % over a year ago, extended to non-target years.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The effective inflation target is the estimated official inflation target plus an adjustment for past \"target misses\". This adjustment is the last 3 years' average gap between actual inflation and the estimated official target mean\n",
      "- The formula implies that central banks gradually lose credibility if they miss their targets consistently on either the high or low side for several years. However, the formula does not allow for a sudden loss in credibility, which may arise from a change in policy or policy annoucements\n",
      "\n",
      "- for **INFTARGET\\_NSA** The calculation of this category is largely similar to \"official inflation target\". However, for periods back to 2000 for which neither a formal nor informal target value was available, pro-forma estimates have been generated based on the average of (i) the past three years' median of CPI core and headline inflation and (ii) the global benchmark for price stability of 2%. This estimate is based on the idea that a generic country targets an inflation between its recent experienced price growth and what is commonly seen as price stability\n",
      "- The main purpose of this series is to have a full set of plausible target reference values across all developed and emerging countries back to 2000\n",
      "\n",
      "###Private credit expansion\n",
      "\n",
      "*Ticker:* **PCREDITBN\\_SJA\\_P1M1ML12**/**PCREDITGDP\\_SJA\\_D1M1ML12**\n",
      "\n",
      "*Label:* Private bank credit, jump-adjusted, % over a year ago/ Banks' private credit expansion, jump-adjusted, as % of GDP over 1 year\n",
      "\n",
      "*Definition:* Private bank credit at the end of the latest reported month, % change over a year ago, seasonal and jump-adjusted/ Change of private credit over 1 year ago, seasonal and jump-adjusted, as % of nominal GDP (1-year moving average) in the base period\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- For most countries the indicator is limited to bank loans to private households and companies\n",
      "- The stock of credit is denominated in local currency\n",
      "- Jump adjustment means that two types of outliers are adjusted for. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for spikes can holiday patterns or tax effects. Second, large one-off jumps in the index are replaced by the local trend. The causes of one-off jumps can be balance sheet restructurings in the banking systems and tax-related asset shifts. The criteria for adjustments are statistical, based on pattern recognition\n",
      "- For **PCREDITGDP\\_SJA\\_D1M1ML12** the expansion is calculated as the change in the stock of bank credit over the latest reported 12 months as % of the annual nominal GDP at the beginning of this 12 month period\n",
      "\n",
      "\n",
      "###Longer-term real IRS yields\n",
      "\n",
      "*Ticker:* **RYLDIRS02Y\\_NSA / RYLDIRS05Y\\_NSA**\n",
      "\n",
      "*Label:* Real IRS yield: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Main local interest rate swap yield minus inflation expectations: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The nominal yield is the yield on a fixed receiver position in the main interest rate swaps contract traded in the currency area. Here inflation expectation is the 2-year or 5-year ahead ahead estimated inflation expectation according to Macrosynergy methodology. See notes on category tickers INFE2Y_JA and INFE5Y_JA on the page \"Inflation expectations (Macrosynergy methodology)\"\n",
      "\n",
      "##Generic returns\n",
      "\n",
      "###Duration Returns\n",
      "\n",
      "*Ticker:* **DU02YXR\\_NSA**/**DU05YXR\\_NSA**\n",
      "\n",
      "*Label:* Duration return, in % of notional: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Return on fixed receiver position in main interest rate swaps contract traded in the currency area, % of notional of the contract, daily roll: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The interest rate derivative for most currency areas is an interest rate swap. For some EM countries (CNY, INR, KRW, THB, TWD) non-deliverable swaps have been used\n",
      "- The returns have been approximated as the sums of the yield spread between the fixed and floating leg and the daily change in yield times duration\n",
      "\n",
      "*Ticker:* **DU02YXR\\_VT10**/**DU05YXR\\_VT10**\n",
      "\n",
      "*Label:* Duration return for 10% vol target: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Return on fixed receiver position, % of risk capital on position scaled to 10% (annualized) volatility target, assuming monthly roll: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Positions are scaled to 10% vol target based on historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month and constrained maximum leverage (notional to risk capital) of 20\n",
      "- See also related important notes on \"Duration return in % of notional\" (DU02YXR_NSA / DU05YXR_NSA)\n",
      "\n",
      "###FX forward returns\n",
      "\n",
      "*Ticker:* **FXXR\\_NSA**/**FXXR\\_VT10**\n",
      "\n",
      "*Label:*  FX forward return, % of notional: dominant cross / FX forward return for 10% vol target: dominant cross\n",
      "\n",
      "*Definition:* 1-month FX forward return, % of notional of the contract, assuming roll back to full 1-month maturity at the end of the month: long against natural benchmark currencies / 1-month FX forward return, % of risk capital on position scaled to 10% (annualized) volatility target, assuming roll back to full 1-month maturity at the end of the month: long against natural benchmark currencies\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The default returns are calculated for a contract that is long the local currency of the cross section against its dominant traded benchmark. For most currencies the benchmark is the dollar. For some European currencies (CHF, CZK, HUF, NOK, PLN, RON, SEK) the benchmark is the euro. And for GBP, TRY, and RUB an equally weighted basked of dollar and euro has been used\n",
      "\n",
      "- For the following currencies returns are at least based on non-deliverable contracts: BRL, CLP, CNY, IDR, INR, KRW, MYR and TWD. CLP has become deliverable as of 2021\n",
      "- For some currencies the returns include periods of low liquidity and FX targeting. If one wishes to 'blacklist' such periods one should use the non-tradability and FX-target dummmies, which have category ticker codes \"FXUNTRADABLE_NSA\" and \"FXTARGETED_NSA\".\n",
      "- for **FXXR\\_VT10** Positions are scaled to 10% vol target based on historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month.Also, a maximum leverage ratio of 5 (of implied notional to cash position) is imposed\n",
      "\n",
      "\n",
      "Real carry versus dominant cross (no hedges)\n",
      "*Ticker:* **FXCRR_NSA**\n",
      "\n",
      "*Label*: Real forward-implied carry vs. dominant cross: % ar\n",
      "\n",
      "*Definition:* 1-month FX forward carry against dominant cross(es), % annualized and adjusted for expected inflation differential: based on notional of the contract\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "In contrast to nominal carry, real carry subtracts the differential between the expected local and benchmark inflation rate from annualized carry. The basis for the adjustment is the quantamental indicator \"Estimated 1-year ahead inflation expectation\" (INFE1Y_JA). The basis of these expectations are the dominant headline and core price indices (50-50) according to popular local conventions. \n",
      "\n",
      "### Exchange rate target dummy\n",
      "\n",
      "*Ticker*: **FXTARGETED_NSA**\n",
      "\n",
      "*Label:* Exchange rate target dummy\n",
      "\n",
      "*Definition:* Binary variable that takes the value 1 (rather than 0) if the exchange rate is targeted through a peg or any regime that significantly reduces exchange rate flexibility.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The exchange rate is considered targeted if the authorities have formally or informally set a peg, target corridor, cap or floor that is known in the market and expected to be defended by intervention and that significantly restricts the variation of the exchange rate.\n",
      "- The dummies are updated once per month at the end of the month based on the assessment of J.P. Morgan market makers and/or research. For example, if a target has been announced in the middle of the month the variable shifts from 0 to 1 at the end of this month.\n",
      "- Older history (prior to launch of JPMaQS)is based on heritage data of Macrosynergy Partners and, before that, replication of real-time liquidity and flexibility status based on an expert review.\n",
      "FX untradability dummy\n",
      "\n",
      "\n",
      "*Ticker*: **FXUNTRADABLE_NSA**\n",
      "\n",
      "*Label:* FX untradability dummy\n",
      "\n",
      "*Definition:* Binary variable that takes the value 1 (rather than 0) if either (i) liquidity in the main FX forward market is limited or (ii) convertibility restrictions significantly distort the link between tradable offshore and untradable onshore contracts.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Liquidity is considered limited if it is not possible to trade at least USD30 million in one ticket without significant market impact.\n",
      "- The dummies are updated once per month at the end of the month based on the assessment of J.P. Morgan market makers and/or research. Brief spells of illiquidity would not be captured by this series.\n",
      "Older history (prior to launch of JPMaQS) is based on heritage data of Macrosynergy Partners and, before that, replication of real-time liquidity and flexibility status based on an expert review.\n",
      "\n",
      "\n",
      "\n",
      "###Equity index future returns\n",
      "\n",
      "*Ticker:* **EQXR\\_NSA**/**EQXR\\_VT10**\n",
      "\n",
      "*Label:* Equity index future returns in % of notional / Equity index future return for 10% vol target\n",
      "\n",
      "*Definition:* Return on front future of main country equity index, % of notional of the contract / Return on front future of main country equity index, % of risk capital on position scaled to 10% (annualized) volatility target\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The return is simply the % change of the futures price. The return calculation assumes rolling futures (from front to second) on IMM (international monetary markets) days\n",
      "- For **EQXR\\_VT10**  positions are scaled to 10% vol target based on the historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month\n",
      "name 'detect' is not defined Language is not detected: fixed-income-returns-and-macro-trends\n",
      "name 'detect' is not defined Language is not detected: macrosynergy\n",
      "name 'detect' is not defined Language is not detected: JPMaQS Quantamental Indicators\n",
      "name 'detect' is not defined Language is not detected: JPMaQS Quantamental Indicators\n",
      "name 'detect' is not defined Language is not detected: ###Background\n",
      " \n",
      "Macrosynergy, a macroeconomic research and technology company rooted in asset management, has partnered with J.P. Morgan to build **JPMaQS**, the J.P. Morgan Macrosynergy Quantamental System. JPMaQS is a service that makes it easy to use quantitative-fundamental (quantamental) information for algorithmic trading, as well as for the development of discretionary trader support tools. The system allows for quick and cost-efficient backtesting of macro quantamental investment strategies. Full JPMaQS data and content are proprietary and offered to J.P. Morgan’s institutional clients as a premium data and content service; however, Macrosynergy is dedicated to promoting economic knowledge to benefit the broader community. We are open to reviewing any potential study with this commitment in mind as long as the data is not used for trading and investment. Please visit here for details:  https://academy.macrosynergy.com/academia/\n",
      "\n",
      "Macrosynergy developed and maintains a free educational site https://academy.macrosynergy.com/strategies/, where we publish examples of how quantamental indicators can create simple trading strategies. This is an excellent resource for both aspiring and seasoned macro investors. \n",
      "\n",
      "For more information on JPMaQS please visit https://www.jpmorgan.com/insights/global-research/markets/macrosynergy-quantamental-system \n",
      "\n",
      "\n",
      "###The example dataset\n",
      "\n",
      "The example dataset contains a small selection of quantamental indicators. JPMaQS data are usually available in panels, with one type of indicator (category) being available for a range of currency areas. The example notebook contains a few select categories for a subset of developed and emerging markets: AUD (Australian dollar), CAD (Canadian dollar), CHF (Swiss franc), CLP (Chilean peso), COP (Colombian peso), CZK (Czech Republic koruna), EUR (euro), GBP (British pound), HUF (Hungarian forint), IDR (Indonesian rupiah), ILS (Israeli shekel), INR (Indian rupee), JPY (Japanese yen), KRW (Korean won), MXN (Mexican peso), NOK (Norwegian krone), NZD (New Zealand dollar), PLN (Polish zloty), SEK (Swedish krona), TRY (Turkish lira), TWD (Taiwanese dollar), USD (U.S. dollar), ZAR (South African rand).\n",
      "\n",
      "\n",
      "Should users find the data inspiring, have questions or want to explore the full set of 5000+ unique macro quantamental indicators (available for JP Morgan clients), please email kaggle@macrosynergy.com.\n",
      "\n",
      "#Categories\n",
      "\n",
      "##Quantamental Indicators\n",
      "\n",
      "###Intuitive growth trends \n",
      "\n",
      "*Ticker:* **INTRGDP\\_NSA\\_P1M1ML12\\_3MMA / INTRGDPv5Y\\_NSA\\_P1M1ML12\\_3MMA**\n",
      "\n",
      "*Label:* Intuitive real GDP growth: % oya, 3mma / Excess intuitive real GDP growth trend: based on 5 year lookback\n",
      "\n",
      "*Definition:*  Latest estimable GDP growth trend based on actual national accounts and monthly activity data, based on sets of regressions that replicate conventional charting methods in markets (Macrosynergy methodology): % over a year ago, 3-month moving average / Latest estimated \"intuitive\" GDP growth trend, % over a year ago, 3-month moving average minus a long-term median of that country's actual GDP growth rate at that time: based on 5 year lookback of the latter \n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Each day on which a new economic indicator is released a full new vintage of monthly-frequency GDP growth rates (% over a year ago) is being estimated\n",
      "- For the __latest months__ of each vintage, for which national accounts data have not yet been published, GDP growth is estimated based on GLS (generalized least squares) regression. This regression predicts GDP growth for a month based on the relevant growth indicators that have so far been published and the autocorrelation effects of past error terms. This means that the error of that same regression for the latest reported quarter carries over into subsequent months. Errors of linear predictions of GDP growth based on a selected few growth indicators are often autocorrelated due to unobservable trends, typically related to sectors for which no data are available\n",
      "- For the older history of each vintage, for which national accounts have already bveen released, the official quarterly-frequency GDP growth rates are decomposed into monthly-frequency growth rates using OLS regression based on monthly activity indicators.\n",
      "- The candidate monthly-frequency indicators are pre-selected according to markets' popular economic release calendars and further narrowed down based their predictive power prior to the point in time at which the selection must have been done. Thus, there is statistical pre-selection based on prior training data. In summary, this means that the set of explanatory monthly-frequency activity indicators changes overtime according to (i) publication schedule and (ii) past predictive power\n",
      "- Only country-specific activity data are used for forestimating country growth. We would not use, for example, U.S. activity data to estimated Canadian growth. This allows differentiating growth information cleanly across countries, which is a good basis for building good relative growth indicators across countries\n",
      "- Important note: The quantamental series of a 3-month moving average, as presented here, is not the same as the 3-month moving average of a quantamental series. Instead the former is a 3-month moving average of the concurrently available vintage. Since the latest month and the previous months may be estimated based on different monthly-frequency data, depending on publication lag, the quantamental 3-month moving averages contain independent information and may look very different from the smoothed monthly quantamental series\n",
      "- The excess intuitive GDP growth trend subtracts a real-time 5-year or 10-year GDP growth median. This serves as a simplistic but fairly objective estimate for potential GDP growth at the time\n",
      "\n",
      "*Ticker:* **RGDP_SA_P1Q1QL4_20QMA**\n",
      "\n",
      "*Label:* Long-term real GDP growth: 5-year moving average\n",
      "\n",
      "*Definition:* Long-term real GDP growth, % over a year ago, based on trailing lookback window: 5-year (20 quarters) moving average\n",
      "\n",
      "*Notes:* \n",
      "\n",
      "This average uses only quarterly national accounts data of the concurrently available vintage. For some older history where quarterly reports were not available, annual data history has been used\n",
      "\n",
      "###Consumer price inflation trends\n",
      "\n",
      "*Ticker:* **CPIH\\_SA\\_P1M1ML12** \n",
      "\n",
      "*Label:* Standard annual headline consumer price inflation\n",
      "\n",
      "*Definition:* Most widely watched consumer price measure of the currency area, % change of latest release over a year ago\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Typically this inflation rate is based on the main CPI index of a country. In some countries other indices were used in the past, such as the UK (retail price index) and India (a wholesale price index)\n",
      "- The headline consumer price index is not necessarily the one preferred by the central bank, who may focus more on core or net inflation measures. However, it typically does have a great weight in the public discussion on inflation\n",
      "- The underlying data series are monthly with the exception of Australia and New Zealand\n",
      "- Australia and New Zealand only produce quarterly official CPIs. For Australia a monthly CPI inflation rate is estimated based on a monthly TD-MI Inflation Gauge. For New Zealand only the latest quarterly official CPI data are used\n",
      "\n",
      "\n",
      "*Ticker:* **CPIH\\_SJA\\_P3M3ML3AR / \\_P6M6ML6AR** \n",
      "\n",
      "*Label:* Adjusted headline consumer price trend: % 3m/3m ar / % 6m/6m ar\n",
      "\n",
      "*Definition:* Headline consumer price index, seasonally- and jump-adjusted: % of latest 3 months over previous 3 months at annualized rate / % of latest 6 months over previous 6 months at an annualized rate\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Seasonal adjustment factors are sequentially re-estimated as new data are being released. This means a new data vintage is being created at every estimation\n",
      "- Jump adjustment means that two types of outliers are adjusted for. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for such spikes are often holiday shifts between two months, while the causes for jumps are mostly indirect taxes, administered prices or FX turbulences. Second, large one-off jumps in the index are replaced by the local trend. The criteria for adjustment are statistical based on pattern recognition\n",
      "- In most markets shorter-horizon price trends are watched mainly by market economists rather then the broader public\n",
      "- Australia and New Zealand only produce quarterly offical CPIs. For Australia a monthly CPI is estimated based on a monthly TD-MI Inflation Gauge. For New Zealand latest quarterly official CPI data are used\n",
      "\n",
      "\n",
      "\n",
      "*Ticker:* **CPIC_SA_P1M1ML12**\n",
      "\n",
      "*Label:* Annual core consumer price inflation\n",
      "\n",
      "*Definition:* Core consumer price index preferred by the central bank or market, % change of latest release over a year ago\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Unlike headline consumer prices, this index is chosen based on local central bank or market convention rather than popular following.\n",
      "- The basic idea behind core price indices is to exclude volatile or erratic prices that cloud the detection of a trend. Often this means the exclusion of specific food and energy products from the basket\n",
      "- The basis for the adjustment is usually a consumer price index. An exception is the U.S. for which the central bank has preferred focusing on a price index of personal consumer expenditures\n",
      "- Australia and New Zealand only produce quarterly offical CPIs. For Australia a monthly CPI is estimated based on a monthly TD-MI Inflation Gauge (core measure, ex volatile items). For New Zealand latest quarterly official CPI data are used\n",
      "\n",
      "Core CPI here means CPI that excludes volatile price components according to preferred local convention. The list of excluded items by market can be checked on https://markets.jpmorgan.com/#jpmaqs (password protected)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*Ticker:* **CPIC\\_SJA\\_P3M3ML3AR / CPIC\\_SJA\\_P6M6ML6AR**\n",
      "\n",
      "*Label:* Adjusted latest core consumer price trend: % 3m/3m ar / % 6m/6m ar\n",
      "\n",
      "*Definition:* Core consumer price index, seasonally and jump-adjusted: % of latest 3 months over previous 3 months at an annualized rate / % of latest 6 months over previous 6 months at an annualized rate\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Seasonal adjustment factors are sequentially re-estimated as new data are released\n",
      "- Jump adjustment means that two types of outliers are adjusted. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for spikes are often holiday shifts between two months, while the causes for jumps are mostly indirect taxes, administered prices or FX turbulences. Second, large one-off jumps in the index are replaced by the local trend. The criteria for adjustment are statistical and based on pattern recognition\n",
      "\n",
      "###Inflation target\n",
      "\n",
      "*Ticker:* **INFTEFF_NSA / INFTARGET\\_NSA**\n",
      "\n",
      "*Label:* Effective official inflation target (Macrosynergy method), % over a year ago / Estimated extended official target for next year, % over a year ago\n",
      "\n",
      "*Definition:* Estimated offical inflation target for next year, adjusted for past target deviations. / Estimate of the inflation rate targeted by the government or central bank for the next calendar year, % over a year ago, extended to non-target years.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The effective inflation target is the estimated official inflation target plus an adjustment for past \"target misses\". This adjustment is the last 3 years' average gap between actual inflation and the estimated official target mean\n",
      "- The formula implies that central banks gradually lose credibility if they miss their targets consistently on either the high or low side for several years. However, the formula does not allow for a sudden loss in credibility, which may arise from a change in policy or policy annoucements\n",
      "\n",
      "- for **INFTARGET\\_NSA** The calculation of this category is largely similar to \"official inflation target\". However, for periods back to 2000 for which neither a formal nor informal target value was available, pro-forma estimates have been generated based on the average of (i) the past three years' median of CPI core and headline inflation and (ii) the global benchmark for price stability of 2%. This estimate is based on the idea that a generic country targets an inflation between its recent experienced price growth and what is commonly seen as price stability\n",
      "- The main purpose of this series is to have a full set of plausible target reference values across all developed and emerging countries back to 2000\n",
      "\n",
      "###Private credit expansion\n",
      "\n",
      "*Ticker:* **PCREDITBN\\_SJA\\_P1M1ML12**/**PCREDITGDP\\_SJA\\_D1M1ML12**\n",
      "\n",
      "*Label:* Private bank credit, jump-adjusted, % over a year ago/ Banks' private credit expansion, jump-adjusted, as % of GDP over 1 year\n",
      "\n",
      "*Definition:* Private bank credit at the end of the latest reported month, % change over a year ago, seasonal and jump-adjusted/ Change of private credit over 1 year ago, seasonal and jump-adjusted, as % of nominal GDP (1-year moving average) in the base period\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- For most countries the indicator is limited to bank loans to private households and companies\n",
      "- The stock of credit is denominated in local currency\n",
      "- Jump adjustment means that two types of outliers are adjusted for. First, large \"spikes\" (i.e. two subsequent large moves of the seasonal index in opposite directions) are averaged. The causes for spikes can holiday patterns or tax effects. Second, large one-off jumps in the index are replaced by the local trend. The causes of one-off jumps can be balance sheet restructurings in the banking systems and tax-related asset shifts. The criteria for adjustments are statistical, based on pattern recognition\n",
      "- For **PCREDITGDP\\_SJA\\_D1M1ML12** the expansion is calculated as the change in the stock of bank credit over the latest reported 12 months as % of the annual nominal GDP at the beginning of this 12 month period\n",
      "\n",
      "\n",
      "###Longer-term real IRS yields\n",
      "\n",
      "*Ticker:* **RYLDIRS02Y\\_NSA / RYLDIRS05Y\\_NSA**\n",
      "\n",
      "*Label:* Real IRS yield: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Main local interest rate swap yield minus inflation expectations: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The nominal yield is the yield on a fixed receiver position in the main interest rate swaps contract traded in the currency area. Here inflation expectation is the 2-year or 5-year ahead ahead estimated inflation expectation according to Macrosynergy methodology. See notes on category tickers INFE2Y_JA and INFE5Y_JA on the page \"Inflation expectations (Macrosynergy methodology)\"\n",
      "\n",
      "##Generic returns\n",
      "\n",
      "###Duration Returns\n",
      "\n",
      "*Ticker:* **DU02YXR\\_NSA**/**DU05YXR\\_NSA**\n",
      "\n",
      "*Label:* Duration return, in % of notional: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Return on fixed receiver position in main interest rate swaps contract traded in the currency area, % of notional of the contract, daily roll: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The interest rate derivative for most currency areas is an interest rate swap. For some EM countries (CNY, INR, KRW, THB, TWD) non-deliverable swaps have been used\n",
      "- The returns have been approximated as the sums of the yield spread between the fixed and floating leg and the daily change in yield times duration\n",
      "\n",
      "*Ticker:* **DU02YXR\\_VT10**/**DU05YXR\\_VT10**\n",
      "\n",
      "*Label:* Duration return for 10% vol target: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Definition:* Return on fixed receiver position, % of risk capital on position scaled to 10% (annualized) volatility target, assuming monthly roll: 2-year maturity / 5-year maturity\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Positions are scaled to 10% vol target based on historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month and constrained maximum leverage (notional to risk capital) of 20\n",
      "- See also related important notes on \"Duration return in % of notional\" (DU02YXR_NSA / DU05YXR_NSA)\n",
      "\n",
      "###FX forward returns\n",
      "\n",
      "*Ticker:* **FXXR\\_NSA**/**FXXR\\_VT10**\n",
      "\n",
      "*Label:*  FX forward return, % of notional: dominant cross / FX forward return for 10% vol target: dominant cross\n",
      "\n",
      "*Definition:* 1-month FX forward return, % of notional of the contract, assuming roll back to full 1-month maturity at the end of the month: long against natural benchmark currencies / 1-month FX forward return, % of risk capital on position scaled to 10% (annualized) volatility target, assuming roll back to full 1-month maturity at the end of the month: long against natural benchmark currencies\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The default returns are calculated for a contract that is long the local currency of the cross section against its dominant traded benchmark. For most currencies the benchmark is the dollar. For some European currencies (CHF, CZK, HUF, NOK, PLN, RON, SEK) the benchmark is the euro. And for GBP, TRY, and RUB an equally weighted basked of dollar and euro has been used\n",
      "\n",
      "- For the following currencies returns are at least based on non-deliverable contracts: BRL, CLP, CNY, IDR, INR, KRW, MYR and TWD. CLP has become deliverable as of 2021\n",
      "- For some currencies the returns include periods of low liquidity and FX targeting. If one wishes to 'blacklist' such periods one should use the non-tradability and FX-target dummmies, which have category ticker codes \"FXUNTRADABLE_NSA\" and \"FXTARGETED_NSA\".\n",
      "- for **FXXR\\_VT10** Positions are scaled to 10% vol target based on historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month.Also, a maximum leverage ratio of 5 (of implied notional to cash position) is imposed\n",
      "\n",
      "\n",
      "Real carry versus dominant cross (no hedges)\n",
      "*Ticker:* **FXCRR_NSA**\n",
      "\n",
      "*Label*: Real forward-implied carry vs. dominant cross: % ar\n",
      "\n",
      "*Definition:* 1-month FX forward carry against dominant cross(es), % annualized and adjusted for expected inflation differential: based on notional of the contract\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "In contrast to nominal carry, real carry subtracts the differential between the expected local and benchmark inflation rate from annualized carry. The basis for the adjustment is the quantamental indicator \"Estimated 1-year ahead inflation expectation\" (INFE1Y_JA). The basis of these expectations are the dominant headline and core price indices (50-50) according to popular local conventions. \n",
      "\n",
      "### Exchange rate target dummy\n",
      "\n",
      "*Ticker*: **FXTARGETED_NSA**\n",
      "\n",
      "*Label:* Exchange rate target dummy\n",
      "\n",
      "*Definition:* Binary variable that takes the value 1 (rather than 0) if the exchange rate is targeted through a peg or any regime that significantly reduces exchange rate flexibility.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The exchange rate is considered targeted if the authorities have formally or informally set a peg, target corridor, cap or floor that is known in the market and expected to be defended by intervention and that significantly restricts the variation of the exchange rate.\n",
      "- The dummies are updated once per month at the end of the month based on the assessment of J.P. Morgan market makers and/or research. For example, if a target has been announced in the middle of the month the variable shifts from 0 to 1 at the end of this month.\n",
      "- Older history (prior to launch of JPMaQS)is based on heritage data of Macrosynergy Partners and, before that, replication of real-time liquidity and flexibility status based on an expert review.\n",
      "FX untradability dummy\n",
      "\n",
      "\n",
      "*Ticker*: **FXUNTRADABLE_NSA**\n",
      "\n",
      "*Label:* FX untradability dummy\n",
      "\n",
      "*Definition:* Binary variable that takes the value 1 (rather than 0) if either (i) liquidity in the main FX forward market is limited or (ii) convertibility restrictions significantly distort the link between tradable offshore and untradable onshore contracts.\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- Liquidity is considered limited if it is not possible to trade at least USD30 million in one ticket without significant market impact.\n",
      "- The dummies are updated once per month at the end of the month based on the assessment of J.P. Morgan market makers and/or research. Brief spells of illiquidity would not be captured by this series.\n",
      "Older history (prior to launch of JPMaQS) is based on heritage data of Macrosynergy Partners and, before that, replication of real-time liquidity and flexibility status based on an expert review.\n",
      "\n",
      "\n",
      "\n",
      "###Equity index future returns\n",
      "\n",
      "*Ticker:* **EQXR\\_NSA**/**EQXR\\_VT10**\n",
      "\n",
      "*Label:* Equity index future returns in % of notional / Equity index future return for 10% vol target\n",
      "\n",
      "*Definition:* Return on front future of main country equity index, % of notional of the contract / Return on front future of main country equity index, % of risk capital on position scaled to 10% (annualized) volatility target\n",
      "\n",
      "*Notes:*\n",
      "\n",
      "- The return is simply the % change of the futures price. The return calculation assumes rolling futures (from front to second) on IMM (international monetary markets) days\n",
      "- For **EQXR\\_VT10**  positions are scaled to 10% vol target based on the historic standard deviation for an exponential moving average with a half-time of 11 days. Positions are rebalanced at the end of each month\n",
      "name 'detect' is not defined Language is not detected: victorbonilla/beijing-multisite-airquality-data-data-set\n",
      "name 'detect' is not defined Language is not detected: beijing-multisite-airquality-data-data-set\n",
      "name 'detect' is not defined Language is not detected: victorbonilla\n",
      "name 'detect' is not defined Language is not detected: Beijing Multi-Site Air-Quality Data Data Set\n",
      "name 'detect' is not defined Language is not detected: Beijing Multi-Site Air-Quality Data\n",
      "name 'detect' is not defined Language is not detected: # Abstract\n",
      "This hourly data set considers 6 main air pollutants and 6 relevant meteorological variables at multiple sites in Beijing.\n",
      "The picture of this data set web page has been acquired from [the NASA Earth Observatory](https://earthobservatory.nasa.gov/images/76888/haze-over-eastern-china).\n",
      "\n",
      "- Data Set Characteristics: Multivariate, Time-Series\n",
      "- Number of Instances: 420768\n",
      "- Area: Physical\n",
      "- Attribute Characteristics: Integer, Real\n",
      "- Number of Attributes: 18\n",
      "- Date Donated: 2019-09-20\n",
      "- Associated Tasks: Regression\n",
      "- Missing Values? Yes\n",
      "\n",
      "# Source\n",
      "[Web page](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data) in UCI Machine Learning Repository\n",
      "\n",
      "# Data Set Information\n",
      "This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.\n",
      "\n",
      "# Relevant Papers\n",
      "Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.\n",
      "\n",
      "# Citation Request (license)\n",
      "Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.\n",
      "name 'detect' is not defined Language is not detected: beijing-multisite-airquality-data-data-set\n",
      "name 'detect' is not defined Language is not detected: victorbonilla\n",
      "name 'detect' is not defined Language is not detected: Beijing Multi-Site Air-Quality Data Data Set\n",
      "name 'detect' is not defined Language is not detected: Beijing Multi-Site Air-Quality Data\n",
      "name 'detect' is not defined Language is not detected: # Abstract\n",
      "This hourly data set considers 6 main air pollutants and 6 relevant meteorological variables at multiple sites in Beijing.\n",
      "The picture of this data set web page has been acquired from [the NASA Earth Observatory](https://earthobservatory.nasa.gov/images/76888/haze-over-eastern-china).\n",
      "\n",
      "- Data Set Characteristics: Multivariate, Time-Series\n",
      "- Number of Instances: 420768\n",
      "- Area: Physical\n",
      "- Attribute Characteristics: Integer, Real\n",
      "- Number of Attributes: 18\n",
      "- Date Donated: 2019-09-20\n",
      "- Associated Tasks: Regression\n",
      "- Missing Values? Yes\n",
      "\n",
      "# Source\n",
      "[Web page](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data) in UCI Machine Learning Repository\n",
      "\n",
      "# Data Set Information\n",
      "This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.\n",
      "\n",
      "# Relevant Papers\n",
      "Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.\n",
      "\n",
      "# Citation Request (license)\n",
      "Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.\n",
      "name 'detect' is not defined Language is not detected: /stock-price-and-news-realted-to-it\n",
      "name 'detect' is not defined Language is not detected: stock-price-and-news-realted-to-it\n",
      "name 'detect' is not defined Language is not detected: Impact of News on the Share closing value\n",
      "name 'detect' is not defined Language is not detected: Stock prices and the News related to the Apple and Microsoft\n",
      "name 'detect' is not defined Language is not detected: The Dataset here consists of Stock Value of Apple(AAPL) and Microsoft(MSFT) from 2006 to 2016 and News summary, abstract and snippets on News featuring these two tech giants during the same period. We are trying to understand and depict the impact of News stories on the stock prices. The News snippets, summary and abstracted were retrieved from The New York Times [API][1]. The stock values were obtained from Yahoo Finance.  \n",
      "The final data set is created by applying sentiment analysis on those News string and converting them into a score. For this purpose, [Natural Language Toolkit(NLTK)][2] was used. The final dataset were further used for a Regression Model.\n",
      "\n",
      "\n",
      "  [1]: https://developer.nytimes.com/\n",
      "  [2]: https://www.nltk.org/\n",
      "name 'detect' is not defined Language is not detected: stock-price-and-news-realted-to-it\n",
      "name 'detect' is not defined Language is not detected: Impact of News on the Share closing value\n",
      "name 'detect' is not defined Language is not detected: Stock prices and the News related to the Apple and Microsoft\n",
      "name 'detect' is not defined Language is not detected: The Dataset here consists of Stock Value of Apple(AAPL) and Microsoft(MSFT) from 2006 to 2016 and News summary, abstract and snippets on News featuring these two tech giants during the same period. We are trying to understand and depict the impact of News stories on the stock prices. The News snippets, summary and abstracted were retrieved from The New York Times [API][1]. The stock values were obtained from Yahoo Finance.  \n",
      "The final data set is created by applying sentiment analysis on those News string and converting them into a score. For this purpose, [Natural Language Toolkit(NLTK)][2] was used. The final dataset were further used for a Regression Model.\n",
      "\n",
      "\n",
      "  [1]: https://developer.nytimes.com/\n",
      "  [2]: https://www.nltk.org/\n",
      "name 'detect' is not defined Language is not detected: roshanm0903/elodmbiproject\n",
      "name 'detect' is not defined Language is not detected: elodmbiproject\n",
      "name 'detect' is not defined Language is not detected: roshanm0903\n",
      "name 'detect' is not defined Language is not detected: elo_dmbi_01\n",
      "name 'detect' is not defined Language is not detected: samples from the data set of ELO merchant category recommendation competition\n",
      "name 'detect' is not defined Language is not detected: elodmbiproject\n",
      "name 'detect' is not defined Language is not detected: roshanm0903\n",
      "name 'detect' is not defined Language is not detected: elo_dmbi_01\n",
      "name 'detect' is not defined Language is not detected: samples from the data set of ELO merchant category recommendation competition\n",
      "name 'detect' is not defined Language is not detected: austcse/embedded-smartphone-sensor-data\n",
      "name 'detect' is not defined Language is not detected: embedded-smartphone-sensor-data\n",
      "name 'detect' is not defined Language is not detected: austcse\n",
      "name 'detect' is not defined Language is not detected: Embedded Smartphone Sensor Data\n",
      "name 'detect' is not defined Language is not detected: ## Context ##\n",
      "This dataset includes time-series data generated by embedded smartphone sensors(accelerometer, gyroscope, magnetometer, battery sensor, GPS, etc.). These data are collected with an android application \"Sensor Data Collector,” developed to accumulate smartphone sensor data.\n",
      "\n",
      "## Inspiration ##\n",
      "By analyzing this dataset, one can easily find different types of correlations between mobile sensor data and human emotion, sleep patterns, movement patterns, etc. It can also be used in different domains like recommendation systems, activities of daily living, and many more.\n",
      "\n",
      "## Content ##\n",
      "**Version 2(UserInfo.csv, Sensors.csv)**\n",
      "For each participant, the study commenced by collecting their 14 days of smartphone sensor data by using an android application \" Sensor data collector\" and obtained 21690 recorded samples of autonomous smartphone sensor data from 145 users.\n",
      "\n",
      "UserInfo.csv contains information that we have collected directly from the users, like their gender and age range. Privacy concerns were taken into significant consideration in our research study. Focusing on protecting participants’ personal information, we have entirely anonymized each participant’s identity with a random hashed user id.\n",
      "\n",
      "Sensors.csv contains mobile sensor data collected from the accelerometer, gravity, gyroscope, magnetometer, orientation, and GPS sensors. Accelerometer, gravity, gyroscope, and magnetometer data are recorded with their tri-axial values. GPS encompasses the device’s increments in latitude, longitude, and altitude.\n",
      "\n",
      "**Version 1(SENSORDATA.csv)**\n",
      "Data is collected through the same application  \"Sensor data collector\"  for 14 days as version 2. This dataset contains 9516 records of embedded smartphone sensor data collected from the accelerometer, gravity, gyroscope, magnetometer, battery sensor, light sensor, orientation, and GPS sensors. \n",
      "\n",
      "## Acknowledgements ##\n",
      "If you use this dataset, please cite the following paper:\n",
      "\n",
      "{\n",
      "Author= {Moontaha Nishat Chowdhury, H M Zabir Haque, Kazi Taqi Tahmid, Fatema-Tuz-Zohora Salma, Nafisa Ahmed}\n",
      "Title= {A Novel Approach for Product Recommendation Using Smartphone Sensor Data}\n",
      "Publication Type= {Journal Article}\n",
      "Country= {Germany}\n",
      "Year of Publication= {2022}\n",
      "Journal Name= {International Journal of Interactive Mobile Technologies}\n",
      "ISSN= {1865-7923}\n",
      "Volume= {16}\n",
      "Publisher= {Kassel University Press GmbH}\n",
      "DOI={https://doi.org/10.3991/ijim.v16i16.31617 }\n",
      "}\n",
      "\n",
      "or,\n",
      "{\n",
      "Author= {Kazi Taqi Tahmid, Khandaker Rezwan Ahmed, Moontaha Nishat Chowdhury, Koushik Mallik, Umme Habiba, and H. M. Zabir Haque}\n",
      "Title= {An Integrated Crowdsourcing Application for Embedded Smartphone Sensor Data Acquisition and Mobility Analysis}\n",
      "Publication Type= {Journal Article}\n",
      "Country= {United States}\n",
      "Year of Publication= {2022}\n",
      "Journal Name= {Journal of Advances in Information Technology}\n",
      "ISSN= {1798-2340}\n",
      "Volume= {13}\n",
      "Number= {5}\n",
      "Publisher= {Engineering and Technology Publishing}\n",
      "DOI={https://doi.org/10.12720/jait.13.5.503-511 }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: embedded-smartphone-sensor-data\n",
      "name 'detect' is not defined Language is not detected: austcse\n",
      "name 'detect' is not defined Language is not detected: Embedded Smartphone Sensor Data\n",
      "name 'detect' is not defined Language is not detected: ## Context ##\n",
      "This dataset includes time-series data generated by embedded smartphone sensors(accelerometer, gyroscope, magnetometer, battery sensor, GPS, etc.). These data are collected with an android application \"Sensor Data Collector,” developed to accumulate smartphone sensor data.\n",
      "\n",
      "## Inspiration ##\n",
      "By analyzing this dataset, one can easily find different types of correlations between mobile sensor data and human emotion, sleep patterns, movement patterns, etc. It can also be used in different domains like recommendation systems, activities of daily living, and many more.\n",
      "\n",
      "## Content ##\n",
      "**Version 2(UserInfo.csv, Sensors.csv)**\n",
      "For each participant, the study commenced by collecting their 14 days of smartphone sensor data by using an android application \" Sensor data collector\" and obtained 21690 recorded samples of autonomous smartphone sensor data from 145 users.\n",
      "\n",
      "UserInfo.csv contains information that we have collected directly from the users, like their gender and age range. Privacy concerns were taken into significant consideration in our research study. Focusing on protecting participants’ personal information, we have entirely anonymized each participant’s identity with a random hashed user id.\n",
      "\n",
      "Sensors.csv contains mobile sensor data collected from the accelerometer, gravity, gyroscope, magnetometer, orientation, and GPS sensors. Accelerometer, gravity, gyroscope, and magnetometer data are recorded with their tri-axial values. GPS encompasses the device’s increments in latitude, longitude, and altitude.\n",
      "\n",
      "**Version 1(SENSORDATA.csv)**\n",
      "Data is collected through the same application  \"Sensor data collector\"  for 14 days as version 2. This dataset contains 9516 records of embedded smartphone sensor data collected from the accelerometer, gravity, gyroscope, magnetometer, battery sensor, light sensor, orientation, and GPS sensors. \n",
      "\n",
      "## Acknowledgements ##\n",
      "If you use this dataset, please cite the following paper:\n",
      "\n",
      "{\n",
      "Author= {Moontaha Nishat Chowdhury, H M Zabir Haque, Kazi Taqi Tahmid, Fatema-Tuz-Zohora Salma, Nafisa Ahmed}\n",
      "Title= {A Novel Approach for Product Recommendation Using Smartphone Sensor Data}\n",
      "Publication Type= {Journal Article}\n",
      "Country= {Germany}\n",
      "Year of Publication= {2022}\n",
      "Journal Name= {International Journal of Interactive Mobile Technologies}\n",
      "ISSN= {1865-7923}\n",
      "Volume= {16}\n",
      "Publisher= {Kassel University Press GmbH}\n",
      "DOI={https://doi.org/10.3991/ijim.v16i16.31617 }\n",
      "}\n",
      "\n",
      "or,\n",
      "{\n",
      "Author= {Kazi Taqi Tahmid, Khandaker Rezwan Ahmed, Moontaha Nishat Chowdhury, Koushik Mallik, Umme Habiba, and H. M. Zabir Haque}\n",
      "Title= {An Integrated Crowdsourcing Application for Embedded Smartphone Sensor Data Acquisition and Mobility Analysis}\n",
      "Publication Type= {Journal Article}\n",
      "Country= {United States}\n",
      "Year of Publication= {2022}\n",
      "Journal Name= {Journal of Advances in Information Technology}\n",
      "ISSN= {1798-2340}\n",
      "Volume= {13}\n",
      "Number= {5}\n",
      "Publisher= {Engineering and Technology Publishing}\n",
      "DOI={https://doi.org/10.12720/jait.13.5.503-511 }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: allegray/book-recommendation-system-dataset\n",
      "name 'detect' is not defined Language is not detected: book-recommendation-system-dataset\n",
      "name 'detect' is not defined Language is not detected: allegray\n",
      "name 'detect' is not defined Language is not detected: book_recommendation_system_dataset\n",
      "name 'detect' is not defined Language is not detected: book-recommendation-system-dataset\n",
      "name 'detect' is not defined Language is not detected: allegray\n",
      "name 'detect' is not defined Language is not detected: book_recommendation_system_dataset\n",
      "name 'detect' is not defined Language is not detected: muralidharbhusal/50000-imdb-tv-and-web-series\n",
      "name 'detect' is not defined Language is not detected: 50000-imdb-tv-and-web-series\n",
      "name 'detect' is not defined Language is not detected: muralidharbhusal\n",
      "name 'detect' is not defined Language is not detected: 50,000 IMDB TV and Web Series\n",
      "name 'detect' is not defined Language is not detected: Top 50,000 IMDB TV and Web series based on popularity\n",
      "name 'detect' is not defined Language is not detected: ## About Dataset\n",
      "\n",
      "### Context \n",
      "Scrapped from IMDb, the dataset is a collection of top 50,000 TV shows worldwide based on their popularity.\n",
      "\n",
      "### Content\n",
      "The data contains 7 columns and 50,000 rows. \n",
      "1. Series Title : The name of the TV show\n",
      "2. Release Year : The Year the show was released in \n",
      "3. Runtime : The runtime of single episode of the Show\n",
      "4. Genre : The genre of the show \n",
      "5. Rating : The rating the specific show has received from users in IMDB\n",
      "6. Cast : The leading stars of the show\n",
      "7. Synopsis : Background and summary of the story of the show\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "One of the most popular use of this dataset can be to create recommendation systems. The series can be categorized based on cast of your choice, rating and the type of genre you are into among others.\n",
      "\n",
      "### Acknowledgement\n",
      "The dataset is prepared by scraping the IMDb's website but is not endorsed by IMDb.\n",
      "name 'detect' is not defined Language is not detected: 50000-imdb-tv-and-web-series\n",
      "name 'detect' is not defined Language is not detected: muralidharbhusal\n",
      "name 'detect' is not defined Language is not detected: 50,000 IMDB TV and Web Series\n",
      "name 'detect' is not defined Language is not detected: Top 50,000 IMDB TV and Web series based on popularity\n",
      "name 'detect' is not defined Language is not detected: ## About Dataset\n",
      "\n",
      "### Context \n",
      "Scrapped from IMDb, the dataset is a collection of top 50,000 TV shows worldwide based on their popularity.\n",
      "\n",
      "### Content\n",
      "The data contains 7 columns and 50,000 rows. \n",
      "1. Series Title : The name of the TV show\n",
      "2. Release Year : The Year the show was released in \n",
      "3. Runtime : The runtime of single episode of the Show\n",
      "4. Genre : The genre of the show \n",
      "5. Rating : The rating the specific show has received from users in IMDB\n",
      "6. Cast : The leading stars of the show\n",
      "7. Synopsis : Background and summary of the story of the show\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "One of the most popular use of this dataset can be to create recommendation systems. The series can be categorized based on cast of your choice, rating and the type of genre you are into among others.\n",
      "\n",
      "### Acknowledgement\n",
      "The dataset is prepared by scraping the IMDb's website but is not endorsed by IMDb.\n",
      "name 'detect' is not defined Language is not detected: varun23/predict-clearsky-global-horizontal-irradianceghi\n",
      "name 'detect' is not defined Language is not detected: predict-clearsky-global-horizontal-irradianceghi\n",
      "name 'detect' is not defined Language is not detected: varun23\n",
      "name 'detect' is not defined Language is not detected: Predict Clearsky Global Horizontal Irradiance(GHI)\n",
      "name 'detect' is not defined Language is not detected: Data Attributes:\n",
      "\n",
      "    ‘Year',\n",
      "    'Month',\n",
      "    'Day',\n",
      "    'Hour',\n",
      "    'Minute',\n",
      "    'Temperature', 0C\n",
      "    'Clearsky DHI', w/m2\n",
      "    'Clearsky DNI', w/m2\n",
      "    'Clearsky GHI', w/m2\n",
      "    'Cloud Type',\n",
      "        Cloud Type 0    Clear\n",
      "        Cloud Type 1    Probably Clear\n",
      "        Cloud Type 2    Fog\n",
      "        Cloud Type 3    Water\n",
      "        Cloud Type 4    Super-Cooled Water\n",
      "        Cloud Type 5    Mixed\n",
      "        Cloud Type 6    Opaque Ice\n",
      "        Cloud Type 7    Cirrus\n",
      "        Cloud Type 8    Overlapping\n",
      "        Cloud Type 9    Overshooting\n",
      "        Cloud Type 10    Unknown\n",
      "        Cloud Type 11    Dust\n",
      "        Cloud Type 12    Smoke\n",
      "        Cloud Type -15    N/A\n",
      "    'Dew Point', C\n",
      "    'Fill Flag',\n",
      "        Fill Flag 0    N/A\n",
      "        Fill Flag 1    Missing Image\n",
      "        Fill Flag 2    Low Irradiance\n",
      "        Fill Flag 3    Exceeds Clearsky\n",
      "        Fill Flag 4    Missing CLoud Properties\n",
      "        Fill Flag 5    Rayleigh Violation\n",
      "        Fill Flag any   N/A\n",
      "    'Relative Humidity', %\n",
      "    'Solar Zenith Angle', Degree to calculate cos(θ)\n",
      "    'Surface Albedo', flux per unit area\n",
      "    'Pressure', mbar\n",
      "    'Precipitable Water', cm\n",
      "    'Wind Direction', Degrees\n",
      "    'Wind Speed' m/s\n",
      "name 'detect' is not defined Language is not detected: predict-clearsky-global-horizontal-irradianceghi\n",
      "name 'detect' is not defined Language is not detected: varun23\n",
      "name 'detect' is not defined Language is not detected: Predict Clearsky Global Horizontal Irradiance(GHI)\n",
      "name 'detect' is not defined Language is not detected: Data Attributes:\n",
      "\n",
      "    ‘Year',\n",
      "    'Month',\n",
      "    'Day',\n",
      "    'Hour',\n",
      "    'Minute',\n",
      "    'Temperature', 0C\n",
      "    'Clearsky DHI', w/m2\n",
      "    'Clearsky DNI', w/m2\n",
      "    'Clearsky GHI', w/m2\n",
      "    'Cloud Type',\n",
      "        Cloud Type 0    Clear\n",
      "        Cloud Type 1    Probably Clear\n",
      "        Cloud Type 2    Fog\n",
      "        Cloud Type 3    Water\n",
      "        Cloud Type 4    Super-Cooled Water\n",
      "        Cloud Type 5    Mixed\n",
      "        Cloud Type 6    Opaque Ice\n",
      "        Cloud Type 7    Cirrus\n",
      "        Cloud Type 8    Overlapping\n",
      "        Cloud Type 9    Overshooting\n",
      "        Cloud Type 10    Unknown\n",
      "        Cloud Type 11    Dust\n",
      "        Cloud Type 12    Smoke\n",
      "        Cloud Type -15    N/A\n",
      "    'Dew Point', C\n",
      "    'Fill Flag',\n",
      "        Fill Flag 0    N/A\n",
      "        Fill Flag 1    Missing Image\n",
      "        Fill Flag 2    Low Irradiance\n",
      "        Fill Flag 3    Exceeds Clearsky\n",
      "        Fill Flag 4    Missing CLoud Properties\n",
      "        Fill Flag 5    Rayleigh Violation\n",
      "        Fill Flag any   N/A\n",
      "    'Relative Humidity', %\n",
      "    'Solar Zenith Angle', Degree to calculate cos(θ)\n",
      "    'Surface Albedo', flux per unit area\n",
      "    'Pressure', mbar\n",
      "    'Precipitable Water', cm\n",
      "    'Wind Direction', Degrees\n",
      "    'Wind Speed' m/s\n",
      "name 'detect' is not defined Language is not detected: apkaayush/tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: apkaayush\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset: Unveiling Cinematic Insights.\n",
      "name 'detect' is not defined Language is not detected: **TMDB 10000 Movies Dataset\n",
      "**\n",
      "Dive into the captivating world of cinema with the TMDB 10000 Movies Dataset. This comprehensive collection brings together detailed information about 10,000 movies, providing a rich source of insights for movie enthusiasts, analysts, and data scientists alike. From timeless classics to modern blockbusters, this dataset offers a treasure trove of cinematic knowledge.\n",
      "\n",
      "****Dataset Contents:\n",
      "****Explore a wide range of movie attributes, including:\n",
      "\n",
      "Genres: Discover the diverse genres that span the cinematic landscape.\n",
      "Cast and Crew: Uncover the talented individuals who brought these movies to life on screen.\n",
      "Release Dates: Trace the timeline of movie releases and unveil trends over the years.\n",
      "Popularity and Ratings: Gain insights into the popularity and audience reception of each film.\n",
      "Synopsis and Overview: Get a sneak peek into the storylines that captured the imaginations of audiences.\n",
      "Whether you're interested in analyzing genre trends, exploring the impact of cast and crew on a film's success, or simply delving into the world of your favorite movies, this dataset has something for everyone.\n",
      "\n",
      "**Potential Use Cases:\n",
      "**\n",
      "Data Analysis: Investigate patterns and correlations between genres, popularity, and ratings.\n",
      "Recommendation Systems: Build personalized movie recommendation algorithms based on genre preferences.\n",
      "Visualizations: Create captivating visual representations of movie trends and insights.\n",
      "Machine Learning: Develop predictive models to estimate a movie's popularity based on its attributes.\n",
      "Join the community of movie enthusiasts and data explorers as we unravel the stories behind 10,000 movies. Whether you're a curious cinephile or a seasoned data professional, this dataset invites you to uncover the magic that unfolds on the silver screen.\n",
      "\n",
      "Note: This dataset has been sourced from The Movie Database (TMDB) API and offers a unique opportunity to analyze a diverse collection of movies from various genres and eras.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tmdb-10000-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: apkaayush\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: TMDB 10000 Movies Dataset: Unveiling Cinematic Insights.\n",
      "name 'detect' is not defined Language is not detected: **TMDB 10000 Movies Dataset\n",
      "**\n",
      "Dive into the captivating world of cinema with the TMDB 10000 Movies Dataset. This comprehensive collection brings together detailed information about 10,000 movies, providing a rich source of insights for movie enthusiasts, analysts, and data scientists alike. From timeless classics to modern blockbusters, this dataset offers a treasure trove of cinematic knowledge.\n",
      "\n",
      "****Dataset Contents:\n",
      "****Explore a wide range of movie attributes, including:\n",
      "\n",
      "Genres: Discover the diverse genres that span the cinematic landscape.\n",
      "Cast and Crew: Uncover the talented individuals who brought these movies to life on screen.\n",
      "Release Dates: Trace the timeline of movie releases and unveil trends over the years.\n",
      "Popularity and Ratings: Gain insights into the popularity and audience reception of each film.\n",
      "Synopsis and Overview: Get a sneak peek into the storylines that captured the imaginations of audiences.\n",
      "Whether you're interested in analyzing genre trends, exploring the impact of cast and crew on a film's success, or simply delving into the world of your favorite movies, this dataset has something for everyone.\n",
      "\n",
      "**Potential Use Cases:\n",
      "**\n",
      "Data Analysis: Investigate patterns and correlations between genres, popularity, and ratings.\n",
      "Recommendation Systems: Build personalized movie recommendation algorithms based on genre preferences.\n",
      "Visualizations: Create captivating visual representations of movie trends and insights.\n",
      "Machine Learning: Develop predictive models to estimate a movie's popularity based on its attributes.\n",
      "Join the community of movie enthusiasts and data explorers as we unravel the stories behind 10,000 movies. Whether you're a curious cinephile or a seasoned data professional, this dataset invites you to uncover the magic that unfolds on the silver screen.\n",
      "\n",
      "Note: This dataset has been sourced from The Movie Database (TMDB) API and offers a unique opportunity to analyze a diverse collection of movies from various genres and eras.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tadakasuryateja/nifty-50-stocks\n",
      "name 'detect' is not defined Language is not detected: nifty-50-stocks\n",
      "name 'detect' is not defined Language is not detected: tadakasuryateja\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 Stocks Price Dataset\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 Stocks Prices\n",
      "name 'detect' is not defined Language is not detected: A dataset containing prices of Nifty 50 Stocks. Contains open, high, low, close prices, volume of the stock. \n",
      "This dataset is meant to use for predicting the stock prices in future. \n",
      "You may use linear regression or neural networks.****\n",
      "name 'detect' is not defined Language is not detected: nifty-50-stocks\n",
      "name 'detect' is not defined Language is not detected: tadakasuryateja\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 Stocks Price Dataset\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 Stocks Prices\n",
      "name 'detect' is not defined Language is not detected: A dataset containing prices of Nifty 50 Stocks. Contains open, high, low, close prices, volume of the stock. \n",
      "This dataset is meant to use for predicting the stock prices in future. \n",
      "You may use linear regression or neural networks.****\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian/sentiment-based-product-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sentiment-based-product-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian\n",
      "name 'detect' is not defined Language is not detected: Sentiment Based Product Recommendation System\n",
      "name 'detect' is not defined Language is not detected: Build a recommendation model given their past reviews and ratings\n",
      "name 'detect' is not defined Language is not detected: The e-commerce business is quite popular today. Here, you do not need to take orders by going to each customer. A company launches its website to sell the items to the end consumer, and customers can order the products that they require from the same website. Famous examples of such e-commerce companies are Amazon, Flipkart, Myntra, Paytm and Snapdeal.\n",
      "\n",
      "Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.\n",
      "\n",
      "With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.\n",
      "\n",
      "As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings. \n",
      "\n",
      "In order to do this, you need to build a sentiment-based product recommendation system, which includes the following tasks.\n",
      "\n",
      "- Data sourcing and sentiment analysis\n",
      "- Building a recommendation system\n",
      "- Improving the recommendations using the sentiment analysis model\n",
      "- Deploying the end-to-end project with a user interface\n",
      "name 'detect' is not defined Language is not detected: sentiment-based-product-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian\n",
      "name 'detect' is not defined Language is not detected: Sentiment Based Product Recommendation System\n",
      "name 'detect' is not defined Language is not detected: Build a recommendation model given their past reviews and ratings\n",
      "name 'detect' is not defined Language is not detected: The e-commerce business is quite popular today. Here, you do not need to take orders by going to each customer. A company launches its website to sell the items to the end consumer, and customers can order the products that they require from the same website. Famous examples of such e-commerce companies are Amazon, Flipkart, Myntra, Paytm and Snapdeal.\n",
      "\n",
      "Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.\n",
      "\n",
      "With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.\n",
      "\n",
      "As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings. \n",
      "\n",
      "In order to do this, you need to build a sentiment-based product recommendation system, which includes the following tasks.\n",
      "\n",
      "- Data sourcing and sentiment analysis\n",
      "- Building a recommendation system\n",
      "- Improving the recommendations using the sentiment analysis model\n",
      "- Deploying the end-to-end project with a user interface\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian/automatic-ticket-classification\n",
      "name 'detect' is not defined Language is not detected: automatic-ticket-classification\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian\n",
      "name 'detect' is not defined Language is not detected: Automatic Ticket Classification\n",
      "name 'detect' is not defined Language is not detected: Classify customer complaints based on products and services from the ticket\n",
      "name 'detect' is not defined Language is not detected: # Problem statement\n",
      "For a financial company, customer complaints carry a lot of importance, as they are often an indicator of the shortcomings in their products and services. If these complaints are resolved efficiently in time, they can bring down customer dissatisfaction to a minimum and retain them with stronger loyalty. This also gives them an idea of how to continuously improve their services to attract more customers. \n",
      "These customer complaints are unstructured text data; so, traditionally, companies need to allocate the task of evaluating and assigning each ticket to the relevant department to multiple support employees. This becomes tedious as the company grows and has a large customer base.\n",
      "In this case study, you will be working as an NLP engineer for a financial company that wants to automate its customer support tickets system. As a financial company, the firm has many products and services such as credit cards, banking and mortgages/loans. \n",
      "\n",
      "# Business goal\n",
      "You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
      "With the help of topic modelling, you will detect patterns and recurring words present in each ticket. This can be then used to understand the important features for each cluster of categories. By segregating the clusters, you will be able to identify the topics of the customer complaints. \n",
      "You will be doing topic modelling on the .json data provided by the company. Since this data is not labelled, you need to apply techniques to analyze patterns and classify tickets into the following five clusters based on their products/services:\n",
      "\n",
      "1. Credit card / Prepaid card\n",
      "\n",
      "2.  Bank account services\n",
      "\n",
      "3. Theft/Dispute reporting\n",
      "\n",
      "4. Mortgages/loans\n",
      "\n",
      "5. Others \n",
      "\n",
      "With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department.\n",
      "\n",
      "# Dataset\n",
      "The data set given to you is in the .json format and contains 78,313 customer complaints with 22 features.\n",
      "name 'detect' is not defined Language is not detected: automatic-ticket-classification\n",
      "name 'detect' is not defined Language is not detected: venkatasubramanian\n",
      "name 'detect' is not defined Language is not detected: Automatic Ticket Classification\n",
      "name 'detect' is not defined Language is not detected: Classify customer complaints based on products and services from the ticket\n",
      "name 'detect' is not defined Language is not detected: # Problem statement\n",
      "For a financial company, customer complaints carry a lot of importance, as they are often an indicator of the shortcomings in their products and services. If these complaints are resolved efficiently in time, they can bring down customer dissatisfaction to a minimum and retain them with stronger loyalty. This also gives them an idea of how to continuously improve their services to attract more customers. \n",
      "These customer complaints are unstructured text data; so, traditionally, companies need to allocate the task of evaluating and assigning each ticket to the relevant department to multiple support employees. This becomes tedious as the company grows and has a large customer base.\n",
      "In this case study, you will be working as an NLP engineer for a financial company that wants to automate its customer support tickets system. As a financial company, the firm has many products and services such as credit cards, banking and mortgages/loans. \n",
      "\n",
      "# Business goal\n",
      "You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
      "With the help of topic modelling, you will detect patterns and recurring words present in each ticket. This can be then used to understand the important features for each cluster of categories. By segregating the clusters, you will be able to identify the topics of the customer complaints. \n",
      "You will be doing topic modelling on the .json data provided by the company. Since this data is not labelled, you need to apply techniques to analyze patterns and classify tickets into the following five clusters based on their products/services:\n",
      "\n",
      "1. Credit card / Prepaid card\n",
      "\n",
      "2.  Bank account services\n",
      "\n",
      "3. Theft/Dispute reporting\n",
      "\n",
      "4. Mortgages/loans\n",
      "\n",
      "5. Others \n",
      "\n",
      "With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department.\n",
      "\n",
      "# Dataset\n",
      "The data set given to you is in the .json format and contains 78,313 customer complaints with 22 features.\n",
      "name 'detect' is not defined Language is not detected: ransakaravihara/anime-recommendation-ltr-dataset\n",
      "name 'detect' is not defined Language is not detected: anime-recommendation-ltr-dataset\n",
      "name 'detect' is not defined Language is not detected: ransakaravihara\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendation LTR Dataset\n",
      "name 'detect' is not defined Language is not detected: anime-recommendation-ltr-dataset\n",
      "name 'detect' is not defined Language is not detected: ransakaravihara\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendation LTR Dataset\n",
      "name 'detect' is not defined Language is not detected: chngyuanlongrandy/hdb-prices-with-closest-mrt-distance\n",
      "name 'detect' is not defined Language is not detected: hdb-prices-with-closest-mrt-distance\n",
      "name 'detect' is not defined Language is not detected: chngyuanlongrandy\n",
      "name 'detect' is not defined Language is not detected: HDB Resale Prices with MRT Details (1990 to Now)\n",
      "name 'detect' is not defined Language is not detected: HDB resale prices enriched with MRT Distance and more!\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Yes this is another dataset on the resale prices in Singapore from data.gov.sg. Yes it has been done to death, so why not give it another go?\n",
      "\n",
      "What's different from other datasets is this has the coordinates of each address and the distance in meters to the nearest MRT station.\n",
      "\n",
      "Information is mainly from data.gov.sg on the HDB resale prices taken on the tail end of 2021. The dataset is enriched with the coordinates from open street map and onemap.sg\n",
      "\n",
      "### Content\n",
      "\n",
      "Some address may be incorrect.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Come and be the best property agent in Singapore!\n",
      "\n",
      "### Updates\n",
      "\n",
      "Version 5:\n",
      "- Contains full set of data from the HDB resale flat prices in data.gov.sg\n",
      "- Removed HDB blocks that was removed due to SERS exercise\n",
      "- Geodata should be more accurate now\n",
      "- Nearest MRT is also appended\n",
      "- Calculation of the Nearest MRT distance and MRTs are affected by old lines (EW and NS) vs new lines (Circle line, etc) between pre2000 and post 2000\n",
      "\n",
      "Version 6:\n",
      "- Removed Other unnecessary files \n",
      "- Added in logic to amend address `MARINE CRES` to `MARINE CRESCENT VILLE` as  `MARINE CRES`  will pick up `SEMBCORP MARINE TUAS CRESCENT YARD`\n",
      "name 'detect' is not defined Language is not detected: hdb-prices-with-closest-mrt-distance\n",
      "name 'detect' is not defined Language is not detected: chngyuanlongrandy\n",
      "name 'detect' is not defined Language is not detected: HDB Resale Prices with MRT Details (1990 to Now)\n",
      "name 'detect' is not defined Language is not detected: HDB resale prices enriched with MRT Distance and more!\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Yes this is another dataset on the resale prices in Singapore from data.gov.sg. Yes it has been done to death, so why not give it another go?\n",
      "\n",
      "What's different from other datasets is this has the coordinates of each address and the distance in meters to the nearest MRT station.\n",
      "\n",
      "Information is mainly from data.gov.sg on the HDB resale prices taken on the tail end of 2021. The dataset is enriched with the coordinates from open street map and onemap.sg\n",
      "\n",
      "### Content\n",
      "\n",
      "Some address may be incorrect.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Come and be the best property agent in Singapore!\n",
      "\n",
      "### Updates\n",
      "\n",
      "Version 5:\n",
      "- Contains full set of data from the HDB resale flat prices in data.gov.sg\n",
      "- Removed HDB blocks that was removed due to SERS exercise\n",
      "- Geodata should be more accurate now\n",
      "- Nearest MRT is also appended\n",
      "- Calculation of the Nearest MRT distance and MRTs are affected by old lines (EW and NS) vs new lines (Circle line, etc) between pre2000 and post 2000\n",
      "\n",
      "Version 6:\n",
      "- Removed Other unnecessary files \n",
      "- Added in logic to amend address `MARINE CRES` to `MARINE CRESCENT VILLE` as  `MARINE CRES`  will pick up `SEMBCORP MARINE TUAS CRESCENT YARD`\n",
      "name 'detect' is not defined Language is not detected: chaitanyakck/stackoverflow-datasets-2011-to-present\n",
      "name 'detect' is not defined Language is not detected: stackoverflow-datasets-2011-to-present\n",
      "name 'detect' is not defined Language is not detected: chaitanyakck\n",
      "name 'detect' is not defined Language is not detected: StackOverflow Datasets 2011 to Present\n",
      "name 'detect' is not defined Language is not detected: StackOverflow Developer Surevey Results from 2011 to Present\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "StackOverflow’s annual Developer Survey is the largest and most comprehensive survey of developers around the world. Each year, StackOverflow conducts a survey covering various aspects from developers’ favorite technologies to their job preferences.\n",
      "\n",
      "\n",
      "### Content\n",
      "This dataset contains individual responses from developers on the Developer Survey fielded by Stack Overflow 2011 - 2020. The files contain - one respondent per row and one column per answer.\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to [StackOverflow](https://insights.stackoverflow.com/survey)\n",
      "Image source: Photo by [Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/code?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
      "name 'detect' is not defined Language is not detected: stackoverflow-datasets-2011-to-present\n",
      "name 'detect' is not defined Language is not detected: chaitanyakck\n",
      "name 'detect' is not defined Language is not detected: StackOverflow Datasets 2011 to Present\n",
      "name 'detect' is not defined Language is not detected: StackOverflow Developer Surevey Results from 2011 to Present\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "StackOverflow’s annual Developer Survey is the largest and most comprehensive survey of developers around the world. Each year, StackOverflow conducts a survey covering various aspects from developers’ favorite technologies to their job preferences.\n",
      "\n",
      "\n",
      "### Content\n",
      "This dataset contains individual responses from developers on the Developer Survey fielded by Stack Overflow 2011 - 2020. The files contain - one respondent per row and one column per answer.\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to [StackOverflow](https://insights.stackoverflow.com/survey)\n",
      "Image source: Photo by [Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/code?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
      "name 'detect' is not defined Language is not detected: abrambeyer/us-hospital-overall-star-ratings-20162020\n",
      "name 'detect' is not defined Language is not detected: us-hospital-overall-star-ratings-20162020\n",
      "name 'detect' is not defined Language is not detected: abrambeyer\n",
      "name 'detect' is not defined Language is not detected: U.S. Hospital Overall Star Ratings 2016-2020\n",
      "name 'detect' is not defined Language is not detected: How does your hospital compare to the other 4,000 Medicare-certified hospitals?\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Every year, all U.S. hospitals that accept payments from Medicare and Medicaid must submit quality data to The Centers for Medicare and Medicaid Services (CMS).  CMS' [Hospital Compare](https://www.medicare.gov/care-compare/?providerType=Hospital&redirect=true) program is a consumer-oriented website that provides information on \"the quality of care hospitals are providing to their patients.\"  CMS releases this quality data publicly in order to encourage hospitals to improve their quality and to help consumer make better decisions about which providers they visit.\n",
      "\n",
      "\"Hospital Compare provides data on over 4,000 Medicare-certified hospitals, including acute care hospitals, critical access hospitals (CAHs), children’s hospitals, Veterans Health Administration (VHA) Medical Centers, and hospital outpatient departments\"\n",
      "\n",
      "The Centers for Medicare & Medicaid Services (CMS) uses a five-star quality rating system to measure the experiences Medicare beneficiaries have with their health plan and health care system — the Star Rating Program. Health plans are rated on a scale of 1 to 5 stars, with 5 being the highest.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "| Dataset Rows  | Dataset Columns  |\n",
      "| --- | --- |\n",
      "|  25082 |  29 |\n",
      "\n",
      "* Includes the most recent Hospital General Information.csv data for each archive year found on CMS' [archive site](https://data.cms.gov/provider-data/archived-data/hospitals).  ***Years: 2016-2020***\n",
      "\n",
      "| Column Name | Data Type | Description |\n",
      "| --- | --- | -- |\n",
      "|  Facility ID |  Char(6) | Facility Medicare ID |\n",
      "|  Facility Name |  Char(72) | Name of the facility |\n",
      "|  Address |  Char(51) | Facility street address |\n",
      "|  City |  Char(20) | Facility City |\n",
      "|  State |  Char(2) | Facility State |\n",
      "| ZIP Code |  Num(8) | Facility ZIP Code |\n",
      "|  County Name | Char(25) | Facility County |\n",
      "|  Phone Number |  Char(14) | Facility Phone Number |\n",
      "|  Hospital Type |  Char(34) | What type of facility is it? |\n",
      "|  Hospital Ownership |  Char(43) | What type of ownership does the facility have? |\n",
      "|  Emergency Services |  Char(3)) | Does the facility have emergency services Yes/No? |\n",
      "|  Meets criteria for promoting interoperability of EHRs |  Char(1) | Does facility meet government EHR standard Yes/No? |\n",
      "|  Hospital overall rating |  Char(13) | Hospital Overall Star Rating 1=Worst; 5=Best.  Aggregate measure of all other measures |\n",
      "|  Hospital overall rating footnote |  Num(8) |   |\n",
      "|  Mortality national comparison |  Char(28) | Facility overall performance on mortality measures compared to other facilities |\n",
      "|  Mortality national comparison footnote |  Num(8) |  |\n",
      "|  Safety of care national comparison |  Char(28) | Facility overall performance on safety measures compared to other facilities |\n",
      "|  Safety of care national comparison footnote |  Num(8) |  |\n",
      "|  Readmission national comparison |  Char(28) | Facility overall performance on readmission measures compared to other facilities |\n",
      "|  Readmission national comparison footnote |  Num(8) |  |\n",
      "|  Patient experience national comparison |  Char(28) | Facility overall performance on pat. exp. measures compared to other facilities |\n",
      "|  Patient experience national comparison footnote |  Char(8) |  |\n",
      "|  Effectiveness of care national comparison |  Char(28) | Facility overall performance on effect. of care measures compared to other facilities |\n",
      "|  Effectiveness of care national comparison footnote |  Char(8) |  |\n",
      "|  Timeliness of care national comparison |  Char(28) | Facility overall performance on timeliness of care measures compared to other facilities |\n",
      "|  Timeliness of care national comparison footnote|  Char(8) |  |\n",
      "|  Efficient use of medical imaging national comparison |  Char(28) | Facility overall performance on efficient use measures compared to other facilities |\n",
      "|  Efficient use of medical imaging national comparison footnote |  Char(8) |  |\n",
      "|  Year |  Char(4) | cms data release year  |\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "A similar dataset called [Hospital General Information](https://www.kaggle.com/cms/hospital-general-information) was previously uploaded to Kaggle.  However, that dataset only includes data from one year (2017).  I was inspired by this dataset to go a little further and try to add a time dimension.  This dataset includes a union of Hospital General Information for the years 2016-2020.  The python script used to collect and union all the datasets can be found on my [github[(https://github.com/abrambeyer/cms_hospital_general_info_file_downloader).  Thanks to this dataset owner for the inspiration.\n",
      "\n",
      "Thanks to CMS for releasing this dataset publicly to help consumers find better hospitals and make better-informed decisions.\n",
      "\n",
      "***All Hospital Compare websites are publically accessible. As works of the U.S. government, Hospital Compare data are in the public domain and permission is not required to reuse them. An attribution to the agency as the source is appreciated. Your materials, however, should not give the false impression of government endorsement of your commercial products or services.***\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Since I work in healthcare, I was inspired to look at my own hospital's performance.  Since CMS provides overall performance data on over 4,000 facilities, I hope this data can help people answer the following questions:\n",
      "\n",
      "1.  Which hospitals have the best overall performance?  Best performance by measure such as Patient Experience?\n",
      "2.  Which states, cities, counties have the most high-performing hospitals?  The lowest performing hospitals?\n",
      "3.  How have individual hospitals overall performance changed between 2016 and 2020?  Are certain locations getting better or worse overall?\n",
      "name 'detect' is not defined Language is not detected: us-hospital-overall-star-ratings-20162020\n",
      "name 'detect' is not defined Language is not detected: abrambeyer\n",
      "name 'detect' is not defined Language is not detected: U.S. Hospital Overall Star Ratings 2016-2020\n",
      "name 'detect' is not defined Language is not detected: How does your hospital compare to the other 4,000 Medicare-certified hospitals?\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Every year, all U.S. hospitals that accept payments from Medicare and Medicaid must submit quality data to The Centers for Medicare and Medicaid Services (CMS).  CMS' [Hospital Compare](https://www.medicare.gov/care-compare/?providerType=Hospital&redirect=true) program is a consumer-oriented website that provides information on \"the quality of care hospitals are providing to their patients.\"  CMS releases this quality data publicly in order to encourage hospitals to improve their quality and to help consumer make better decisions about which providers they visit.\n",
      "\n",
      "\"Hospital Compare provides data on over 4,000 Medicare-certified hospitals, including acute care hospitals, critical access hospitals (CAHs), children’s hospitals, Veterans Health Administration (VHA) Medical Centers, and hospital outpatient departments\"\n",
      "\n",
      "The Centers for Medicare & Medicaid Services (CMS) uses a five-star quality rating system to measure the experiences Medicare beneficiaries have with their health plan and health care system — the Star Rating Program. Health plans are rated on a scale of 1 to 5 stars, with 5 being the highest.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "| Dataset Rows  | Dataset Columns  |\n",
      "| --- | --- |\n",
      "|  25082 |  29 |\n",
      "\n",
      "* Includes the most recent Hospital General Information.csv data for each archive year found on CMS' [archive site](https://data.cms.gov/provider-data/archived-data/hospitals).  ***Years: 2016-2020***\n",
      "\n",
      "| Column Name | Data Type | Description |\n",
      "| --- | --- | -- |\n",
      "|  Facility ID |  Char(6) | Facility Medicare ID |\n",
      "|  Facility Name |  Char(72) | Name of the facility |\n",
      "|  Address |  Char(51) | Facility street address |\n",
      "|  City |  Char(20) | Facility City |\n",
      "|  State |  Char(2) | Facility State |\n",
      "| ZIP Code |  Num(8) | Facility ZIP Code |\n",
      "|  County Name | Char(25) | Facility County |\n",
      "|  Phone Number |  Char(14) | Facility Phone Number |\n",
      "|  Hospital Type |  Char(34) | What type of facility is it? |\n",
      "|  Hospital Ownership |  Char(43) | What type of ownership does the facility have? |\n",
      "|  Emergency Services |  Char(3)) | Does the facility have emergency services Yes/No? |\n",
      "|  Meets criteria for promoting interoperability of EHRs |  Char(1) | Does facility meet government EHR standard Yes/No? |\n",
      "|  Hospital overall rating |  Char(13) | Hospital Overall Star Rating 1=Worst; 5=Best.  Aggregate measure of all other measures |\n",
      "|  Hospital overall rating footnote |  Num(8) |   |\n",
      "|  Mortality national comparison |  Char(28) | Facility overall performance on mortality measures compared to other facilities |\n",
      "|  Mortality national comparison footnote |  Num(8) |  |\n",
      "|  Safety of care national comparison |  Char(28) | Facility overall performance on safety measures compared to other facilities |\n",
      "|  Safety of care national comparison footnote |  Num(8) |  |\n",
      "|  Readmission national comparison |  Char(28) | Facility overall performance on readmission measures compared to other facilities |\n",
      "|  Readmission national comparison footnote |  Num(8) |  |\n",
      "|  Patient experience national comparison |  Char(28) | Facility overall performance on pat. exp. measures compared to other facilities |\n",
      "|  Patient experience national comparison footnote |  Char(8) |  |\n",
      "|  Effectiveness of care national comparison |  Char(28) | Facility overall performance on effect. of care measures compared to other facilities |\n",
      "|  Effectiveness of care national comparison footnote |  Char(8) |  |\n",
      "|  Timeliness of care national comparison |  Char(28) | Facility overall performance on timeliness of care measures compared to other facilities |\n",
      "|  Timeliness of care national comparison footnote|  Char(8) |  |\n",
      "|  Efficient use of medical imaging national comparison |  Char(28) | Facility overall performance on efficient use measures compared to other facilities |\n",
      "|  Efficient use of medical imaging national comparison footnote |  Char(8) |  |\n",
      "|  Year |  Char(4) | cms data release year  |\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "A similar dataset called [Hospital General Information](https://www.kaggle.com/cms/hospital-general-information) was previously uploaded to Kaggle.  However, that dataset only includes data from one year (2017).  I was inspired by this dataset to go a little further and try to add a time dimension.  This dataset includes a union of Hospital General Information for the years 2016-2020.  The python script used to collect and union all the datasets can be found on my [github[(https://github.com/abrambeyer/cms_hospital_general_info_file_downloader).  Thanks to this dataset owner for the inspiration.\n",
      "\n",
      "Thanks to CMS for releasing this dataset publicly to help consumers find better hospitals and make better-informed decisions.\n",
      "\n",
      "***All Hospital Compare websites are publically accessible. As works of the U.S. government, Hospital Compare data are in the public domain and permission is not required to reuse them. An attribution to the agency as the source is appreciated. Your materials, however, should not give the false impression of government endorsement of your commercial products or services.***\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Since I work in healthcare, I was inspired to look at my own hospital's performance.  Since CMS provides overall performance data on over 4,000 facilities, I hope this data can help people answer the following questions:\n",
      "\n",
      "1.  Which hospitals have the best overall performance?  Best performance by measure such as Patient Experience?\n",
      "2.  Which states, cities, counties have the most high-performing hospitals?  The lowest performing hospitals?\n",
      "3.  How have individual hospitals overall performance changed between 2016 and 2020?  Are certain locations getting better or worse overall?\n",
      "name 'detect' is not defined Language is not detected: ajaysh/women-apparel-recommendation-engine-amazoncom\n",
      "name 'detect' is not defined Language is not detected: women-apparel-recommendation-engine-amazoncom\n",
      "name 'detect' is not defined Language is not detected: ajaysh\n",
      "name 'detect' is not defined Language is not detected: Women Apparel Recommendation Engine (amazon.com)\n",
      "name 'detect' is not defined Language is not detected: women-apparel-recommendation-engine-amazoncom\n",
      "name 'detect' is not defined Language is not detected: ajaysh\n",
      "name 'detect' is not defined Language is not detected: Women Apparel Recommendation Engine (amazon.com)\n",
      "name 'detect' is not defined Language is not detected: mrsantos/hcc-dataset\n",
      "name 'detect' is not defined Language is not detected: hcc-dataset\n",
      "name 'detect' is not defined Language is not detected: mrsantos\n",
      "name 'detect' is not defined Language is not detected: HCC dataset\n",
      "name 'detect' is not defined Language is not detected: Hepatocellular Carcinoma Dataset\n",
      "name 'detect' is not defined Language is not detected: **Data Set Name:** Hepatocellular Carcinoma Dataset (HCC dataset)\n",
      "\n",
      "**Abstract:** Hepatocellular Carcinoma dataset (HCC dataset) was collected at a University Hospital in Portugal. It contains real clinical data of 165 patients diagnosed with HCC.\n",
      "\n",
      "**Donors:** \n",
      "Miriam Seoane Santos (miriams@student.dei.uc.pt) and Pedro Henriques Abreu (pha@dei.uc.pt), Department of Informatics Engineering, Faculty of Sciences and Technology, University of Coimbra \n",
      "Armando Carvalho (aspcarvalho@gmail.com) and Adélia Simão (adeliasimao@gmail.com), Internal Medicine Service, Hospital and University Centre of Coimbra \n",
      "\n",
      "**Data Type:**  Multivariate\n",
      "**Task:** Classification, Regression, Clustering, Casual Discovery\n",
      "**Attribute Type**: Categorical, Integer and Real\n",
      "\n",
      "**Area:** Life Sciences \n",
      "**Format Type:** Matrix\n",
      "**Missing values:** Yes\n",
      "\n",
      "**Instances and Attributes:**\n",
      "Number of Instances (records in your data set): 165\n",
      "Number of attributes (fields within each record): 49\n",
      "\n",
      "\n",
      "**Relevant Information:**\n",
      "HCC dataset was obtained at a University Hospital in Portugal and contais several demographic, risk factors, laboratory and overall survival features of 165 real patients diagnosed with HCC. The dataset contains 49 features selected according to the EASL-EORTC (European Association for the Study of the Liver - European Organisation for Research and Treatment of Cancer) Clinical Practice Guidelines, which are the current state-of-the-art on the management of HCC.\n",
      "\n",
      "This is an heterogeneous dataset, with 23 quantitative variables, and 26 qualitative variables. Overall, missing data represents 10.22% of the whole dataset and only eight patients have complete information in all fields (4.85%). The target variables is the survival at 1 year, and was encoded as a binary variable: 0 (dies) and 1 (lives). A certain degree of class-imbalance is also present (63 cases labeled as “dies” and 102 as “lives”).\n",
      "\n",
      "A detailed description of the HCC dataset (feature’s type/scale, range, mean/mode and missing data percentages) is provided in Santos et al. “A new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients”, Journal of biomedical informatics, 58, 49-59, 2015.\n",
      "name 'detect' is not defined Language is not detected: hcc-dataset\n",
      "name 'detect' is not defined Language is not detected: mrsantos\n",
      "name 'detect' is not defined Language is not detected: HCC dataset\n",
      "name 'detect' is not defined Language is not detected: Hepatocellular Carcinoma Dataset\n",
      "name 'detect' is not defined Language is not detected: **Data Set Name:** Hepatocellular Carcinoma Dataset (HCC dataset)\n",
      "\n",
      "**Abstract:** Hepatocellular Carcinoma dataset (HCC dataset) was collected at a University Hospital in Portugal. It contains real clinical data of 165 patients diagnosed with HCC.\n",
      "\n",
      "**Donors:** \n",
      "Miriam Seoane Santos (miriams@student.dei.uc.pt) and Pedro Henriques Abreu (pha@dei.uc.pt), Department of Informatics Engineering, Faculty of Sciences and Technology, University of Coimbra \n",
      "Armando Carvalho (aspcarvalho@gmail.com) and Adélia Simão (adeliasimao@gmail.com), Internal Medicine Service, Hospital and University Centre of Coimbra \n",
      "\n",
      "**Data Type:**  Multivariate\n",
      "**Task:** Classification, Regression, Clustering, Casual Discovery\n",
      "**Attribute Type**: Categorical, Integer and Real\n",
      "\n",
      "**Area:** Life Sciences \n",
      "**Format Type:** Matrix\n",
      "**Missing values:** Yes\n",
      "\n",
      "**Instances and Attributes:**\n",
      "Number of Instances (records in your data set): 165\n",
      "Number of attributes (fields within each record): 49\n",
      "\n",
      "\n",
      "**Relevant Information:**\n",
      "HCC dataset was obtained at a University Hospital in Portugal and contais several demographic, risk factors, laboratory and overall survival features of 165 real patients diagnosed with HCC. The dataset contains 49 features selected according to the EASL-EORTC (European Association for the Study of the Liver - European Organisation for Research and Treatment of Cancer) Clinical Practice Guidelines, which are the current state-of-the-art on the management of HCC.\n",
      "\n",
      "This is an heterogeneous dataset, with 23 quantitative variables, and 26 qualitative variables. Overall, missing data represents 10.22% of the whole dataset and only eight patients have complete information in all fields (4.85%). The target variables is the survival at 1 year, and was encoded as a binary variable: 0 (dies) and 1 (lives). A certain degree of class-imbalance is also present (63 cases labeled as “dies” and 102 as “lives”).\n",
      "\n",
      "A detailed description of the HCC dataset (feature’s type/scale, range, mean/mode and missing data percentages) is provided in Santos et al. “A new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients”, Journal of biomedical informatics, 58, 49-59, 2015.\n",
      "name 'detect' is not defined Language is not detected: filipdomovic/cars-data\n",
      "name 'detect' is not defined Language is not detected: cars-data\n",
      "name 'detect' is not defined Language is not detected: filipdomovic\n",
      "name 'detect' is not defined Language is not detected: Cars data\n",
      "name 'detect' is not defined Language is not detected: Most comprehensive cars specifications dataset on Kaggle\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information about various cars and their specifications, with a focus on key attributes that are relevant for predicting car prices, drag race performance, and similar automotive-related predictions. The data is derived from a car website and may be updated in the future with additional performance-related information. \n",
      "Below is a description of each column in the dataset:\n",
      "\n",
      "**brand:** The brand or manufacturer of the car.\n",
      "**car_id:** An identifier for each car in the dataset.\n",
      "**model:** The model or name of the car.\n",
      "**cylinders:** The number of cylinders in the car's engine.\n",
      "**transmission:** The type of transmission (e.g., automatic, manual).\n",
      "**drive_wheel:** The type of drive wheel configuration (e.g., front-wheel drive, rear-wheel drive, all-wheel drive).\n",
      "**power:** The car's power output in horsepower (HP).\n",
      "**max_power_rpm:** The RPM (revolutions per minute) at which the maximum power is achieved.\n",
      "**torque:** The car's torque output in Newton-meters (Nm).\n",
      "**max_torque_rpm:** The RPM at which the maximum torque is achieved.\n",
      "**turbo:** Indicates whether the car has a turbocharger and which type of turbocharger.\n",
      "**fuel:** The type of fuel the car uses (e.g., gasoline, diesel).\n",
      "**top_speed:** The car's maximum attainable speed in kilometers per hour (km/h).\n",
      "**acc_0_100:** The time it takes for the car to accelerate from 0 to 100 km/h in seconds.\n",
      "**gear_1 to gear_9:** Information about the gear ratios for each gear (if applicable).\n",
      "**gear_r:** Information about the reverse gear (if applicable).\n",
      "**gear_final:** The final drive ratio of the car's transmission.\n",
      "**front_tire:** Specifications of the front tires.\n",
      "**rear_tire:** Specifications of the rear tires.\n",
      "**eng_capacity:** The engine capacity in cubic centimeters (cc).\n",
      "**weight:** The weight of the car in kilograms (kg).\n",
      "**height:** The height of the car in millimeters (mm).\n",
      "**width:** The width of the car in millimeters (mm).\n",
      "**length:** The length of the car in millimeters (mm).\n",
      "**wheelbase:** The wheelbase of the car in millimeters (mm).\n",
      "\n",
      "This dataset is well-suited for various predictive modeling tasks, including:\n",
      "\n",
      "**Car Price Prediction:** The dataset provides key features like brand, model, engine specifications, and more, making it suitable for predicting car prices.\n",
      "\n",
      "**Drag Race Performance Prediction:** With attributes such as power, torque, and acceleration data, this dataset can be used to predict a car's performance in drag races.\n",
      "\n",
      "**Automotive Analytics:** Researchers and enthusiasts can use this dataset to conduct in-depth analysis of various car attributes and their impact on performance and pricing.\n",
      "\n",
      "**Recommendation Systems:** The dataset can be used to build recommendation systems for car buyers based on their preferences and needs.\n",
      "\n",
      "**Machine Learning Projects:** It serves as a valuable resource for machine learning projects related to cars, automotive technology, and performance analysis.\n",
      "\n",
      "Keep in mind that as the dataset is updated with more performance-related data in the future, its utility for predicting various automotive-related outcomes is likely to increase.\n",
      "name 'detect' is not defined Language is not detected: cars-data\n",
      "name 'detect' is not defined Language is not detected: filipdomovic\n",
      "name 'detect' is not defined Language is not detected: Cars data\n",
      "name 'detect' is not defined Language is not detected: Most comprehensive cars specifications dataset on Kaggle\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information about various cars and their specifications, with a focus on key attributes that are relevant for predicting car prices, drag race performance, and similar automotive-related predictions. The data is derived from a car website and may be updated in the future with additional performance-related information. \n",
      "Below is a description of each column in the dataset:\n",
      "\n",
      "**brand:** The brand or manufacturer of the car.\n",
      "**car_id:** An identifier for each car in the dataset.\n",
      "**model:** The model or name of the car.\n",
      "**cylinders:** The number of cylinders in the car's engine.\n",
      "**transmission:** The type of transmission (e.g., automatic, manual).\n",
      "**drive_wheel:** The type of drive wheel configuration (e.g., front-wheel drive, rear-wheel drive, all-wheel drive).\n",
      "**power:** The car's power output in horsepower (HP).\n",
      "**max_power_rpm:** The RPM (revolutions per minute) at which the maximum power is achieved.\n",
      "**torque:** The car's torque output in Newton-meters (Nm).\n",
      "**max_torque_rpm:** The RPM at which the maximum torque is achieved.\n",
      "**turbo:** Indicates whether the car has a turbocharger and which type of turbocharger.\n",
      "**fuel:** The type of fuel the car uses (e.g., gasoline, diesel).\n",
      "**top_speed:** The car's maximum attainable speed in kilometers per hour (km/h).\n",
      "**acc_0_100:** The time it takes for the car to accelerate from 0 to 100 km/h in seconds.\n",
      "**gear_1 to gear_9:** Information about the gear ratios for each gear (if applicable).\n",
      "**gear_r:** Information about the reverse gear (if applicable).\n",
      "**gear_final:** The final drive ratio of the car's transmission.\n",
      "**front_tire:** Specifications of the front tires.\n",
      "**rear_tire:** Specifications of the rear tires.\n",
      "**eng_capacity:** The engine capacity in cubic centimeters (cc).\n",
      "**weight:** The weight of the car in kilograms (kg).\n",
      "**height:** The height of the car in millimeters (mm).\n",
      "**width:** The width of the car in millimeters (mm).\n",
      "**length:** The length of the car in millimeters (mm).\n",
      "**wheelbase:** The wheelbase of the car in millimeters (mm).\n",
      "\n",
      "This dataset is well-suited for various predictive modeling tasks, including:\n",
      "\n",
      "**Car Price Prediction:** The dataset provides key features like brand, model, engine specifications, and more, making it suitable for predicting car prices.\n",
      "\n",
      "**Drag Race Performance Prediction:** With attributes such as power, torque, and acceleration data, this dataset can be used to predict a car's performance in drag races.\n",
      "\n",
      "**Automotive Analytics:** Researchers and enthusiasts can use this dataset to conduct in-depth analysis of various car attributes and their impact on performance and pricing.\n",
      "\n",
      "**Recommendation Systems:** The dataset can be used to build recommendation systems for car buyers based on their preferences and needs.\n",
      "\n",
      "**Machine Learning Projects:** It serves as a valuable resource for machine learning projects related to cars, automotive technology, and performance analysis.\n",
      "\n",
      "Keep in mind that as the dataset is updated with more performance-related data in the future, its utility for predicting various automotive-related outcomes is likely to increase.\n",
      "name 'detect' is not defined Language is not detected: aslibaraf/goibibo-indian-hotels-dataset-scraped\n",
      "name 'detect' is not defined Language is not detected: goibibo-indian-hotels-dataset-scraped\n",
      "name 'detect' is not defined Language is not detected: aslibaraf\n",
      "name 'detect' is not defined Language is not detected: Goibibo Indian Hotels DataSet RecommendationEngine\n",
      "name 'detect' is not defined Language is not detected: Build Recommendation Engine for Indian Hotels\n",
      "name 'detect' is not defined Language is not detected: This dataset consists of several different CSV files namely Hotel Dataset, Nearby Landmarks, and Hotels review. **Hotel Code is Primary Key which is a Unique ID for each hotel. **\n",
      "name 'detect' is not defined Language is not detected: goibibo-indian-hotels-dataset-scraped\n",
      "name 'detect' is not defined Language is not detected: aslibaraf\n",
      "name 'detect' is not defined Language is not detected: Goibibo Indian Hotels DataSet RecommendationEngine\n",
      "name 'detect' is not defined Language is not detected: Build Recommendation Engine for Indian Hotels\n",
      "name 'detect' is not defined Language is not detected: This dataset consists of several different CSV files namely Hotel Dataset, Nearby Landmarks, and Hotels review. **Hotel Code is Primary Key which is a Unique ID for each hotel. **\n",
      "name 'detect' is not defined Language is not detected: adinishad/yts-movie-dataset\n",
      "name 'detect' is not defined Language is not detected: yts-movie-dataset\n",
      "name 'detect' is not defined Language is not detected: adinishad\n",
      "name 'detect' is not defined Language is not detected: YTS movie dataset\n",
      "name 'detect' is not defined Language is not detected: Movie dataset for analysis and recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Create JSON to make movie dataset one step harder, try and analyze.\n",
      "\n",
      "\n",
      "### Content\n",
      "\"name\":string\"Movie Name\"\n",
      "\"year\":string\"Movie Release Year\"\n",
      "\"imdb_rating\":string\"Movie imdb rating\"\n",
      "\"movie_poster\":NULL\n",
      "\"genres\":string\"Drama , Romance , War\"\n",
      "\"directors\":string\"Director\"\n",
      "\"cast_0, cast_1, cast_2, cast_3\": string\"Actor Name\"\n",
      "\"similar movie\" : recommended movie name\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Let's create a recommendation system\n",
      "name 'detect' is not defined Language is not detected: yts-movie-dataset\n",
      "name 'detect' is not defined Language is not detected: adinishad\n",
      "name 'detect' is not defined Language is not detected: YTS movie dataset\n",
      "name 'detect' is not defined Language is not detected: Movie dataset for analysis and recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Create JSON to make movie dataset one step harder, try and analyze.\n",
      "\n",
      "\n",
      "### Content\n",
      "\"name\":string\"Movie Name\"\n",
      "\"year\":string\"Movie Release Year\"\n",
      "\"imdb_rating\":string\"Movie imdb rating\"\n",
      "\"movie_poster\":NULL\n",
      "\"genres\":string\"Drama , Romance , War\"\n",
      "\"directors\":string\"Director\"\n",
      "\"cast_0, cast_1, cast_2, cast_3\": string\"Actor Name\"\n",
      "\"similar movie\" : recommended movie name\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Let's create a recommendation system\n",
      "name 'detect' is not defined Language is not detected: chocozzz/t-academy-recommendation2\n",
      "name 'detect' is not defined Language is not detected: t-academy-recommendation2\n",
      "name 'detect' is not defined Language is not detected: chocozzz\n",
      "name 'detect' is not defined Language is not detected: T Academy Recommendation2\n",
      "name 'detect' is not defined Language is not detected: 82. Recommendation Introduction \n",
      "name 'detect' is not defined Language is not detected: t-academy-recommendation2\n",
      "name 'detect' is not defined Language is not detected: chocozzz\n",
      "name 'detect' is not defined Language is not detected: T Academy Recommendation2\n",
      "name 'detect' is not defined Language is not detected: 82. Recommendation Introduction \n",
      "name 'detect' is not defined Language is not detected: deepaksiva/movie-recommendation-datasetimdbnetflix\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-datasetimdbnetflix\n",
      "name 'detect' is not defined Language is not detected: deepaksiva\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation dataset-imdb-netflix\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-datasetimdbnetflix\n",
      "name 'detect' is not defined Language is not detected: deepaksiva\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation dataset-imdb-netflix\n",
      "name 'detect' is not defined Language is not detected: santhoshchinnu/personalized-product-recommedations-data-set\n",
      "name 'detect' is not defined Language is not detected: personalized-product-recommedations-data-set\n",
      "name 'detect' is not defined Language is not detected: santhoshchinnu\n",
      "name 'detect' is not defined Language is not detected: personalized product recommedation\n",
      "name 'detect' is not defined Language is not detected: product data analysis\n",
      "name 'detect' is not defined Language is not detected: \n",
      "               Personalized product recommendations have become a pivotal component of e-commerce and digital marketing strategies. Leveraging user data and advanced algorithms, these recommendations aim to enhance the online shopping experience by providing tailored product suggestions to individual customers. This abstract delves into the key aspects of personalized product recommendations, including data collection, analysis, segmentation, and recommendation algorithms. The utilization of collaborative filtering, content-based filtering, and hybrid methods is explored as the foundation for generating personalized suggestions. Real-time updates and presentation strategies are also discussed as essential elements for effective recommendation systems. The benefits of personalized product recommendations, such as improved user experiences, increased sales, customer retention, and enhanced inventory management, are highlighted. Lastly, the abstract underscores the importance of privacy and data security in implementing these recommendations to maintain user trust and comply with relevant regulations.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: personalized-product-recommedations-data-set\n",
      "name 'detect' is not defined Language is not detected: santhoshchinnu\n",
      "name 'detect' is not defined Language is not detected: personalized product recommedation\n",
      "name 'detect' is not defined Language is not detected: product data analysis\n",
      "name 'detect' is not defined Language is not detected: \n",
      "               Personalized product recommendations have become a pivotal component of e-commerce and digital marketing strategies. Leveraging user data and advanced algorithms, these recommendations aim to enhance the online shopping experience by providing tailored product suggestions to individual customers. This abstract delves into the key aspects of personalized product recommendations, including data collection, analysis, segmentation, and recommendation algorithms. The utilization of collaborative filtering, content-based filtering, and hybrid methods is explored as the foundation for generating personalized suggestions. Real-time updates and presentation strategies are also discussed as essential elements for effective recommendation systems. The benefits of personalized product recommendations, such as improved user experiences, increased sales, customer retention, and enhanced inventory management, are highlighted. Lastly, the abstract underscores the importance of privacy and data security in implementing these recommendations to maintain user trust and comply with relevant regulations.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: vinayakshanawad/us-news-dataset\n",
      "name 'detect' is not defined Language is not detected: us-news-dataset\n",
      "name 'detect' is not defined Language is not detected: vinayakshanawad\n",
      "name 'detect' is not defined Language is not detected: USA News Dataset\n",
      "name 'detect' is not defined Language is not detected: Information of news articles and click histories\n",
      "name 'detect' is not defined Language is not detected: ### Problem Description\n",
      "\n",
      "Construct two types of models -- (A) a deep learning classifier such as LSTM or similar model to predict the category of a news article given its title and abstract, and (B) A recommendation system to recommend posts that a user is most likely to click.\n",
      "\n",
      "The dataset consists of two files -- (1) user_news_clicks.csv, and (2) news_text.csv.\n",
      "\n",
      "Model A, the deep learning classifier only requires the news_text.csv dataset. The goal is to predict the ‘category’ label using the ‘title’ and ‘abstract; columns. Model B, the recommendation system only requires user_news_clicks.csv but you can use the news_text.csv in addition if you’d like though it is not necessary for this exercise. The goal is to be able to recommend users news articles that they’re likely to click.\n",
      "\n",
      "### Data Description\n",
      "\n",
      "In news_text.csv - each record consists of three attributes and a target variable:\n",
      "- Category - There are lots of news categories available in this dataset, as requested we need to only 3 categories - news, sports and finance\n",
      "- news_id - Identification number of the news\n",
      "- title - Title of the news\n",
      "- abstract - Abstract of the news\n",
      "\n",
      "In user_news_clicks.csv - each record consists of two attributes and a target variable:\n",
      "- click - User has clicked the articles or not\n",
      "- user_id - Identification number of the user\n",
      "- item - Identification number of an item\n",
      "\n",
      "### Goals\n",
      "\n",
      "- Design the deep learning classifier and the recommendation system models\n",
      "- Build and train the models using a Python deep learning library such as Tensorflow or PyTorch\n",
      "- Test the model’s performance using a set of metrics\n",
      "- Report on the performance of the model\n",
      "\n",
      "### Instructions\n",
      "\n",
      "- Read about the dataset at\n",
      "https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md\n",
      "\n",
      "**NOTE**: We do not need to use the entire dataset, if resources are limited. Feel free to sample.\n",
      "- For Model A, use only the top 3 categories -- namely news, sports, and finance for model training and validation.\n",
      "- Code and build the models A and B using a Python library such as Pytorch or Tensorflow\n",
      "name 'detect' is not defined Language is not detected: us-news-dataset\n",
      "name 'detect' is not defined Language is not detected: vinayakshanawad\n",
      "name 'detect' is not defined Language is not detected: USA News Dataset\n",
      "name 'detect' is not defined Language is not detected: Information of news articles and click histories\n",
      "name 'detect' is not defined Language is not detected: ### Problem Description\n",
      "\n",
      "Construct two types of models -- (A) a deep learning classifier such as LSTM or similar model to predict the category of a news article given its title and abstract, and (B) A recommendation system to recommend posts that a user is most likely to click.\n",
      "\n",
      "The dataset consists of two files -- (1) user_news_clicks.csv, and (2) news_text.csv.\n",
      "\n",
      "Model A, the deep learning classifier only requires the news_text.csv dataset. The goal is to predict the ‘category’ label using the ‘title’ and ‘abstract; columns. Model B, the recommendation system only requires user_news_clicks.csv but you can use the news_text.csv in addition if you’d like though it is not necessary for this exercise. The goal is to be able to recommend users news articles that they’re likely to click.\n",
      "\n",
      "### Data Description\n",
      "\n",
      "In news_text.csv - each record consists of three attributes and a target variable:\n",
      "- Category - There are lots of news categories available in this dataset, as requested we need to only 3 categories - news, sports and finance\n",
      "- news_id - Identification number of the news\n",
      "- title - Title of the news\n",
      "- abstract - Abstract of the news\n",
      "\n",
      "In user_news_clicks.csv - each record consists of two attributes and a target variable:\n",
      "- click - User has clicked the articles or not\n",
      "- user_id - Identification number of the user\n",
      "- item - Identification number of an item\n",
      "\n",
      "### Goals\n",
      "\n",
      "- Design the deep learning classifier and the recommendation system models\n",
      "- Build and train the models using a Python deep learning library such as Tensorflow or PyTorch\n",
      "- Test the model’s performance using a set of metrics\n",
      "- Report on the performance of the model\n",
      "\n",
      "### Instructions\n",
      "\n",
      "- Read about the dataset at\n",
      "https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md\n",
      "\n",
      "**NOTE**: We do not need to use the entire dataset, if resources are limited. Feel free to sample.\n",
      "- For Model A, use only the top 3 categories -- namely news, sports, and finance for model training and validation.\n",
      "- Code and build the models A and B using a Python library such as Pytorch or Tensorflow\n",
      "name 'detect' is not defined Language is not detected: yakhyojon/customer-satisfaction-in-airline\n",
      "name 'detect' is not defined Language is not detected: customer-satisfaction-in-airline\n",
      "name 'detect' is not defined Language is not detected: yakhyojon\n",
      "name 'detect' is not defined Language is not detected: Customer Satisfaction in Airline\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction in Invistico Airline\n",
      "name 'detect' is not defined Language is not detected: The data is for a sample size of 129,880 customers. It includes data points such as class, flight distance, and inflight entertainment to be used to predict whether a customer will be satisfied with their flight experience.\n",
      "name 'detect' is not defined Language is not detected: customer-satisfaction-in-airline\n",
      "name 'detect' is not defined Language is not detected: yakhyojon\n",
      "name 'detect' is not defined Language is not detected: Customer Satisfaction in Airline\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction in Invistico Airline\n",
      "name 'detect' is not defined Language is not detected: The data is for a sample size of 129,880 customers. It includes data points such as class, flight distance, and inflight entertainment to be used to predict whether a customer will be satisfied with their flight experience.\n",
      "name 'detect' is not defined Language is not detected: ibrahimmukherjee/gdp-world-bank-data\n",
      "name 'detect' is not defined Language is not detected: gdp-world-bank-data\n",
      "name 'detect' is not defined Language is not detected: ibrahimmukherjee\n",
      "name 'detect' is not defined Language is not detected: GDP World Bank Data\n",
      "name 'detect' is not defined Language is not detected: Calculate the GDP from primary indicators of the country.\n",
      "name 'detect' is not defined Language is not detected: The World Bank is a treasure trove of information. :- https://data.worldbank.org/\n",
      "\n",
      "Generally the Gross Domestic Product of a country = the total output of the country = measure of development/total affluence of the country is measured by indicators such as household spending, government spending, level of investments etc.\n",
      "\n",
      "Please see Bank of England explanation of GDP here :- http://edu.bankofengland.co.uk/knowledgebank/what-is-gdp/\n",
      "\n",
      "I have argued that GDP could instead be measured better by primary indicators that lead to these what I call \"secondary indicators\".\n",
      "\n",
      "Primary indicators are such as :- level of education. I hypothesize that a higher level of education leads to higher household income and hence higher household spending. So does knowing education levels of a country allow us to predict the GDP of the country?\n",
      "\n",
      "I have used the list of primary indicators below to do a regression of the GDP per person :-\n",
      "(1) Women making informed choices regarding healthcare - The null hypotheses (H0)----&gt; is the higher the level of women's education - the higher the level of national education and lesser infant mortality rates(which might be a stretch) and hence higher household income --&gt; higher household spending ---&gt; higher GDP. \n",
      "(2) Rural Population % - The null hypotheses (H0) is -----&gt; higher rural population ----&gt; lower per capita household income----&gt; lower level of household spending----&gt; lower GDP.\n",
      "(3) Ratio of Population having education ----&gt; similar to above. You get the point hopefully by now... if not read a introductory macroeconomics textbook or course like this :- https://www.edx.org/course/introduction-economics-macroeconomics-snux-snu044-088-2x-0\n",
      "(4) Legal Rights Strength Index-----&gt; This actually comes from Islam. In Islam - the affluence of a country is related to truthfulness, rule of law being abided in the country etc.. For those who can understand Urdu/Hindi - please watch this video :- https://www.youtube.com/watch?v=XLjicUv0KYs\n",
      "(5) Credit to Private Sector -----&gt; easier it is to open a business, work on ideas-----&gt; higher should be the output of the country\n",
      "(6) Births attended by Skilled Staff ------&gt; less infant mortality ----&gt; indicates higher level of education and health care in the country ------&gt; can indicate higher government spending among other factors ------&gt;and should translate to higher level of GDP.\n",
      "(6) ATMMachines Ratio per 1000 people ---------&gt; Higher level -----&gt; shows finance is easily available -----&gt; institutions are developed -----&gt; maybe even indicates better public infrastructure-----&gt; should indicate higher personal and government funding.\n",
      "(7) Agricultural Machines per hectare of land ------&gt; higher automation -----&gt; better access to finance for rural areas ------&gt; should lead to higher GDP.\n",
      "(8) Literacy Rate Adults -----&gt; the higher level of education in adults ----&gt; higher private spending -----&gt; should lead to higher GDP.\n",
      "(9) Accounts Ratio Financial Institutions -----&gt; how many people have bank accounts who are male and over 15 ------&gt; shows level of private spending-----&gt; level of finance and infrastructure and hence government funding maybe -----&gt; higher GDP.\n",
      "name 'detect' is not defined Language is not detected: gdp-world-bank-data\n",
      "name 'detect' is not defined Language is not detected: ibrahimmukherjee\n",
      "name 'detect' is not defined Language is not detected: GDP World Bank Data\n",
      "name 'detect' is not defined Language is not detected: Calculate the GDP from primary indicators of the country.\n",
      "name 'detect' is not defined Language is not detected: The World Bank is a treasure trove of information. :- https://data.worldbank.org/\n",
      "\n",
      "Generally the Gross Domestic Product of a country = the total output of the country = measure of development/total affluence of the country is measured by indicators such as household spending, government spending, level of investments etc.\n",
      "\n",
      "Please see Bank of England explanation of GDP here :- http://edu.bankofengland.co.uk/knowledgebank/what-is-gdp/\n",
      "\n",
      "I have argued that GDP could instead be measured better by primary indicators that lead to these what I call \"secondary indicators\".\n",
      "\n",
      "Primary indicators are such as :- level of education. I hypothesize that a higher level of education leads to higher household income and hence higher household spending. So does knowing education levels of a country allow us to predict the GDP of the country?\n",
      "\n",
      "I have used the list of primary indicators below to do a regression of the GDP per person :-\n",
      "(1) Women making informed choices regarding healthcare - The null hypotheses (H0)----&gt; is the higher the level of women's education - the higher the level of national education and lesser infant mortality rates(which might be a stretch) and hence higher household income --&gt; higher household spending ---&gt; higher GDP. \n",
      "(2) Rural Population % - The null hypotheses (H0) is -----&gt; higher rural population ----&gt; lower per capita household income----&gt; lower level of household spending----&gt; lower GDP.\n",
      "(3) Ratio of Population having education ----&gt; similar to above. You get the point hopefully by now... if not read a introductory macroeconomics textbook or course like this :- https://www.edx.org/course/introduction-economics-macroeconomics-snux-snu044-088-2x-0\n",
      "(4) Legal Rights Strength Index-----&gt; This actually comes from Islam. In Islam - the affluence of a country is related to truthfulness, rule of law being abided in the country etc.. For those who can understand Urdu/Hindi - please watch this video :- https://www.youtube.com/watch?v=XLjicUv0KYs\n",
      "(5) Credit to Private Sector -----&gt; easier it is to open a business, work on ideas-----&gt; higher should be the output of the country\n",
      "(6) Births attended by Skilled Staff ------&gt; less infant mortality ----&gt; indicates higher level of education and health care in the country ------&gt; can indicate higher government spending among other factors ------&gt;and should translate to higher level of GDP.\n",
      "(6) ATMMachines Ratio per 1000 people ---------&gt; Higher level -----&gt; shows finance is easily available -----&gt; institutions are developed -----&gt; maybe even indicates better public infrastructure-----&gt; should indicate higher personal and government funding.\n",
      "(7) Agricultural Machines per hectare of land ------&gt; higher automation -----&gt; better access to finance for rural areas ------&gt; should lead to higher GDP.\n",
      "(8) Literacy Rate Adults -----&gt; the higher level of education in adults ----&gt; higher private spending -----&gt; should lead to higher GDP.\n",
      "(9) Accounts Ratio Financial Institutions -----&gt; how many people have bank accounts who are male and over 15 ------&gt; shows level of private spending-----&gt; level of finance and infrastructure and hence government funding maybe -----&gt; higher GDP.\n",
      "name 'detect' is not defined Language is not detected: sahilsaxenass/events-blog\n",
      "name 'detect' is not defined Language is not detected: events-blog\n",
      "name 'detect' is not defined Language is not detected: sahilsaxenass\n",
      "name 'detect' is not defined Language is not detected: Events blog\n",
      "name 'detect' is not defined Language is not detected: Big job related events to apply NLP, EDA (usage-recommendation system)\n",
      "name 'detect' is not defined Language is not detected: events-blog\n",
      "name 'detect' is not defined Language is not detected: sahilsaxenass\n",
      "name 'detect' is not defined Language is not detected: Events blog\n",
      "name 'detect' is not defined Language is not detected: Big job related events to apply NLP, EDA (usage-recommendation system)\n",
      "name 'detect' is not defined Language is not detected: chenqinzhang/torchhub-efficientnet\n",
      "name 'detect' is not defined Language is not detected: torchhub-efficientnet\n",
      "name 'detect' is not defined Language is not detected: chenqinzhang\n",
      "name 'detect' is not defined Language is not detected: torchhub_efficientnet\n",
      "name 'detect' is not defined Language is not detected: torchhub-efficientnet\n",
      "name 'detect' is not defined Language is not detected: chenqinzhang\n",
      "name 'detect' is not defined Language is not detected: torchhub_efficientnet\n",
      "name 'detect' is not defined Language is not detected: joyshil0599/movie-reviews-dataset-10k-scraped-data\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-dataset-10k-scraped-data\n",
      "name 'detect' is not defined Language is not detected: joyshil0599\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Dataset: 10k+ Scraped Data\n",
      "name 'detect' is not defined Language is not detected: Explore sentiments,ratings,and more with our comprehensive movie review dataset.\n",
      "name 'detect' is not defined Language is not detected: The dataset contains over 10,000 movie reviews that were scraped from two popular movie review websites, **Letterboxd** and **Metacritic**. \n",
      "\n",
      "**Letterboxd:** Letterboxd is a global social network for grass-roots film discussion and discovery. Use it as a diary to record and share your opinion about films as you watch them, or just to keep track of films you’ve seen in the past. Showcase your favorites on your profile page. Rate, review and tag films as you add them. Find and follow your friends to see what they’re enjoying. Keep a watchlist of films you’d like to see, and create lists/collections on any given topic. We’ve been described as “like GoodReads for movies”.\n",
      "We have apps for iOS, Android and Apple TV. Paying members get an ad-free experience, personalized all-time and annual stats pages, the option to select and filter by what’s available on their favorite streaming services, watchlist notifications, the ability to filter activity streams by entry type and to clone other members’ lists, and much more. Learn about Pro.Read more about the genesis of Letterboxd and follow us on Twitter, Facebook, Instagram, YouTube and TikTok.\n",
      "\n",
      "**Metacritic:** It began as a simple idea back in the summer of 1999: a single score could summarize the many entertainment reviews available for a movie or a video game. Metacritic's three founding members—all former attorneys who were happy to find a more constructive but less profitable use of their time—launched the site in January 2001 and Metacritic has evolved over the last decade to reflect their experience distilling many critics' voices into the single Metascore, a weighted average of the most respected critics writing reviews online and in print.\n",
      "Metacritic's mission is to help consumers make an informed decision about how to spend their time and money on entertainment. We believe that multiple opinions are better than one, user voices can be as important as critics, and opinions must be scored to be easy to use.\n",
      "Our Metascore system is unique and merits its own explanation page.\n",
      "\n",
      "Each review includes various data points such as the **movie title**, **release year**,  **review text** , **rating** , and **sentiment** . The data can be used for sentiment analysis, natural language processing, and machine learning applications such as text classification, recommendation systems, and more.\n",
      "\n",
      "Since the dataset includes reviews from two different websites, it offers a diverse range of opinions and perspectives on a wide variety of movies. Additionally, the large size of the dataset makes it a valuable resource for researchers, analysts, and data scientists who are interested in studying movie reviews and developing new insights into audience preferences, trends, and behaviors.\n",
      "\n",
      "Overall, this dataset offers a rich source of information for anyone who wants to analyze or explore movie reviews in depth.\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-dataset-10k-scraped-data\n",
      "name 'detect' is not defined Language is not detected: joyshil0599\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Dataset: 10k+ Scraped Data\n",
      "name 'detect' is not defined Language is not detected: Explore sentiments,ratings,and more with our comprehensive movie review dataset.\n",
      "name 'detect' is not defined Language is not detected: The dataset contains over 10,000 movie reviews that were scraped from two popular movie review websites, **Letterboxd** and **Metacritic**. \n",
      "\n",
      "**Letterboxd:** Letterboxd is a global social network for grass-roots film discussion and discovery. Use it as a diary to record and share your opinion about films as you watch them, or just to keep track of films you’ve seen in the past. Showcase your favorites on your profile page. Rate, review and tag films as you add them. Find and follow your friends to see what they’re enjoying. Keep a watchlist of films you’d like to see, and create lists/collections on any given topic. We’ve been described as “like GoodReads for movies”.\n",
      "We have apps for iOS, Android and Apple TV. Paying members get an ad-free experience, personalized all-time and annual stats pages, the option to select and filter by what’s available on their favorite streaming services, watchlist notifications, the ability to filter activity streams by entry type and to clone other members’ lists, and much more. Learn about Pro.Read more about the genesis of Letterboxd and follow us on Twitter, Facebook, Instagram, YouTube and TikTok.\n",
      "\n",
      "**Metacritic:** It began as a simple idea back in the summer of 1999: a single score could summarize the many entertainment reviews available for a movie or a video game. Metacritic's three founding members—all former attorneys who were happy to find a more constructive but less profitable use of their time—launched the site in January 2001 and Metacritic has evolved over the last decade to reflect their experience distilling many critics' voices into the single Metascore, a weighted average of the most respected critics writing reviews online and in print.\n",
      "Metacritic's mission is to help consumers make an informed decision about how to spend their time and money on entertainment. We believe that multiple opinions are better than one, user voices can be as important as critics, and opinions must be scored to be easy to use.\n",
      "Our Metascore system is unique and merits its own explanation page.\n",
      "\n",
      "Each review includes various data points such as the **movie title**, **release year**,  **review text** , **rating** , and **sentiment** . The data can be used for sentiment analysis, natural language processing, and machine learning applications such as text classification, recommendation systems, and more.\n",
      "\n",
      "Since the dataset includes reviews from two different websites, it offers a diverse range of opinions and perspectives on a wide variety of movies. Additionally, the large size of the dataset makes it a valuable resource for researchers, analysts, and data scientists who are interested in studying movie reviews and developing new insights into audience preferences, trends, and behaviors.\n",
      "\n",
      "Overall, this dataset offers a rich source of information for anyone who wants to analyze or explore movie reviews in depth.\n",
      "name 'detect' is not defined Language is not detected: sanjushasuresh/list-of-songs-for-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: list-of-songs-for-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sanjushasuresh\n",
      "name 'detect' is not defined Language is not detected: List of songs for recommendation system\n",
      "name 'detect' is not defined Language is not detected: list-of-songs-for-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sanjushasuresh\n",
      "name 'detect' is not defined Language is not detected: List of songs for recommendation system\n",
      "name 'detect' is not defined Language is not detected: jonathandumas/liege-microgrid-open-data\n",
      "name 'detect' is not defined Language is not detected: liege-microgrid-open-data\n",
      "name 'detect' is not defined Language is not detected: jonathandumas\n",
      "name 'detect' is not defined Language is not detected: Liege Microgrid Open Data\n",
      "name 'detect' is not defined Language is not detected: Consumption, PV monitored and forecast dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This open dataset is used in the scientific study \"Coordination of operational planning and real-time optimization in microgrids\" currently in the submission process of the Power Systems Computation Conference (PSCC) 2020.\n",
      "\n",
      "The paper abstract is:\n",
      "\n",
      "*Hierarchical microgrid control levels range from distributed device level controllers that run at a high frequency to centralized controllers optimizing market integration that run much less frequently. Centralized controllers are often subdivided in operational planning controllers that optimize decisions over a time horizon of one or several days, and real-time optimization controllers that deal with actions in the current market period. The coordination of these levels is of paramount importance. In this paper we propose a value function based approach as a way to propagate information from operational planning to real-time optimization. We apply this method to an environment where operational planning, using day-ahead forecasts, optimizes at a market period resolution the decisions to minimize the total energy cost and revenues, the peak consumption and injection related costs, and plans for reserve requirements, while real-time optimization copes with the forecast errors and yields implementable actions based on real-time measurements. The approach is compared to a rule-based controller on three use cases, and its sensitivity to forecast error is assessed.*\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset is composed:\n",
      "\n",
      "- 15 minutes resolution weather forecast (solar irradiation, air temperature, etc) produced by the Laboratory of Climatology of the university of Liège, based on the MAR regional climate model;\n",
      "- 5 seconds resolution monitored PV and consumption of the MiRIS microgrid;\n",
      "- 15 minutes resolution PV and consumption weather based forecasts produced by our forecaster and used in the study.\n",
      "\n",
      "The weather based forecasts are multi outputs with an horizon of 24 hours ahead and a resolution of 15 minutes. They are quarterly produced on rolling basis with a learning set of one week. Every six hours the model is refreshed and the learning set is moved consequently.\n",
      "This means, each quarter a PV and consumption forecast is produced, composed of 96 values (one per quarter of the 24 hours ahead). \n",
      "\n",
      "More information about the MiRIS microgrid located at the John Cockerill Group’s international headquarters in Seraing, Belgium, is available at https://johncockerill.com/fr/energy/stockage-denergie/.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank John Cockerill and Nethys for their financial support, and Xavier Fettweis of the Laboratory of Climatology of ULiège who produced the weather forecasts based on the MAR regional climate model.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "You can freely use the data to reproduce the numerical results of the study or to produce better weather based forecasts.\n",
      "\n",
      "### Weather based forecast description\n",
      "\n",
      "Two \"classic\" deterministic techniques are implemented, a Recurrent Neural Network(RNN) with the keras python library and a Gradient Boosting Regression (GBR) with the scikit-learn python library.\n",
      "\n",
      "The RNN is a LSTM with one hidden layer, 5000 epochs, RELU as activation function (hidden layer and output), a batch size of 200 and drop out rate of 0.4.\n",
      "\n",
      "The GBR is the multi output GBR of sklearn with 200 estimator and 20 as max depth.\n",
      "\n",
      "### Weather forecast description\n",
      "\n",
      "The weather forecast is made at the MiRIS microgrid loation.\n",
      "\n",
      "The file is composed of forecast of several weather variables:\n",
      "- CD = low clouds (0 to 1)\n",
      "- CM = medium clouds (0 to 1)\n",
      "- CU = high clouds (0 to 1)\n",
      "- PREC = precipitation (mm / 15 min)\n",
      "- RH2m = relative humidity (%)\n",
      "- SNOW = snow height (mm)\n",
      "- ST = Surface Temperature (°C)\n",
      "- SWD = Global Horizontal Irradiance (W/m2)\n",
      "- SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
      "- TT2M = temperature 2 meters above the ground (°C)\n",
      "- WS100m = Wind speed at 100m from the ground (m/s)\n",
      "- WS10m =Wind speed at 10m from the ground (m/s)\n",
      "name 'detect' is not defined Language is not detected: liege-microgrid-open-data\n",
      "name 'detect' is not defined Language is not detected: jonathandumas\n",
      "name 'detect' is not defined Language is not detected: Liege Microgrid Open Data\n",
      "name 'detect' is not defined Language is not detected: Consumption, PV monitored and forecast dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This open dataset is used in the scientific study \"Coordination of operational planning and real-time optimization in microgrids\" currently in the submission process of the Power Systems Computation Conference (PSCC) 2020.\n",
      "\n",
      "The paper abstract is:\n",
      "\n",
      "*Hierarchical microgrid control levels range from distributed device level controllers that run at a high frequency to centralized controllers optimizing market integration that run much less frequently. Centralized controllers are often subdivided in operational planning controllers that optimize decisions over a time horizon of one or several days, and real-time optimization controllers that deal with actions in the current market period. The coordination of these levels is of paramount importance. In this paper we propose a value function based approach as a way to propagate information from operational planning to real-time optimization. We apply this method to an environment where operational planning, using day-ahead forecasts, optimizes at a market period resolution the decisions to minimize the total energy cost and revenues, the peak consumption and injection related costs, and plans for reserve requirements, while real-time optimization copes with the forecast errors and yields implementable actions based on real-time measurements. The approach is compared to a rule-based controller on three use cases, and its sensitivity to forecast error is assessed.*\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset is composed:\n",
      "\n",
      "- 15 minutes resolution weather forecast (solar irradiation, air temperature, etc) produced by the Laboratory of Climatology of the university of Liège, based on the MAR regional climate model;\n",
      "- 5 seconds resolution monitored PV and consumption of the MiRIS microgrid;\n",
      "- 15 minutes resolution PV and consumption weather based forecasts produced by our forecaster and used in the study.\n",
      "\n",
      "The weather based forecasts are multi outputs with an horizon of 24 hours ahead and a resolution of 15 minutes. They are quarterly produced on rolling basis with a learning set of one week. Every six hours the model is refreshed and the learning set is moved consequently.\n",
      "This means, each quarter a PV and consumption forecast is produced, composed of 96 values (one per quarter of the 24 hours ahead). \n",
      "\n",
      "More information about the MiRIS microgrid located at the John Cockerill Group’s international headquarters in Seraing, Belgium, is available at https://johncockerill.com/fr/energy/stockage-denergie/.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank John Cockerill and Nethys for their financial support, and Xavier Fettweis of the Laboratory of Climatology of ULiège who produced the weather forecasts based on the MAR regional climate model.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "You can freely use the data to reproduce the numerical results of the study or to produce better weather based forecasts.\n",
      "\n",
      "### Weather based forecast description\n",
      "\n",
      "Two \"classic\" deterministic techniques are implemented, a Recurrent Neural Network(RNN) with the keras python library and a Gradient Boosting Regression (GBR) with the scikit-learn python library.\n",
      "\n",
      "The RNN is a LSTM with one hidden layer, 5000 epochs, RELU as activation function (hidden layer and output), a batch size of 200 and drop out rate of 0.4.\n",
      "\n",
      "The GBR is the multi output GBR of sklearn with 200 estimator and 20 as max depth.\n",
      "\n",
      "### Weather forecast description\n",
      "\n",
      "The weather forecast is made at the MiRIS microgrid loation.\n",
      "\n",
      "The file is composed of forecast of several weather variables:\n",
      "- CD = low clouds (0 to 1)\n",
      "- CM = medium clouds (0 to 1)\n",
      "- CU = high clouds (0 to 1)\n",
      "- PREC = precipitation (mm / 15 min)\n",
      "- RH2m = relative humidity (%)\n",
      "- SNOW = snow height (mm)\n",
      "- ST = Surface Temperature (°C)\n",
      "- SWD = Global Horizontal Irradiance (W/m2)\n",
      "- SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
      "- TT2M = temperature 2 meters above the ground (°C)\n",
      "- WS100m = Wind speed at 100m from the ground (m/s)\n",
      "- WS10m =Wind speed at 10m from the ground (m/s)\n",
      "name 'detect' is not defined Language is not detected: paramvir705/netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: paramvir705\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset\n",
      "name 'detect' is not defined Language is not detected: Discover the pulse of Netflix's cinematic offerings with this comprehensive data\n",
      "name 'detect' is not defined Language is not detected: Dive into the vast and diverse world of Netflix entertainment with this extensive dataset. From blockbuster movies to binge-worthy TV shows, this collection offers insights into the rich content library available on the popular streaming platform. Gain access to detailed information about each title, including genre, release year, cast, director, and synopsis. Whether you're a data enthusiast, a movie buff, or a TV aficionado, this dataset provides valuable resources for analysis, recommendation systems, and exploring trends in the ever-evolving landscape of digital entertainment. Uncover hidden gems, track audience preferences, and embark on a journey through the dynamic realm of Netflix originals and licensed content.\n",
      "name 'detect' is not defined Language is not detected: netflix-dataset\n",
      "name 'detect' is not defined Language is not detected: paramvir705\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset\n",
      "name 'detect' is not defined Language is not detected: Discover the pulse of Netflix's cinematic offerings with this comprehensive data\n",
      "name 'detect' is not defined Language is not detected: Dive into the vast and diverse world of Netflix entertainment with this extensive dataset. From blockbuster movies to binge-worthy TV shows, this collection offers insights into the rich content library available on the popular streaming platform. Gain access to detailed information about each title, including genre, release year, cast, director, and synopsis. Whether you're a data enthusiast, a movie buff, or a TV aficionado, this dataset provides valuable resources for analysis, recommendation systems, and exploring trends in the ever-evolving landscape of digital entertainment. Uncover hidden gems, track audience preferences, and embark on a journey through the dynamic realm of Netflix originals and licensed content.\n",
      "name 'detect' is not defined Language is not detected: naidukarthi2193/geeks-for-geeks-articles-dataset\n",
      "name 'detect' is not defined Language is not detected: geeks-for-geeks-articles-dataset\n",
      "name 'detect' is not defined Language is not detected: naidukarthi2193\n",
      "name 'detect' is not defined Language is not detected: Geeks for Geeks Articles Dataset\n",
      "name 'detect' is not defined Language is not detected: A Web scraped CSV of 24500 articles from Geeks for Geeks website  \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This Data was created using Beautiful Soup Package.\n",
      "I hope this helps in Creating recommendation Systems for Related Article using NLP systems. \n",
      "\n",
      "name 'detect' is not defined Language is not detected: geeks-for-geeks-articles-dataset\n",
      "name 'detect' is not defined Language is not detected: naidukarthi2193\n",
      "name 'detect' is not defined Language is not detected: Geeks for Geeks Articles Dataset\n",
      "name 'detect' is not defined Language is not detected: A Web scraped CSV of 24500 articles from Geeks for Geeks website  \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This Data was created using Beautiful Soup Package.\n",
      "I hope this helps in Creating recommendation Systems for Related Article using NLP systems. \n",
      "\n",
      "name 'detect' is not defined Language is not detected: samuelshanthanaraja/recommendation-movie-rating\n",
      "name 'detect' is not defined Language is not detected: recommendation-movie-rating\n",
      "name 'detect' is not defined Language is not detected: samuelshanthanaraja\n",
      "name 'detect' is not defined Language is not detected: Recommendation_movie rating\n",
      "name 'detect' is not defined Language is not detected: recommendation-movie-rating\n",
      "name 'detect' is not defined Language is not detected: samuelshanthanaraja\n",
      "name 'detect' is not defined Language is not detected: Recommendation_movie rating\n",
      "name 'detect' is not defined Language is not detected: muniryadi/gasturbine-co-and-nox-emission-data\n",
      "name 'detect' is not defined Language is not detected: gasturbine-co-and-nox-emission-data\n",
      "name 'detect' is not defined Language is not detected: muniryadi\n",
      "name 'detect' is not defined Language is not detected: Gas-Turbine CO and NOx Emission Data\n",
      "name 'detect' is not defined Language is not detected: Industrial gas turbines for power generation \n",
      "name 'detect' is not defined Language is not detected: A powerplant  engine( gas-turbine) is mainly used to generate electricity. Since the engine can different types of fuels, the engine can have different levels of CO2 and NO emission gases. \n",
      "This dataset is generated from a gas turbine in Turkey. More info:\n",
      "https://journals.tubitak.gov.tr/elektrik/issues/elk-19-27-6/elk-27-6-54-1807-87.pdf\n",
      "\n",
      "Column name descriptions:\n",
      "Variable (Abbr.) Unit Min Max Mean\n",
      "Ambient temperature (AT) C â€“6.23 37.10 17.71\n",
      "Ambient pressure (AP) mbar 985.85 1036.56 1013.07\n",
      "Ambient humidity (AH) (%) 24.08 100.20 77.87\n",
      "Air filter difference pressure (AFDP) mbar 2.09 7.61 3.93\n",
      "Gas turbine exhaust pressure (GTEP) mbar 17.70 40.72 25.56\n",
      "Turbine inlet temperature (TIT) C 1000.85 1100.89 1081.43\n",
      "Turbine after temperature (TAT) C 511.04 550.61 546.16\n",
      "Compressor discharge pressure (CDP) mbar 9.85 15.16 12.06\n",
      "Turbine energy yield (TEY) MWH 100.02 179.50 133.51\n",
      "Carbon monoxide (CO) mg/m3 0.00 44.10 2.37\n",
      "Nitrogen oxides (NOx) mg/m3 25.90 119.91 65.29\n",
      "What you can do with this dataset:\n",
      "\n",
      "You can create a regression model to generate the level of Carbon monoxide (CO) and Nitrogen oxides (NOx). \n",
      "You can also find out which features are most correlated to CO2 and NO. \n",
      "name 'detect' is not defined Language is not detected: gasturbine-co-and-nox-emission-data\n",
      "name 'detect' is not defined Language is not detected: muniryadi\n",
      "name 'detect' is not defined Language is not detected: Gas-Turbine CO and NOx Emission Data\n",
      "name 'detect' is not defined Language is not detected: Industrial gas turbines for power generation \n",
      "name 'detect' is not defined Language is not detected: A powerplant  engine( gas-turbine) is mainly used to generate electricity. Since the engine can different types of fuels, the engine can have different levels of CO2 and NO emission gases. \n",
      "This dataset is generated from a gas turbine in Turkey. More info:\n",
      "https://journals.tubitak.gov.tr/elektrik/issues/elk-19-27-6/elk-27-6-54-1807-87.pdf\n",
      "\n",
      "Column name descriptions:\n",
      "Variable (Abbr.) Unit Min Max Mean\n",
      "Ambient temperature (AT) C â€“6.23 37.10 17.71\n",
      "Ambient pressure (AP) mbar 985.85 1036.56 1013.07\n",
      "Ambient humidity (AH) (%) 24.08 100.20 77.87\n",
      "Air filter difference pressure (AFDP) mbar 2.09 7.61 3.93\n",
      "Gas turbine exhaust pressure (GTEP) mbar 17.70 40.72 25.56\n",
      "Turbine inlet temperature (TIT) C 1000.85 1100.89 1081.43\n",
      "Turbine after temperature (TAT) C 511.04 550.61 546.16\n",
      "Compressor discharge pressure (CDP) mbar 9.85 15.16 12.06\n",
      "Turbine energy yield (TEY) MWH 100.02 179.50 133.51\n",
      "Carbon monoxide (CO) mg/m3 0.00 44.10 2.37\n",
      "Nitrogen oxides (NOx) mg/m3 25.90 119.91 65.29\n",
      "What you can do with this dataset:\n",
      "\n",
      "You can create a regression model to generate the level of Carbon monoxide (CO) and Nitrogen oxides (NOx). \n",
      "You can also find out which features are most correlated to CO2 and NO. \n",
      "name 'detect' is not defined Language is not detected: mubashirrahim/wind-power-generation-data-forecasting\n",
      "name 'detect' is not defined Language is not detected: wind-power-generation-data-forecasting\n",
      "name 'detect' is not defined Language is not detected: mubashirrahim\n",
      "name 'detect' is not defined Language is not detected: Wind Power Generation Data - Forecasting\n",
      "name 'detect' is not defined Language is not detected: Wind Energy Dataset from 4 different locations\n",
      "name 'detect' is not defined Language is not detected: This dataset is a unique compilation of field-based meteorological observations and wind power generation data, collected directly from one of our company's operational sites. The dataset represents a detailed hourly record, starting from January 2, 2017. This rich dataset provides real-world insights into the interplay between various weather conditions and wind energy production.\n",
      "\n",
      "**Context and Inspiration:**\n",
      "The dataset was conceived out of the necessity to understand the dynamic relationship between meteorological variables and their impact on wind power generation. By collecting data directly from the field and the wind turbine installations, we aim to provide a comprehensive and authentic dataset that can be instrumental for industry-specific research, operational optimization, and academic purposes.\n",
      "\n",
      "**Data Collection:**\n",
      "Data was meticulously gathered using state-of-the-art equipment installed at the site. The meteorological instruments measured temperature, humidity, dew point, and wind characteristics at different heights, while power generation data was recorded from the wind turbines' output.\n",
      "This dataset is a unique compilation of field-based meteorological observations and wind power generation data, collected directly from one of our company's operational sites. The dataset represents a detailed hourly record, starting from January 2, 2017. This rich dataset provides real-world insights into the interplay between various weather conditions and wind energy production.\n",
      "\n",
      "**Potential Uses:**\n",
      "This dataset is ideal for industry experts, researchers, and data scientists exploring renewable energy, especially wind power. It can aid in developing predictive models for power generation, studying environmental impacts on renewable energy sources, and enhancing operational efficiency in wind farms.\n",
      "name 'detect' is not defined Language is not detected: wind-power-generation-data-forecasting\n",
      "name 'detect' is not defined Language is not detected: mubashirrahim\n",
      "name 'detect' is not defined Language is not detected: Wind Power Generation Data - Forecasting\n",
      "name 'detect' is not defined Language is not detected: Wind Energy Dataset from 4 different locations\n",
      "name 'detect' is not defined Language is not detected: This dataset is a unique compilation of field-based meteorological observations and wind power generation data, collected directly from one of our company's operational sites. The dataset represents a detailed hourly record, starting from January 2, 2017. This rich dataset provides real-world insights into the interplay between various weather conditions and wind energy production.\n",
      "\n",
      "**Context and Inspiration:**\n",
      "The dataset was conceived out of the necessity to understand the dynamic relationship between meteorological variables and their impact on wind power generation. By collecting data directly from the field and the wind turbine installations, we aim to provide a comprehensive and authentic dataset that can be instrumental for industry-specific research, operational optimization, and academic purposes.\n",
      "\n",
      "**Data Collection:**\n",
      "Data was meticulously gathered using state-of-the-art equipment installed at the site. The meteorological instruments measured temperature, humidity, dew point, and wind characteristics at different heights, while power generation data was recorded from the wind turbines' output.\n",
      "This dataset is a unique compilation of field-based meteorological observations and wind power generation data, collected directly from one of our company's operational sites. The dataset represents a detailed hourly record, starting from January 2, 2017. This rich dataset provides real-world insights into the interplay between various weather conditions and wind energy production.\n",
      "\n",
      "**Potential Uses:**\n",
      "This dataset is ideal for industry experts, researchers, and data scientists exploring renewable energy, especially wind power. It can aid in developing predictive models for power generation, studying environmental impacts on renewable energy sources, and enhancing operational efficiency in wind farms.\n",
      "name 'detect' is not defined Language is not detected: rahulvyasm/netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: rahulvyasm\n",
      "name 'detect' is not defined Language is not detected: Netflix Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: Exploring the Depths of Netflix: A Comprehensive Dataset of Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: Netflix stands as a leading force in the realm of media and video streaming. With a staggering array of over 8,000 movies and TV shows accessible on their platform, as of mid-2021, their global subscriber count exceeds 200 million. This tabulated dataset comprehensively catalogues all offerings on Netflix, including vital details such as cast, directors, ratings, release year, duration, and more.\n",
      "\n",
      "# Dataset Overview:\n",
      "The Netflix Titles dataset is a comprehensive compilation of movies and TV shows available on Netflix, covering various aspects such as the title type, director, cast, country of production, release year, rating, duration, genres (listed in), and a brief description. This dataset is instrumental for analyzing trends in Netflix content, understanding genre popularity, and examining the distribution of content across different regions and time periods.\n",
      "\n",
      "## Key Details:\n",
      "- **Total Entries:** The dataset contains 8,809 entries, each representing a unique movie or TV show.\n",
      "- **Columns:** There are 12 columns in the dataset:\n",
      "  1. **show_id:** A unique identifier for each title.\n",
      "  2. **type:** The category of the title, which is either 'Movie' or 'TV Show'.\n",
      "  3. **title:** The name of the movie or TV show.\n",
      "  4. **director:** The director(s) of the movie or TV show. (Contains null values for some entries, especially TV shows where this information might not be applicable.)\n",
      "  5. **cast:** The list of main actors/actresses in the title. (Some entries might not have this information.)\n",
      "  6. **country:** The country or countries where the movie or TV show was produced.\n",
      "  7. **date_added:** The date the title was added to Netflix.\n",
      "  8. **release_year:** The year the movie or TV show was originally released.\n",
      "  9. **rating:** The age rating of the title.\n",
      "  10. **duration:** The duration of the title, in minutes for movies and seasons for TV shows.\n",
      "  11. **listed_in:** The genres the title falls under.\n",
      "  12. **description:** A brief summary of the title.\n",
      "\n",
      "## Potential Use Cases:\n",
      "- **Content Analysis:** This dataset can be used to perform detailed content analysis, such as genre popularity over time, distribution of content production across different countries, and trends in movie versus TV show production.\n",
      "- **Recommendation Systems:** For developers and data scientists working on recommendation systems, this dataset provides a rich source of metadata for content similarity and user preference modeling.\n",
      "- **Market Analysis:** Market researchers can utilize this dataset to analyze Netflix's content strategy, including their focus on international markets, genre diversification, and investment in original content.\n",
      "\n",
      "Whether you are a data enthusiast, a content creator, or a market analyst, the Netflix Titles dataset offers valuable insights into the evolving landscape of digital content. Explore this dataset to uncover trends, patterns, and opportunities in the world of streaming entertainment.\n",
      "\n",
      "If you find the dataset intriguing, please consider upvoting. Thank you.\n",
      "name 'detect' is not defined Language is not detected: netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: rahulvyasm\n",
      "name 'detect' is not defined Language is not detected: Netflix Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: Exploring the Depths of Netflix: A Comprehensive Dataset of Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: Netflix stands as a leading force in the realm of media and video streaming. With a staggering array of over 8,000 movies and TV shows accessible on their platform, as of mid-2021, their global subscriber count exceeds 200 million. This tabulated dataset comprehensively catalogues all offerings on Netflix, including vital details such as cast, directors, ratings, release year, duration, and more.\n",
      "\n",
      "# Dataset Overview:\n",
      "The Netflix Titles dataset is a comprehensive compilation of movies and TV shows available on Netflix, covering various aspects such as the title type, director, cast, country of production, release year, rating, duration, genres (listed in), and a brief description. This dataset is instrumental for analyzing trends in Netflix content, understanding genre popularity, and examining the distribution of content across different regions and time periods.\n",
      "\n",
      "## Key Details:\n",
      "- **Total Entries:** The dataset contains 8,809 entries, each representing a unique movie or TV show.\n",
      "- **Columns:** There are 12 columns in the dataset:\n",
      "  1. **show_id:** A unique identifier for each title.\n",
      "  2. **type:** The category of the title, which is either 'Movie' or 'TV Show'.\n",
      "  3. **title:** The name of the movie or TV show.\n",
      "  4. **director:** The director(s) of the movie or TV show. (Contains null values for some entries, especially TV shows where this information might not be applicable.)\n",
      "  5. **cast:** The list of main actors/actresses in the title. (Some entries might not have this information.)\n",
      "  6. **country:** The country or countries where the movie or TV show was produced.\n",
      "  7. **date_added:** The date the title was added to Netflix.\n",
      "  8. **release_year:** The year the movie or TV show was originally released.\n",
      "  9. **rating:** The age rating of the title.\n",
      "  10. **duration:** The duration of the title, in minutes for movies and seasons for TV shows.\n",
      "  11. **listed_in:** The genres the title falls under.\n",
      "  12. **description:** A brief summary of the title.\n",
      "\n",
      "## Potential Use Cases:\n",
      "- **Content Analysis:** This dataset can be used to perform detailed content analysis, such as genre popularity over time, distribution of content production across different countries, and trends in movie versus TV show production.\n",
      "- **Recommendation Systems:** For developers and data scientists working on recommendation systems, this dataset provides a rich source of metadata for content similarity and user preference modeling.\n",
      "- **Market Analysis:** Market researchers can utilize this dataset to analyze Netflix's content strategy, including their focus on international markets, genre diversification, and investment in original content.\n",
      "\n",
      "Whether you are a data enthusiast, a content creator, or a market analyst, the Netflix Titles dataset offers valuable insights into the evolving landscape of digital content. Explore this dataset to uncover trends, patterns, and opportunities in the world of streaming entertainment.\n",
      "\n",
      "If you find the dataset intriguing, please consider upvoting. Thank you.\n",
      "name 'detect' is not defined Language is not detected: trnduythanhkhttt/uelstoredataset\n",
      "name 'detect' is not defined Language is not detected: uelstoredataset\n",
      "name 'detect' is not defined Language is not detected: trnduythanhkhttt\n",
      "name 'detect' is not defined Language is not detected: Ecommerce Rating Dataset\n",
      "name 'detect' is not defined Language is not detected: Ecommerce Rating Dataset is collected from http://ecom.uelstore.com/ \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset is from ecommerce website http://ecom.uelstore.com/\n",
      "And community can use this dataset to build the recommendation system.\n",
      "\n",
      "Volunteers are mainly students of the University of Economics - Law, Vietnam National University, Ho Chi Minh City, using their accounts to register and conduct ratings.\n",
      "\n",
      "### Content\n",
      "We provide 3 file datasets with JSon format\n",
      "-678 users (id, nickname)\n",
      "-732 products of all kinds (id, name)\n",
      "-130754 product - reviews (rating, date)\n",
      "\n",
      "### Source code Python for E-Commerce recommendation:\n",
      "\n",
      "https://www.kaggle.com/code/trnduythanhkhttt/e-commerce-recommendation\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This Research from KMOU (Korea Maritime & Ocean University) - Data Science Lab - Room 407.\n",
      "\n",
      "Authors: Duy Thanh Tran, Prof. Jun-Ho Huh\n",
      "\n",
      "Any question, please free to contact me: thanhtd@uel.edu.vn\n",
      "\n",
      "My full name: TRAN DUY THANH\n",
      "\n",
      "Blog study coding: https://duythanhcse.wordpress.com/\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This is use to build recommendation\n",
      "name 'detect' is not defined Language is not detected: uelstoredataset\n",
      "name 'detect' is not defined Language is not detected: trnduythanhkhttt\n",
      "name 'detect' is not defined Language is not detected: Ecommerce Rating Dataset\n",
      "name 'detect' is not defined Language is not detected: Ecommerce Rating Dataset is collected from http://ecom.uelstore.com/ \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset is from ecommerce website http://ecom.uelstore.com/\n",
      "And community can use this dataset to build the recommendation system.\n",
      "\n",
      "Volunteers are mainly students of the University of Economics - Law, Vietnam National University, Ho Chi Minh City, using their accounts to register and conduct ratings.\n",
      "\n",
      "### Content\n",
      "We provide 3 file datasets with JSon format\n",
      "-678 users (id, nickname)\n",
      "-732 products of all kinds (id, name)\n",
      "-130754 product - reviews (rating, date)\n",
      "\n",
      "### Source code Python for E-Commerce recommendation:\n",
      "\n",
      "https://www.kaggle.com/code/trnduythanhkhttt/e-commerce-recommendation\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This Research from KMOU (Korea Maritime & Ocean University) - Data Science Lab - Room 407.\n",
      "\n",
      "Authors: Duy Thanh Tran, Prof. Jun-Ho Huh\n",
      "\n",
      "Any question, please free to contact me: thanhtd@uel.edu.vn\n",
      "\n",
      "My full name: TRAN DUY THANH\n",
      "\n",
      "Blog study coding: https://duythanhcse.wordpress.com/\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This is use to build recommendation\n",
      "name 'detect' is not defined Language is not detected: vjchoudhary7/hr-analytics-case-study\n",
      "name 'detect' is not defined Language is not detected: hr-analytics-case-study\n",
      "name 'detect' is not defined Language is not detected: vjchoudhary7\n",
      "name 'detect' is not defined Language is not detected: HR Analytics Case Study\n",
      "name 'detect' is not defined Language is not detected: Employee Attrition Analysis (Logistic Regression Model)\n",
      "name 'detect' is not defined Language is not detected: **Problem Statement**\n",
      "A large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n",
      "\n",
      " 1. The former employees’ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n",
      " 2. A sizeable department has to be maintained, for the purposes of recruiting new talent\n",
      " 3. More often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n",
      "\n",
      "Hence, the management has contracted an HR analytics firm to understand what factors they should focus on, in order to curb attrition. In other words, they want to know what changes they should make to their workplace, in order to get most of their employees to stay. Also, they want to know which of these variables is most important and needs to be addressed right away.\n",
      "\n",
      "Since you are one of the star analysts at the firm, this project has been given to you.\n",
      "\n",
      "\n",
      "\n",
      "**Goal of the case study**\n",
      "You are required to model the probability of attrition using a logistic regression. The results thus obtained will be used by the management to understand what changes they should make to their workplace, in order to get most of their employees to stay.\n",
      "name 'detect' is not defined Language is not detected: hr-analytics-case-study\n",
      "name 'detect' is not defined Language is not detected: vjchoudhary7\n",
      "name 'detect' is not defined Language is not detected: HR Analytics Case Study\n",
      "name 'detect' is not defined Language is not detected: Employee Attrition Analysis (Logistic Regression Model)\n",
      "name 'detect' is not defined Language is not detected: **Problem Statement**\n",
      "A large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n",
      "\n",
      " 1. The former employees’ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n",
      " 2. A sizeable department has to be maintained, for the purposes of recruiting new talent\n",
      " 3. More often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n",
      "\n",
      "Hence, the management has contracted an HR analytics firm to understand what factors they should focus on, in order to curb attrition. In other words, they want to know what changes they should make to their workplace, in order to get most of their employees to stay. Also, they want to know which of these variables is most important and needs to be addressed right away.\n",
      "\n",
      "Since you are one of the star analysts at the firm, this project has been given to you.\n",
      "\n",
      "\n",
      "\n",
      "**Goal of the case study**\n",
      "You are required to model the probability of attrition using a logistic regression. The results thus obtained will be used by the management to understand what changes they should make to their workplace, in order to get most of their employees to stay.\n",
      "name 'detect' is not defined Language is not detected: mohamedhamad21/spotify-tracks-dataset\n",
      "name 'detect' is not defined Language is not detected: spotify-tracks-dataset\n",
      "name 'detect' is not defined Language is not detected: mohamedhamad21\n",
      "name 'detect' is not defined Language is not detected: Spotify Tracks Dataset\n",
      "name 'detect' is not defined Language is not detected: This is a dataset of Spotify tracks over a range of 125 different genres. Each t\n",
      "name 'detect' is not defined Language is not detected: # **Content**\n",
      "This is a dataset of Spotify tracks over a range of 125 different genres. Each track has some audio features associated with it. The data is in CSV format which is tabular and can be loaded quickly.\n",
      "\n",
      "\n",
      "# **Usage**\n",
      "## The dataset can be used for:\n",
      "* Building a Recommendation System based on some user input or preference.\n",
      "\n",
      "* Classification purposes based on audio features and available genres.\n",
      "\n",
      "* Any other application that you can think of. \n",
      "\n",
      "\n",
      "# **Column Descriptors**\n",
      "track_id: The Spotify ID for the track\n",
      "\n",
      "artists: The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
      "\n",
      "album_name: The album name in which the track appears\n",
      "\n",
      "popularity: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
      "\n",
      "duration_ms: The track length in milliseconds.\n",
      "\n",
      "explicit: Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown).\n",
      "\n",
      "danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
      "energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\n",
      "\n",
      "key: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
      "\n",
      "loudness: The overall loudness of a track in decibels (dB).\n",
      "\n",
      "mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
      "\n",
      "speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
      "\n",
      "acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
      "\n",
      "instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.\n",
      "\n",
      "liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
      "\n",
      "valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
      "\n",
      "tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
      "\n",
      "time_signature: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
      "\n",
      "track_genre: The genre in which the track belongs\n",
      "name 'detect' is not defined Language is not detected: spotify-tracks-dataset\n",
      "name 'detect' is not defined Language is not detected: mohamedhamad21\n",
      "name 'detect' is not defined Language is not detected: Spotify Tracks Dataset\n",
      "name 'detect' is not defined Language is not detected: This is a dataset of Spotify tracks over a range of 125 different genres. Each t\n",
      "name 'detect' is not defined Language is not detected: # **Content**\n",
      "This is a dataset of Spotify tracks over a range of 125 different genres. Each track has some audio features associated with it. The data is in CSV format which is tabular and can be loaded quickly.\n",
      "\n",
      "\n",
      "# **Usage**\n",
      "## The dataset can be used for:\n",
      "* Building a Recommendation System based on some user input or preference.\n",
      "\n",
      "* Classification purposes based on audio features and available genres.\n",
      "\n",
      "* Any other application that you can think of. \n",
      "\n",
      "\n",
      "# **Column Descriptors**\n",
      "track_id: The Spotify ID for the track\n",
      "\n",
      "artists: The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
      "\n",
      "album_name: The album name in which the track appears\n",
      "\n",
      "popularity: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
      "\n",
      "duration_ms: The track length in milliseconds.\n",
      "\n",
      "explicit: Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown).\n",
      "\n",
      "danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
      "energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\n",
      "\n",
      "key: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
      "\n",
      "loudness: The overall loudness of a track in decibels (dB).\n",
      "\n",
      "mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
      "\n",
      "speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
      "\n",
      "acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
      "\n",
      "instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.\n",
      "\n",
      "liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
      "\n",
      "valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
      "\n",
      "tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
      "\n",
      "time_signature: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
      "\n",
      "track_genre: The genre in which the track belongs\n",
      "name 'detect' is not defined Language is not detected: radioactive11/tmdb-movies-dataset-2020\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset-2020\n",
      "name 'detect' is not defined Language is not detected: radioactive11\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset (2020)\n",
      "name 'detect' is not defined Language is not detected: This dataset contains movies upto Nov 2020 scrapped using TMDB API \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "These files contain metadata for all 58,000+ movies listed in [The Movie Database](https://www.themoviedb.org). The dataset consists of movies released on or before November 2020. Data points include Title, Genre(s), Poster, Keywords and is sorted by Popularity. \n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains a single CSV file, ```moviesTMDB2020.csv```.\n",
      "Since I created this dataset for making a Content-Based  Recommendation Engine, it has the following columns:\n",
      "* ```id```: TMDB ID of the movie\n",
      "* ```original_title```: Title of the movie\n",
      "* ```poster_path```: Path to Poster of the movie\n",
      "* ```Overview```: Brief Description of the movie\n",
      "* ```Genre```: Genre(s) the movie belongs to\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset-2020\n",
      "name 'detect' is not defined Language is not detected: radioactive11\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset (2020)\n",
      "name 'detect' is not defined Language is not detected: This dataset contains movies upto Nov 2020 scrapped using TMDB API \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "These files contain metadata for all 58,000+ movies listed in [The Movie Database](https://www.themoviedb.org). The dataset consists of movies released on or before November 2020. Data points include Title, Genre(s), Poster, Keywords and is sorted by Popularity. \n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains a single CSV file, ```moviesTMDB2020.csv```.\n",
      "Since I created this dataset for making a Content-Based  Recommendation Engine, it has the following columns:\n",
      "* ```id```: TMDB ID of the movie\n",
      "* ```original_title```: Title of the movie\n",
      "* ```poster_path```: Path to Poster of the movie\n",
      "* ```Overview```: Brief Description of the movie\n",
      "* ```Genre```: Genre(s) the movie belongs to\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: subhajournal/job-fraud-detection\n",
      "name 'detect' is not defined Language is not detected: job-fraud-detection\n",
      "name 'detect' is not defined Language is not detected: subhajournal\n",
      "name 'detect' is not defined Language is not detected: Job Fraud Detection\n",
      "name 'detect' is not defined Language is not detected: Detection of Job Frauds and Job Recommendation System\n",
      "name 'detect' is not defined Language is not detected: job-fraud-detection\n",
      "name 'detect' is not defined Language is not detected: subhajournal\n",
      "name 'detect' is not defined Language is not detected: Job Fraud Detection\n",
      "name 'detect' is not defined Language is not detected: Detection of Job Frauds and Job Recommendation System\n",
      "name 'detect' is not defined Language is not detected: subhajournal/drug-recommendations\n",
      "name 'detect' is not defined Language is not detected: drug-recommendations\n",
      "name 'detect' is not defined Language is not detected: subhajournal\n",
      "name 'detect' is not defined Language is not detected: Drug Recommendations\n",
      "name 'detect' is not defined Language is not detected: Drug Recommendation by type of Disease and Review Analysis\n",
      "name 'detect' is not defined Language is not detected: drug-recommendations\n",
      "name 'detect' is not defined Language is not detected: subhajournal\n",
      "name 'detect' is not defined Language is not detected: Drug Recommendations\n",
      "name 'detect' is not defined Language is not detected: Drug Recommendation by type of Disease and Review Analysis\n",
      "name 'detect' is not defined Language is not detected: ebrahimhaquebhatti/75000-used-cars-dataset-with-specifications\n",
      "name 'detect' is not defined Language is not detected: 75000-used-cars-dataset-with-specifications\n",
      "name 'detect' is not defined Language is not detected: ebrahimhaquebhatti\n",
      "name 'detect' is not defined Language is not detected: 75000+ used cars dataset with specifications\n",
      "name 'detect' is not defined Language is not detected: Make, Engine Type, Engine Capacity, Price and lots of features of 75000+ cars\n",
      "name 'detect' is not defined Language is not detected: Pakistan is the world's 5th largest country by population. As the population increases, the demand for goods and the necessities of life also increases, and the car is one of them. PakWheels is the largest online marketplace for car shoppers and sellers in Pakistan. The data in this dataset is web scrapped from the PakWheels website using the Python library BeautifulSoup.\n",
      "name 'detect' is not defined Language is not detected: 75000-used-cars-dataset-with-specifications\n",
      "name 'detect' is not defined Language is not detected: ebrahimhaquebhatti\n",
      "name 'detect' is not defined Language is not detected: 75000+ used cars dataset with specifications\n",
      "name 'detect' is not defined Language is not detected: Make, Engine Type, Engine Capacity, Price and lots of features of 75000+ cars\n",
      "name 'detect' is not defined Language is not detected: Pakistan is the world's 5th largest country by population. As the population increases, the demand for goods and the necessities of life also increases, and the car is one of them. PakWheels is the largest online marketplace for car shoppers and sellers in Pakistan. The data in this dataset is web scrapped from the PakWheels website using the Python library BeautifulSoup.\n",
      "name 'detect' is not defined Language is not detected: deepakburi062/power-plant-optimization-problem\n",
      "name 'detect' is not defined Language is not detected: power-plant-optimization-problem\n",
      "name 'detect' is not defined Language is not detected: deepakburi062\n",
      "name 'detect' is not defined Language is not detected: Power Plant Optimization Problem\n",
      "name 'detect' is not defined Language is not detected: Optimization of Process parameter to minimize Reheater spray (Super critical)\n",
      "name 'detect' is not defined Language is not detected: power-plant-optimization-problem\n",
      "name 'detect' is not defined Language is not detected: deepakburi062\n",
      "name 'detect' is not defined Language is not detected: Power Plant Optimization Problem\n",
      "name 'detect' is not defined Language is not detected: Optimization of Process parameter to minimize Reheater spray (Super critical)\n",
      "name 'detect' is not defined Language is not detected: syxlicheng/bone-tumor-predicted\n",
      "name 'detect' is not defined Language is not detected: bone-tumor-predicted\n",
      "name 'detect' is not defined Language is not detected: syxlicheng\n",
      "name 'detect' is not defined Language is not detected: Bone_tumor_predicted\n",
      "name 'detect' is not defined Language is not detected: Bone_tumor_predicted-with deeplearning network\n",
      "name 'detect' is not defined Language is not detected: The purpose of this study was to develop a deep learning algorithm that can differentiate benign and malignant bone lesions using MR.\n",
      "\n",
      "1. Imaging data models\n",
      "Models for image classification should be developed by adapting the EfficientNet or any other model deep learning architecture. final classification layer with a single node and sigmoid activation need to used for perform the binary classification task.\n",
      "\n",
      "2. Clinical data model\n",
      "A logistic regression model using clinical variables was separately developed for the classification task. Inputs were patient age, sex, and lesion location. 21 locations (clavicle, cranium, proximal femur, distal femur, foot, proximal radius, distal radius, proximal ulna, distal,ulna, hand, hip, proximal humerus, distal humerus, proximal tibia, distal tibia, proximal fibula, distal fibula, mandible, rib/chest wall, scapula, or spine) were one-hot encoded such that the model\n",
      "received 23 distinct quantified input variables.\n",
      "\n",
      "3.Ensemble model\n",
      "The imaging and clinical feature models need to be combine by using a stacked ensemble approach in which a voting ensemble received malignancy probabilities from the imaging and clinical feature models as inputs and created outputs based upon a summation of the predicted probabilities. Each ensemble classification model consisted of the outputs of a model trained upon T1W imaging studies, a model trained upon T2W imaging studies, and a logistic regression model based upon clinical features.\n",
      "name 'detect' is not defined Language is not detected: bone-tumor-predicted\n",
      "name 'detect' is not defined Language is not detected: syxlicheng\n",
      "name 'detect' is not defined Language is not detected: Bone_tumor_predicted\n",
      "name 'detect' is not defined Language is not detected: Bone_tumor_predicted-with deeplearning network\n",
      "name 'detect' is not defined Language is not detected: The purpose of this study was to develop a deep learning algorithm that can differentiate benign and malignant bone lesions using MR.\n",
      "\n",
      "1. Imaging data models\n",
      "Models for image classification should be developed by adapting the EfficientNet or any other model deep learning architecture. final classification layer with a single node and sigmoid activation need to used for perform the binary classification task.\n",
      "\n",
      "2. Clinical data model\n",
      "A logistic regression model using clinical variables was separately developed for the classification task. Inputs were patient age, sex, and lesion location. 21 locations (clavicle, cranium, proximal femur, distal femur, foot, proximal radius, distal radius, proximal ulna, distal,ulna, hand, hip, proximal humerus, distal humerus, proximal tibia, distal tibia, proximal fibula, distal fibula, mandible, rib/chest wall, scapula, or spine) were one-hot encoded such that the model\n",
      "received 23 distinct quantified input variables.\n",
      "\n",
      "3.Ensemble model\n",
      "The imaging and clinical feature models need to be combine by using a stacked ensemble approach in which a voting ensemble received malignancy probabilities from the imaging and clinical feature models as inputs and created outputs based upon a summation of the predicted probabilities. Each ensemble classification model consisted of the outputs of a model trained upon T1W imaging studies, a model trained upon T2W imaging studies, and a logistic regression model based upon clinical features.\n",
      "name 'detect' is not defined Language is not detected: saralattarulo/efige1\n",
      "name 'detect' is not defined Language is not detected: efige1\n",
      "name 'detect' is not defined Language is not detected: saralattarulo\n",
      "name 'detect' is not defined Language is not detected: European Firms in a Global Economy (EFIGE)\n",
      "name 'detect' is not defined Language is not detected: Dataset containing information about a sample of 14758 firms\n",
      "name 'detect' is not defined Language is not detected: The database, for the first time in Europe, combines measures of firms’ international activities (eg exports, outsourcing, FDI, imports) with quantitative and qualitative information on about 150 items ranging from R&D and innovation, labour organisation, financing and organisational activities, and pricing behaviour. Data consists of a representative sample (at the country level for the manufacturing industry) of almost 15,000 surveyed firms (above 10 employees) in seven European economies (Germany, France, Italy, Spain, United Kingdom, Austria, Hungary). Data was collected in 2010, covering the years from 2007 to 2009. Special questions related to the behaviour of firms during the crisis were also included in the survey.\n",
      "name 'detect' is not defined Language is not detected: efige1\n",
      "name 'detect' is not defined Language is not detected: saralattarulo\n",
      "name 'detect' is not defined Language is not detected: European Firms in a Global Economy (EFIGE)\n",
      "name 'detect' is not defined Language is not detected: Dataset containing information about a sample of 14758 firms\n",
      "name 'detect' is not defined Language is not detected: The database, for the first time in Europe, combines measures of firms’ international activities (eg exports, outsourcing, FDI, imports) with quantitative and qualitative information on about 150 items ranging from R&D and innovation, labour organisation, financing and organisational activities, and pricing behaviour. Data consists of a representative sample (at the country level for the manufacturing industry) of almost 15,000 surveyed firms (above 10 employees) in seven European economies (Germany, France, Italy, Spain, United Kingdom, Austria, Hungary). Data was collected in 2010, covering the years from 2007 to 2009. Special questions related to the behaviour of firms during the crisis were also included in the survey.\n",
      "name 'detect' is not defined Language is not detected: srikaranelakurthy/online-news-popularity\n",
      "name 'detect' is not defined Language is not detected: online-news-popularity\n",
      "name 'detect' is not defined Language is not detected: srikaranelakurthy\n",
      "name 'detect' is not defined Language is not detected: Online News Popularity\n",
      "name 'detect' is not defined Language is not detected: Category Classification & Prediction shares\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "As digital media is growing the competition between online platforms also has rapidly increased. Online platforms like Buzzfeed, Mashable, Medium, towards data science publish hundreds of articles of every day. In this report, we analyze the Mashable dataset which consists of articles data information mainly as a number of unique words, number of non-stop words, the postpositive polarity of words, negative polarity of words, etc. Here we intend to predict the number of shares that articles can be shared. This will be very helpful for Mashable to decide which articles should they publish because they can actually predict which articles will be having the maximum number of shares. Random forest regression has been used to predict the number of shares and it can achieve an accuracy of 70% with Parameter tuning. As there is the number of articles that will be collected from different ways but to classify or group these articles into separate categories for an online platform it will be a difficult job. To handle this problem, in this report we have used neural-networks to classify the articles into different categories. By doing so, the people doesn't need to do an extensive search because the Mashable can keep an interface with articles classified into different categories which in-turn will help people to choose the category and directly search their articles. \n",
      "\n",
      "### Content\n",
      "\n",
      "With the growth of the Internet in daily life, people are in a minute away to read the news or watch any entertainment or read articles of different categories. As the growth of the internet, even the usage by the people of it has increased rapidly, it actually became their part of life. Nowadays as people using the internet more, they are studying the articles for their knowledge or news or of any sector online. As the demand is increased even online platforms rivalry has increased. Due to this, every online platform is striving to publish the articles on their site which have great value and bring most shares. In this project, we do the prediction of shares of an article based on the data produced by ‘Mashable’ where they collected data of around 39000 articles. For this prediction, we have used Random forest Regression. In this report will be discussing why the Random forest  Regression has been choosing for the prediction of shares by analyzing the Data set and doing cross-tabulation, what is the variance of the dataset and how many levels of bias it is with-holding. Even discuss about the features selection and why decided to do some feature engineering and how it will be helpful in increasing the accuracy.  Even in this report, we discuss how these predictions will be helpful for Mashable organization on their decision of publishing the articles.\n",
      "\t\n",
      "In this paper, we will see to handle the issue of classifying articles such as entertainment, news, lifestyle, technology, etc. To obtain this classification used the neural networks. In this paper, we will discuss why did we choose the neural networks for classification and what type of feature engineering has been used. At what levels of hidden layers and neurons the model is being affected at what stages model got started getting overfitted. For classification after the output layer, we used soft-max function. In this paper, an 11 layer neural network classifier has been used and achieved around 80% of accuracy. Methods used to achieve this accuracy are constant check rate of accuracy with different layers and neurons, standardization technique and feature selection using a correlation matrix.\n",
      "\n",
      "Related work on the study and analysis of Online News Popularity is done by the Shuo Zhang from Australian National University where they predicted the article will be popular or not and used binary neural network classification. The other related works also achieved greater accuracy of 70% but here they actually predicted the shares by applying different regression techniques. This paper was worked by He Ren and Quan Yang work in DepaDepartment of Electrical Engineering at Stanford University.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Bringing value from a heavy data set. How does this value will be helpful to Organizations. Analyzing the large volumes of data and how to bring the values from it. Correlating the features and calculating the predictability power to the target variable which we are predicting. Selection of different Machine Learning algorithms and their compatibility. Neural Networks works efficient for high dimensional data sets but Needed a very high computational time.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "•\tPredicting the number of shares an article can get it\n",
      "•\tClassifying the articles into different categories?\n",
      "•\tWhich category of article should be published maximum for higher number of shares?\n",
      "•\tOn What week-day What type of article should Mashable post more?\n",
      "•\tFor different categories of articles what should be their min and max content length?\n",
      "name 'detect' is not defined Language is not detected: online-news-popularity\n",
      "name 'detect' is not defined Language is not detected: srikaranelakurthy\n",
      "name 'detect' is not defined Language is not detected: Online News Popularity\n",
      "name 'detect' is not defined Language is not detected: Category Classification & Prediction shares\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "As digital media is growing the competition between online platforms also has rapidly increased. Online platforms like Buzzfeed, Mashable, Medium, towards data science publish hundreds of articles of every day. In this report, we analyze the Mashable dataset which consists of articles data information mainly as a number of unique words, number of non-stop words, the postpositive polarity of words, negative polarity of words, etc. Here we intend to predict the number of shares that articles can be shared. This will be very helpful for Mashable to decide which articles should they publish because they can actually predict which articles will be having the maximum number of shares. Random forest regression has been used to predict the number of shares and it can achieve an accuracy of 70% with Parameter tuning. As there is the number of articles that will be collected from different ways but to classify or group these articles into separate categories for an online platform it will be a difficult job. To handle this problem, in this report we have used neural-networks to classify the articles into different categories. By doing so, the people doesn't need to do an extensive search because the Mashable can keep an interface with articles classified into different categories which in-turn will help people to choose the category and directly search their articles. \n",
      "\n",
      "### Content\n",
      "\n",
      "With the growth of the Internet in daily life, people are in a minute away to read the news or watch any entertainment or read articles of different categories. As the growth of the internet, even the usage by the people of it has increased rapidly, it actually became their part of life. Nowadays as people using the internet more, they are studying the articles for their knowledge or news or of any sector online. As the demand is increased even online platforms rivalry has increased. Due to this, every online platform is striving to publish the articles on their site which have great value and bring most shares. In this project, we do the prediction of shares of an article based on the data produced by ‘Mashable’ where they collected data of around 39000 articles. For this prediction, we have used Random forest Regression. In this report will be discussing why the Random forest  Regression has been choosing for the prediction of shares by analyzing the Data set and doing cross-tabulation, what is the variance of the dataset and how many levels of bias it is with-holding. Even discuss about the features selection and why decided to do some feature engineering and how it will be helpful in increasing the accuracy.  Even in this report, we discuss how these predictions will be helpful for Mashable organization on their decision of publishing the articles.\n",
      "\t\n",
      "In this paper, we will see to handle the issue of classifying articles such as entertainment, news, lifestyle, technology, etc. To obtain this classification used the neural networks. In this paper, we will discuss why did we choose the neural networks for classification and what type of feature engineering has been used. At what levels of hidden layers and neurons the model is being affected at what stages model got started getting overfitted. For classification after the output layer, we used soft-max function. In this paper, an 11 layer neural network classifier has been used and achieved around 80% of accuracy. Methods used to achieve this accuracy are constant check rate of accuracy with different layers and neurons, standardization technique and feature selection using a correlation matrix.\n",
      "\n",
      "Related work on the study and analysis of Online News Popularity is done by the Shuo Zhang from Australian National University where they predicted the article will be popular or not and used binary neural network classification. The other related works also achieved greater accuracy of 70% but here they actually predicted the shares by applying different regression techniques. This paper was worked by He Ren and Quan Yang work in DepaDepartment of Electrical Engineering at Stanford University.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Bringing value from a heavy data set. How does this value will be helpful to Organizations. Analyzing the large volumes of data and how to bring the values from it. Correlating the features and calculating the predictability power to the target variable which we are predicting. Selection of different Machine Learning algorithms and their compatibility. Neural Networks works efficient for high dimensional data sets but Needed a very high computational time.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "•\tPredicting the number of shares an article can get it\n",
      "•\tClassifying the articles into different categories?\n",
      "•\tWhich category of article should be published maximum for higher number of shares?\n",
      "•\tOn What week-day What type of article should Mashable post more?\n",
      "•\tFor different categories of articles what should be their min and max content length?\n",
      "name 'detect' is not defined Language is not detected: venessagreen/predicting-taxi-fares-by-utilizing-random-forests\n",
      "name 'detect' is not defined Language is not detected: predicting-taxi-fares-by-utilizing-random-forests\n",
      "name 'detect' is not defined Language is not detected: venessagreen\n",
      "name 'detect' is not defined Language is not detected: Predicting Taxi Fares By Utilizing Random Forests\n",
      "name 'detect' is not defined Language is not detected:  Exploring taxi trips in New York City with regression trees and random forests\n",
      "name 'detect' is not defined Language is not detected: In this notebook, we analyze data on taxi trips in New York City to gain insights into how different factors affect trip fares. We start by visualizing the spatial distribution of trip origins and the relationship between fare and distance. Then, we fit a regression tree and a random forest to predict trip fares based on variables such as pickup location, time of day, day of the week, and month. We compare the performance of the two methods and highlight the most important predictors. Finally, we visualize the predicted fares and explore how they vary across the city.\n",
      "name 'detect' is not defined Language is not detected: predicting-taxi-fares-by-utilizing-random-forests\n",
      "name 'detect' is not defined Language is not detected: venessagreen\n",
      "name 'detect' is not defined Language is not detected: Predicting Taxi Fares By Utilizing Random Forests\n",
      "name 'detect' is not defined Language is not detected:  Exploring taxi trips in New York City with regression trees and random forests\n",
      "name 'detect' is not defined Language is not detected: In this notebook, we analyze data on taxi trips in New York City to gain insights into how different factors affect trip fares. We start by visualizing the spatial distribution of trip origins and the relationship between fare and distance. Then, we fit a regression tree and a random forest to predict trip fares based on variables such as pickup location, time of day, day of the week, and month. We compare the performance of the two methods and highlight the most important predictors. Finally, we visualize the predicted fares and explore how they vary across the city.\n",
      "name 'detect' is not defined Language is not detected: piyushagni5/sentiment-analysis-for-steam-reviews\n",
      "name 'detect' is not defined Language is not detected: sentiment-analysis-for-steam-reviews\n",
      "name 'detect' is not defined Language is not detected: piyushagni5\n",
      "name 'detect' is not defined Language is not detected: Sentiment Analysis for Steam Reviews\n",
      "name 'detect' is not defined Language is not detected: Game Reviews Classification\n",
      "name 'detect' is not defined Language is not detected: ### Sentiment Analysis for Steam Reviews\n",
      "Steam is a video game digital distribution service with a vast community of gamers globally. A lot of gamers write reviews on the game page and have the option of choosing whether they would recommend this game to others or not. However, determining this sentiment automatically from the text can help Steam to automatically tag such reviews extracted from other forums across the internet and can help them better judge the popularity of games.\n",
      "\n",
      "Given the review text with user recommendation and other information related to each game for 64 game titles, the task is to create a test set by making a split from the training set and try to predict whether the reviewer recommended the game titles available in the test set on the basis of review text and other information.\n",
      "\n",
      "Game overview information for the train is available in single file game_overview.csv.\n",
      "\n",
      "### About Data Source: Steam Platform\n",
      "* train.csv\n",
      "\n",
      "**review_id** --&gt; Unique ID for each review\n",
      "\n",
      "**title** --&gt; Title of the game\n",
      "\n",
      "**year** --&gt; Year in which the review was posted\n",
      "\n",
      "**user_review** --&gt; Full Text of the review posted by a user\n",
      "\n",
      "**user_suggestion** --&gt; (Target) Game marked Recommended(1) and Not Recommended(0) by the user\n",
      "\n",
      "\n",
      "* game_overview.csv\n",
      "\n",
      "**title** --&gt; Title of the game\n",
      "\n",
      "**developer** --&gt; Name of the developer of the game\n",
      "\n",
      "**publisher** --&gt; Name of the publisher of the game\n",
      "\n",
      "**tags** --&gt; Popular user-defined tags for the game\n",
      "\n",
      "**overview** --&gt; Overview of the game provided by the publisher.\n",
      "\n",
      "### Acknowledgements\n",
      "The data is collected from the Analytic Vidhya, JanataHack: NLP Hackathon. \n",
      "name 'detect' is not defined Language is not detected: sentiment-analysis-for-steam-reviews\n",
      "name 'detect' is not defined Language is not detected: piyushagni5\n",
      "name 'detect' is not defined Language is not detected: Sentiment Analysis for Steam Reviews\n",
      "name 'detect' is not defined Language is not detected: Game Reviews Classification\n",
      "name 'detect' is not defined Language is not detected: ### Sentiment Analysis for Steam Reviews\n",
      "Steam is a video game digital distribution service with a vast community of gamers globally. A lot of gamers write reviews on the game page and have the option of choosing whether they would recommend this game to others or not. However, determining this sentiment automatically from the text can help Steam to automatically tag such reviews extracted from other forums across the internet and can help them better judge the popularity of games.\n",
      "\n",
      "Given the review text with user recommendation and other information related to each game for 64 game titles, the task is to create a test set by making a split from the training set and try to predict whether the reviewer recommended the game titles available in the test set on the basis of review text and other information.\n",
      "\n",
      "Game overview information for the train is available in single file game_overview.csv.\n",
      "\n",
      "### About Data Source: Steam Platform\n",
      "* train.csv\n",
      "\n",
      "**review_id** --&gt; Unique ID for each review\n",
      "\n",
      "**title** --&gt; Title of the game\n",
      "\n",
      "**year** --&gt; Year in which the review was posted\n",
      "\n",
      "**user_review** --&gt; Full Text of the review posted by a user\n",
      "\n",
      "**user_suggestion** --&gt; (Target) Game marked Recommended(1) and Not Recommended(0) by the user\n",
      "\n",
      "\n",
      "* game_overview.csv\n",
      "\n",
      "**title** --&gt; Title of the game\n",
      "\n",
      "**developer** --&gt; Name of the developer of the game\n",
      "\n",
      "**publisher** --&gt; Name of the publisher of the game\n",
      "\n",
      "**tags** --&gt; Popular user-defined tags for the game\n",
      "\n",
      "**overview** --&gt; Overview of the game provided by the publisher.\n",
      "\n",
      "### Acknowledgements\n",
      "The data is collected from the Analytic Vidhya, JanataHack: NLP Hackathon. \n",
      "name 'detect' is not defined Language is not detected: officialprojecto/youtubeset\n",
      "name 'detect' is not defined Language is not detected: youtubeset\n",
      "name 'detect' is not defined Language is not detected: officialprojecto\n",
      "name 'detect' is not defined Language is not detected: YouTubeSet\n",
      "name 'detect' is not defined Language is not detected: The perfect dataset for creating a model that boost YouTubers productivity!\n",
      "name 'detect' is not defined Language is not detected: This dataset has been created by scraping YouTube home page (so it contains suggested videos) where video thumbnails have been associated with number of views and title of the video. It could be a useful dataset for infering how much a good thumbnail and a good title drive the views of a video.\n",
      "name 'detect' is not defined Language is not detected: youtubeset\n",
      "name 'detect' is not defined Language is not detected: officialprojecto\n",
      "name 'detect' is not defined Language is not detected: YouTubeSet\n",
      "name 'detect' is not defined Language is not detected: The perfect dataset for creating a model that boost YouTubers productivity!\n",
      "name 'detect' is not defined Language is not detected: This dataset has been created by scraping YouTube home page (so it contains suggested videos) where video thumbnails have been associated with number of views and title of the video. It could be a useful dataset for infering how much a good thumbnail and a good title drive the views of a video.\n",
      "name 'detect' is not defined Language is not detected: gabrieloolugbenga/case-study-dataset\n",
      "name 'detect' is not defined Language is not detected: case-study-dataset\n",
      "name 'detect' is not defined Language is not detected: gabrieloolugbenga\n",
      "name 'detect' is not defined Language is not detected: case_study_dataset\n",
      "name 'detect' is not defined Language is not detected: Cyclistic bikes dataset (Google recommendation)\n",
      "name 'detect' is not defined Language is not detected: case-study-dataset\n",
      "name 'detect' is not defined Language is not detected: gabrieloolugbenga\n",
      "name 'detect' is not defined Language is not detected: case_study_dataset\n",
      "name 'detect' is not defined Language is not detected: Cyclistic bikes dataset (Google recommendation)\n",
      "name 'detect' is not defined Language is not detected: sinderpreet/movies-recommender-system\n",
      "name 'detect' is not defined Language is not detected: movies-recommender-system\n",
      "name 'detect' is not defined Language is not detected: sinderpreet\n",
      "name 'detect' is not defined Language is not detected: Movies Recommender System\n",
      "name 'detect' is not defined Language is not detected: \"Reel Insights: Crafting a Movies Recommender System - Unveiling Patterns, Predi\n",
      "name 'detect' is not defined Language is not detected: Description:\n",
      "\n",
      "🎬 **Reel Insights: Movies Recommender System Dataset Project** 🎥\n",
      "\n",
      "Unlock the cinematic universe with our meticulously curated dataset for building a cutting-edge Movies Recommender System. Dive into the world of film recommendations, fueled by a rich collection of user preferences, ratings, and movie metadata. This dataset is designed to empower data enthusiasts, machine learning practitioners, and movie buffs to explore, analyze, and innovate within the realm of personalized movie recommendations.\n",
      "\n",
      "📚 **Context:**\n",
      "Movies hold a special place in our hearts, and the quest for discovering new and tailored cinematic experiences has never been more exciting. This dataset is a treasure trove of information, encompassing user interactions, ratings, genres, release dates, and more. Whether you're a seasoned data scientist or a passionate moviegoer, this project provides a unique opportunity to delve into the dynamics of user behavior and preferences in the vast landscape of cinema.\n",
      "\n",
      "🌐 **Sources:**\n",
      "Our dataset draws inspiration and data from a diverse range of sources, amalgamating information from reputable movie databases, user ratings platforms, and industry-standard metadata repositories. The convergence of these sources ensures a comprehensive and authentic representation of the cinematic landscape, spanning decades and genres.\n",
      "\n",
      "💡 **Inspiration:**\n",
      "The inspiration behind this dataset project is driven by the desire to enhance the movie-watching experience through intelligent recommendations. We believe in the power of data to unravel patterns, unveil hidden gems, and provide users with personalized suggestions that go beyond generic genre or popularity-based recommendations. As we embark on this journey, we invite you to join us in exploring the vast potential of machine learning in the realm of entertainment.\n",
      "\n",
      "🚀 **Get Ready to Transform Movie Recommendations!**\n",
      "This dataset is your passport to innovation in the world of movies. Unleash the power of data science, machine learning algorithms, and your creative flair to redefine how users discover and enjoy their favorite films. Let's bring the magic of the silver screen to every individual, one personalized recommendation at a time.\n",
      "\n",
      "🎞️ **Lights, Camera, Data!** 🍿\n",
      "name 'detect' is not defined Language is not detected: movies-recommender-system\n",
      "name 'detect' is not defined Language is not detected: sinderpreet\n",
      "name 'detect' is not defined Language is not detected: Movies Recommender System\n",
      "name 'detect' is not defined Language is not detected: \"Reel Insights: Crafting a Movies Recommender System - Unveiling Patterns, Predi\n",
      "name 'detect' is not defined Language is not detected: Description:\n",
      "\n",
      "🎬 **Reel Insights: Movies Recommender System Dataset Project** 🎥\n",
      "\n",
      "Unlock the cinematic universe with our meticulously curated dataset for building a cutting-edge Movies Recommender System. Dive into the world of film recommendations, fueled by a rich collection of user preferences, ratings, and movie metadata. This dataset is designed to empower data enthusiasts, machine learning practitioners, and movie buffs to explore, analyze, and innovate within the realm of personalized movie recommendations.\n",
      "\n",
      "📚 **Context:**\n",
      "Movies hold a special place in our hearts, and the quest for discovering new and tailored cinematic experiences has never been more exciting. This dataset is a treasure trove of information, encompassing user interactions, ratings, genres, release dates, and more. Whether you're a seasoned data scientist or a passionate moviegoer, this project provides a unique opportunity to delve into the dynamics of user behavior and preferences in the vast landscape of cinema.\n",
      "\n",
      "🌐 **Sources:**\n",
      "Our dataset draws inspiration and data from a diverse range of sources, amalgamating information from reputable movie databases, user ratings platforms, and industry-standard metadata repositories. The convergence of these sources ensures a comprehensive and authentic representation of the cinematic landscape, spanning decades and genres.\n",
      "\n",
      "💡 **Inspiration:**\n",
      "The inspiration behind this dataset project is driven by the desire to enhance the movie-watching experience through intelligent recommendations. We believe in the power of data to unravel patterns, unveil hidden gems, and provide users with personalized suggestions that go beyond generic genre or popularity-based recommendations. As we embark on this journey, we invite you to join us in exploring the vast potential of machine learning in the realm of entertainment.\n",
      "\n",
      "🚀 **Get Ready to Transform Movie Recommendations!**\n",
      "This dataset is your passport to innovation in the world of movies. Unleash the power of data science, machine learning algorithms, and your creative flair to redefine how users discover and enjoy their favorite films. Let's bring the magic of the silver screen to every individual, one personalized recommendation at a time.\n",
      "\n",
      "🎞️ **Lights, Camera, Data!** 🍿\n",
      "name 'detect' is not defined Language is not detected: owaiskhan9654/rise-research-innovate-solve-excel\n",
      "name 'detect' is not defined Language is not detected: rise-research-innovate-solve-excel\n",
      "name 'detect' is not defined Language is not detected: owaiskhan9654\n",
      "name 'detect' is not defined Language is not detected: R.I.S.E. – Research. Innovate. Solve. Excel. \n",
      "name 'detect' is not defined Language is not detected: Sony R.I.S.E Research Challenge Dataset\n",
      "name 'detect' is not defined Language is not detected: This Dataset has been Taken from Recent Challenge organized by Sony. This was a pretty competitive challenge. Out of this Dataset we have to build a Recommendation System which would be able to recommend top 10 Movies based on User Locations and Its Preferences.  \n",
      "name 'detect' is not defined Language is not detected: rise-research-innovate-solve-excel\n",
      "name 'detect' is not defined Language is not detected: owaiskhan9654\n",
      "name 'detect' is not defined Language is not detected: R.I.S.E. – Research. Innovate. Solve. Excel. \n",
      "name 'detect' is not defined Language is not detected: Sony R.I.S.E Research Challenge Dataset\n",
      "name 'detect' is not defined Language is not detected: This Dataset has been Taken from Recent Challenge organized by Sony. This was a pretty competitive challenge. Out of this Dataset we have to build a Recommendation System which would be able to recommend top 10 Movies based on User Locations and Its Preferences.  \n",
      "name 'detect' is not defined Language is not detected: ujjwalchowdhury/walmartcleaned\n",
      "name 'detect' is not defined Language is not detected: walmartcleaned\n",
      "name 'detect' is not defined Language is not detected: ujjwalchowdhury\n",
      "name 'detect' is not defined Language is not detected: Walmart Cleaned Data\n",
      "name 'detect' is not defined Language is not detected: Walmart Store Sales - Time Series Data for Regression and Anomaly Detection\n",
      "name 'detect' is not defined Language is not detected: ## Description:\n",
      "One of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc.\n",
      "\n",
      "Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.\n",
      "\n",
      "## Acknowledgements\n",
      "The dataset is taken from world.data\n",
      "\n",
      "## Objective:\n",
      "- Understand the Dataset.\n",
      "- Time Series Forecasting.\n",
      "- Anomaly Detection.\n",
      "- Build Regression models to predict the sales w.r.t single & multiple features.\n",
      "- Also evaluate the models & compare their respective scores like R2, RMSE, etc.\n",
      "name 'detect' is not defined Language is not detected: walmartcleaned\n",
      "name 'detect' is not defined Language is not detected: ujjwalchowdhury\n",
      "name 'detect' is not defined Language is not detected: Walmart Cleaned Data\n",
      "name 'detect' is not defined Language is not detected: Walmart Store Sales - Time Series Data for Regression and Anomaly Detection\n",
      "name 'detect' is not defined Language is not detected: ## Description:\n",
      "One of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc.\n",
      "\n",
      "Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.\n",
      "\n",
      "## Acknowledgements\n",
      "The dataset is taken from world.data\n",
      "\n",
      "## Objective:\n",
      "- Understand the Dataset.\n",
      "- Time Series Forecasting.\n",
      "- Anomaly Detection.\n",
      "- Build Regression models to predict the sales w.r.t single & multiple features.\n",
      "- Also evaluate the models & compare their respective scores like R2, RMSE, etc.\n",
      "name 'detect' is not defined Language is not detected: yehyachali/top2k-books-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: top2k-books-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: yehyachali\n",
      "name 'detect' is not defined Language is not detected: TOP2K Books with Descriptions\n",
      "name 'detect' is not defined Language is not detected: 2000 most rated books from Goodreads with descriptions and tags\n",
      "name 'detect' is not defined Language is not detected: ### Context \n",
      "\n",
      "I am working on a book recommendation system, and I wasn't able to find a dataset with all the information I needed(descriptions and tags, all in one) so I used Goodreads API to download descriptions for each book and I added them to [goodbooks-10k](https://www.kaggle.com/zygmunt/goodbooks-10k/notebooks?sortBy=dateRun&group=profile&pageSize=20&datasetId=1938).\n",
      "I organized the tags in one list and removed all the tags with numbers, I found them useless for my recommendation system.\n",
      " \n",
      "\n",
      "### Content\n",
      "This dataset contains top2K most rated books from [goodbooks-10k](https://www.kaggle.com/zygmunt/goodbooks-10k/notebooks?sortBy=dateRun&group=profile&pageSize=20&datasetId=1938), with descriptions and all tags in one list.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "thanks to Goodreads and [Foxtrot](https://www.kaggle.com/zygmunt)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: top2k-books-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: yehyachali\n",
      "name 'detect' is not defined Language is not detected: TOP2K Books with Descriptions\n",
      "name 'detect' is not defined Language is not detected: 2000 most rated books from Goodreads with descriptions and tags\n",
      "name 'detect' is not defined Language is not detected: ### Context \n",
      "\n",
      "I am working on a book recommendation system, and I wasn't able to find a dataset with all the information I needed(descriptions and tags, all in one) so I used Goodreads API to download descriptions for each book and I added them to [goodbooks-10k](https://www.kaggle.com/zygmunt/goodbooks-10k/notebooks?sortBy=dateRun&group=profile&pageSize=20&datasetId=1938).\n",
      "I organized the tags in one list and removed all the tags with numbers, I found them useless for my recommendation system.\n",
      " \n",
      "\n",
      "### Content\n",
      "This dataset contains top2K most rated books from [goodbooks-10k](https://www.kaggle.com/zygmunt/goodbooks-10k/notebooks?sortBy=dateRun&group=profile&pageSize=20&datasetId=1938), with descriptions and all tags in one list.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "thanks to Goodreads and [Foxtrot](https://www.kaggle.com/zygmunt)\n",
      "\n",
      "name 'detect' is not defined Language is not detected: leomauro/brazilian-stock-market-data-warehouse\n",
      "name 'detect' is not defined Language is not detected: brazilian-stock-market-data-warehouse\n",
      "name 'detect' is not defined Language is not detected: leomauro\n",
      "name 'detect' is not defined Language is not detected: Brazil Stock Market - Data Warehouse\n",
      "name 'detect' is not defined Language is not detected: Stocks prices and companies details from the Brazil Stock Market (1994 - 2020)\n",
      "name 'detect' is not defined Language is not detected: <i>Photo by <a href=\"https://unsplash.com/@nampoh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Maxim Hopman</a> on <a href=\"https://unsplash.com/s/photos/stock-market\">Unsplash</a>.</i>\n",
      "\n",
      "# Introduction\n",
      "\n",
      "According to [Economatica](https://economatica.com/), a company specializing in the Latin American stock market, the Brazilian stock exchange market, governed by Brasil, Bolsa, Balcão (B3), exchanged BRL ~25.9 billion per day in the first half of 2020, during the coronavirus epidemic. Furthermore, it is estimated that in this same period there was an 18% growth in the number of Brazilian investors, totaling ~2.6 million active investors. Therefore, the financial market moves a large amount of values and, consequently, produces a vast amount of information and data daily; These data represent the movements of shares, their respective prices, dollar exchange values, and so on. This dataset contains daily stock values and information about their companies.\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Data Analysis - Spark\n",
      "- Price Prediction - Regression task\n",
      "- Best Group of Stocks - Association Rules task\n",
      "\n",
      "This dataset provides an environment (Data Warehouse-like) for analysis and visualization of financial business for users of decision support systems. Specifically, the data allow compare different assets (i.e. stocks) listed on B3, according to the sectors of the economy in which these assets operate. For example, with this Data Warehouse, the user will be able to answer questions similar to this one: **What are the most profitable sectors for investment in a given period of time?** In this way, the user can identify which are the sectors that are standing out, as well as which are the most profitable companies in the sector.\n",
      "\n",
      "# Dataset\n",
      "\n",
      "<img src=\"https://i.imgur.com/28Mf0sN.png\" alt=\"Data Warehouse\">\n",
      "\n",
      "This dataset is split into five files:\n",
      "- `dimCoin.csv` - Dimension table with information about the coins.\n",
      "- `dimCompany.csv` - Dimension table with information about the companies.\n",
      "- `dimTime.csv` - Dimension table with information about the `datetime`.\n",
      "- `factCoins.csv` - Fact table with coin value over time.\n",
      "- `factStocks.csv` - Fact table with stock prices over time.\n",
      "\n",
      "## Source\n",
      "\n",
      "The data were available by B3. You can access in https://www.b3.com.br/en_us/market-data-and-indices/ .I just structure and model the data as Data Warehouse tables. You can access my code in https://github.com/leomaurodesenv/b3-stock-indexes\n",
      "\n",
      "name 'detect' is not defined Language is not detected: brazilian-stock-market-data-warehouse\n",
      "name 'detect' is not defined Language is not detected: leomauro\n",
      "name 'detect' is not defined Language is not detected: Brazil Stock Market - Data Warehouse\n",
      "name 'detect' is not defined Language is not detected: Stocks prices and companies details from the Brazil Stock Market (1994 - 2020)\n",
      "name 'detect' is not defined Language is not detected: <i>Photo by <a href=\"https://unsplash.com/@nampoh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Maxim Hopman</a> on <a href=\"https://unsplash.com/s/photos/stock-market\">Unsplash</a>.</i>\n",
      "\n",
      "# Introduction\n",
      "\n",
      "According to [Economatica](https://economatica.com/), a company specializing in the Latin American stock market, the Brazilian stock exchange market, governed by Brasil, Bolsa, Balcão (B3), exchanged BRL ~25.9 billion per day in the first half of 2020, during the coronavirus epidemic. Furthermore, it is estimated that in this same period there was an 18% growth in the number of Brazilian investors, totaling ~2.6 million active investors. Therefore, the financial market moves a large amount of values and, consequently, produces a vast amount of information and data daily; These data represent the movements of shares, their respective prices, dollar exchange values, and so on. This dataset contains daily stock values and information about their companies.\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Data Analysis - Spark\n",
      "- Price Prediction - Regression task\n",
      "- Best Group of Stocks - Association Rules task\n",
      "\n",
      "This dataset provides an environment (Data Warehouse-like) for analysis and visualization of financial business for users of decision support systems. Specifically, the data allow compare different assets (i.e. stocks) listed on B3, according to the sectors of the economy in which these assets operate. For example, with this Data Warehouse, the user will be able to answer questions similar to this one: **What are the most profitable sectors for investment in a given period of time?** In this way, the user can identify which are the sectors that are standing out, as well as which are the most profitable companies in the sector.\n",
      "\n",
      "# Dataset\n",
      "\n",
      "<img src=\"https://i.imgur.com/28Mf0sN.png\" alt=\"Data Warehouse\">\n",
      "\n",
      "This dataset is split into five files:\n",
      "- `dimCoin.csv` - Dimension table with information about the coins.\n",
      "- `dimCompany.csv` - Dimension table with information about the companies.\n",
      "- `dimTime.csv` - Dimension table with information about the `datetime`.\n",
      "- `factCoins.csv` - Fact table with coin value over time.\n",
      "- `factStocks.csv` - Fact table with stock prices over time.\n",
      "\n",
      "## Source\n",
      "\n",
      "The data were available by B3. You can access in https://www.b3.com.br/en_us/market-data-and-indices/ .I just structure and model the data as Data Warehouse tables. You can access my code in https://github.com/leomaurodesenv/b3-stock-indexes\n",
      "\n",
      "name 'detect' is not defined Language is not detected: fermatsavant/airbnb-dataset-of-barcelona-city\n",
      "name 'detect' is not defined Language is not detected: airbnb-dataset-of-barcelona-city\n",
      "name 'detect' is not defined Language is not detected: fermatsavant\n",
      "name 'detect' is not defined Language is not detected: Airbnb dataset of barcelona city\n",
      "name 'detect' is not defined Language is not detected: Dataset for recommendation analysis\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "The data was taken from http://tomslee.net/airbnb-data-collection-get-the-data.\n",
      "The data was collected from the public Airbnb web site and the code was used is available on https://github.com/tomslee/airbnb-data-collection.\n",
      "\n",
      "### Content\n",
      "    room_id: A unique number identifying an Airbnb listing. The listing has a URL on the Airbnb web site of http://airbnb.com/rooms/room_id\n",
      "    host_id: A unique number identifying an Airbnb host. The host’s page has a URL on the Airbnb web site of http://airbnb.com/users/show/host_id\n",
      "    room_type: One of “Entire home/apt”, “Private room”, or “Shared room”\n",
      "    borough: A subregion of the city or search area for which the survey is carried out. The borough is taken from a shapefile of the city that is obtained independently of the Airbnb web site. For some cities, there is no borough information; for others the borough may be a number. If you have better shapefiles for a city of interest, please send them to me.\n",
      "    neighborhood: As with borough: a subregion of the city or search area for which the survey is carried out. For cities that have both, a neighbourhood is smaller than a borough. For some cities there is no neighbourhood information.\n",
      "    reviews: The number of reviews that a listing has received. Airbnb has said that 70% of visits end up with a review, so the number of reviews can be used to estimate the number of visits. Note that such an estimate will not be reliable for an individual listing (especially as reviews occasionally vanish from the site), but over a city as a whole it should be a useful metric of traffic.\n",
      "    overall_satisfaction: The average rating (out of five) that the listing has received from those visitors who left a review.\n",
      "    accommodates: The number of guests a listing can accommodate.\n",
      "    bedrooms: The number of bedrooms a listing offers.\n",
      "    price: The price (in $US) for a night stay. In early surveys, there may be some values that were recorded by month.\n",
      "    minstay: The minimum stay for a visit, as posted by the host.\n",
      "    latitude and longitude: The latitude and longitude of the listing as posted on the Airbnb site: this may be off by a few hundred metres. I do not have a way to track individual listing locations with\n",
      "    last_modified: the date and time that the values were read from the Airbnb web site.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: airbnb-dataset-of-barcelona-city\n",
      "name 'detect' is not defined Language is not detected: fermatsavant\n",
      "name 'detect' is not defined Language is not detected: Airbnb dataset of barcelona city\n",
      "name 'detect' is not defined Language is not detected: Dataset for recommendation analysis\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "The data was taken from http://tomslee.net/airbnb-data-collection-get-the-data.\n",
      "The data was collected from the public Airbnb web site and the code was used is available on https://github.com/tomslee/airbnb-data-collection.\n",
      "\n",
      "### Content\n",
      "    room_id: A unique number identifying an Airbnb listing. The listing has a URL on the Airbnb web site of http://airbnb.com/rooms/room_id\n",
      "    host_id: A unique number identifying an Airbnb host. The host’s page has a URL on the Airbnb web site of http://airbnb.com/users/show/host_id\n",
      "    room_type: One of “Entire home/apt”, “Private room”, or “Shared room”\n",
      "    borough: A subregion of the city or search area for which the survey is carried out. The borough is taken from a shapefile of the city that is obtained independently of the Airbnb web site. For some cities, there is no borough information; for others the borough may be a number. If you have better shapefiles for a city of interest, please send them to me.\n",
      "    neighborhood: As with borough: a subregion of the city or search area for which the survey is carried out. For cities that have both, a neighbourhood is smaller than a borough. For some cities there is no neighbourhood information.\n",
      "    reviews: The number of reviews that a listing has received. Airbnb has said that 70% of visits end up with a review, so the number of reviews can be used to estimate the number of visits. Note that such an estimate will not be reliable for an individual listing (especially as reviews occasionally vanish from the site), but over a city as a whole it should be a useful metric of traffic.\n",
      "    overall_satisfaction: The average rating (out of five) that the listing has received from those visitors who left a review.\n",
      "    accommodates: The number of guests a listing can accommodate.\n",
      "    bedrooms: The number of bedrooms a listing offers.\n",
      "    price: The price (in $US) for a night stay. In early surveys, there may be some values that were recorded by month.\n",
      "    minstay: The minimum stay for a visit, as posted by the host.\n",
      "    latitude and longitude: The latitude and longitude of the listing as posted on the Airbnb site: this may be off by a few hundred metres. I do not have a way to track individual listing locations with\n",
      "    last_modified: the date and time that the values were read from the Airbnb web site.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: gauravtopre/credit-card-defaulter-prediction\n",
      "name 'detect' is not defined Language is not detected: credit-card-defaulter-prediction\n",
      "name 'detect' is not defined Language is not detected: gauravtopre\n",
      "name 'detect' is not defined Language is not detected: Credit Card Defaulter Prediction\n",
      "name 'detect' is not defined Language is not detected: Credit Card Default Payment in Taiwan\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "In recent years, the credit card issuers in Taiwan faced the cash and credit card debt crisis and the delinquency is expected to peak in the third quarter of 2006 (Chou,2006). In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, overused credit card for consumption and accumulated heavy credit and cash–card debts. The crisis caused the blow to consumer finance confidence and it is a big challenge for both banks and cardholders\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Credit for this dataset belongs to [UCI ML Repository ](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
      "\n",
      "### Column Information\n",
      "\n",
      "PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months,8=payment delay for eight - months, 9=payment delay for nine months and above)\n",
      "PAY_2: Repayment status in August, 2005 (scale same as above)\n",
      "PAY_3: Repayment status in July, 2005 (scale same as above)\n",
      "PAY_4: Repayment status in June, 2005 (scale same as above)\n",
      "PAY_5: Repayment status in May, 2005 (scale same as above)\n",
      "PAY_6: Repayment status in April, 2005 (scale same as above)\n",
      "BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
      "BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
      "BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
      "BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
      "BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
      "BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
      "PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
      "PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
      "PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
      "PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
      "PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
      "PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: credit-card-defaulter-prediction\n",
      "name 'detect' is not defined Language is not detected: gauravtopre\n",
      "name 'detect' is not defined Language is not detected: Credit Card Defaulter Prediction\n",
      "name 'detect' is not defined Language is not detected: Credit Card Default Payment in Taiwan\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "In recent years, the credit card issuers in Taiwan faced the cash and credit card debt crisis and the delinquency is expected to peak in the third quarter of 2006 (Chou,2006). In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, overused credit card for consumption and accumulated heavy credit and cash–card debts. The crisis caused the blow to consumer finance confidence and it is a big challenge for both banks and cardholders\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Credit for this dataset belongs to [UCI ML Repository ](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
      "\n",
      "### Column Information\n",
      "\n",
      "PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months,8=payment delay for eight - months, 9=payment delay for nine months and above)\n",
      "PAY_2: Repayment status in August, 2005 (scale same as above)\n",
      "PAY_3: Repayment status in July, 2005 (scale same as above)\n",
      "PAY_4: Repayment status in June, 2005 (scale same as above)\n",
      "PAY_5: Repayment status in May, 2005 (scale same as above)\n",
      "PAY_6: Repayment status in April, 2005 (scale same as above)\n",
      "BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
      "BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
      "BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
      "BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
      "BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
      "BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
      "PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
      "PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
      "PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
      "PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
      "PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
      "PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ayanzadeh93/color-classification\n",
      "name 'detect' is not defined Language is not detected: color-classification\n",
      "name 'detect' is not defined Language is not detected: ayanzadeh93\n",
      "name 'detect' is not defined Language is not detected: color classification\n",
      "name 'detect' is not defined Language is not detected: Dataset for color classification\n",
      "name 'detect' is not defined Language is not detected: **Introduction**\n",
      "\n",
      "Color classification is an important application that is used in many areas. For example, systems that perform daily. SVM classifier with an optimal hyperplane life analysis can benefit from this classification process. For the classification process, lots of classification algorithms can be used. Among them, the most popular machine learning algorithms are neural networks, decision trees, k-nearest neighbors, Bayes network, support vector machines. In this work for training, SVMs are used and a classifier model was tried to be obtained. SVMs algorithm is one of the supervised learning methods. SVM calls for solutions to regression and classification problems as in all supervised learning methods. This algorithm is usually used to training for separate and classify different labeled samples. As a result of training with SVM, it is aimed to create an optimum hyperplane and classify the data in different\n",
      "classes. This hyperplane is located as far away from the data as possible to avoid error conditions.\n",
      "\n",
      "**Dataset**\n",
      "\n",
      "The datasets have contained about 80 images for trainset datasets for whole color classes and 90 images for the test set. colors which are prepared for this application is y yellow, black, white, green, red, orange, blue a and violet. In this implementation, basic colors are preferred for classification. and created a dataset containing images of these basic colors. The dataset also includes masks for all images. we create these masks by binarizing the image. we did the masking on the images I collected and painted the pixels belonging to the class color to white and remaining pixels to the black color.\n",
      "name 'detect' is not defined Language is not detected: color-classification\n",
      "name 'detect' is not defined Language is not detected: ayanzadeh93\n",
      "name 'detect' is not defined Language is not detected: color classification\n",
      "name 'detect' is not defined Language is not detected: Dataset for color classification\n",
      "name 'detect' is not defined Language is not detected: **Introduction**\n",
      "\n",
      "Color classification is an important application that is used in many areas. For example, systems that perform daily. SVM classifier with an optimal hyperplane life analysis can benefit from this classification process. For the classification process, lots of classification algorithms can be used. Among them, the most popular machine learning algorithms are neural networks, decision trees, k-nearest neighbors, Bayes network, support vector machines. In this work for training, SVMs are used and a classifier model was tried to be obtained. SVMs algorithm is one of the supervised learning methods. SVM calls for solutions to regression and classification problems as in all supervised learning methods. This algorithm is usually used to training for separate and classify different labeled samples. As a result of training with SVM, it is aimed to create an optimum hyperplane and classify the data in different\n",
      "classes. This hyperplane is located as far away from the data as possible to avoid error conditions.\n",
      "\n",
      "**Dataset**\n",
      "\n",
      "The datasets have contained about 80 images for trainset datasets for whole color classes and 90 images for the test set. colors which are prepared for this application is y yellow, black, white, green, red, orange, blue a and violet. In this implementation, basic colors are preferred for classification. and created a dataset containing images of these basic colors. The dataset also includes masks for all images. we create these masks by binarizing the image. we did the masking on the images I collected and painted the pixels belonging to the class color to white and remaining pixels to the black color.\n",
      "name 'detect' is not defined Language is not detected: raviiloveyou/predict-taxi-fare-with-a-bigquery-ml-forecasting\n",
      "name 'detect' is not defined Language is not detected: predict-taxi-fare-with-a-bigquery-ml-forecasting\n",
      "name 'detect' is not defined Language is not detected: raviiloveyou\n",
      "name 'detect' is not defined Language is not detected: Taxi trip fare prediction\n",
      "name 'detect' is not defined Language is not detected: This dataset is designed not just to put your skills under the microscope\n",
      "name 'detect' is not defined Language is not detected: Overview\n",
      "\n",
      "BigQuery is Google's fully managed, NoOps, low cost analytics database. With BigQuery you can query terabytes and terabytes of data without having any infrastructure to manage, or needing a database administrator.\n",
      "\n",
      "BigQuery Machine Learning BQML is where data analysts can create, train, evaluate, and predict with machine learning models with minimal coding.\n",
      "\n",
      "In this  you will explore millions of New York City yellow taxi cab trips available in a BigQuery Public Dataset. You will create a machine learning model inside of BigQuery to predict the fare of the cab ride given your model inputs and evaluate the performance of your model and make predictions with it.\n",
      "\n",
      "perform the following tasks:\n",
      "\n",
      "\n",
      "Query and explore the public taxi cab dataset.\n",
      "Create a training and evaluation dataset to be used for batch prediction.\n",
      "Create a forecasting (linear regression) model in BQML.\n",
      "Evaluate the performance of your machine learning model.\n",
      "\n",
      "There are several model types to choose from:\n",
      "\n",
      "Forecasting numeric values like next month's sales with Linear Regression (linear_reg).\n",
      "Binary or Multiclass Classification like spam or not spam email by using Logistic Regression (logistic_reg).\n",
      "k-Means Clustering for when you want unsupervised learning for exploration (kmeans).\n",
      "\n",
      "Note: There are many additional model types used in Machine Learning (like Neural Networks and decision trees) and available using libraries like TensorFlow. At this time, BQML supports the three listed above. Follow the BQML roadmap for more information.\n",
      "\n",
      "For reference sake of you we also released notebook which is available in this try to explore from that .use AutoMl foundational Models to automatically selecting important features from dataset and Model selection .\n",
      "\n",
      "you can also go with spectral clustering algorithms upcourse it is not an unsupervised task but it is correlated ,visualize  the Fare trip prices .so that cab drive easily identifies fare trips in their respective locations .\n",
      "\n",
      "Build a Forecasting model which helps for cab drives like (uber,rapido) which reach their customers easily and short time\n",
      "\n",
      "Dataset :\n",
      "             ⏱️ 'trip_duration': How long did the journey last?[in Seconds]\n",
      "             🛣️ 'distance_traveled': How far did the taxi travel?[in Km]\n",
      "             🧑‍🤝‍🧑 'num_of_passengers': How many passengers were in the taxi?\n",
      "              💵 'fare': What's the base fare for the journey?[In INR]\n",
      "              💲 'tip': How much did the driver receive in tips?[In INR]\n",
      "               🎀 'miscellaneous_fees': Were there any additional charges during the trip?e.g. tolls, convenience fees, GST \n",
      "                     etc.[In INR]\n",
      "               💰 'total_fare': The grand total for the ride (this is your prediction target!).[In INR]\n",
      "               ⚡ 'surge_applied': Was there a surge pricing applied? Yes or no?\n",
      "\n",
      "\n",
      "IF IT IS USEFUL UPVOTE THE DATASET.\n",
      "                                                      THANK YOU!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: predict-taxi-fare-with-a-bigquery-ml-forecasting\n",
      "name 'detect' is not defined Language is not detected: raviiloveyou\n",
      "name 'detect' is not defined Language is not detected: Taxi trip fare prediction\n",
      "name 'detect' is not defined Language is not detected: This dataset is designed not just to put your skills under the microscope\n",
      "name 'detect' is not defined Language is not detected: Overview\n",
      "\n",
      "BigQuery is Google's fully managed, NoOps, low cost analytics database. With BigQuery you can query terabytes and terabytes of data without having any infrastructure to manage, or needing a database administrator.\n",
      "\n",
      "BigQuery Machine Learning BQML is where data analysts can create, train, evaluate, and predict with machine learning models with minimal coding.\n",
      "\n",
      "In this  you will explore millions of New York City yellow taxi cab trips available in a BigQuery Public Dataset. You will create a machine learning model inside of BigQuery to predict the fare of the cab ride given your model inputs and evaluate the performance of your model and make predictions with it.\n",
      "\n",
      "perform the following tasks:\n",
      "\n",
      "\n",
      "Query and explore the public taxi cab dataset.\n",
      "Create a training and evaluation dataset to be used for batch prediction.\n",
      "Create a forecasting (linear regression) model in BQML.\n",
      "Evaluate the performance of your machine learning model.\n",
      "\n",
      "There are several model types to choose from:\n",
      "\n",
      "Forecasting numeric values like next month's sales with Linear Regression (linear_reg).\n",
      "Binary or Multiclass Classification like spam or not spam email by using Logistic Regression (logistic_reg).\n",
      "k-Means Clustering for when you want unsupervised learning for exploration (kmeans).\n",
      "\n",
      "Note: There are many additional model types used in Machine Learning (like Neural Networks and decision trees) and available using libraries like TensorFlow. At this time, BQML supports the three listed above. Follow the BQML roadmap for more information.\n",
      "\n",
      "For reference sake of you we also released notebook which is available in this try to explore from that .use AutoMl foundational Models to automatically selecting important features from dataset and Model selection .\n",
      "\n",
      "you can also go with spectral clustering algorithms upcourse it is not an unsupervised task but it is correlated ,visualize  the Fare trip prices .so that cab drive easily identifies fare trips in their respective locations .\n",
      "\n",
      "Build a Forecasting model which helps for cab drives like (uber,rapido) which reach their customers easily and short time\n",
      "\n",
      "Dataset :\n",
      "             ⏱️ 'trip_duration': How long did the journey last?[in Seconds]\n",
      "             🛣️ 'distance_traveled': How far did the taxi travel?[in Km]\n",
      "             🧑‍🤝‍🧑 'num_of_passengers': How many passengers were in the taxi?\n",
      "              💵 'fare': What's the base fare for the journey?[In INR]\n",
      "              💲 'tip': How much did the driver receive in tips?[In INR]\n",
      "               🎀 'miscellaneous_fees': Were there any additional charges during the trip?e.g. tolls, convenience fees, GST \n",
      "                     etc.[In INR]\n",
      "               💰 'total_fare': The grand total for the ride (this is your prediction target!).[In INR]\n",
      "               ⚡ 'surge_applied': Was there a surge pricing applied? Yes or no?\n",
      "\n",
      "\n",
      "IF IT IS USEFUL UPVOTE THE DATASET.\n",
      "                                                      THANK YOU!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: edusanketdk/electronics\n",
      "name 'detect' is not defined Language is not detected: electronics\n",
      "name 'detect' is not defined Language is not detected: edusanketdk\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics Products Sales\n",
      "name 'detect' is not defined Language is not detected: Used for product recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset was used in a data science contest by Capgemini. The main concept was to build a product recommendation system. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset gives us electronics sales data at Amazon. It contains user ratings for various electronics items sold, along with category of each item and time of sell.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset is shared at \n",
      "https://www.techgig.com/files/contest_upload_files/electronics.csv\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The dataset is a great start for building a product recommendation system based on user ratings, brands, and timestamps. \n",
      "name 'detect' is not defined Language is not detected: electronics\n",
      "name 'detect' is not defined Language is not detected: edusanketdk\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics Products Sales\n",
      "name 'detect' is not defined Language is not detected: Used for product recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset was used in a data science contest by Capgemini. The main concept was to build a product recommendation system. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset gives us electronics sales data at Amazon. It contains user ratings for various electronics items sold, along with category of each item and time of sell.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset is shared at \n",
      "https://www.techgig.com/files/contest_upload_files/electronics.csv\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The dataset is a great start for building a product recommendation system based on user ratings, brands, and timestamps. \n",
      "name 'detect' is not defined Language is not detected: thedevastator/predicting-co-branded-credit-card-defaults-in-re\n",
      "name 'detect' is not defined Language is not detected: predicting-co-branded-credit-card-defaults-in-re\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Co-Branded Credit Card Defaults\n",
      "name 'detect' is not defined Language is not detected: Analyzing Application and Credit History Data\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Co-Branded Credit Card Defaults\n",
      "### Analyzing Application and Credit History Data\n",
      "By Amit Kishore [[source]](https://data.world/amitkishore)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset provides insights into the predictability of co-branded credit card default in a retail network of a company. With over [x] columns of data, this dataset contains information ranging from applicants' demographics and credit scores to their limits and payment history. This comprehensive dataset was constructed with the goal of understanding how demographic factors influence credit risk and ultimately, co-branded credit card default rates. From age to income, marital status to educational background, each variable is used to create an understanding of the risks associated with applicants taking out co-branded cards in the retail network. Additionally, get an inside look at current trends in loan application behavior — see how often customers use loan or have applied for new cards over set time intervals — as well as monthly payments and query history. Use this unique dataset to develop an improved model for predicting credit card default that could help financial institutions assess potential cusotmers more accuracyly!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset aims to help predict co-branded credit card defaults in retail networks by providing a variety of information about the applicants. The dataset includes information such as age, gender, marital status, employment status, education level, monthly income and expenses, credit history length, number of loans and credit cards owned by the applicant, number of times they applied for loan/credit card inquiries and how many times they used each loan/credit card in the last months.  \n",
      "&gt; - In order to use this dataset effectively to predict co-branded credit card default rates in a retail network it is important to understand the data and how it's related each other. It is also important to consider any external factors that can influence an individual's likelihood of defaulting on a loan.  \n",
      "&gt; - The first step is to look at the descriptive statistics for each column so that we can get some idea as to what kind of values are seen most often among our data points and if there are any outliers present. This will give us an idea about which features may be most relevant when predicting defaults or if our model may need more contextual information from outside sources like socio-economic or political factors.  \n",
      "&gt; - Once we have identified any relevant features from our descriptive statistics analysis we'll then want to start exploring different ways these variables are related with one another and what kind of relationship these variables have with regards to defaults (both positively correlated/directly increase default risk plus negatively correlated/directly decrease default risk). This can be done through simple pair plots which show distribution and correlations between two given columns or triangular heatmaps which allow us explore correlations among multiple columns at once. Building upon these relationships further allows us then determine possible causes behind the observed correlations between different variable groups – allowing us get even more insight into why certain individuals are more likely than others be defaulters on their co-branded cards (whether it because they simply had bad luck or because there were larger systematic factors playing out).   \n",
      "&gt; - Having identified all relevant features from this data exploration process along with any external “background” data points - we finally move into constructing our machine learning models using appropriate algorithms suitable for predicting probability outcomes such as SVM or XGBoost tree ensembles etc.. When building out your ML model you’ll want ensure that all parameters necessary for accurate predictions have been included before deploying them on production systems so as not compromise neither customer privacy nor product quality standards set by regulatory authorities governing such models across countries globally\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Using the given dataset to create a predictive model that can be used to identify customers at risk of defaulting on their co-branded credit cards. This could help determine which customers should be offered special incentives or strategies in order to reduce their risk of defaulting.  \n",
      "&gt; - Using the given dataset to create a financial health recommendation engine that analyzes customer’s existing credit cards and recommends other ways they can improve their financial situation (e.g., balance transfers, better rewards programs, etc.). \n",
      "&gt; - Extracting insights from the data by performing sentiment analysis on customer’s online reviews of legitimate co-branded credit card companies, in order to enable more effective marketing and outreach campaign targeting toward specific demographics or geographic regions with different purchasing preferences or concerns regarding co-branded credit cards\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/amitkishore)\n",
      "&gt;  \n",
      "\n",
      "### License \n",
      "&gt; \n",
      "&gt; **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "&gt; - You are free to:\n",
      "&gt;      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      "&gt;      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "&gt; - You must:\n",
      "&gt;      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      "&gt;      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      "&gt;      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Training_dataset_Original.csv**\n",
      "| Column name         | Description                                                                                   |\n",
      "|:--------------------|:----------------------------------------------------------------------------------------------|\n",
      "| **application_key** | Unique identifier for each application. (Numeric)                                             |\n",
      "| **mvar1**           | Age of the applicant. (Numeric)                                                               |\n",
      "| **mvar2**           | Gender of the applicant. (Categorical)                                                        |\n",
      "| **mvar3**           | Marital status of the applicant. (Categorical)                                                |\n",
      "| **mvar4**           | Education level of the applicant. (Categorical)                                               |\n",
      "| **mvar5**           | Employment status of the applicant. (Categorical)                                             |\n",
      "| **mvar6**           | Income level of the applicant. (Numeric)                                                      |\n",
      "| **mvar7**           | Number of loans owned by the applicant. (Numeric)                                             |\n",
      "| **mvar8**           | Number of credit cards owned by the applicant. (Numeric)                                      |\n",
      "| **mvar9**           | Monthly expenses of the applicant. (Numeric)                                                  |\n",
      "| **mvar10**          | Current credit standing of the applicant. (Categorical)                                       |\n",
      "| **mvar11**          | Months since last payment or delinquency. (Numeric)                                           |\n",
      "| **mvar12**          | Number of times the applicant has used their co-branded card in the past 6 months. (Numeric)  |\n",
      "| **mvar13**          | Number of years of credit history. (Numeric)                                                  |\n",
      "| **mvar14**          | Number of times the applicant has used their co-branded card in the past 12 months. (Numeric) |\n",
      "| **mvar15**          | Number of times the applicant has used their co-branded card in the past 24 months. (Numeric) |\n",
      "| **mvar16**          | Number of times the applicant has used their co-branded card in the past 36 months. (Numeric) |\n",
      "| **mvar17**          | Number of times the applicant has used their co-branded card in the past 48 months. (Numeric) |\n",
      "| **mvar18**          | Number of times the applicant has used their co-branded card in the past 60 months. (Numeric) |\n",
      "| **mvar19**          | Default indicator (0/1). (Categorical)                                                        |\n",
      "| **default_ind**     | Default indicator (0/1). (Categorical)                                                        |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: Data_Dictionary.csv**\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Amit Kishore](https://data.world/amitkishore).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: predicting-co-branded-credit-card-defaults-in-re\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Co-Branded Credit Card Defaults\n",
      "name 'detect' is not defined Language is not detected: Analyzing Application and Credit History Data\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Co-Branded Credit Card Defaults\n",
      "### Analyzing Application and Credit History Data\n",
      "By Amit Kishore [[source]](https://data.world/amitkishore)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset provides insights into the predictability of co-branded credit card default in a retail network of a company. With over [x] columns of data, this dataset contains information ranging from applicants' demographics and credit scores to their limits and payment history. This comprehensive dataset was constructed with the goal of understanding how demographic factors influence credit risk and ultimately, co-branded credit card default rates. From age to income, marital status to educational background, each variable is used to create an understanding of the risks associated with applicants taking out co-branded cards in the retail network. Additionally, get an inside look at current trends in loan application behavior — see how often customers use loan or have applied for new cards over set time intervals — as well as monthly payments and query history. Use this unique dataset to develop an improved model for predicting credit card default that could help financial institutions assess potential cusotmers more accuracyly!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset aims to help predict co-branded credit card defaults in retail networks by providing a variety of information about the applicants. The dataset includes information such as age, gender, marital status, employment status, education level, monthly income and expenses, credit history length, number of loans and credit cards owned by the applicant, number of times they applied for loan/credit card inquiries and how many times they used each loan/credit card in the last months.  \n",
      "&gt; - In order to use this dataset effectively to predict co-branded credit card default rates in a retail network it is important to understand the data and how it's related each other. It is also important to consider any external factors that can influence an individual's likelihood of defaulting on a loan.  \n",
      "&gt; - The first step is to look at the descriptive statistics for each column so that we can get some idea as to what kind of values are seen most often among our data points and if there are any outliers present. This will give us an idea about which features may be most relevant when predicting defaults or if our model may need more contextual information from outside sources like socio-economic or political factors.  \n",
      "&gt; - Once we have identified any relevant features from our descriptive statistics analysis we'll then want to start exploring different ways these variables are related with one another and what kind of relationship these variables have with regards to defaults (both positively correlated/directly increase default risk plus negatively correlated/directly decrease default risk). This can be done through simple pair plots which show distribution and correlations between two given columns or triangular heatmaps which allow us explore correlations among multiple columns at once. Building upon these relationships further allows us then determine possible causes behind the observed correlations between different variable groups – allowing us get even more insight into why certain individuals are more likely than others be defaulters on their co-branded cards (whether it because they simply had bad luck or because there were larger systematic factors playing out).   \n",
      "&gt; - Having identified all relevant features from this data exploration process along with any external “background” data points - we finally move into constructing our machine learning models using appropriate algorithms suitable for predicting probability outcomes such as SVM or XGBoost tree ensembles etc.. When building out your ML model you’ll want ensure that all parameters necessary for accurate predictions have been included before deploying them on production systems so as not compromise neither customer privacy nor product quality standards set by regulatory authorities governing such models across countries globally\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Using the given dataset to create a predictive model that can be used to identify customers at risk of defaulting on their co-branded credit cards. This could help determine which customers should be offered special incentives or strategies in order to reduce their risk of defaulting.  \n",
      "&gt; - Using the given dataset to create a financial health recommendation engine that analyzes customer’s existing credit cards and recommends other ways they can improve their financial situation (e.g., balance transfers, better rewards programs, etc.). \n",
      "&gt; - Extracting insights from the data by performing sentiment analysis on customer’s online reviews of legitimate co-branded credit card companies, in order to enable more effective marketing and outreach campaign targeting toward specific demographics or geographic regions with different purchasing preferences or concerns regarding co-branded credit cards\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/amitkishore)\n",
      "&gt;  \n",
      "\n",
      "### License \n",
      "&gt; \n",
      "&gt; **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "&gt; - You are free to:\n",
      "&gt;      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      "&gt;      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "&gt; - You must:\n",
      "&gt;      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      "&gt;      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      "&gt;      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Training_dataset_Original.csv**\n",
      "| Column name         | Description                                                                                   |\n",
      "|:--------------------|:----------------------------------------------------------------------------------------------|\n",
      "| **application_key** | Unique identifier for each application. (Numeric)                                             |\n",
      "| **mvar1**           | Age of the applicant. (Numeric)                                                               |\n",
      "| **mvar2**           | Gender of the applicant. (Categorical)                                                        |\n",
      "| **mvar3**           | Marital status of the applicant. (Categorical)                                                |\n",
      "| **mvar4**           | Education level of the applicant. (Categorical)                                               |\n",
      "| **mvar5**           | Employment status of the applicant. (Categorical)                                             |\n",
      "| **mvar6**           | Income level of the applicant. (Numeric)                                                      |\n",
      "| **mvar7**           | Number of loans owned by the applicant. (Numeric)                                             |\n",
      "| **mvar8**           | Number of credit cards owned by the applicant. (Numeric)                                      |\n",
      "| **mvar9**           | Monthly expenses of the applicant. (Numeric)                                                  |\n",
      "| **mvar10**          | Current credit standing of the applicant. (Categorical)                                       |\n",
      "| **mvar11**          | Months since last payment or delinquency. (Numeric)                                           |\n",
      "| **mvar12**          | Number of times the applicant has used their co-branded card in the past 6 months. (Numeric)  |\n",
      "| **mvar13**          | Number of years of credit history. (Numeric)                                                  |\n",
      "| **mvar14**          | Number of times the applicant has used their co-branded card in the past 12 months. (Numeric) |\n",
      "| **mvar15**          | Number of times the applicant has used their co-branded card in the past 24 months. (Numeric) |\n",
      "| **mvar16**          | Number of times the applicant has used their co-branded card in the past 36 months. (Numeric) |\n",
      "| **mvar17**          | Number of times the applicant has used their co-branded card in the past 48 months. (Numeric) |\n",
      "| **mvar18**          | Number of times the applicant has used their co-branded card in the past 60 months. (Numeric) |\n",
      "| **mvar19**          | Default indicator (0/1). (Categorical)                                                        |\n",
      "| **default_ind**     | Default indicator (0/1). (Categorical)                                                        |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: Data_Dictionary.csv**\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Amit Kishore](https://data.world/amitkishore).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/sigfox-and-lorawan-localization-tool\n",
      "name 'detect' is not defined Language is not detected: sigfox-and-lorawan-localization-tool\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Sigfox and LoRaWAN Localization Tool\n",
      "name 'detect' is not defined Language is not detected: Evaluating Fingerprinting Localization Algorithms in Large Outdoor Areas\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Sigfox and LoRaWAN Localization Tool\n",
      "### Evaluating Fingerprinting Localization Algorithms in Large Outdoor Areas\n",
      "By  [[source]](https://zenodo.org/record/3904158#.Y9ZAptJBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset offers a powerful benchmark to evaluate the performance and accuracy of finger printing localization algorithms in a range of outdoor environments. Featuring 130,430 LoRaWAN messages collected in the city center of Antwerp, 14,378 Sigfox messages recorded in Antwerp city center and 25,638 Sigfox messages collected between Antwerp and Ghent rural regions. The data is presented as an extensive collection of metadata including receiving time stamps, base station identifiers and Received Signal Strength Indicators (RSSI). To offer different perspectives on analysis possibilities this dataset also features Signal-to-Noise Ratio (SNR) for all signals along with Estimated Signal Power (ESP) metrics. In addition to that the coordinates for all LoRaWAN gateways in Antwerp are pinned within the data set opening up potential opportunities for cross comparison between urban and rural environments. Analyzing this data will provide details on current testing practices for fingerprinting algorithms allowing further insight into how robust these methods can be across diverse conditions ranging from metropolitan city centers to rural areas. This invaluable benchmark will ultimately enable researchers to improve their existing system models while comparing the performance results from both Sigfox and LoRaWAN networks!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; - Download the dataset from kaggle to get started. \n",
      "&gt; - Examine the data to better understand what is included in the set and how it is formatted. The dataset contains signal strength of BS 1-BS 84, RX Time, SF (Spreading Factor), HDOP (Horizontal Dilution of Precision) as well as Latitude and Longitude information for each location. \n",
      "&gt; - Data preparation is a necessary process prior to more extensive analysis of these data sets. This involves cleaning data by removing irrelevant variables that are not suitable for use in modeling or further processing, normalizing values so they can be used in comparison and sorting out any potential errors or inconsistencies that may exist in the data sets before getting started with your analysis. \n",
      "&gt; - Define measurable objectives that you would want to determine or learn from this dataset such as developing a model which accurately predicts locations associated with signals strengths collected across various Signal frequencies (SFs).   \n",
      "&gt; - Decide on an evaluation strategy such as using Mean-Squared Error (MSE) to measure performance of predicted hypothesized locations compared against observed longitudes/latitudes provided within this dataset before proceeding ahead into actual analysis phase \t  \n",
      "&gt; 6 In order to make best use of the given datasets here, it would be beneficial to apply separate algorithms separately onto them depending on their types - Regression algorithms should be used onto numerical variables like SF & HDOP while Classification models should employed onto categorical variables like BS 1- BS 84 . Find out about different Machine Learning techniques applied suitable for respective variable types and finalize an algorithm application list before moving onto implementation phase!     \t  \n",
      "&gt; 7 Once algorithms have been applied, validation checks need be done test the accuracy performance against objective within established criteria period -  once satisfied results can now safely interpreted accordingly towards decisions making exercise!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Comparing the performance of Sigfox and LoRaWAN networks in urban and rural environments to prioritize the most cost effective network for a given location.\n",
      "&gt; - Analyzing the Received Signal Strength Indicator (RSSI) of each received base station signal to evaluate the accuracy of fingerprinting localization algorithms in different types of large outdoor areas with various environmental features.\n",
      "&gt; - Utilizingthe Estimated Signal Power (ESP) and Signal-to-Noise Ratio (SNR) columns to further analyze how strength or noise affect signal integrity, thus maximizing accuracy when conducting finger printing localization testing in outdoor conditions\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://zenodo.org/record/3904158#.Y9ZAptJBwUE)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: sigfox_dataset_antwerp.csv**\n",
      "| Column name     | Description                                                                    |\n",
      "|:----------------|:-------------------------------------------------------------------------------|\n",
      "| **'BS 1'**      | Received Signal Strength Indicator (RSSI) value for Base Station 1. (Integer)  |\n",
      "| **'BS 2'**      | Received Signal Strength Indicator (RSSI) value for Base Station 2. (Integer)  |\n",
      "| **'BS 3'**      | Received Signal Strength Indicator (RSSI) value for Base Station 3. (Integer)  |\n",
      "| **'BS 4'**      | Received Signal Strength Indicator (RSSI) value for Base Station 4. (Integer)  |\n",
      "| **'BS 5'**      | Received Signal Strength Indicator (RSSI) value for Base Station 5. (Integer)  |\n",
      "| **'BS 6'**      | Received Signal Strength Indicator (RSSI) value for Base Station 6. (Integer)  |\n",
      "| **'BS 7'**      | Received Signal Strength Indicator (RSSI) value for Base Station 7. (Integer)  |\n",
      "| **'BS 8'**      | Received Signal Strength Indicator (RSSI) value for Base Station 8. (Integer)  |\n",
      "| **'BS 9'**      | Received Signal Strength Indicator (RSSI) value for Base Station 9. (Integer)  |\n",
      "| **'BS 10'**     | Received Signal Strength Indicator (RSSI) value for Base Station 10. (Integer) |\n",
      "| **'BS 11'**     | Received Signal Strength Indicator (RSSI) value for Base Station 11. (Integer) |\n",
      "| **'BS 12'**     | Received Signal Strength Indicator (RSSI) value for Base Station 12. (Integer) |\n",
      "| **'BS 13'**     | Received Signal Strength Indicator (RSSI) value for Base Station 13. (Integer) |\n",
      "| **'BS 14'**     | Received Signal Strength Indicator (RSSI) value for Base Station 14. (Integer) |\n",
      "| **'BS 15'**     | Received Signal Strength Indicator (RSSI) value for Base Station 15. (Integer) |\n",
      "| **'BS 16'**     | Received Signal Strength Indicator (RSSI) value for Base Station 16. (Integer) |\n",
      "| **'BS 17'**     | Received Signal Strength Indicator (RSSI) value for Base Station 17. (Integer) |\n",
      "| **'BS 18'**     | Received Signal Strength Indicator (RSSI) value for Base Station 18. (Integer) |\n",
      "| **'BS 19'**     | Received Signal Strength Indicator (RSSI) value for Base Station 19. (Integer) |\n",
      "| **'BS 21'**     | Received Signal Strength Indicator (RSSI) value for Base Station 21. (Integer) |\n",
      "| **'BS 22'**     | Received Signal Strength Indicator (RSSI) value for Base Station 22. (Integer) |\n",
      "| **'BS 23'**     | Received Signal Strength Indicator (RSSI) value for Base Station 23. (Integer) |\n",
      "| **'BS 24'**     | Received Signal Strength Indicator (RSSI) value for Base Station 24. (Integer) |\n",
      "| **'BS 25'**     | Received Signal Strength Indicator (RSSI) value for Base Station 25. (Integer) |\n",
      "| **'BS 26'**     | Received Signal Strength Indicator (RSSI) value for Base Station 26. (Integer) |\n",
      "| **'BS 27'**     | Received Signal Strength Indicator (RSSI) value for Base Station 27. (Integer) |\n",
      "| **'BS 28'**     | Received Signal Strength Indicator (RSSI) value for Base Station 28. (Integer) |\n",
      "| **'BS 29'**     | Received Signal Strength Indicator (RSSI) value for Base Station 29. (Integer) |\n",
      "| **'BS 30'**     | Received Signal Strength Indicator (RSSI) value for Base Station 30. (Integer) |\n",
      "| **'BS 31'**     | Received Signal Strength Indicator (RSSI) value for Base Station 31. (Integer) |\n",
      "| **'BS 32'**     | Received Signal Strength Indicator (RSSI) value for Base Station 32. (Integer) |\n",
      "| **'BS 33'**     | Received Signal Strength Indicator (RSSI) value for Base Station 33. (Integer) |\n",
      "| **'BS 34'**     | Received Signal Strength Indicator (RSSI) value for Base Station 34. (Integer) |\n",
      "| **'BS 35'**     | Received Signal Strength Indicator (RSSI) value for Base Station 35. (Integer) |\n",
      "| **'BS 36'**     | Received Signal Strength Indicator (RSSI) value for Base Station 36. (Integer) |\n",
      "| **'BS 37'**     | Received Signal Strength Indicator (RSSI) value for Base Station 37. (Integer) |\n",
      "| **'BS 38'**     | Received Signal Strength Indicator (RSSI) value for Base Station 38. (Integer) |\n",
      "| **'BS 39'**     | Received Signal                                                                |\n",
      "| **'BS 41'**     | Received Signal Strength Indicator (RSSI) value for Base Station 41. (Integer) |\n",
      "| **'BS 42'**     | Received Signal Strength Indicator (RSSI) value for Base Station 42. (Integer) |\n",
      "| **'BS 43'**     | Received Signal Strength Indicator (RSSI) value for Base Station 43. (Integer) |\n",
      "| **'BS 44'**     | Received Signal Strength Indicator (RSSI) value for Base Station 44. (Integer) |\n",
      "| **'BS 45'**     | Received Signal Strength Indicator (RSSI) value for Base Station 45. (Integer) |\n",
      "| **'BS 46'**     | Received Signal Strength Indicator (RSSI) value for Base Station 46. (Integer) |\n",
      "| **'BS 47'**     | Received Signal Strength Indicator (RSSI) value for Base Station 47. (Integer) |\n",
      "| **'BS 48'**     | Received Signal Strength Indicator (RSSI) value for Base Station 48. (Integer) |\n",
      "| **'BS 49'**     | Received Signal Strength Indicator (RSSI) value for Base Station 49. (Integer) |\n",
      "| **'BS 50'**     | Received Signal Strength Indicator (RSSI) value for Base Station 50. (Integer) |\n",
      "| **'BS 51'**     | Received Signal Strength Indicator (RSSI) value for Base Station 51. (Integer) |\n",
      "| **'BS 52'**     | Received Signal Strength Indicator (RSSI) value for Base Station 52. (Integer) |\n",
      "| **'BS 53'**     | Received Signal Strength Indicator (RSSI) value for Base Station 53. (Integer) |\n",
      "| **'BS 54'**     | Received Signal Strength Indicator (RSSI) value for Base Station 54. (Integer) |\n",
      "| **'BS 55'**     | Received Signal Strength Indicator (RSSI) value for Base Station 55. (Integer) |\n",
      "| **'BS 56'**     | Received Signal Strength Indicator (RSSI) value for Base Station 56. (Integer) |\n",
      "| **'BS 57'**     | Received Signal Strength Indicator (RSSI) value for Base Station 57. (Integer) |\n",
      "| **'BS 58'**     | Received Signal Strength Indicator (RSSI) value for Base Station 58. (Integer) |\n",
      "| **'BS 59'**     | Received Signal                                                                |\n",
      "| **'BS 61'**     | Received Signal Strength Indicator (RSSI) value for Base Station 61. (Integer) |\n",
      "| **'BS 62'**     | Received Signal Strength Indicator (RSSI) value for Base Station 62. (Integer) |\n",
      "| **'BS 63'**     | Received Signal Strength Indicator (RSSI) value for Base Station 63. (Integer) |\n",
      "| **'BS 64'**     | Received Signal Strength Indicator (RSSI) value for Base Station 64. (Integer) |\n",
      "| **'BS 65'**     | Received Signal Strength Indicator (RSSI) value for Base Station 65. (Integer) |\n",
      "| **'BS 66'**     | Received Signal Strength Indicator (RSSI) value for Base Station 66. (Integer) |\n",
      "| **'BS 67'**     | Received Signal Strength Indicator (RSSI) value for Base Station 67. (Integer) |\n",
      "| **'BS 68'**     | Received Signal Strength Indicator (RSSI) value for Base Station 68. (Integer) |\n",
      "| **'BS 69'**     | Received Signal Strength Indicator (RSSI) value for Base Station 69. (Integer) |\n",
      "| **'BS 70'**     | Received Signal Strength Indicator (RSSI) value for Base Station 70. (Integer) |\n",
      "| **'BS 71'**     | Received Signal Strength Indicator (RSSI) value for Base Station 71. (Integer) |\n",
      "| **'BS 72'**     | Received Signal Strength Indicator (RSSI) value for Base Station 72. (Integer) |\n",
      "| **'BS 73'**     | Received Signal Strength Indicator (RSSI) value for Base Station 73. (Integer) |\n",
      "| **'BS 74'**     | Received Signal Strength Indicator (RSSI) value for Base Station 74. (Integer) |\n",
      "| **'BS 75'**     | Received Signal Strength Indicator (RSSI) value for Base Station 75. (Integer) |\n",
      "| **'BS 76'**     | Received Signal Strength Indicator (RSSI) value for Base Station 76. (Integer) |\n",
      "| **'BS 77'**     | Received Signal Strength Indicator (RSSI) value for Base Station 77. (Integer) |\n",
      "| **'BS 78'**     | Received Signal Strength Indicator (RSSI) value for Base Station 78. (Integer) |\n",
      "| **'BS 79'**     | Received Signal                                                                |\n",
      "| **'BS 81'**     | Received Signal Strength Indicator (RSSI) value for Base Station 81. (Integer) |\n",
      "| **'BS 82'**     | Received Signal Strength Indicator (RSSI) value for Base Station 82. (Integer) |\n",
      "| **'BS 83'**     | Received Signal Strength Indicator (RSSI) value for Base Station 83. (Integer) |\n",
      "| **'BS 84'**     | Received Signal Strength Indicator (RSSI) value for Base Station 84. (Integer) |\n",
      "| **'RX Time'**   | Time of message reception. (DateTime)                                          |\n",
      "| **'Latitude'**  | Latitude coordinate of the LoRaWAN gateway. (Float)                            |\n",
      "| **'Longitude'** | Longitude coordinate of the LoRaWAN gateway. (Float)                           |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: lorawan_antwerp_2019_dataset.csv**\n",
      "| Column name   | Description                                                                    |\n",
      "|:--------------|:-------------------------------------------------------------------------------|\n",
      "| **BS 1**      | Received Signal Strength Indicator (RSSI) value for Base Station 1. (Integer)  |\n",
      "| **BS 2**      | Received Signal Strength Indicator (RSSI) value for Base Station 2. (Integer)  |\n",
      "| **BS 3**      | Received Signal Strength Indicator (RSSI) value for Base Station 3. (Integer)  |\n",
      "| **BS 4**      | Received Signal Strength Indicator (RSSI) value for Base Station 4. (Integer)  |\n",
      "| **BS 5**      | Received Signal Strength Indicator (RSSI) value for Base Station 5. (Integer)  |\n",
      "| **BS 6**      | Received Signal Strength Indicator (RSSI) value for Base Station 6. (Integer)  |\n",
      "| **BS 7**      | Received Signal Strength Indicator (RSSI) value for Base Station 7. (Integer)  |\n",
      "| **BS 8**      | Received Signal Strength Indicator (RSSI) value for Base Station 8. (Integer)  |\n",
      "| **BS 9**      | Received Signal Strength Indicator (RSSI) value for Base Station 9. (Integer)  |\n",
      "| **BS 10**     | Received Signal Strength Indicator (RSSI) value for Base Station 10. (Integer) |\n",
      "| **BS 11**     | Received Signal Strength Indicator (RSSI) value for Base Station 11. (Integer) |\n",
      "| **BS 12**     | Received Signal Strength Indicator (RSSI) value for Base Station 12. (Integer) |\n",
      "| **BS 13**     | Received Signal Strength Indicator (RSSI) value for Base Station 13. (Integer) |\n",
      "| **BS 14**     | Received Signal Strength Indicator (RSSI) value for Base Station 14. (Integer) |\n",
      "| **BS 15**     | Received Signal Strength Indicator (RSSI) value for Base Station 15. (Integer) |\n",
      "| **BS 16**     | Received Signal Strength Indicator (RSSI) value for Base Station 16. (Integer) |\n",
      "| **BS 17**     | Received Signal Strength Indicator (RSSI) value for Base Station 17. (Integer) |\n",
      "| **BS 18**     | Received Signal Strength Indicator (RSSI) value for Base Station 18. (Integer) |\n",
      "| **BS 19**     | Received Signal Strength Indicator (RSSI) value for Base Station 19. (Integer) |\n",
      "| **BS 20**     | Received Signal Strength Indicator (RSSI) value for                            |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/3904158#.Y9ZAptJBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sigfox-and-lorawan-localization-tool\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Sigfox and LoRaWAN Localization Tool\n",
      "name 'detect' is not defined Language is not detected: Evaluating Fingerprinting Localization Algorithms in Large Outdoor Areas\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Sigfox and LoRaWAN Localization Tool\n",
      "### Evaluating Fingerprinting Localization Algorithms in Large Outdoor Areas\n",
      "By  [[source]](https://zenodo.org/record/3904158#.Y9ZAptJBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset offers a powerful benchmark to evaluate the performance and accuracy of finger printing localization algorithms in a range of outdoor environments. Featuring 130,430 LoRaWAN messages collected in the city center of Antwerp, 14,378 Sigfox messages recorded in Antwerp city center and 25,638 Sigfox messages collected between Antwerp and Ghent rural regions. The data is presented as an extensive collection of metadata including receiving time stamps, base station identifiers and Received Signal Strength Indicators (RSSI). To offer different perspectives on analysis possibilities this dataset also features Signal-to-Noise Ratio (SNR) for all signals along with Estimated Signal Power (ESP) metrics. In addition to that the coordinates for all LoRaWAN gateways in Antwerp are pinned within the data set opening up potential opportunities for cross comparison between urban and rural environments. Analyzing this data will provide details on current testing practices for fingerprinting algorithms allowing further insight into how robust these methods can be across diverse conditions ranging from metropolitan city centers to rural areas. This invaluable benchmark will ultimately enable researchers to improve their existing system models while comparing the performance results from both Sigfox and LoRaWAN networks!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; - Download the dataset from kaggle to get started. \n",
      "&gt; - Examine the data to better understand what is included in the set and how it is formatted. The dataset contains signal strength of BS 1-BS 84, RX Time, SF (Spreading Factor), HDOP (Horizontal Dilution of Precision) as well as Latitude and Longitude information for each location. \n",
      "&gt; - Data preparation is a necessary process prior to more extensive analysis of these data sets. This involves cleaning data by removing irrelevant variables that are not suitable for use in modeling or further processing, normalizing values so they can be used in comparison and sorting out any potential errors or inconsistencies that may exist in the data sets before getting started with your analysis. \n",
      "&gt; - Define measurable objectives that you would want to determine or learn from this dataset such as developing a model which accurately predicts locations associated with signals strengths collected across various Signal frequencies (SFs).   \n",
      "&gt; - Decide on an evaluation strategy such as using Mean-Squared Error (MSE) to measure performance of predicted hypothesized locations compared against observed longitudes/latitudes provided within this dataset before proceeding ahead into actual analysis phase \t  \n",
      "&gt; 6 In order to make best use of the given datasets here, it would be beneficial to apply separate algorithms separately onto them depending on their types - Regression algorithms should be used onto numerical variables like SF & HDOP while Classification models should employed onto categorical variables like BS 1- BS 84 . Find out about different Machine Learning techniques applied suitable for respective variable types and finalize an algorithm application list before moving onto implementation phase!     \t  \n",
      "&gt; 7 Once algorithms have been applied, validation checks need be done test the accuracy performance against objective within established criteria period -  once satisfied results can now safely interpreted accordingly towards decisions making exercise!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Comparing the performance of Sigfox and LoRaWAN networks in urban and rural environments to prioritize the most cost effective network for a given location.\n",
      "&gt; - Analyzing the Received Signal Strength Indicator (RSSI) of each received base station signal to evaluate the accuracy of fingerprinting localization algorithms in different types of large outdoor areas with various environmental features.\n",
      "&gt; - Utilizingthe Estimated Signal Power (ESP) and Signal-to-Noise Ratio (SNR) columns to further analyze how strength or noise affect signal integrity, thus maximizing accuracy when conducting finger printing localization testing in outdoor conditions\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://zenodo.org/record/3904158#.Y9ZAptJBwUE)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: sigfox_dataset_antwerp.csv**\n",
      "| Column name     | Description                                                                    |\n",
      "|:----------------|:-------------------------------------------------------------------------------|\n",
      "| **'BS 1'**      | Received Signal Strength Indicator (RSSI) value for Base Station 1. (Integer)  |\n",
      "| **'BS 2'**      | Received Signal Strength Indicator (RSSI) value for Base Station 2. (Integer)  |\n",
      "| **'BS 3'**      | Received Signal Strength Indicator (RSSI) value for Base Station 3. (Integer)  |\n",
      "| **'BS 4'**      | Received Signal Strength Indicator (RSSI) value for Base Station 4. (Integer)  |\n",
      "| **'BS 5'**      | Received Signal Strength Indicator (RSSI) value for Base Station 5. (Integer)  |\n",
      "| **'BS 6'**      | Received Signal Strength Indicator (RSSI) value for Base Station 6. (Integer)  |\n",
      "| **'BS 7'**      | Received Signal Strength Indicator (RSSI) value for Base Station 7. (Integer)  |\n",
      "| **'BS 8'**      | Received Signal Strength Indicator (RSSI) value for Base Station 8. (Integer)  |\n",
      "| **'BS 9'**      | Received Signal Strength Indicator (RSSI) value for Base Station 9. (Integer)  |\n",
      "| **'BS 10'**     | Received Signal Strength Indicator (RSSI) value for Base Station 10. (Integer) |\n",
      "| **'BS 11'**     | Received Signal Strength Indicator (RSSI) value for Base Station 11. (Integer) |\n",
      "| **'BS 12'**     | Received Signal Strength Indicator (RSSI) value for Base Station 12. (Integer) |\n",
      "| **'BS 13'**     | Received Signal Strength Indicator (RSSI) value for Base Station 13. (Integer) |\n",
      "| **'BS 14'**     | Received Signal Strength Indicator (RSSI) value for Base Station 14. (Integer) |\n",
      "| **'BS 15'**     | Received Signal Strength Indicator (RSSI) value for Base Station 15. (Integer) |\n",
      "| **'BS 16'**     | Received Signal Strength Indicator (RSSI) value for Base Station 16. (Integer) |\n",
      "| **'BS 17'**     | Received Signal Strength Indicator (RSSI) value for Base Station 17. (Integer) |\n",
      "| **'BS 18'**     | Received Signal Strength Indicator (RSSI) value for Base Station 18. (Integer) |\n",
      "| **'BS 19'**     | Received Signal Strength Indicator (RSSI) value for Base Station 19. (Integer) |\n",
      "| **'BS 21'**     | Received Signal Strength Indicator (RSSI) value for Base Station 21. (Integer) |\n",
      "| **'BS 22'**     | Received Signal Strength Indicator (RSSI) value for Base Station 22. (Integer) |\n",
      "| **'BS 23'**     | Received Signal Strength Indicator (RSSI) value for Base Station 23. (Integer) |\n",
      "| **'BS 24'**     | Received Signal Strength Indicator (RSSI) value for Base Station 24. (Integer) |\n",
      "| **'BS 25'**     | Received Signal Strength Indicator (RSSI) value for Base Station 25. (Integer) |\n",
      "| **'BS 26'**     | Received Signal Strength Indicator (RSSI) value for Base Station 26. (Integer) |\n",
      "| **'BS 27'**     | Received Signal Strength Indicator (RSSI) value for Base Station 27. (Integer) |\n",
      "| **'BS 28'**     | Received Signal Strength Indicator (RSSI) value for Base Station 28. (Integer) |\n",
      "| **'BS 29'**     | Received Signal Strength Indicator (RSSI) value for Base Station 29. (Integer) |\n",
      "| **'BS 30'**     | Received Signal Strength Indicator (RSSI) value for Base Station 30. (Integer) |\n",
      "| **'BS 31'**     | Received Signal Strength Indicator (RSSI) value for Base Station 31. (Integer) |\n",
      "| **'BS 32'**     | Received Signal Strength Indicator (RSSI) value for Base Station 32. (Integer) |\n",
      "| **'BS 33'**     | Received Signal Strength Indicator (RSSI) value for Base Station 33. (Integer) |\n",
      "| **'BS 34'**     | Received Signal Strength Indicator (RSSI) value for Base Station 34. (Integer) |\n",
      "| **'BS 35'**     | Received Signal Strength Indicator (RSSI) value for Base Station 35. (Integer) |\n",
      "| **'BS 36'**     | Received Signal Strength Indicator (RSSI) value for Base Station 36. (Integer) |\n",
      "| **'BS 37'**     | Received Signal Strength Indicator (RSSI) value for Base Station 37. (Integer) |\n",
      "| **'BS 38'**     | Received Signal Strength Indicator (RSSI) value for Base Station 38. (Integer) |\n",
      "| **'BS 39'**     | Received Signal                                                                |\n",
      "| **'BS 41'**     | Received Signal Strength Indicator (RSSI) value for Base Station 41. (Integer) |\n",
      "| **'BS 42'**     | Received Signal Strength Indicator (RSSI) value for Base Station 42. (Integer) |\n",
      "| **'BS 43'**     | Received Signal Strength Indicator (RSSI) value for Base Station 43. (Integer) |\n",
      "| **'BS 44'**     | Received Signal Strength Indicator (RSSI) value for Base Station 44. (Integer) |\n",
      "| **'BS 45'**     | Received Signal Strength Indicator (RSSI) value for Base Station 45. (Integer) |\n",
      "| **'BS 46'**     | Received Signal Strength Indicator (RSSI) value for Base Station 46. (Integer) |\n",
      "| **'BS 47'**     | Received Signal Strength Indicator (RSSI) value for Base Station 47. (Integer) |\n",
      "| **'BS 48'**     | Received Signal Strength Indicator (RSSI) value for Base Station 48. (Integer) |\n",
      "| **'BS 49'**     | Received Signal Strength Indicator (RSSI) value for Base Station 49. (Integer) |\n",
      "| **'BS 50'**     | Received Signal Strength Indicator (RSSI) value for Base Station 50. (Integer) |\n",
      "| **'BS 51'**     | Received Signal Strength Indicator (RSSI) value for Base Station 51. (Integer) |\n",
      "| **'BS 52'**     | Received Signal Strength Indicator (RSSI) value for Base Station 52. (Integer) |\n",
      "| **'BS 53'**     | Received Signal Strength Indicator (RSSI) value for Base Station 53. (Integer) |\n",
      "| **'BS 54'**     | Received Signal Strength Indicator (RSSI) value for Base Station 54. (Integer) |\n",
      "| **'BS 55'**     | Received Signal Strength Indicator (RSSI) value for Base Station 55. (Integer) |\n",
      "| **'BS 56'**     | Received Signal Strength Indicator (RSSI) value for Base Station 56. (Integer) |\n",
      "| **'BS 57'**     | Received Signal Strength Indicator (RSSI) value for Base Station 57. (Integer) |\n",
      "| **'BS 58'**     | Received Signal Strength Indicator (RSSI) value for Base Station 58. (Integer) |\n",
      "| **'BS 59'**     | Received Signal                                                                |\n",
      "| **'BS 61'**     | Received Signal Strength Indicator (RSSI) value for Base Station 61. (Integer) |\n",
      "| **'BS 62'**     | Received Signal Strength Indicator (RSSI) value for Base Station 62. (Integer) |\n",
      "| **'BS 63'**     | Received Signal Strength Indicator (RSSI) value for Base Station 63. (Integer) |\n",
      "| **'BS 64'**     | Received Signal Strength Indicator (RSSI) value for Base Station 64. (Integer) |\n",
      "| **'BS 65'**     | Received Signal Strength Indicator (RSSI) value for Base Station 65. (Integer) |\n",
      "| **'BS 66'**     | Received Signal Strength Indicator (RSSI) value for Base Station 66. (Integer) |\n",
      "| **'BS 67'**     | Received Signal Strength Indicator (RSSI) value for Base Station 67. (Integer) |\n",
      "| **'BS 68'**     | Received Signal Strength Indicator (RSSI) value for Base Station 68. (Integer) |\n",
      "| **'BS 69'**     | Received Signal Strength Indicator (RSSI) value for Base Station 69. (Integer) |\n",
      "| **'BS 70'**     | Received Signal Strength Indicator (RSSI) value for Base Station 70. (Integer) |\n",
      "| **'BS 71'**     | Received Signal Strength Indicator (RSSI) value for Base Station 71. (Integer) |\n",
      "| **'BS 72'**     | Received Signal Strength Indicator (RSSI) value for Base Station 72. (Integer) |\n",
      "| **'BS 73'**     | Received Signal Strength Indicator (RSSI) value for Base Station 73. (Integer) |\n",
      "| **'BS 74'**     | Received Signal Strength Indicator (RSSI) value for Base Station 74. (Integer) |\n",
      "| **'BS 75'**     | Received Signal Strength Indicator (RSSI) value for Base Station 75. (Integer) |\n",
      "| **'BS 76'**     | Received Signal Strength Indicator (RSSI) value for Base Station 76. (Integer) |\n",
      "| **'BS 77'**     | Received Signal Strength Indicator (RSSI) value for Base Station 77. (Integer) |\n",
      "| **'BS 78'**     | Received Signal Strength Indicator (RSSI) value for Base Station 78. (Integer) |\n",
      "| **'BS 79'**     | Received Signal                                                                |\n",
      "| **'BS 81'**     | Received Signal Strength Indicator (RSSI) value for Base Station 81. (Integer) |\n",
      "| **'BS 82'**     | Received Signal Strength Indicator (RSSI) value for Base Station 82. (Integer) |\n",
      "| **'BS 83'**     | Received Signal Strength Indicator (RSSI) value for Base Station 83. (Integer) |\n",
      "| **'BS 84'**     | Received Signal Strength Indicator (RSSI) value for Base Station 84. (Integer) |\n",
      "| **'RX Time'**   | Time of message reception. (DateTime)                                          |\n",
      "| **'Latitude'**  | Latitude coordinate of the LoRaWAN gateway. (Float)                            |\n",
      "| **'Longitude'** | Longitude coordinate of the LoRaWAN gateway. (Float)                           |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: lorawan_antwerp_2019_dataset.csv**\n",
      "| Column name   | Description                                                                    |\n",
      "|:--------------|:-------------------------------------------------------------------------------|\n",
      "| **BS 1**      | Received Signal Strength Indicator (RSSI) value for Base Station 1. (Integer)  |\n",
      "| **BS 2**      | Received Signal Strength Indicator (RSSI) value for Base Station 2. (Integer)  |\n",
      "| **BS 3**      | Received Signal Strength Indicator (RSSI) value for Base Station 3. (Integer)  |\n",
      "| **BS 4**      | Received Signal Strength Indicator (RSSI) value for Base Station 4. (Integer)  |\n",
      "| **BS 5**      | Received Signal Strength Indicator (RSSI) value for Base Station 5. (Integer)  |\n",
      "| **BS 6**      | Received Signal Strength Indicator (RSSI) value for Base Station 6. (Integer)  |\n",
      "| **BS 7**      | Received Signal Strength Indicator (RSSI) value for Base Station 7. (Integer)  |\n",
      "| **BS 8**      | Received Signal Strength Indicator (RSSI) value for Base Station 8. (Integer)  |\n",
      "| **BS 9**      | Received Signal Strength Indicator (RSSI) value for Base Station 9. (Integer)  |\n",
      "| **BS 10**     | Received Signal Strength Indicator (RSSI) value for Base Station 10. (Integer) |\n",
      "| **BS 11**     | Received Signal Strength Indicator (RSSI) value for Base Station 11. (Integer) |\n",
      "| **BS 12**     | Received Signal Strength Indicator (RSSI) value for Base Station 12. (Integer) |\n",
      "| **BS 13**     | Received Signal Strength Indicator (RSSI) value for Base Station 13. (Integer) |\n",
      "| **BS 14**     | Received Signal Strength Indicator (RSSI) value for Base Station 14. (Integer) |\n",
      "| **BS 15**     | Received Signal Strength Indicator (RSSI) value for Base Station 15. (Integer) |\n",
      "| **BS 16**     | Received Signal Strength Indicator (RSSI) value for Base Station 16. (Integer) |\n",
      "| **BS 17**     | Received Signal Strength Indicator (RSSI) value for Base Station 17. (Integer) |\n",
      "| **BS 18**     | Received Signal Strength Indicator (RSSI) value for Base Station 18. (Integer) |\n",
      "| **BS 19**     | Received Signal Strength Indicator (RSSI) value for Base Station 19. (Integer) |\n",
      "| **BS 20**     | Received Signal Strength Indicator (RSSI) value for                            |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/3904158#.Y9ZAptJBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/imdb-movie-review-sentiment-dataset\n",
      "name 'detect' is not defined Language is not detected: imdb-movie-review-sentiment-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: IMDb Movie Review Sentiment \n",
      "name 'detect' is not defined Language is not detected: Movie Review Sentiment \n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# IMDb Movie Review Sentiment \n",
      "### Movie Review Sentiment \n",
      "By imdb (From Huggingface) [[source]](https://huggingface.co/datasets/imdb)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The IMDb Large Movie Review Dataset is a comprehensive collection of movie reviews used for sentiment classification. The dataset includes a wide range of movie reviews along with their corresponding sentiment labels, which indicate whether the review is positive or negative in nature. This invaluable dataset is aimed at facilitating sentiment analysis and classification tasks in the field of natural language processing.\n",
      "&gt; \n",
      "&gt; The main purpose of the train.csv file within this dataset is to provide a curated collection of movie reviews, each accompanied by its respective sentiment label. This file proves particularly useful for training machine learning models to accurately predict sentiment and classify reviews based on their emotional tone.\n",
      "&gt; \n",
      "&gt; Similarly, the test.csv file contains another set of movie reviews along with corresponding sentiment labels. Meant for testing and validating the performance of trained models, this dataset enables researchers and developers to evaluate their models' effectiveness in real-world scenarios.\n",
      "&gt; \n",
      "&gt; Additionally, the unsupervised.csv file offers an alternative subset within the dataset. Unlike train.csv and test.csv, unsupervised.csv does not include any associated sentiment labels for individual movie reviews. This specific subset serves as a valuable resource for exploring unsupervised learning techniques within the domain of sentiment classification.\n",
      "&gt; \n",
      "&gt; By utilizing this meticulously compiled IMDb Large Movie Review Dataset, researchers and data scientists can delve into various aspects related to analyzing sentiments in textual data. With its carefully labeled data points covering both positive and negative sentiments expressed in diverse film critiques, this dataset empowers users to develop sophisticated machine learning algorithms that accurately assess subjective opinions from text data\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; Introduction:\n",
      "&gt; \n",
      "&gt; Dataset Overview:\n",
      "&gt; - Train.csv: This file contains a set of movie reviews along with their sentiment labels. It is intended for training your sentiment analysis models.\n",
      "&gt; - Test.csv: This file provides another set of movie reviews along with their corresponding sentiment labels. You can use this file to evaluate the performance of your trained models.\n",
      "&gt; - Unsupervised.csv: This file includes movie reviews without any associated sentiment labels. It can be used for unsupervised sentiment classification tasks.\n",
      "&gt; \n",
      "&gt; Columns in the Dataset:\n",
      "&gt; - text: The main column containing the text of each movie review.\n",
      "&gt; - label: The sentiment label assigned to each review, indicating whether it is positive or negative.\n",
      "&gt; \n",
      "&gt; Guidelines for Using the Dataset:\n",
      "&gt; \n",
      "&gt; - Training Your Model:\n",
      "&gt;    - Begin by loading and preprocessing the data from train.csv\n",
      "&gt;    - Treat 'text' as your input feature and 'label' as your target variable\n",
      "&gt;    - Explore different machine learning or deep learning algorithms suitable for text classification\n",
      "&gt;    - Train your model using various techniques, such as bag-of-words, word embeddings, or transformers\n",
      "&gt;    - Evaluate and fine-tune your model's performance using test.csv\n",
      "&gt; \n",
      "&gt; - Evaluating Your Model:\n",
      "&gt;    - Load test.csv and preprocess the data similar to what you did with train.csv\n",
      "&gt;    - Use this preprocessed test data to evaluate the accuracy, precision, recall, F1 score or other relevant metrics of your trained model on unseen data\n",
      "&gt;    - Analyze these metrics to understand how well your model is performing in predicting sentiments\n",
      "&gt; \n",
      "&gt; - Advancing Your Model (Unsupervised Classification):\n",
      "&gt;    - Utilize unsupervised.csv for unsupervised sentiment classification tasks\n",
      "&gt;    - Preprocess the movie reviews in this file and explore techniques like clustering, topic modeling, or self-supervised learning\n",
      "&gt;    - Extract patterns, themes, or sentiments from the reviews without any guidance from labeled data\n",
      "&gt; \n",
      "&gt; Conclusion:\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Sentiment Analysis: This dataset can be used to train models for sentiment analysis, where the goal is to predict whether a movie review is positive or negative based on its text.\n",
      "&gt; - NLP Research: The dataset can be used for various natural language processing (NLP) tasks such as text classification, information extraction, or named entity recognition. Researchers and practitioners can leverage this dataset to develop and evaluate new algorithms and techniques in the field of NLP.\n",
      "&gt; - Recommendation Systems: The sentiment labels in this dataset can be used as a source of feedback or user preferences for recommendation systems. By analyzing the sentiments expressed in reviews, recommendation algorithms can better understand users' tastes and preferences to provide more personalized recommendations\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/imdb)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: test.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: unsupervised.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [imdb (From Huggingface)](https://huggingface.co/datasets/imdb).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: imdb-movie-review-sentiment-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: IMDb Movie Review Sentiment \n",
      "name 'detect' is not defined Language is not detected: Movie Review Sentiment \n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# IMDb Movie Review Sentiment \n",
      "### Movie Review Sentiment \n",
      "By imdb (From Huggingface) [[source]](https://huggingface.co/datasets/imdb)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The IMDb Large Movie Review Dataset is a comprehensive collection of movie reviews used for sentiment classification. The dataset includes a wide range of movie reviews along with their corresponding sentiment labels, which indicate whether the review is positive or negative in nature. This invaluable dataset is aimed at facilitating sentiment analysis and classification tasks in the field of natural language processing.\n",
      "&gt; \n",
      "&gt; The main purpose of the train.csv file within this dataset is to provide a curated collection of movie reviews, each accompanied by its respective sentiment label. This file proves particularly useful for training machine learning models to accurately predict sentiment and classify reviews based on their emotional tone.\n",
      "&gt; \n",
      "&gt; Similarly, the test.csv file contains another set of movie reviews along with corresponding sentiment labels. Meant for testing and validating the performance of trained models, this dataset enables researchers and developers to evaluate their models' effectiveness in real-world scenarios.\n",
      "&gt; \n",
      "&gt; Additionally, the unsupervised.csv file offers an alternative subset within the dataset. Unlike train.csv and test.csv, unsupervised.csv does not include any associated sentiment labels for individual movie reviews. This specific subset serves as a valuable resource for exploring unsupervised learning techniques within the domain of sentiment classification.\n",
      "&gt; \n",
      "&gt; By utilizing this meticulously compiled IMDb Large Movie Review Dataset, researchers and data scientists can delve into various aspects related to analyzing sentiments in textual data. With its carefully labeled data points covering both positive and negative sentiments expressed in diverse film critiques, this dataset empowers users to develop sophisticated machine learning algorithms that accurately assess subjective opinions from text data\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; Introduction:\n",
      "&gt; \n",
      "&gt; Dataset Overview:\n",
      "&gt; - Train.csv: This file contains a set of movie reviews along with their sentiment labels. It is intended for training your sentiment analysis models.\n",
      "&gt; - Test.csv: This file provides another set of movie reviews along with their corresponding sentiment labels. You can use this file to evaluate the performance of your trained models.\n",
      "&gt; - Unsupervised.csv: This file includes movie reviews without any associated sentiment labels. It can be used for unsupervised sentiment classification tasks.\n",
      "&gt; \n",
      "&gt; Columns in the Dataset:\n",
      "&gt; - text: The main column containing the text of each movie review.\n",
      "&gt; - label: The sentiment label assigned to each review, indicating whether it is positive or negative.\n",
      "&gt; \n",
      "&gt; Guidelines for Using the Dataset:\n",
      "&gt; \n",
      "&gt; - Training Your Model:\n",
      "&gt;    - Begin by loading and preprocessing the data from train.csv\n",
      "&gt;    - Treat 'text' as your input feature and 'label' as your target variable\n",
      "&gt;    - Explore different machine learning or deep learning algorithms suitable for text classification\n",
      "&gt;    - Train your model using various techniques, such as bag-of-words, word embeddings, or transformers\n",
      "&gt;    - Evaluate and fine-tune your model's performance using test.csv\n",
      "&gt; \n",
      "&gt; - Evaluating Your Model:\n",
      "&gt;    - Load test.csv and preprocess the data similar to what you did with train.csv\n",
      "&gt;    - Use this preprocessed test data to evaluate the accuracy, precision, recall, F1 score or other relevant metrics of your trained model on unseen data\n",
      "&gt;    - Analyze these metrics to understand how well your model is performing in predicting sentiments\n",
      "&gt; \n",
      "&gt; - Advancing Your Model (Unsupervised Classification):\n",
      "&gt;    - Utilize unsupervised.csv for unsupervised sentiment classification tasks\n",
      "&gt;    - Preprocess the movie reviews in this file and explore techniques like clustering, topic modeling, or self-supervised learning\n",
      "&gt;    - Extract patterns, themes, or sentiments from the reviews without any guidance from labeled data\n",
      "&gt; \n",
      "&gt; Conclusion:\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Sentiment Analysis: This dataset can be used to train models for sentiment analysis, where the goal is to predict whether a movie review is positive or negative based on its text.\n",
      "&gt; - NLP Research: The dataset can be used for various natural language processing (NLP) tasks such as text classification, information extraction, or named entity recognition. Researchers and practitioners can leverage this dataset to develop and evaluate new algorithms and techniques in the field of NLP.\n",
      "&gt; - Recommendation Systems: The sentiment labels in this dataset can be used as a source of feedback or user preferences for recommendation systems. By analyzing the sentiments expressed in reviews, recommendation algorithms can better understand users' tastes and preferences to provide more personalized recommendations\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/imdb)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: test.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: unsupervised.csv**\n",
      "| Column name   | Description                                                                                                                                    |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **text**      | The actual text content of each movie review. (Text)                                                                                           |\n",
      "| **label**     | Indicates whether a review has positive or negative sentiment. It is categorical and can have two values (positive or negative). (Categorical) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [imdb (From Huggingface)](https://huggingface.co/datasets/imdb).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/comprehensive-literary-greats-dataset\n",
      "name 'detect' is not defined Language is not detected: comprehensive-literary-greats-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Comprehensive Literary Greats Dataset\n",
      "name 'detect' is not defined Language is not detected: 50,000+ Books Rated and Awarded Across Language, Genre, and Format\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Comprehensive Literary Greats Dataset\n",
      "### 50,000+ Books Rated and Awarded Across Language, Genre, and Format\n",
      "By  [[source]](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This remarkable dataset provides an awe-inspiring collection of over 50,000 books, encompassing the world's best practices in literature, poetry, and authorship. For each book included in the dataset, users can gain access to a wealth of insightful information such as title, author(s), average rating given by readers and critics alike, a brief description highlighting its plot or characteristics; language it is written in; unique ISBN which enables potential buyers to locate their favorite works with ease; genres it belongs to; any awards it has won or characters that inhabit its storyworld. \n",
      "> \n",
      "> Additionally, seeking out readers' opinions on exceptional books is made easier due to the availability of bbeScore (best books ever score) alongside details for the most accurate ratings given through well-detailed breakdowns in “ratingsByStars” section. Making sure visibility and recognition are granted fairly – be it a classic novel from time immemorial or merely recently released newcomers - this source also allows us to evaluate new stories based off readers' engagement rate highlighted by likedPercent column (the percentage of readers who liked the book), bbeVotes (number of votes casted) as well as entries related to date published - including showstopping firstPublishDate! \n",
      "> \n",
      "> Aspiring literature researchers; literary historians and those seeking hidden literary gems alike would no doubt benefit from delving into this magnificent collection – 25 variables regarding different novels & poets that are presented by Kaggle open source dataset “Best Books Ever: A Comprehensive Historical Collection of Literary Greats”. What worlds awaits you?\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> Whether you are a student, researcher, or enthusiast of literature, this dataset provides a valuable source for exploring literary works from varied time periods and genres. By accessing all 25 variables in the dataset, readers have the opportunity to use them for building visualizations, creating new analysis tools and models, or finding books you might be interested in reading. \n",
      "> \n",
      "> First after downloading the dataset into Kaggle Notebooks platform or other programming interfaces of your choice such as R Studio/Python Jupyter Notebooks (Pandas) - make sure that data is arranged into columns with clearly labeled title names. This will help you understand which variable is related to what precise information. Afterwards explore each variable by finding any patterns across particular titles or interesting findings about certain authors/ratings that are available in your research interests.  \n",
      "> \n",
      "> Utilize the vital columns of Title (title), Author(author), Rating (rating), Description (description), Language (language), Genres (genres) and Characters(characters) - these can assist you in discovering different trends between books according to style of composition or character types etc. Move further down on examining more specific details offered by Book Format(bookFormat), Edition(edition) Pages(pages). Peruse publisher info along with Publish Date(publishDate). Besides these structural elements also take note of Awards column considering recent recognition different titles have received; also observe how much ratings has been collected per text through Numbers Ratings column-(numRatings); analyze reader's feedback according on Ratings By Stars(_ratingsByStars); view LikedPercentage rate provided by readers when analyzing particular book(_likedPercent).  \n",
      ">  \n",
      "> Apart from more accessible factors mentioned previously delve deeper onto more sophisticated data presented: Setting (_setting); Cover Image (_coverImg); BbeScore_bbeScore); BbeVotes_bbeVotes). All those should provide greater insight when trying to explain why certain book has made its way onto GoodReads top selections list! To find value estimate test out Price (_price)) column too - determining if some texts retain large popularity despite rather costly publishing options cost-wise available on market currently?   \n",
      "> \n",
      ">  Finally combine different aspects observed while researching concerning individual titles- create personalized recommendations based upon released comprehensive lists! To achieve that utilize ISUBN code provided; compare publication Vs first publication dates historically recorded; verify awards labeling procedure relied upon give context information on discussed here books progress over years\n",
      "\n",
      "### Research Ideas\n",
      "> - Creating a web or mobile application to enable users to find books by genre, author, setting, etc. Users would be able to search for books based on their favorite criteria and also see bbeScore and reviews from other users. \n",
      "> - Using machine learning algorithms such as clustering or topic modeling to analyze the text from book descriptions in order to better understand common themes in literature and literary eras throughout history.\n",
      "> - Developing a recommendation engine that takes into account the user's interests and favorites with book ratings and bbeVotes in order to provide more personalized recommendations of titles they might like\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: books_1.Best_Books_Ever.csv**\n",
      "| Column name          | Description                                                             |\n",
      "|:---------------------|:------------------------------------------------------------------------|\n",
      "| **title**            | The title of the book. (String)                                         |\n",
      "| **series**           | The series the book belongs to, if any. (String)                        |\n",
      "| **author**           | The author of the book. (String)                                        |\n",
      "| **rating**           | The rating of the book on a scale of 1-5. (Integer)                     |\n",
      "| **description**      | A brief description of the book. (String)                               |\n",
      "| **language**         | The language the book is written in. (String)                           |\n",
      "| **isbn**             | The ISBN number of the book. (String)                                   |\n",
      "| **genres**           | The genres the book belongs to. (String)                                |\n",
      "| **characters**       | The characters featured in the book. (String)                           |\n",
      "| **bookFormat**       | The format of the book (e.g. paperback, hardcover, etc.). (String)      |\n",
      "| **edition**          | The edition of the book. (String)                                       |\n",
      "| **pages**            | The number of pages in the book. (Integer)                              |\n",
      "| **publisher**        | The publisher of the book. (String)                                     |\n",
      "| **publishDate**      | The date the book was published. (Date)                                 |\n",
      "| **firstPublishDate** | The date the book was first published. (Date)                           |\n",
      "| **awards**           | Any awards the book has won. (String)                                   |\n",
      "| **numRatings**       | The number of ratings the book has received. (Integer)                  |\n",
      "| **ratingsByStars**   | The ratings the book has received broken down by star rating. (Integer) |\n",
      "| **likedPercent**     | The percentage of people who liked the book. (Integer)                  |\n",
      "| **setting**          | The setting of the book. (String)                                       |\n",
      "| **coverImg**         | The cover image of the book. (Image)                                    |\n",
      "| **bbeScore**         | The best books ever score of the book. (Integer)                        |\n",
      "| **bbeVotes**         | The number of votes the book has received. (Integer)                    |\n",
      "| **price**            | The price of the book. (Integer)                                        |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: comprehensive-literary-greats-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Comprehensive Literary Greats Dataset\n",
      "name 'detect' is not defined Language is not detected: 50,000+ Books Rated and Awarded Across Language, Genre, and Format\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Comprehensive Literary Greats Dataset\n",
      "### 50,000+ Books Rated and Awarded Across Language, Genre, and Format\n",
      "By  [[source]](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This remarkable dataset provides an awe-inspiring collection of over 50,000 books, encompassing the world's best practices in literature, poetry, and authorship. For each book included in the dataset, users can gain access to a wealth of insightful information such as title, author(s), average rating given by readers and critics alike, a brief description highlighting its plot or characteristics; language it is written in; unique ISBN which enables potential buyers to locate their favorite works with ease; genres it belongs to; any awards it has won or characters that inhabit its storyworld. \n",
      "> \n",
      "> Additionally, seeking out readers' opinions on exceptional books is made easier due to the availability of bbeScore (best books ever score) alongside details for the most accurate ratings given through well-detailed breakdowns in “ratingsByStars” section. Making sure visibility and recognition are granted fairly – be it a classic novel from time immemorial or merely recently released newcomers - this source also allows us to evaluate new stories based off readers' engagement rate highlighted by likedPercent column (the percentage of readers who liked the book), bbeVotes (number of votes casted) as well as entries related to date published - including showstopping firstPublishDate! \n",
      "> \n",
      "> Aspiring literature researchers; literary historians and those seeking hidden literary gems alike would no doubt benefit from delving into this magnificent collection – 25 variables regarding different novels & poets that are presented by Kaggle open source dataset “Best Books Ever: A Comprehensive Historical Collection of Literary Greats”. What worlds awaits you?\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> Whether you are a student, researcher, or enthusiast of literature, this dataset provides a valuable source for exploring literary works from varied time periods and genres. By accessing all 25 variables in the dataset, readers have the opportunity to use them for building visualizations, creating new analysis tools and models, or finding books you might be interested in reading. \n",
      "> \n",
      "> First after downloading the dataset into Kaggle Notebooks platform or other programming interfaces of your choice such as R Studio/Python Jupyter Notebooks (Pandas) - make sure that data is arranged into columns with clearly labeled title names. This will help you understand which variable is related to what precise information. Afterwards explore each variable by finding any patterns across particular titles or interesting findings about certain authors/ratings that are available in your research interests.  \n",
      "> \n",
      "> Utilize the vital columns of Title (title), Author(author), Rating (rating), Description (description), Language (language), Genres (genres) and Characters(characters) - these can assist you in discovering different trends between books according to style of composition or character types etc. Move further down on examining more specific details offered by Book Format(bookFormat), Edition(edition) Pages(pages). Peruse publisher info along with Publish Date(publishDate). Besides these structural elements also take note of Awards column considering recent recognition different titles have received; also observe how much ratings has been collected per text through Numbers Ratings column-(numRatings); analyze reader's feedback according on Ratings By Stars(_ratingsByStars); view LikedPercentage rate provided by readers when analyzing particular book(_likedPercent).  \n",
      ">  \n",
      "> Apart from more accessible factors mentioned previously delve deeper onto more sophisticated data presented: Setting (_setting); Cover Image (_coverImg); BbeScore_bbeScore); BbeVotes_bbeVotes). All those should provide greater insight when trying to explain why certain book has made its way onto GoodReads top selections list! To find value estimate test out Price (_price)) column too - determining if some texts retain large popularity despite rather costly publishing options cost-wise available on market currently?   \n",
      "> \n",
      ">  Finally combine different aspects observed while researching concerning individual titles- create personalized recommendations based upon released comprehensive lists! To achieve that utilize ISUBN code provided; compare publication Vs first publication dates historically recorded; verify awards labeling procedure relied upon give context information on discussed here books progress over years\n",
      "\n",
      "### Research Ideas\n",
      "> - Creating a web or mobile application to enable users to find books by genre, author, setting, etc. Users would be able to search for books based on their favorite criteria and also see bbeScore and reviews from other users. \n",
      "> - Using machine learning algorithms such as clustering or topic modeling to analyze the text from book descriptions in order to better understand common themes in literature and literary eras throughout history.\n",
      "> - Developing a recommendation engine that takes into account the user's interests and favorites with book ratings and bbeVotes in order to provide more personalized recommendations of titles they might like\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: books_1.Best_Books_Ever.csv**\n",
      "| Column name          | Description                                                             |\n",
      "|:---------------------|:------------------------------------------------------------------------|\n",
      "| **title**            | The title of the book. (String)                                         |\n",
      "| **series**           | The series the book belongs to, if any. (String)                        |\n",
      "| **author**           | The author of the book. (String)                                        |\n",
      "| **rating**           | The rating of the book on a scale of 1-5. (Integer)                     |\n",
      "| **description**      | A brief description of the book. (String)                               |\n",
      "| **language**         | The language the book is written in. (String)                           |\n",
      "| **isbn**             | The ISBN number of the book. (String)                                   |\n",
      "| **genres**           | The genres the book belongs to. (String)                                |\n",
      "| **characters**       | The characters featured in the book. (String)                           |\n",
      "| **bookFormat**       | The format of the book (e.g. paperback, hardcover, etc.). (String)      |\n",
      "| **edition**          | The edition of the book. (String)                                       |\n",
      "| **pages**            | The number of pages in the book. (Integer)                              |\n",
      "| **publisher**        | The publisher of the book. (String)                                     |\n",
      "| **publishDate**      | The date the book was published. (Date)                                 |\n",
      "| **firstPublishDate** | The date the book was first published. (Date)                           |\n",
      "| **awards**           | Any awards the book has won. (String)                                   |\n",
      "| **numRatings**       | The number of ratings the book has received. (Integer)                  |\n",
      "| **ratingsByStars**   | The ratings the book has received broken down by star rating. (Integer) |\n",
      "| **likedPercent**     | The percentage of people who liked the book. (Integer)                  |\n",
      "| **setting**          | The setting of the book. (String)                                       |\n",
      "| **coverImg**         | The cover image of the book. (Image)                                    |\n",
      "| **bbeScore**         | The best books ever score of the book. (Integer)                        |\n",
      "| **bbeVotes**         | The number of votes the book has received. (Integer)                    |\n",
      "| **price**            | The price of the book. (Integer)                                        |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4265096#.Y9Y_O9JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/steam-games-user-statistics-and-features\n",
      "name 'detect' is not defined Language is not detected: steam-games-user-statistics-and-features\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Steam Games User Statistics and Features\n",
      "name 'detect' is not defined Language is not detected: Steam Games: User Statistics, Features, and Support Details\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Steam Games User Statistics and Features\n",
      "### Steam Games: User Statistics, Features, and Support Details\n",
      "By Craig Kelly [[source]](https://data.world/craigkelly)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> Steam Game Feature and User Statistic Dataset\n",
      "> ===============================================\n",
      "> \n",
      "> This dataset, titled Steam Game Feature and User Statistics, offers researchers, game developers, and enthusiasts an opportunity to analyze detailed information on various features of games available on the Steam platform. In addition to this, it also records user statistics and provides support details for each game. These data points include the release date of a game, the number of developers involved in its creation, its genre classification among several others.\n",
      "> \n",
      "> More specifically, through this dataset one can glean broadly diverse yet interconnected segments of information about a game’s details including integral values such as the developer count or DLC availability or boolean values which could range from determining if a game is free or if it caters to multiple gaming platforms like Windows, Linux or Mac. It offers rich text data providing insights into specific categories like in-app purchases or VR support. \n",
      "> \n",
      "> Moreover with more specific fields available one can through this dataset determine individual genre elements associated with each listed title breaking down segments covering non-gaming activities all the way up to massively multiplayer adventures! The financial aspect of any title is not overlooked either providing pivotal tracking markers related to a games nominal price point (initial & final rates) along with variances based on region specifics.\n",
      "> \n",
      "> Further adding depth are support email specifics that are listed along with any special notices like external account requirements which paint precise details related to individual titles facilitating perfect clarity for users – be it potential purchasers looking for specific features before deciding on their purchase choice or detailed market research studies trying to gauge user behaviour based on these complex interactions.\n",
      "> \n",
      "> Leaning more towards technical analysis there is wealth of datasets mapping minimum & recommended PC specifications split clearly across categories outlining requirements needed for smooth running across variations including Windows , Linux & MAC operating systems presenting clear performance indicators crucial towards gauging probable market success given typical end-user system capabilities!\n",
      "> \n",
      "> Taking contextual relevance into consideration the provided data has been processed using UTF-8 unicode conversion ensuring clear language localizations enhanced by the incorporation of specialized Python standard library sequences that prevents loss in translation as can be witnessed from the highly defined string values.\n",
      "> \n",
      "> To ensure seamless interaction for further analytical processing, each field was sanitized and set up to handle all common data types. Text strings were cleaned up via several iterative steps to facilitate easy interpretation and removal of any unwanted characters or white spaces that would otherwise throw off automated text analysis algorithms. \n",
      "> \n",
      "> In sum, this Steam Game Features and User Statistics dataset is an extremely comprehensive source of information regarding games available on the Steam platform offering multi-faceted insights into gaming trends & opportunities for those savvy enough to interpret\n",
      "\n",
      "### How to use the dataset\n",
      "> ## Guide: How to Use the Steam Game User Statistics and Features Dataset\n",
      "> \n",
      "> \n",
      "> _Follow these steps:_\n",
      "> \n",
      "> **1) Understand The Data Types**\n",
      "> \n",
      "> The dataset comprises Textual, Integer, Float and Boolean values.\n",
      ">   - Text has been processed for easy handling. Strings have been cleaned up with stripping leading/trailing whitespace and converting strings to lower case.\n",
      ">   - Integer & Float are numeric values; missing ones are represented as '0' & '0.0', respectively.\n",
      ">   - Boolean fields can only contain two literal values: True or False.\n",
      "> \n",
      "> **2) Familiarize Yourself With The Fields**\n",
      "> \n",
      "> The dataset offers a diverse range of field categories. To use it effectively:\n",
      "> \n",
      ">    - **Review:** Overview columns clarify details such as game name (`QueryName`, `ResponseName`), `ReleaseDate`, `RequiredAge`, etc.\n",
      ">    - **Examine developer features:** Columns such as developer count (`DeveloperCount`), publisher count (`PublisherCount`) package count (`PackageCount`) provide information related to game production demographics.\n",
      ">    - **Look for user-focused information:** User statistics such as owner count(`SteamSpyOwners`,`SteamSpyOwnersVariance`), player estimates(`SteamSpyPlayersEstimate`,`SteamSpyPlayersVariance`), recommendation counts(`RecommendationCount`) offer valuable data on community preferences/reception of a game.\n",
      ">    - **Check Platform functionalities:** Various columns signify software specifications like PC requirements (eg:`PCReqsHaveMin`,`PCReqsHaveRec`etc.), supported platforms (like `PlatformWindows`, `PlatformMac`),\n",
      ">    support contact details (`SupportEmail`, `SupportURL`)\n",
      ">     \n",
      ">  _Additionally, there are columns signifying-_\n",
      ">  \n",
      ">      * Genre type (like `GenreIsAction`, `GenreIsAdventure`)\n",
      ">      * Game Categories(`CategorySinglePlayer`, `CategoryMultiplayer`)\n",
      ">      * Price details (`PriceInitial`, `PriceFinal`), and more.\n",
      "> \n",
      "> **3) Know Your Objective**\n",
      "> \n",
      "> Different objectives merit focus on different data fields. \n",
      ">   - If you're analysing trends in user preferences, concentrate on user stats columns.\n",
      ">   - A market research analysis might require examination of information from genre type, pricing and platform columns.\n",
      ">   - To gauge game development tendencies across the industry, review developer-focused features.\n",
      "> \n",
      "> **4) Prepare & Cleanse Your Data**\n",
      "> \n",
      "> Before you start\n",
      "\n",
      "### Research Ideas\n",
      "> - Game Recommendation Engine: Analyzing the features and popularity of different games, this dataset can be used to develop a recommendation system for Steam users. The system can recommend new games based on players' preferences, play history, required hardware specifications or other parameters.\n",
      "> - Market Analysis: Companies in the gaming industry can use this dataset for market analysis with regards to publishing game types that are currently popular among users. They could evaluate trends based on factors such as genre, cooperative play availability or package price variability.\n",
      ">    \n",
      "> - Game Development Inspiration: Independent game developers could use this data to understand player preferences better and tailor their developments accordingly. They might also get inspiration about implementing special features (like VR Support, cooperative plays), see which genres are popular amongst players and what pricing model they should go with\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/craigkelly)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "> - You are free to:\n",
      ">      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      ">      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "> - You must:\n",
      ">      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      ">      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      ">      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: games-features.csv**\n",
      "| Column name                     | Description                                                                                    |\n",
      "|:--------------------------------|:-----------------------------------------------------------------------------------------------|\n",
      "| **QueryName**                   | The name of the game as queried in the Steam database. (String)                                |\n",
      "| **ResponseName**                | The name of the game as responded by the Steam database. (String)                              |\n",
      "| **ReleaseDate**                 | The date when the game was released on the Steam platform. (Date)                              |\n",
      "| **RequiredAge**                 | The minimum age required to play the game as suggested by the game's rating. (Integer)         |\n",
      "| **DemoCount**                   | The number of demo versions available for the game. (Integer)                                  |\n",
      "| **DeveloperCount**              | The number of developers involved in the creation of the game. (Integer)                       |\n",
      "| **DLCCount**                    | The number of downloadable content (DLC) packages available for the game. (Integer)            |\n",
      "| **Metacritic**                  | The game's score on Metacritic, a website that aggregates reviews of media products. (Integer) |\n",
      "| **MovieCount**                  | The number of promotional movies or trailers available for the game. (Integer)                 |\n",
      "| **PackageCount**                | The number of packages in which the game is sold. (Integer)                                    |\n",
      "| **RecommendationCount**         | The number of times the game has been recommended by users. (Integer)                          |\n",
      "| **PublisherCount**              | The number of publishers that have published the game. (Integer)                               |\n",
      "| **ScreenshotCount**             | The number of screenshots available for the game. (Integer)                                    |\n",
      "| **SteamSpyOwners**              | The estimated number of people who own the game, according to SteamSpy. (Integer)              |\n",
      "| **SteamSpyOwnersVariance**      | The variance in the estimated number of owners, according to SteamSpy. (Integer)               |\n",
      "| **SteamSpyPlayersEstimate**     | The estimated number of people who have played the game, according to SteamSpy. (Integer)      |\n",
      "| **SteamSpyPlayersVariance**     | The variance in the estimated number of players, according to SteamSpy. (Integer)              |\n",
      "| **AchievementCount**            | The number of achievements available in the game. (Integer)                                    |\n",
      "| **AchievementHighlightedCount** | The number of achievements that are highlighted in the game. (Integer)                         |\n",
      "| **ControllerSupport**           | Whether the game supports controller input. (Boolean)                                          |\n",
      "| **IsFree**                      | Indicates whether the game is free to play. (Boolean)                                          |\n",
      "| **FreeVerAvail**                | Indicates whether a free version of the game is available. (Boolean)                           |\n",
      "| **PurchaseAvail**               | Indicates whether the game is available for purchase. (Boolean)                                |\n",
      "| **SubscriptionAvail**           | Indicates whether the game is available through a subscription service. (Boolean)              |\n",
      "| **PlatformWindows**             | Indicates whether the game is compatible with the Windows operating system. (Boolean)          |\n",
      "| **PlatformLinux**               | Indicates whether the game is compatible with the Linux operating system. (Boolean)            |\n",
      "| **PlatformMac**                 | Indicates whether the game is compatible with the Mac operating system. (Boolean)              |\n",
      "| **PCReqsHaveMin**               | Indicates whether the game has minimum system requirements for PC. (Boolean)                   |\n",
      "| **PCReqsHaveRec**               | Indicates whether the game has recommended system requirements for PC. (Boolean)               |\n",
      "| **LinuxReqsHaveMin**            | Indicates whether the game has minimum system requirements for Linux. (Boolean)                |\n",
      "| **LinuxReqsHaveRec**            | Indicates whether the game has recommended system requirements for Linux. (Boolean)            |\n",
      "| **MacReqsHaveMin**              | Indicates whether the game has minimum system requirements for Mac. (Boolean)                  |\n",
      "| **MacReqsHaveRec**              | Indicates whether the game has recommended system requirements for Mac. (Boolean)              |\n",
      "| **CategorySinglePlayer**        | Indicates whether the game has a single-player mode. (Boolean)                                 |\n",
      "| **CategoryMultiplayer**         | Indicates whether the game has a multiplayer mode. (Boolean)                                   |\n",
      "| **CategoryCoop**                | Indicates whether the game has a cooperative play mode. (Boolean)                              |\n",
      "| **CategoryMMO**                 | Indicates whether the game is a massively multiplayer online game. (Boolean)                   |\n",
      "| **CategoryInAppPurchase**       | Indicates whether the game offers in-app purchases. (Boolean)                                  |\n",
      "| **CategoryIncludeSrcSDK**       | Indicates whether the game includes a source software development kit. (Boolean)               |\n",
      "| **CategoryIncludeLevelEditor**  | Indicates whether the game includes a level editor. (Boolean)                                  |\n",
      "| **CategoryVRSupport**           | Indicates whether the game supports virtual reality (VR). (Boolean)                            |\n",
      "| **GenreIsNonGame**              | Indicates whether the game is categorized as a non-game. (Boolean)                             |\n",
      "| **GenreIsIndie**                | Indicates whether the game is categorized as an indie game. (Boolean)                          |\n",
      "| **GenreIsAction**               | Indicates whether the game is categorized as an action game. (Boolean)                         |\n",
      "| **GenreIsAdventure**            | Indicates whether the game is categorized as an adventure game. (Boolean)                      |\n",
      "| **GenreIsCasual**               | Indicates whether the game is categorized as a casual game. (Boolean)                          |\n",
      "| **GenreIsStrategy**             | Indicates whether the game is categorized as a strategy game. (Boolean)                        |\n",
      "| **GenreIsRPG**                  | Indicates whether the game is categorized as a role-playing game (RPG). (Boolean)              |\n",
      "| **GenreIsSimulation**           | Indicates whether the game is categorized as a simulation game. (Boolean)                      |\n",
      "| **GenreIsEarlyAccess**          | Indicates whether the game is in early access. (Boolean)                                       |\n",
      "| **GenreIsFreeToPlay**           | Indicates whether the game is free to play. (Boolean)                                          |\n",
      "| **GenreIsSports**               | Indicates whether the game is categorized as a sports game. (Boolean)                          |\n",
      "| **GenreIsRacing**               | Indicates whether the game is categorized as a racing game. (Boolean)                          |\n",
      "| **GenreIsMassivelyMultiplayer** | Indicates whether the game is categorized as a massively multiplayer online game. (Boolean)    |\n",
      "| **PriceCurrency**               | The currency in which the game's price is listed. (String)                                     |\n",
      "| **PriceInitial**                | The initial price of the game. (Float)                                                         |\n",
      "| **PriceFinal**                  | The final price of the game after any discounts. (Float)                                       |\n",
      "| **SupportEmail**                | The email address for support related to the game. (String)                                    |\n",
      "| **SupportURL**                  | The URL for support related to the game. (String)                                              |\n",
      "| **AboutText**                   | A description of the game. (String)                                                            |\n",
      "| **Background**                  | The URL of the game's background image. (String)                                               |\n",
      "| **ShortDescrip**                | A short description of the game. (String)                                                      |\n",
      "| **DetailedDescrip**             | A detailed description of the game. (String)                                                   |\n",
      "| **DRMNotice**                   | Any notices related to the game's digital rights management (DRM). (String)                    |\n",
      "| **ExtUserAcctNotice**           | Any notices related to external user accounts for the game. (String)                           |\n",
      "| **HeaderImage**                 | The URL of the game's header image. (String)                                                   |\n",
      "| **LegalNotice**                 | Any legal notices related to the game. (String)                                                |\n",
      "| **Reviews**                     | User reviews of the game. (String)                                                             |\n",
      "| **SupportedLanguages**          | The languages supported by the game. (String)                                                  |\n",
      "| **Website**                     | The game's official website. (String)                                                          |\n",
      "| **PCMinReqsText**               | The minimum system requirements for the game on PC. (String)                                   |\n",
      "| **PCRecReqsText**               | The recommended system requirements for the game on PC. (String)                               |\n",
      "| **LinuxMinReqsText**            | The minimum system requirements for the game on Linux. (String)                                |\n",
      "| **LinuxRecReqsText**            | The recommended system requirements for the game on Linux. (String)                            |\n",
      "| **MacMinReqsText**              | The minimum system requirements for the game on Mac. (String)                                  |\n",
      "| **MacRecReqsText**              | The recommended system requirements for the game on Mac. (String)                              |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Craig Kelly](https://data.world/craigkelly).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: steam-games-user-statistics-and-features\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Steam Games User Statistics and Features\n",
      "name 'detect' is not defined Language is not detected: Steam Games: User Statistics, Features, and Support Details\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Steam Games User Statistics and Features\n",
      "### Steam Games: User Statistics, Features, and Support Details\n",
      "By Craig Kelly [[source]](https://data.world/craigkelly)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> Steam Game Feature and User Statistic Dataset\n",
      "> ===============================================\n",
      "> \n",
      "> This dataset, titled Steam Game Feature and User Statistics, offers researchers, game developers, and enthusiasts an opportunity to analyze detailed information on various features of games available on the Steam platform. In addition to this, it also records user statistics and provides support details for each game. These data points include the release date of a game, the number of developers involved in its creation, its genre classification among several others.\n",
      "> \n",
      "> More specifically, through this dataset one can glean broadly diverse yet interconnected segments of information about a game’s details including integral values such as the developer count or DLC availability or boolean values which could range from determining if a game is free or if it caters to multiple gaming platforms like Windows, Linux or Mac. It offers rich text data providing insights into specific categories like in-app purchases or VR support. \n",
      "> \n",
      "> Moreover with more specific fields available one can through this dataset determine individual genre elements associated with each listed title breaking down segments covering non-gaming activities all the way up to massively multiplayer adventures! The financial aspect of any title is not overlooked either providing pivotal tracking markers related to a games nominal price point (initial & final rates) along with variances based on region specifics.\n",
      "> \n",
      "> Further adding depth are support email specifics that are listed along with any special notices like external account requirements which paint precise details related to individual titles facilitating perfect clarity for users – be it potential purchasers looking for specific features before deciding on their purchase choice or detailed market research studies trying to gauge user behaviour based on these complex interactions.\n",
      "> \n",
      "> Leaning more towards technical analysis there is wealth of datasets mapping minimum & recommended PC specifications split clearly across categories outlining requirements needed for smooth running across variations including Windows , Linux & MAC operating systems presenting clear performance indicators crucial towards gauging probable market success given typical end-user system capabilities!\n",
      "> \n",
      "> Taking contextual relevance into consideration the provided data has been processed using UTF-8 unicode conversion ensuring clear language localizations enhanced by the incorporation of specialized Python standard library sequences that prevents loss in translation as can be witnessed from the highly defined string values.\n",
      "> \n",
      "> To ensure seamless interaction for further analytical processing, each field was sanitized and set up to handle all common data types. Text strings were cleaned up via several iterative steps to facilitate easy interpretation and removal of any unwanted characters or white spaces that would otherwise throw off automated text analysis algorithms. \n",
      "> \n",
      "> In sum, this Steam Game Features and User Statistics dataset is an extremely comprehensive source of information regarding games available on the Steam platform offering multi-faceted insights into gaming trends & opportunities for those savvy enough to interpret\n",
      "\n",
      "### How to use the dataset\n",
      "> ## Guide: How to Use the Steam Game User Statistics and Features Dataset\n",
      "> \n",
      "> \n",
      "> _Follow these steps:_\n",
      "> \n",
      "> **1) Understand The Data Types**\n",
      "> \n",
      "> The dataset comprises Textual, Integer, Float and Boolean values.\n",
      ">   - Text has been processed for easy handling. Strings have been cleaned up with stripping leading/trailing whitespace and converting strings to lower case.\n",
      ">   - Integer & Float are numeric values; missing ones are represented as '0' & '0.0', respectively.\n",
      ">   - Boolean fields can only contain two literal values: True or False.\n",
      "> \n",
      "> **2) Familiarize Yourself With The Fields**\n",
      "> \n",
      "> The dataset offers a diverse range of field categories. To use it effectively:\n",
      "> \n",
      ">    - **Review:** Overview columns clarify details such as game name (`QueryName`, `ResponseName`), `ReleaseDate`, `RequiredAge`, etc.\n",
      ">    - **Examine developer features:** Columns such as developer count (`DeveloperCount`), publisher count (`PublisherCount`) package count (`PackageCount`) provide information related to game production demographics.\n",
      ">    - **Look for user-focused information:** User statistics such as owner count(`SteamSpyOwners`,`SteamSpyOwnersVariance`), player estimates(`SteamSpyPlayersEstimate`,`SteamSpyPlayersVariance`), recommendation counts(`RecommendationCount`) offer valuable data on community preferences/reception of a game.\n",
      ">    - **Check Platform functionalities:** Various columns signify software specifications like PC requirements (eg:`PCReqsHaveMin`,`PCReqsHaveRec`etc.), supported platforms (like `PlatformWindows`, `PlatformMac`),\n",
      ">    support contact details (`SupportEmail`, `SupportURL`)\n",
      ">     \n",
      ">  _Additionally, there are columns signifying-_\n",
      ">  \n",
      ">      * Genre type (like `GenreIsAction`, `GenreIsAdventure`)\n",
      ">      * Game Categories(`CategorySinglePlayer`, `CategoryMultiplayer`)\n",
      ">      * Price details (`PriceInitial`, `PriceFinal`), and more.\n",
      "> \n",
      "> **3) Know Your Objective**\n",
      "> \n",
      "> Different objectives merit focus on different data fields. \n",
      ">   - If you're analysing trends in user preferences, concentrate on user stats columns.\n",
      ">   - A market research analysis might require examination of information from genre type, pricing and platform columns.\n",
      ">   - To gauge game development tendencies across the industry, review developer-focused features.\n",
      "> \n",
      "> **4) Prepare & Cleanse Your Data**\n",
      "> \n",
      "> Before you start\n",
      "\n",
      "### Research Ideas\n",
      "> - Game Recommendation Engine: Analyzing the features and popularity of different games, this dataset can be used to develop a recommendation system for Steam users. The system can recommend new games based on players' preferences, play history, required hardware specifications or other parameters.\n",
      "> - Market Analysis: Companies in the gaming industry can use this dataset for market analysis with regards to publishing game types that are currently popular among users. They could evaluate trends based on factors such as genre, cooperative play availability or package price variability.\n",
      ">    \n",
      "> - Game Development Inspiration: Independent game developers could use this data to understand player preferences better and tailor their developments accordingly. They might also get inspiration about implementing special features (like VR Support, cooperative plays), see which genres are popular amongst players and what pricing model they should go with\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/craigkelly)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "> - You are free to:\n",
      ">      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      ">      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "> - You must:\n",
      ">      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      ">      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      ">      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: games-features.csv**\n",
      "| Column name                     | Description                                                                                    |\n",
      "|:--------------------------------|:-----------------------------------------------------------------------------------------------|\n",
      "| **QueryName**                   | The name of the game as queried in the Steam database. (String)                                |\n",
      "| **ResponseName**                | The name of the game as responded by the Steam database. (String)                              |\n",
      "| **ReleaseDate**                 | The date when the game was released on the Steam platform. (Date)                              |\n",
      "| **RequiredAge**                 | The minimum age required to play the game as suggested by the game's rating. (Integer)         |\n",
      "| **DemoCount**                   | The number of demo versions available for the game. (Integer)                                  |\n",
      "| **DeveloperCount**              | The number of developers involved in the creation of the game. (Integer)                       |\n",
      "| **DLCCount**                    | The number of downloadable content (DLC) packages available for the game. (Integer)            |\n",
      "| **Metacritic**                  | The game's score on Metacritic, a website that aggregates reviews of media products. (Integer) |\n",
      "| **MovieCount**                  | The number of promotional movies or trailers available for the game. (Integer)                 |\n",
      "| **PackageCount**                | The number of packages in which the game is sold. (Integer)                                    |\n",
      "| **RecommendationCount**         | The number of times the game has been recommended by users. (Integer)                          |\n",
      "| **PublisherCount**              | The number of publishers that have published the game. (Integer)                               |\n",
      "| **ScreenshotCount**             | The number of screenshots available for the game. (Integer)                                    |\n",
      "| **SteamSpyOwners**              | The estimated number of people who own the game, according to SteamSpy. (Integer)              |\n",
      "| **SteamSpyOwnersVariance**      | The variance in the estimated number of owners, according to SteamSpy. (Integer)               |\n",
      "| **SteamSpyPlayersEstimate**     | The estimated number of people who have played the game, according to SteamSpy. (Integer)      |\n",
      "| **SteamSpyPlayersVariance**     | The variance in the estimated number of players, according to SteamSpy. (Integer)              |\n",
      "| **AchievementCount**            | The number of achievements available in the game. (Integer)                                    |\n",
      "| **AchievementHighlightedCount** | The number of achievements that are highlighted in the game. (Integer)                         |\n",
      "| **ControllerSupport**           | Whether the game supports controller input. (Boolean)                                          |\n",
      "| **IsFree**                      | Indicates whether the game is free to play. (Boolean)                                          |\n",
      "| **FreeVerAvail**                | Indicates whether a free version of the game is available. (Boolean)                           |\n",
      "| **PurchaseAvail**               | Indicates whether the game is available for purchase. (Boolean)                                |\n",
      "| **SubscriptionAvail**           | Indicates whether the game is available through a subscription service. (Boolean)              |\n",
      "| **PlatformWindows**             | Indicates whether the game is compatible with the Windows operating system. (Boolean)          |\n",
      "| **PlatformLinux**               | Indicates whether the game is compatible with the Linux operating system. (Boolean)            |\n",
      "| **PlatformMac**                 | Indicates whether the game is compatible with the Mac operating system. (Boolean)              |\n",
      "| **PCReqsHaveMin**               | Indicates whether the game has minimum system requirements for PC. (Boolean)                   |\n",
      "| **PCReqsHaveRec**               | Indicates whether the game has recommended system requirements for PC. (Boolean)               |\n",
      "| **LinuxReqsHaveMin**            | Indicates whether the game has minimum system requirements for Linux. (Boolean)                |\n",
      "| **LinuxReqsHaveRec**            | Indicates whether the game has recommended system requirements for Linux. (Boolean)            |\n",
      "| **MacReqsHaveMin**              | Indicates whether the game has minimum system requirements for Mac. (Boolean)                  |\n",
      "| **MacReqsHaveRec**              | Indicates whether the game has recommended system requirements for Mac. (Boolean)              |\n",
      "| **CategorySinglePlayer**        | Indicates whether the game has a single-player mode. (Boolean)                                 |\n",
      "| **CategoryMultiplayer**         | Indicates whether the game has a multiplayer mode. (Boolean)                                   |\n",
      "| **CategoryCoop**                | Indicates whether the game has a cooperative play mode. (Boolean)                              |\n",
      "| **CategoryMMO**                 | Indicates whether the game is a massively multiplayer online game. (Boolean)                   |\n",
      "| **CategoryInAppPurchase**       | Indicates whether the game offers in-app purchases. (Boolean)                                  |\n",
      "| **CategoryIncludeSrcSDK**       | Indicates whether the game includes a source software development kit. (Boolean)               |\n",
      "| **CategoryIncludeLevelEditor**  | Indicates whether the game includes a level editor. (Boolean)                                  |\n",
      "| **CategoryVRSupport**           | Indicates whether the game supports virtual reality (VR). (Boolean)                            |\n",
      "| **GenreIsNonGame**              | Indicates whether the game is categorized as a non-game. (Boolean)                             |\n",
      "| **GenreIsIndie**                | Indicates whether the game is categorized as an indie game. (Boolean)                          |\n",
      "| **GenreIsAction**               | Indicates whether the game is categorized as an action game. (Boolean)                         |\n",
      "| **GenreIsAdventure**            | Indicates whether the game is categorized as an adventure game. (Boolean)                      |\n",
      "| **GenreIsCasual**               | Indicates whether the game is categorized as a casual game. (Boolean)                          |\n",
      "| **GenreIsStrategy**             | Indicates whether the game is categorized as a strategy game. (Boolean)                        |\n",
      "| **GenreIsRPG**                  | Indicates whether the game is categorized as a role-playing game (RPG). (Boolean)              |\n",
      "| **GenreIsSimulation**           | Indicates whether the game is categorized as a simulation game. (Boolean)                      |\n",
      "| **GenreIsEarlyAccess**          | Indicates whether the game is in early access. (Boolean)                                       |\n",
      "| **GenreIsFreeToPlay**           | Indicates whether the game is free to play. (Boolean)                                          |\n",
      "| **GenreIsSports**               | Indicates whether the game is categorized as a sports game. (Boolean)                          |\n",
      "| **GenreIsRacing**               | Indicates whether the game is categorized as a racing game. (Boolean)                          |\n",
      "| **GenreIsMassivelyMultiplayer** | Indicates whether the game is categorized as a massively multiplayer online game. (Boolean)    |\n",
      "| **PriceCurrency**               | The currency in which the game's price is listed. (String)                                     |\n",
      "| **PriceInitial**                | The initial price of the game. (Float)                                                         |\n",
      "| **PriceFinal**                  | The final price of the game after any discounts. (Float)                                       |\n",
      "| **SupportEmail**                | The email address for support related to the game. (String)                                    |\n",
      "| **SupportURL**                  | The URL for support related to the game. (String)                                              |\n",
      "| **AboutText**                   | A description of the game. (String)                                                            |\n",
      "| **Background**                  | The URL of the game's background image. (String)                                               |\n",
      "| **ShortDescrip**                | A short description of the game. (String)                                                      |\n",
      "| **DetailedDescrip**             | A detailed description of the game. (String)                                                   |\n",
      "| **DRMNotice**                   | Any notices related to the game's digital rights management (DRM). (String)                    |\n",
      "| **ExtUserAcctNotice**           | Any notices related to external user accounts for the game. (String)                           |\n",
      "| **HeaderImage**                 | The URL of the game's header image. (String)                                                   |\n",
      "| **LegalNotice**                 | Any legal notices related to the game. (String)                                                |\n",
      "| **Reviews**                     | User reviews of the game. (String)                                                             |\n",
      "| **SupportedLanguages**          | The languages supported by the game. (String)                                                  |\n",
      "| **Website**                     | The game's official website. (String)                                                          |\n",
      "| **PCMinReqsText**               | The minimum system requirements for the game on PC. (String)                                   |\n",
      "| **PCRecReqsText**               | The recommended system requirements for the game on PC. (String)                               |\n",
      "| **LinuxMinReqsText**            | The minimum system requirements for the game on Linux. (String)                                |\n",
      "| **LinuxRecReqsText**            | The recommended system requirements for the game on Linux. (String)                            |\n",
      "| **MacMinReqsText**              | The minimum system requirements for the game on Mac. (String)                                  |\n",
      "| **MacRecReqsText**              | The recommended system requirements for the game on Mac. (String)                              |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Craig Kelly](https://data.world/craigkelly).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/tripadvisor-hotel-review-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: tripadvisor-hotel-review-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: TripAdvisor Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Exploring User Experiences and Emotional Responses\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# TripAdvisor Hotel Reviews\n",
      "### Exploring User Experiences and Emotional Responses\n",
      "By  [[source]](https://zenodo.org/record/1219899)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; Welcome to the TripAdvisor Hotel Reviews Sentiment Analysis dataset, where you can explore the emotional responses of visitors to various hotels around the world. This comprehensive dataset contains reviews and ratings from TripAdvisor which provide an in-depth look into how visitors really feel about their experiences at all kinds of hotels. From boutique hideaways to four-star resorts, discover what makes travelers truly happy or disappointed with each hotel and plan for bettering your own. With detailed sentiment analysis of every review, this dataset is a valuable tool for hoteliers looking to improve their customer's experience and satisfaction levels. Unlock people's true opinions about a range of establishments today - Dive deep into the TripAdvisor reviews!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains hotel reviews from TripAdvisor and the ratings and sentiment of the review. It can be used to gain valuable insight into how people feel about hotels, which can be important when making decisions involving holidays, business travel or other reasons for booking an accommodation. \n",
      "&gt; \n",
      "&gt; The dataset consists of three columns: S.No., Review, Rating. The S.No field is a unique identifier for each review, while the Review field contains the text of the review itself. Finally, there is a Rating field with an integer rating which can range from 1-5 stars depending on how satisfied or dissatisfied a reviewer felt about their experience at that particular hotel. \n",
      "&gt; \n",
      "&gt; In order to make use of this dataset it's best to examine both qualitative and quantitative data in order to determine what customers are saying about each hotel as well as their overall sentiment towards them based on their rating score (1-5 stars). For more in-depth analysis into customer's emotions related to these reviews you may consider using Natural Language Processing (NLP) methods such as sentiment analysis tools or word clouds in order to visualize key sentiments in relation to your chosen suite of hotels for further exploration beyond numerical scores alone. \n",
      "&gt; \n",
      "&gt; Overall this dataset could provide great insight into how people felt when they stayed at certain hotels so you could get a better idea if those establishments might fit your own needs before booking a stay!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Analyzing trends in customer sentiment over time to track the success of marketing campaigns, service initiatives, and hotel improvements.\n",
      "&gt; - Creating a personalized recommendation tool for users who want to find hotels with similar experiences to their previous reviews.\n",
      "&gt; - Leveraging sentiment analysis techniques to identify key themes in feedback and use the data to develop strategies that increase customer satisfaction levels at the individual hotel level\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://zenodo.org/record/1219899)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: b.csv**\n",
      "| Column name   | Description                                          |\n",
      "|:--------------|:-----------------------------------------------------|\n",
      "| **S.No.**     | Unique identifier for each review. (Integer)         |\n",
      "| **Review**    | Text of the review. (String)                         |\n",
      "| **Rating**    | Rating given to the hotel by the reviewer. (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/1219899).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tripadvisor-hotel-review-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: TripAdvisor Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Exploring User Experiences and Emotional Responses\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# TripAdvisor Hotel Reviews\n",
      "### Exploring User Experiences and Emotional Responses\n",
      "By  [[source]](https://zenodo.org/record/1219899)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; Welcome to the TripAdvisor Hotel Reviews Sentiment Analysis dataset, where you can explore the emotional responses of visitors to various hotels around the world. This comprehensive dataset contains reviews and ratings from TripAdvisor which provide an in-depth look into how visitors really feel about their experiences at all kinds of hotels. From boutique hideaways to four-star resorts, discover what makes travelers truly happy or disappointed with each hotel and plan for bettering your own. With detailed sentiment analysis of every review, this dataset is a valuable tool for hoteliers looking to improve their customer's experience and satisfaction levels. Unlock people's true opinions about a range of establishments today - Dive deep into the TripAdvisor reviews!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains hotel reviews from TripAdvisor and the ratings and sentiment of the review. It can be used to gain valuable insight into how people feel about hotels, which can be important when making decisions involving holidays, business travel or other reasons for booking an accommodation. \n",
      "&gt; \n",
      "&gt; The dataset consists of three columns: S.No., Review, Rating. The S.No field is a unique identifier for each review, while the Review field contains the text of the review itself. Finally, there is a Rating field with an integer rating which can range from 1-5 stars depending on how satisfied or dissatisfied a reviewer felt about their experience at that particular hotel. \n",
      "&gt; \n",
      "&gt; In order to make use of this dataset it's best to examine both qualitative and quantitative data in order to determine what customers are saying about each hotel as well as their overall sentiment towards them based on their rating score (1-5 stars). For more in-depth analysis into customer's emotions related to these reviews you may consider using Natural Language Processing (NLP) methods such as sentiment analysis tools or word clouds in order to visualize key sentiments in relation to your chosen suite of hotels for further exploration beyond numerical scores alone. \n",
      "&gt; \n",
      "&gt; Overall this dataset could provide great insight into how people felt when they stayed at certain hotels so you could get a better idea if those establishments might fit your own needs before booking a stay!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Analyzing trends in customer sentiment over time to track the success of marketing campaigns, service initiatives, and hotel improvements.\n",
      "&gt; - Creating a personalized recommendation tool for users who want to find hotels with similar experiences to their previous reviews.\n",
      "&gt; - Leveraging sentiment analysis techniques to identify key themes in feedback and use the data to develop strategies that increase customer satisfaction levels at the individual hotel level\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://zenodo.org/record/1219899)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: b.csv**\n",
      "| Column name   | Description                                          |\n",
      "|:--------------|:-----------------------------------------------------|\n",
      "| **S.No.**     | Unique identifier for each review. (Integer)         |\n",
      "| **Review**    | Text of the review. (String)                         |\n",
      "| **Rating**    | Rating given to the hotel by the reviewer. (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/1219899).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/databricks-chatgpt-dataset\n",
      "name 'detect' is not defined Language is not detected: databricks-chatgpt-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Databricks Dolly (15K)\n",
      "name 'detect' is not defined Language is not detected: Over 15,000 Language Models and Dialogues for Interactive Chat Applications\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Databricks Dolly (15K)\n",
      "### Over 15,000 Language Models and Dialogues for Interactive Chat Applications\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This exceptional dataset, created by Databricks employees, provides 15,000+ language models and dialogues to power dynamic ChatGPT applications. By generating prompt-response pairs from 8 different instruction categories, our goal is to facilitate the use of large language models for interactive dialogue interactions—all while avoiding information taken from any web sources except Wikipedia for particular instruction sets. Use this open-source dataset to explore the boundaries of text-based conversations and uncover new insights about natural language processing!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; First, let's take a look at the columns in this dataset: Instruction (string), Context (string), Response (string), Category (string). Each record represents a prompt-response pair or conversation between two people. The Instruction and Context fields contain what is said by one individual and the Response holds what is said back by another, culminating in a conversation. These paired entries are then classified into one of 8 different categories based on their content. Knowing this information can help you best utilize the corpus to your desired purposes. \n",
      "&gt; \n",
      "&gt; For example: if you are training a dialogue system you could develop multiple funneling pipelines using this dataset to enrich your model with real-world conversations or create intelligent chatbot interactions. If you want to generate natural language answers as part of Q&A systems then you could utilize excerpts from Wikipedia for particular subsets of instruction categories as well drawing upon prompt-response pairs within those given instructions all from within the Databricks set. Furthermore, since each record is independently labeled into one of 8 defined categories - such as make reservations or compare products - there are many possibilities for leveraging these classification labels with supervised learning techniques such as multi-class classification neural networks or logistic regression classifiers. \n",
      "&gt; \n",
      "&gt; In short, this substantial resource offers an array of creative ways to explore different types of dialogue related applications without being limited by needing data from external web sources – all that’s needed from here is your own imagination!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Generating deep learning models to detect and respond to conversational intent. \n",
      "&gt; - Training language models to use natural language processing (NLP) for customer service queries. \n",
      "&gt; - Creating custom dialogue agents that are better able to handle more complex conversational interactions, such as those powered by machine learning techniques like supervised or unsupervised learning methods\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name     | Description                                                                                                                                         |\n",
      "|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **Instruction** | Text prompt that should generate an appropriate response from a machine learning model/chatbot using natural language processing techniques. (Text) |\n",
      "| **Context**     | Provides context to improve accuracy by giving the model more information about what’s happening in a conversation or request execution. (Text)     |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/databricks/databricks-dolly-15k).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: databricks-chatgpt-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Databricks Dolly (15K)\n",
      "name 'detect' is not defined Language is not detected: Over 15,000 Language Models and Dialogues for Interactive Chat Applications\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Databricks Dolly (15K)\n",
      "### Over 15,000 Language Models and Dialogues for Interactive Chat Applications\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This exceptional dataset, created by Databricks employees, provides 15,000+ language models and dialogues to power dynamic ChatGPT applications. By generating prompt-response pairs from 8 different instruction categories, our goal is to facilitate the use of large language models for interactive dialogue interactions—all while avoiding information taken from any web sources except Wikipedia for particular instruction sets. Use this open-source dataset to explore the boundaries of text-based conversations and uncover new insights about natural language processing!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; First, let's take a look at the columns in this dataset: Instruction (string), Context (string), Response (string), Category (string). Each record represents a prompt-response pair or conversation between two people. The Instruction and Context fields contain what is said by one individual and the Response holds what is said back by another, culminating in a conversation. These paired entries are then classified into one of 8 different categories based on their content. Knowing this information can help you best utilize the corpus to your desired purposes. \n",
      "&gt; \n",
      "&gt; For example: if you are training a dialogue system you could develop multiple funneling pipelines using this dataset to enrich your model with real-world conversations or create intelligent chatbot interactions. If you want to generate natural language answers as part of Q&A systems then you could utilize excerpts from Wikipedia for particular subsets of instruction categories as well drawing upon prompt-response pairs within those given instructions all from within the Databricks set. Furthermore, since each record is independently labeled into one of 8 defined categories - such as make reservations or compare products - there are many possibilities for leveraging these classification labels with supervised learning techniques such as multi-class classification neural networks or logistic regression classifiers. \n",
      "&gt; \n",
      "&gt; In short, this substantial resource offers an array of creative ways to explore different types of dialogue related applications without being limited by needing data from external web sources – all that’s needed from here is your own imagination!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Generating deep learning models to detect and respond to conversational intent. \n",
      "&gt; - Training language models to use natural language processing (NLP) for customer service queries. \n",
      "&gt; - Creating custom dialogue agents that are better able to handle more complex conversational interactions, such as those powered by machine learning techniques like supervised or unsupervised learning methods\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name     | Description                                                                                                                                         |\n",
      "|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **Instruction** | Text prompt that should generate an appropriate response from a machine learning model/chatbot using natural language processing techniques. (Text) |\n",
      "| **Context**     | Provides context to improve accuracy by giving the model more information about what’s happening in a conversation or request execution. (Text)     |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/databricks/databricks-dolly-15k).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/tripadvisor-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: tripadvisor-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: TripAdvisor Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Uncovering Semantic Aspects of Online Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# TripAdvisor Hotel Reviews\n",
      "### Uncovering Semantic Aspects of Online Reviews\n",
      "By  [[source]](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> Are you looking for a way to dive deeper into the sentiment of online hotel reviews?  Our TripAdvisor dataset is designed to help researchers uncover the underlying semantic aspects of online reviews, such as sentiment and sentiment orientation. This collection of 6,444 ratings contains reviews rated by users on a five-star scale. By utilizing joint multi-grain topic sentiment modeling, researchers are able to get an in-depth understanding of how people feel about their experiences. Unlock new insights with TripAdvisor's Hotel Reviews Sentiment Analysis dataset now!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset contains hotel reviews from the popular booking website TripAdvisor. It is a valuable source of information for researchers and marketers looking to gauge sentiment, sentiment orientation and other semantic aspects of online reviews. In this guide, we will show you how to use this dataset to uncover the insights it holds. \n",
      "> \n",
      "> - Analyzing Sentiment: The most straightforward way to use this data set is for analyzing sentiment in online reviews. Every review has a rating on a five star scale, which can be used as an indication of how satisfied the customer was with their experience at the hotel. You can use various methods such as text mining or machine learning models to deeper understand what is driving those ratings and determine which customers are more likely to rate positively or negatively based on their words used in the review itself. \n",
      "> \n",
      "> - Understanding Aspects: The review text included in each observation also provides valuable insight into what exactly customers liked or disliked about their stay. This data can be further inspected using techniques such as topic modeling to uncover topics that people mention when writing positive/negative reviews - making this model ideal for understanding what aspects are driving satisfaction with hotels \n",
      "> \n",
      "> - Prediction Models: Finally, you could build upon all these insights by using them in predictive models such as logistic regression or support vector machines which predict customer ratings based on certain features extracted from the text through natural language processing (NLP). By identifying relationships between certain topics mentioned and ratings given, businesses can better target campaigns that strengthen relationships with existing customers while justifying budgets allotted towards acquiring new ones!  \n",
      "> \n",
      ">  In conclusion, by exploring characteristics such as customer sentiment and its orientation through textual analysis of online reviews you can get an invaluable perspective into your products overall performance - allowing decisions makers to make well informed marketing decisions!\n",
      "\n",
      "### Research Ideas\n",
      "> - Training a text classifier to determine positive or negative sentiment in hotel reviews.\n",
      "> - Developing an automated summarization tool to quickly identify the most important points discussed in a review.\n",
      "> - Identifying specific topics that customers commonly complain about when writing product reviews on TripAdvisor, as well as surfacing constructive feedback for businesses looking to improve their services and offerings\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: b.csv**\n",
      "| Column name   | Description                                          |\n",
      "|:--------------|:-----------------------------------------------------|\n",
      "| **S.No.**     | Unique identifier for each review. (Integer)         |\n",
      "| **Review**    | Text of the review. (String)                         |\n",
      "| **Rating**    | Rating of the review on a five-star scale. (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tripadvisor-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: TripAdvisor Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Uncovering Semantic Aspects of Online Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# TripAdvisor Hotel Reviews\n",
      "### Uncovering Semantic Aspects of Online Reviews\n",
      "By  [[source]](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> Are you looking for a way to dive deeper into the sentiment of online hotel reviews?  Our TripAdvisor dataset is designed to help researchers uncover the underlying semantic aspects of online reviews, such as sentiment and sentiment orientation. This collection of 6,444 ratings contains reviews rated by users on a five-star scale. By utilizing joint multi-grain topic sentiment modeling, researchers are able to get an in-depth understanding of how people feel about their experiences. Unlock new insights with TripAdvisor's Hotel Reviews Sentiment Analysis dataset now!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset contains hotel reviews from the popular booking website TripAdvisor. It is a valuable source of information for researchers and marketers looking to gauge sentiment, sentiment orientation and other semantic aspects of online reviews. In this guide, we will show you how to use this dataset to uncover the insights it holds. \n",
      "> \n",
      "> - Analyzing Sentiment: The most straightforward way to use this data set is for analyzing sentiment in online reviews. Every review has a rating on a five star scale, which can be used as an indication of how satisfied the customer was with their experience at the hotel. You can use various methods such as text mining or machine learning models to deeper understand what is driving those ratings and determine which customers are more likely to rate positively or negatively based on their words used in the review itself. \n",
      "> \n",
      "> - Understanding Aspects: The review text included in each observation also provides valuable insight into what exactly customers liked or disliked about their stay. This data can be further inspected using techniques such as topic modeling to uncover topics that people mention when writing positive/negative reviews - making this model ideal for understanding what aspects are driving satisfaction with hotels \n",
      "> \n",
      "> - Prediction Models: Finally, you could build upon all these insights by using them in predictive models such as logistic regression or support vector machines which predict customer ratings based on certain features extracted from the text through natural language processing (NLP). By identifying relationships between certain topics mentioned and ratings given, businesses can better target campaigns that strengthen relationships with existing customers while justifying budgets allotted towards acquiring new ones!  \n",
      "> \n",
      ">  In conclusion, by exploring characteristics such as customer sentiment and its orientation through textual analysis of online reviews you can get an invaluable perspective into your products overall performance - allowing decisions makers to make well informed marketing decisions!\n",
      "\n",
      "### Research Ideas\n",
      "> - Training a text classifier to determine positive or negative sentiment in hotel reviews.\n",
      "> - Developing an automated summarization tool to quickly identify the most important points discussed in a review.\n",
      "> - Identifying specific topics that customers commonly complain about when writing product reviews on TripAdvisor, as well as surfacing constructive feedback for businesses looking to improve their services and offerings\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: b.csv**\n",
      "| Column name   | Description                                          |\n",
      "|:--------------|:-----------------------------------------------------|\n",
      "| **S.No.**     | Unique identifier for each review. (Integer)         |\n",
      "| **Review**    | Text of the review. (String)                         |\n",
      "| **Rating**    | Rating of the review on a five-star scale. (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/1219899#.Y9Y_N9JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/online-shopping-consumer-behavior-dataset\n",
      "name 'detect' is not defined Language is not detected: online-shopping-consumer-behavior-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Online Shopping Consumer Behavior Dataset\n",
      "name 'detect' is not defined Language is not detected: Consumer Buying Patterns in E-Commerce\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Online Shopping Consumer Behavior Dataset\n",
      "### Consumer Buying Patterns in E-Commerce\n",
      "By Weitong Li [[source]](https://data.world/jackjo999)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset is a rich compilation of data that thoroughly guides us through consumers' behavior and their buying intentions while engaged in online shopping. It has been constructed with immense care to ensure it effectively examines an array of factors that influence customers' purchasing intentions in the increasingly significant realm of digital commerce.\n",
      "&gt; \n",
      "&gt; The dataset is exhaustively composed with careful attention to collecting a diverse set of information, thus allowing a broad view into what affects online shopping behavior. Specific columns included cover customer's existing awareness about the website or source from where they are shopping, their information regarding the products they wish to purchase, and more importantly, their satisfaction level related to previous purchases.\n",
      "&gt; \n",
      "&gt; Additionally, the dataset delves deep into investigating both objective and subjective aspects impacting customer behavior online. As such, it includes data on various webpage factors like loading speed, user-friendly interface design, webpage aesthetics, etc., which could significantly persuade the consumer's decision-making process during online shopping. The completion and submission convenience provided by those websites also form part of this database.\n",
      "&gt; \n",
      "&gt; In order to fully understand consumer behavior within an online environment from multiple facets', individual consumers' subjective views are also captured in this dataset; it explores how consumers perceive their trust towards an e-commerce site or if they believe it’s convenient for them to shop via these platforms versus traditional methods? Do they feel relaxed when doing so?\n",
      "&gt; \n",
      "&gt; In recognizing how crucial products competitiveness within such landscapes influences buyer intention - columns that provide details on critical characteristics like price comparisons against offline stores or similar product competitors across different websites have been included too.\n",
      "&gt; \n",
      "&gt; Overall this comprehensive aggregated data collection aims not only at understanding fundamental consumer preferences but also towards predicting future buying behaviors hence forth enabling businesses capitalize on emerging trends within online retail spaces more efficiently & profitably\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; In an online-focused world, understanding consumer behavioral data is crucial. The 'Online Shopping Purchasing Intention Dataset' provides a comprehensive collection of consumer-based insights based on their behavior in virtual shopping environments. This dataset explores various factors that might affect a customer's decision to purchase. Here's how you can harness this dataset:\n",
      "&gt; \n",
      "&gt; #### Defining the Problem \n",
      "&gt; Identify a problem or question this data may answer. This might be: understanding what factors influence buying decisions, predicting whether a visit will result in a purchase based on user behavior, analyzing the impact of the month, operating system or traffic type on online purchasing intention etc.\n",
      "&gt; \n",
      "&gt; #### Data Exploration\n",
      "&gt; Understand the structure of the dataset by getting to know each variable and its meaning:\n",
      "&gt; - Administrative: Counting different types of pages visited by the user in that session.\n",
      "&gt; - Informational & Product Related: Measures how many informational/product related pages are viewed.\n",
      "&gt; - Bounce Rates, Exit Rate, Page Values: Assess these metrics as they provide significant insight about visitor activity.\n",
      "&gt; - Special Day: Explore correlation between proximity to special days (like Mother’s day and Valentine’s Day) with transactions.\n",
      "&gt; - Operating Systems / Browser / Region / Traffic Type: Uncover behavioral patterns associated with technical specs/geo location/ source of traffic.\n",
      "&gt; \n",
      "&gt; #### Analysis and Visualization\n",
      "&gt; Use appropriate statistical analysis techniques to scrutinize relationships between variables such as correlation analysis or chi-square tests for independence etc.\n",
      "&gt; \n",
      "&gt; Visualize your findings using plots like bar graphs for categorical features comparison or scatter plots for multivariate relationships etc.\n",
      "&gt; \n",
      "&gt; #### Model Building\n",
      "&gt; Use machine learning algorithms (like logistic regression or decision tree models) potentially useful if your goal is predicting purchase intention based on given features.\n",
      "&gt; \n",
      "&gt; This could also involve feature selection - choosing most relevant predictors; training & testing model and finally assessing model performance through metrics like accuracy score, precision-recall scores etc.\n",
      "&gt; \n",
      "&gt; Remember to appropriately handle missing values if any before diving into predictive modeling\n",
      "&gt; \n",
      "&gt; The comprehensive nature of this dataset caters to deep-dives into various aspects of consumer behavior in online shopping environments. Whether you aim to understand consumers better or build a predictive model, this dataset avails all information necessary. \n",
      "&gt; \n",
      "&gt; Keep it ethical - use this data responsibly while acknowledging privacy concerns by abstaining from attempting any form of personal identification using the data available. Happy exploring!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Predictive Analysis: The dataset can be used for creating machine learning models to predict whether a particular user will eventually make a purchase based on their online browsing behaviour. This can help businesses to identify potential customers and implement targeted marketing strategies.\n",
      "&gt; - Consumer Behaviour Analysis: An in-depth study of the data can reveal patterns and trends in consumer behaviour, such as what kind of products they are more likely to buy, at what times they are most likely to shop or which pages they spend most time on. Businesses can use this information to optimise their online platforms for better customer experience and increased sales.\n",
      "&gt; - Personalized Recommendation Systems: The dataset could be used in developing personalized product recommendation systems that recommend products based on individual customer's browsing history, region, administrative details etc., increasing the chance of conversions or repeat purchases\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/jackjo999)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Weitong Li](https://data.world/jackjo999).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: online-shopping-consumer-behavior-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Online Shopping Consumer Behavior Dataset\n",
      "name 'detect' is not defined Language is not detected: Consumer Buying Patterns in E-Commerce\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Online Shopping Consumer Behavior Dataset\n",
      "### Consumer Buying Patterns in E-Commerce\n",
      "By Weitong Li [[source]](https://data.world/jackjo999)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset is a rich compilation of data that thoroughly guides us through consumers' behavior and their buying intentions while engaged in online shopping. It has been constructed with immense care to ensure it effectively examines an array of factors that influence customers' purchasing intentions in the increasingly significant realm of digital commerce.\n",
      "&gt; \n",
      "&gt; The dataset is exhaustively composed with careful attention to collecting a diverse set of information, thus allowing a broad view into what affects online shopping behavior. Specific columns included cover customer's existing awareness about the website or source from where they are shopping, their information regarding the products they wish to purchase, and more importantly, their satisfaction level related to previous purchases.\n",
      "&gt; \n",
      "&gt; Additionally, the dataset delves deep into investigating both objective and subjective aspects impacting customer behavior online. As such, it includes data on various webpage factors like loading speed, user-friendly interface design, webpage aesthetics, etc., which could significantly persuade the consumer's decision-making process during online shopping. The completion and submission convenience provided by those websites also form part of this database.\n",
      "&gt; \n",
      "&gt; In order to fully understand consumer behavior within an online environment from multiple facets', individual consumers' subjective views are also captured in this dataset; it explores how consumers perceive their trust towards an e-commerce site or if they believe it’s convenient for them to shop via these platforms versus traditional methods? Do they feel relaxed when doing so?\n",
      "&gt; \n",
      "&gt; In recognizing how crucial products competitiveness within such landscapes influences buyer intention - columns that provide details on critical characteristics like price comparisons against offline stores or similar product competitors across different websites have been included too.\n",
      "&gt; \n",
      "&gt; Overall this comprehensive aggregated data collection aims not only at understanding fundamental consumer preferences but also towards predicting future buying behaviors hence forth enabling businesses capitalize on emerging trends within online retail spaces more efficiently & profitably\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; In an online-focused world, understanding consumer behavioral data is crucial. The 'Online Shopping Purchasing Intention Dataset' provides a comprehensive collection of consumer-based insights based on their behavior in virtual shopping environments. This dataset explores various factors that might affect a customer's decision to purchase. Here's how you can harness this dataset:\n",
      "&gt; \n",
      "&gt; #### Defining the Problem \n",
      "&gt; Identify a problem or question this data may answer. This might be: understanding what factors influence buying decisions, predicting whether a visit will result in a purchase based on user behavior, analyzing the impact of the month, operating system or traffic type on online purchasing intention etc.\n",
      "&gt; \n",
      "&gt; #### Data Exploration\n",
      "&gt; Understand the structure of the dataset by getting to know each variable and its meaning:\n",
      "&gt; - Administrative: Counting different types of pages visited by the user in that session.\n",
      "&gt; - Informational & Product Related: Measures how many informational/product related pages are viewed.\n",
      "&gt; - Bounce Rates, Exit Rate, Page Values: Assess these metrics as they provide significant insight about visitor activity.\n",
      "&gt; - Special Day: Explore correlation between proximity to special days (like Mother’s day and Valentine’s Day) with transactions.\n",
      "&gt; - Operating Systems / Browser / Region / Traffic Type: Uncover behavioral patterns associated with technical specs/geo location/ source of traffic.\n",
      "&gt; \n",
      "&gt; #### Analysis and Visualization\n",
      "&gt; Use appropriate statistical analysis techniques to scrutinize relationships between variables such as correlation analysis or chi-square tests for independence etc.\n",
      "&gt; \n",
      "&gt; Visualize your findings using plots like bar graphs for categorical features comparison or scatter plots for multivariate relationships etc.\n",
      "&gt; \n",
      "&gt; #### Model Building\n",
      "&gt; Use machine learning algorithms (like logistic regression or decision tree models) potentially useful if your goal is predicting purchase intention based on given features.\n",
      "&gt; \n",
      "&gt; This could also involve feature selection - choosing most relevant predictors; training & testing model and finally assessing model performance through metrics like accuracy score, precision-recall scores etc.\n",
      "&gt; \n",
      "&gt; Remember to appropriately handle missing values if any before diving into predictive modeling\n",
      "&gt; \n",
      "&gt; The comprehensive nature of this dataset caters to deep-dives into various aspects of consumer behavior in online shopping environments. Whether you aim to understand consumers better or build a predictive model, this dataset avails all information necessary. \n",
      "&gt; \n",
      "&gt; Keep it ethical - use this data responsibly while acknowledging privacy concerns by abstaining from attempting any form of personal identification using the data available. Happy exploring!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Predictive Analysis: The dataset can be used for creating machine learning models to predict whether a particular user will eventually make a purchase based on their online browsing behaviour. This can help businesses to identify potential customers and implement targeted marketing strategies.\n",
      "&gt; - Consumer Behaviour Analysis: An in-depth study of the data can reveal patterns and trends in consumer behaviour, such as what kind of products they are more likely to buy, at what times they are most likely to shop or which pages they spend most time on. Businesses can use this information to optimise their online platforms for better customer experience and increased sales.\n",
      "&gt; - Personalized Recommendation Systems: The dataset could be used in developing personalized product recommendation systems that recommend products based on individual customer's browsing history, region, administrative details etc., increasing the chance of conversions or repeat purchases\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/jackjo999)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Weitong Li](https://data.world/jackjo999).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/amazon-prime-movies-and-information\n",
      "name 'detect' is not defined Language is not detected: amazon-prime-movies-and-information\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Amazon Prime Movies and Information\n",
      "name 'detect' is not defined Language is not detected: A Wide Collection of Rated Movies for Analyzing Global Trends in Media\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Amazon Prime Movies and Information\n",
      "### A Wide Collection of Rated Movies for Analyzing Global Trends in Media\n",
      "By Irnadia Fardila [[source]](https://data.world/nadnadnad)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains a comprehensive list of popular Amazon Prime titles and their associated information. With over 700 movies from around the world, viewers have plenty from which to choose. Included in the data are details on genre, runtime, release year, country of origin, cast and crew along with added-value information such as a synopsis and IMDB ratings. Get lost in cinematic gems ranging from foreign classics to major blockbusters or simply start your journey by finding out which movie has been most recently added! Making it easy to browse through all that Amazon Prime offers without missing one interesting title\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains information about Amazon Prime movies, including title, director, cast, country, release year, rating, duration and description. It is useful for anyone interested in streaming media on Amazon Prime. \n",
      "&gt; \n",
      "\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Recommending similar films or TV shows based on genre, director, cast, and other data from the dataset.\n",
      "&gt; - Constructing a collaborative filtering recommendation engine using the titles and ratings of films/TV shows in this dataset.\n",
      "&gt; - Creating a visual representation of viewing trends of different genres in different countries over time using release year data and plotting them on graphs or maps using country information from the dataset\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/nadnadnad)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: amazon_prime_titles.csv**\n",
      "| Column name   | Description                                      |\n",
      "|:--------------|:-------------------------------------------------|\n",
      "| **Title**     | The title of the media in question. (String)     |\n",
      "| **Director**  | The director of the media. (String)              |\n",
      "| **Country**   | The country where the media was filmed. (String) |\n",
      "| **Rating**    | The rating of the media. (String)                |\n",
      "| **Type**      | The type of media (movie or TV show). (String)   |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Irnadia Fardila](https://data.world/nadnadnad).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: amazon-prime-movies-and-information\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Amazon Prime Movies and Information\n",
      "name 'detect' is not defined Language is not detected: A Wide Collection of Rated Movies for Analyzing Global Trends in Media\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Amazon Prime Movies and Information\n",
      "### A Wide Collection of Rated Movies for Analyzing Global Trends in Media\n",
      "By Irnadia Fardila [[source]](https://data.world/nadnadnad)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains a comprehensive list of popular Amazon Prime titles and their associated information. With over 700 movies from around the world, viewers have plenty from which to choose. Included in the data are details on genre, runtime, release year, country of origin, cast and crew along with added-value information such as a synopsis and IMDB ratings. Get lost in cinematic gems ranging from foreign classics to major blockbusters or simply start your journey by finding out which movie has been most recently added! Making it easy to browse through all that Amazon Prime offers without missing one interesting title\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains information about Amazon Prime movies, including title, director, cast, country, release year, rating, duration and description. It is useful for anyone interested in streaming media on Amazon Prime. \n",
      "&gt; \n",
      "\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Recommending similar films or TV shows based on genre, director, cast, and other data from the dataset.\n",
      "&gt; - Constructing a collaborative filtering recommendation engine using the titles and ratings of films/TV shows in this dataset.\n",
      "&gt; - Creating a visual representation of viewing trends of different genres in different countries over time using release year data and plotting them on graphs or maps using country information from the dataset\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/nadnadnad)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: amazon_prime_titles.csv**\n",
      "| Column name   | Description                                      |\n",
      "|:--------------|:-------------------------------------------------|\n",
      "| **Title**     | The title of the media in question. (String)     |\n",
      "| **Director**  | The director of the media. (String)              |\n",
      "| **Country**   | The country where the media was filmed. (String) |\n",
      "| **Rating**    | The rating of the media. (String)                |\n",
      "| **Type**      | The type of media (movie or TV show). (String)   |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Irnadia Fardila](https://data.world/nadnadnad).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/kubernetes-kubectl-command-dataset\n",
      "name 'detect' is not defined Language is not detected: kubernetes-kubectl-command-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Kubernetes Commands\n",
      "name 'detect' is not defined Language is not detected: kubectl commands and descriptions for Kubernetes\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Kubernetes Commands\n",
      "### kubectl commands and descriptions for Kubernetes\n",
      "By ComponentSoft (From Huggingface) [[source]](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The ComponentSoft/k8s-kubectl-cot-20k dataset is a valuable resource for researchers and developers who are interested in gaining proficiency in Kubernetes and its kubectl command line interface. This training dataset provides a wide-ranging collection of kubectl commands paired with detailed descriptions, allowing users to enhance their understanding and mastery of these essential commands in the Kubernetes ecosystem. With this dataset, users can explore the objective or purpose behind each kubectl command, grasp their syntax or format, familiarize themselves with the available flags or options for customization, and even discover additional insights through thought-provoking questions related to each command. By leveraging this comprehensive dataset, individuals can acquire a deep understanding of various kubectl commands while honing their skills to efficiently manage Kubernetes clusters and streamline their workflow\n",
      "\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing a kubectl command recommendation system: The dataset can be used to train machine learning models to recommend the most relevant kubectl commands based on user input or specific scenarios. This can help users streamline their workflow and improve productivity.\n",
      "&gt; - Evaluation of documentation quality: By analyzing the descriptions provided in the dataset, researchers can evaluate the clarity and effectiveness of existing documentation for various kubectl commands. This feedback can be used to improve official Kubernetes documentation or develop better tutorials and guides.\n",
      "&gt; - Command usage analysis: The dataset can be used to analyze the popularity and frequency of different kubectl commands in real-world scenarios. This analysis can provide insights into commonly used commands, potential areas for optimization, or identify underutilized features that developers may not be aware of\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name          | Description                                                                                       |\n",
      "|:---------------------|:--------------------------------------------------------------------------------------------------|\n",
      "| **objective**        | The objective or purpose of the kubectl command. (Text)                                           |\n",
      "| **command_name**     | The name of the kubectl command. (Text)                                                           |\n",
      "| **description**      | A description of what the kubectl command does. (Text)                                            |\n",
      "| **syntax**           | The syntax or format of the kubectl command. (Text)                                               |\n",
      "| **flags**            | Any flags or options that can be used with the kubectl command. (Text)                            |\n",
      "| **question**         | A question related to the usage or behavior of the kubectl command. (Text)                        |\n",
      "| **chain_of_thought** | A series of steps or thought process related to utilizing the kubectl command effectively. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [ComponentSoft (From Huggingface)](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: kubernetes-kubectl-command-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Kubernetes Commands\n",
      "name 'detect' is not defined Language is not detected: kubectl commands and descriptions for Kubernetes\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Kubernetes Commands\n",
      "### kubectl commands and descriptions for Kubernetes\n",
      "By ComponentSoft (From Huggingface) [[source]](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The ComponentSoft/k8s-kubectl-cot-20k dataset is a valuable resource for researchers and developers who are interested in gaining proficiency in Kubernetes and its kubectl command line interface. This training dataset provides a wide-ranging collection of kubectl commands paired with detailed descriptions, allowing users to enhance their understanding and mastery of these essential commands in the Kubernetes ecosystem. With this dataset, users can explore the objective or purpose behind each kubectl command, grasp their syntax or format, familiarize themselves with the available flags or options for customization, and even discover additional insights through thought-provoking questions related to each command. By leveraging this comprehensive dataset, individuals can acquire a deep understanding of various kubectl commands while honing their skills to efficiently manage Kubernetes clusters and streamline their workflow\n",
      "\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing a kubectl command recommendation system: The dataset can be used to train machine learning models to recommend the most relevant kubectl commands based on user input or specific scenarios. This can help users streamline their workflow and improve productivity.\n",
      "&gt; - Evaluation of documentation quality: By analyzing the descriptions provided in the dataset, researchers can evaluate the clarity and effectiveness of existing documentation for various kubectl commands. This feedback can be used to improve official Kubernetes documentation or develop better tutorials and guides.\n",
      "&gt; - Command usage analysis: The dataset can be used to analyze the popularity and frequency of different kubectl commands in real-world scenarios. This analysis can provide insights into commonly used commands, potential areas for optimization, or identify underutilized features that developers may not be aware of\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name          | Description                                                                                       |\n",
      "|:---------------------|:--------------------------------------------------------------------------------------------------|\n",
      "| **objective**        | The objective or purpose of the kubectl command. (Text)                                           |\n",
      "| **command_name**     | The name of the kubectl command. (Text)                                                           |\n",
      "| **description**      | A description of what the kubectl command does. (Text)                                            |\n",
      "| **syntax**           | The syntax or format of the kubectl command. (Text)                                               |\n",
      "| **flags**            | Any flags or options that can be used with the kubectl command. (Text)                            |\n",
      "| **question**         | A question related to the usage or behavior of the kubectl command. (Text)                        |\n",
      "| **chain_of_thought** | A series of steps or thought process related to utilizing the kubectl command effectively. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [ComponentSoft (From Huggingface)](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/video-characteristics-and-transcoding-time\n",
      "name 'detect' is not defined Language is not detected: video-characteristics-and-transcoding-time\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Video Characteristics and Transcoding Time\n",
      "name 'detect' is not defined Language is not detected: Video Characteristics and Transcoding Time for YouTube Videos\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Video Characteristics and Transcoding Time\n",
      "### Video Characteristics and Transcoding Time for YouTube Videos\n",
      "By UCI [[source]](https://data.world/uci)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The dataset contains two files: youtube_videos.csv and transcoding_mesurment.csv. The first file consists of data for 1.6 million YouTube videos, including their ID, duration, bitrate, height and width (in pixels), frame rate, estimated frame rate, codec used, video category, and direct link to the video. This dataset provides information about consumer videos found on UGC platforms like YouTube.\n",
      "&gt; \n",
      "&gt; The second file includes information about the transcoding process for different video formats. It contains input and output video characteristics such as duration, bitrate (total and per video), height and width of the frames, actual frame rate along with its estimation value using an Intel i7-3720QM CPU. Moreover it has features like codec used in the transcoding process (input/output codecs) as categorical variables . Additionally transcoding time is included in both datasets which would be helpful while making a transcoding time prediction model in section 6.The column names are relevant enough so that understanding purpose is easier\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; # How to Use This Dataset\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Overview of the Dataset\n",
      "&gt; \n",
      "&gt; The dataset consists of two TSV (Tab-Separated Values) files: youtube_videos.tsv and transcoding_mesurment.tsv. Let's take a closer look at each file:\n",
      "&gt; \n",
      "&gt; ### youtube_videos.tsv:\n",
      "&gt; \n",
      "&gt; This file contains 1.6 million randomly sampled YouTube videos with 10 fundamental video characteristics. Each row represents a unique video instance, and the columns provide information about the following characteristics:\n",
      "&gt; \n",
      "&gt; - **id:** The YouTube video ID.\n",
      "&gt; - **duration:** The duration of the video in seconds.\n",
      "&gt; - **bitrate:** The total bitrate of the video in Kbits.\n",
      "&gt; - **height:** The height of the video in pixels.\n",
      "&gt; - **width:** The width of the video in pixels.\n",
      "&gt; - **frame rate:** The actual frame rate of the video.\n",
      "&gt; - **frame rate(est.):** The estimated frame rate of the vi deo .\n",
      "&gt; - **codec:** The coding standard used for encoding th e vide o .\n",
      "&gt; - **category:**\n",
      "&gt;     - Categorical value representing YouTube vid eo categ ory .\n",
      "&gt;     - Useful for categorizing videos based on their content .\n",
      "&gt; \n",
      "&gt; ### transcoding_mesurment.tsv:\n",
      "&gt; \n",
      "&gt; This file contains additional information about input and outputvideo characteristics, transcoding time, and memory resource requirements for differentvideo formats using ffmpeg 4 as a transcoding application. Each row represents an experimental setup where two rows from youtube_videos.tsv were randomly selected as input parameters to ffmpeg's transcoding process. The columns provide information about various aspects, including:\n",
      "&gt; \n",
      "&gt; **Input Video Characteristics:**\n",
      "&gt; \n",
      "&gt; Inverse details such as (*i_size*, *p_size*, *b_size*), *frames* total number of frames in video.\n",
      "&gt; \n",
      "&gt; **Output Video Characteristics:**\n",
      "&gt; \n",
      "&gt; - **o_codec:** The output codec used for transcoding.\n",
      "&gt; - **o_bitrate:** The output bitrate used for transcoding.\n",
      "&gt; - **o_framerate:** The output frame rate used for transcoding.\n",
      "&gt; - **o_height:** The output height in pixels used for transcoding.\n",
      "&gt; \n",
      "&gt; **Transcoding Time and Memory Resource Requirements:**\n",
      "&gt; \n",
      "&gt; - **umem:** The total codec allocated memory for transcod ing .\n",
      "&gt; - **utime:** The total transcoding time in seconds.\n",
      "&gt; \n",
      "&gt; ## Potential Applications of the Dataset\n",
      "&gt; \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Video Quality Analysis: With information about bitrate, height, width, and frame rate, this dataset can be used to analyze the quality of YouTube videos in terms of their visual characteristics. By comparing these characteristics across different categories or codecs, insights can be gained about how different factors impact video quality.\n",
      "&gt; - YouTube Video Classification: By utilizing the category information in this dataset along with other fundamental characteristics, machine learning algorithms can be trained to classify YouTube videos into different categories automatically. This could help improve content classification on platforms like YouTube and aid in recommendation systems for users\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/uci)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: transcoding_mesurment.csv**\n",
      "| Column name     | Description                                                              |\n",
      "|:----------------|:-------------------------------------------------------------------------|\n",
      "| **duration**    | The length of the video in seconds. (Numeric)                            |\n",
      "| **codec**       | The coding standard used for encoding the video. (Categorical)           |\n",
      "| **height**      | The vertical resolution of the video in pixels. (Numeric)                |\n",
      "| **bitrate**     | The total bitrate or data transfer rate of the video in Kbits. (Numeric) |\n",
      "| **framerate**   | The number of frames per second in the video. (Numeric)                  |\n",
      "| **i_size**      | The size in bytes of the I-frames in the video. (Numeric)                |\n",
      "| **p_size**      | The size in bytes of the P-frames in the video. (Numeric)                |\n",
      "| **b_size**      | The size in bytes of the B-frames in the video. (Numeric)                |\n",
      "| **o_codec**     | The output codec used for transcoding. (Categorical)                     |\n",
      "| **o_bitrate**   | The output bitrate used for transcoding. (Numeric)                       |\n",
      "| **o_framerate** | The output frame rate used for transcoding. (Numeric)                    |\n",
      "| **o_height**    | The output vertical resolution of the video after transcoding. (Numeric) |\n",
      "| **umem**        | The total codec allocated memory for transcoding. (Numeric)              |\n",
      "| **utime**       | The transcoding time in seconds. (Numeric)                               |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: youtube_videos.csv**\n",
      "| Column name          | Description                                                                             |\n",
      "|:---------------------|:----------------------------------------------------------------------------------------|\n",
      "| **duration**         | The length of the video in seconds. (Numeric)                                           |\n",
      "| **bitrate**          | The total bitrate or data transfer rate of the video in Kbits. (Numeric)                |\n",
      "| **height**           | The vertical resolution of the video in pixels. (Numeric)                               |\n",
      "| **codec**            | The coding standard used for encoding the video. (Categorical)                          |\n",
      "| **frame rate**       | The number of frames per second in a video. (Numeric)                                   |\n",
      "| **frame rate(est.)** | An estimated value for the frame rate of the video. (Numeric)                           |\n",
      "| **category**         | The categorization of the video based on YouTube's predefined categories. (Categorical) |\n",
      "| **url**              | The direct link to the video. (Text)                                                    |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [UCI](https://data.world/uci).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: video-characteristics-and-transcoding-time\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Video Characteristics and Transcoding Time\n",
      "name 'detect' is not defined Language is not detected: Video Characteristics and Transcoding Time for YouTube Videos\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Video Characteristics and Transcoding Time\n",
      "### Video Characteristics and Transcoding Time for YouTube Videos\n",
      "By UCI [[source]](https://data.world/uci)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The dataset contains two files: youtube_videos.csv and transcoding_mesurment.csv. The first file consists of data for 1.6 million YouTube videos, including their ID, duration, bitrate, height and width (in pixels), frame rate, estimated frame rate, codec used, video category, and direct link to the video. This dataset provides information about consumer videos found on UGC platforms like YouTube.\n",
      "&gt; \n",
      "&gt; The second file includes information about the transcoding process for different video formats. It contains input and output video characteristics such as duration, bitrate (total and per video), height and width of the frames, actual frame rate along with its estimation value using an Intel i7-3720QM CPU. Moreover it has features like codec used in the transcoding process (input/output codecs) as categorical variables . Additionally transcoding time is included in both datasets which would be helpful while making a transcoding time prediction model in section 6.The column names are relevant enough so that understanding purpose is easier\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; # How to Use This Dataset\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Overview of the Dataset\n",
      "&gt; \n",
      "&gt; The dataset consists of two TSV (Tab-Separated Values) files: youtube_videos.tsv and transcoding_mesurment.tsv. Let's take a closer look at each file:\n",
      "&gt; \n",
      "&gt; ### youtube_videos.tsv:\n",
      "&gt; \n",
      "&gt; This file contains 1.6 million randomly sampled YouTube videos with 10 fundamental video characteristics. Each row represents a unique video instance, and the columns provide information about the following characteristics:\n",
      "&gt; \n",
      "&gt; - **id:** The YouTube video ID.\n",
      "&gt; - **duration:** The duration of the video in seconds.\n",
      "&gt; - **bitrate:** The total bitrate of the video in Kbits.\n",
      "&gt; - **height:** The height of the video in pixels.\n",
      "&gt; - **width:** The width of the video in pixels.\n",
      "&gt; - **frame rate:** The actual frame rate of the video.\n",
      "&gt; - **frame rate(est.):** The estimated frame rate of the vi deo .\n",
      "&gt; - **codec:** The coding standard used for encoding th e vide o .\n",
      "&gt; - **category:**\n",
      "&gt;     - Categorical value representing YouTube vid eo categ ory .\n",
      "&gt;     - Useful for categorizing videos based on their content .\n",
      "&gt; \n",
      "&gt; ### transcoding_mesurment.tsv:\n",
      "&gt; \n",
      "&gt; This file contains additional information about input and outputvideo characteristics, transcoding time, and memory resource requirements for differentvideo formats using ffmpeg 4 as a transcoding application. Each row represents an experimental setup where two rows from youtube_videos.tsv were randomly selected as input parameters to ffmpeg's transcoding process. The columns provide information about various aspects, including:\n",
      "&gt; \n",
      "&gt; **Input Video Characteristics:**\n",
      "&gt; \n",
      "&gt; Inverse details such as (*i_size*, *p_size*, *b_size*), *frames* total number of frames in video.\n",
      "&gt; \n",
      "&gt; **Output Video Characteristics:**\n",
      "&gt; \n",
      "&gt; - **o_codec:** The output codec used for transcoding.\n",
      "&gt; - **o_bitrate:** The output bitrate used for transcoding.\n",
      "&gt; - **o_framerate:** The output frame rate used for transcoding.\n",
      "&gt; - **o_height:** The output height in pixels used for transcoding.\n",
      "&gt; \n",
      "&gt; **Transcoding Time and Memory Resource Requirements:**\n",
      "&gt; \n",
      "&gt; - **umem:** The total codec allocated memory for transcod ing .\n",
      "&gt; - **utime:** The total transcoding time in seconds.\n",
      "&gt; \n",
      "&gt; ## Potential Applications of the Dataset\n",
      "&gt; \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Video Quality Analysis: With information about bitrate, height, width, and frame rate, this dataset can be used to analyze the quality of YouTube videos in terms of their visual characteristics. By comparing these characteristics across different categories or codecs, insights can be gained about how different factors impact video quality.\n",
      "&gt; - YouTube Video Classification: By utilizing the category information in this dataset along with other fundamental characteristics, machine learning algorithms can be trained to classify YouTube videos into different categories automatically. This could help improve content classification on platforms like YouTube and aid in recommendation systems for users\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/uci)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: transcoding_mesurment.csv**\n",
      "| Column name     | Description                                                              |\n",
      "|:----------------|:-------------------------------------------------------------------------|\n",
      "| **duration**    | The length of the video in seconds. (Numeric)                            |\n",
      "| **codec**       | The coding standard used for encoding the video. (Categorical)           |\n",
      "| **height**      | The vertical resolution of the video in pixels. (Numeric)                |\n",
      "| **bitrate**     | The total bitrate or data transfer rate of the video in Kbits. (Numeric) |\n",
      "| **framerate**   | The number of frames per second in the video. (Numeric)                  |\n",
      "| **i_size**      | The size in bytes of the I-frames in the video. (Numeric)                |\n",
      "| **p_size**      | The size in bytes of the P-frames in the video. (Numeric)                |\n",
      "| **b_size**      | The size in bytes of the B-frames in the video. (Numeric)                |\n",
      "| **o_codec**     | The output codec used for transcoding. (Categorical)                     |\n",
      "| **o_bitrate**   | The output bitrate used for transcoding. (Numeric)                       |\n",
      "| **o_framerate** | The output frame rate used for transcoding. (Numeric)                    |\n",
      "| **o_height**    | The output vertical resolution of the video after transcoding. (Numeric) |\n",
      "| **umem**        | The total codec allocated memory for transcoding. (Numeric)              |\n",
      "| **utime**       | The transcoding time in seconds. (Numeric)                               |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: youtube_videos.csv**\n",
      "| Column name          | Description                                                                             |\n",
      "|:---------------------|:----------------------------------------------------------------------------------------|\n",
      "| **duration**         | The length of the video in seconds. (Numeric)                                           |\n",
      "| **bitrate**          | The total bitrate or data transfer rate of the video in Kbits. (Numeric)                |\n",
      "| **height**           | The vertical resolution of the video in pixels. (Numeric)                               |\n",
      "| **codec**            | The coding standard used for encoding the video. (Categorical)                          |\n",
      "| **frame rate**       | The number of frames per second in a video. (Numeric)                                   |\n",
      "| **frame rate(est.)** | An estimated value for the frame rate of the video. (Numeric)                           |\n",
      "| **category**         | The categorization of the video based on YouTube's predefined categories. (Categorical) |\n",
      "| **url**              | The direct link to the video. (Text)                                                    |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [UCI](https://data.world/uci).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/boost-student-success-with-college-completion-da\n",
      "name 'detect' is not defined Language is not detected: boost-student-success-with-college-completion-da\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: College Completion Dataset\n",
      "name 'detect' is not defined Language is not detected: Graduation Rates, Race, Efficiency Measures and More\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# College Completion Dataset\n",
      "### Graduation Rates, Race, Efficiency Measures and More\n",
      "By Jonathan Ortiz [[source]](https://data.world/databeats)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This College Completion dataset provides an invaluable insight into the success and progress of college students in the United States. It contains graduation rates, race and other data to offer a comprehensive view of college completion in America. The data is sourced from two primary sources – the National Center for Education Statistics (NCES)’ Integrated Postsecondary Education System (IPEDS) and Voluntary System of Accountability’s Student Success and Progress rate. \n",
      "&gt; \n",
      "&gt; At four-year institutions, the graduation figures come from IPEDS for first-time, full-time degree seeking students at the undergraduate level, who entered college six years earlier at four-year institutions or three years earlier at two-year institutions. Furthermore, colleges report how many students completed their program within 100 percent and 150 percent of normal time which corresponds with graduation within four years or six year respectively. Students reported as being of two or more races are included in totals but not shown separately \n",
      "&gt; \n",
      "&gt; When analyzing race and ethnicity data NCES have classified student demographics since 2009 into seven categories; White non-Hispanic; Black non Hispanic; American Indian/ Alaskan native ; Asian/ Pacific Islander ; Unknown race or ethnicity ; Non resident with two new categorize Native Hawaiian or Other Pacific Islander combined with Asian plus students belonging to several races. Also worth noting is that different classifications for graduate data stemming from 2008 could be due to variations in time frame examined & groupings used by particular colleges – those who can’t be identified from National Student Clearinghouse records won’t be subjected to penalty by these locations .  \n",
      "&gt; \n",
      "&gt; When it comes down to efficiency measures parameters like “Awards per 100 Full Time Undergraduate Students which includes all undergraduate completions reported by a particular institution including associate degrees & certificates less than 4 year programme will assist us here while we also take into consideration measures like expenditure categories , Pell grant percentage , endowment values , average student aid amounts & full time faculty members contributing outstandingly towards instructional research / public service initiatives . \n",
      "&gt; \n",
      "&gt;  When trying to quantify outcomes back up Median Estimated SAT score metric helps us when it is derived either on 25th percentile basis / 75th percentile basis with all these factors further qualified by identifying required criteria meeting 90% threshold when incoming students are considered for relevance .  Last but not least , Average Student Aid equalizes amount granted by institution dividing same over total sum received against what was allotted that particular year .   \n",
      "&gt; \n",
      "&gt;  All this analysis gives an opportunity get a holistic overview about performance , potential deficits &\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains data on student success, graduation rates, race and gender demographics, an efficiency measure to compare colleges across states and more. It is a great source of information to help you better understand college completion and student success in the United States. \n",
      "&gt; \n",
      "&gt; In this guide we’ll explain how to use the data so that you can find out the best colleges for students with certain characteristics or focus on your target completion rate. We’ll also provide some useful tips for getting the most out of this dataset when seeking guidance on which institutions offer the highest graduation rates or have a good reputation for success in terms of completing programs within normal timeframes. \n",
      "&gt; \n",
      "&gt; Before getting into specifics about interpreting this dataset, it is important that you understand that each row represents information about a particular institution – such as its state affiliation, level (two-year vs four-year), control (public vs private), name and website. Each column contains various demographic information such as rate of awarding degrees compared to other institutions in its sector; race/ethnicity Makeup; full-time faculty percentage; median SAT score among first-time students; awards/grants comparison versus national average/state average - all applicable depending on institution location — and more! \n",
      "&gt; \n",
      "&gt; When using this dataset, our suggestion is that you begin by forming a hypothesis or research question concerning student completion at a given school based upon observable characteristics like financial resources invested per award or correlations between institutional control vs outcomes within five different sectors: public 2‑year schools, public 4‑year schools, private non‑profit 2‑year schools, private non‑profit 4‑year schools and for-profit postsecondary institutions. You may choose certain demographic groups as part of your hypothesis or study outcomes trends across entire population groups over time with these cohorts selected from 2000–2014 years listed in comparative chart below: \n",
      "&gt; \n",
      "&gt;     Cohort\tGroup\t        Numeric Equivalent Code\n",
      "&gt; \n",
      "&gt;         G12\tGraduates 12th grade   \t    1 \n",
      "&gt;         FW14 Freshman who will graduate 14th grade   2  \n",
      "&gt; FTUG14       First Time Undergraduate ‑ Graduates 14th year     3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
      "&gt; &nbsp; TTUG14 &nbsp; &nbsp; Transfer Undergraduates ‑ Graduates 14th year     4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
      "&gt; \n",
      "&gt;  Our recommendation would be using similar code method suggested above when combining “Group” & pressurizing corresponding numeric figure to specify cohort drop down menu under Statistical Filter tab formatting user experience accordingly). After forming initial\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing targeted programs to increase graduation rates and reduce the racial achievement gap: This dataset can be used to analyze college completion trends at both the state and institutional levels, focus on differential rates of student graduation that vary based on gender, race, or other factors, and measure performance against set benchmarks. This analysis could be used to create solutions tailored to individual institutions and states aimed at reducing disparities in college completion between different demographic groups.\n",
      "&gt; \n",
      "&gt; - Identifying financial efficiency measures for higher education institutions: Using this dataset, researchers can explore expenditure patterns among different types of schools (e.g., publics versus privates) and focus on total spending per award as a way of gauging financial efficiency among higher education institutions. By understanding the links between spending patterns and student outcomes such as graduation rates, policy makers can design strategies to optimize institutional resources while increasing overall collegiate success. \n",
      "&gt; \n",
      "&gt; - Developing a correlation model between SAT scores/test results and college completion: Researchers could use this dataset's estimated median SAT score information as well as its state-level comparison data to develop a model exploring the relationship between initial test scores (or other pre-college indications) with eventual college matriculation or graduation\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/databeats)\n",
      "&gt; \n",
      "&gt; \n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: cc_state_sector_grads.csv**\n",
      "| Column name        | Description                                                                        |\n",
      "|:-------------------|:-----------------------------------------------------------------------------------|\n",
      "| **state**          | The state in which the college is located. (String)                                |\n",
      "| **state_abbr**     | The two-letter abbreviation of the state in which the college is located. (String) |\n",
      "| **control**        | The type of college (public or private). (String)                                  |\n",
      "| **level**          | The type of institution (two-year or four-year). (String)                          |\n",
      "| **gender**         | The gender of the student. (String)                                                |\n",
      "| **race**           | The race/ethnicity of the student. (String)                                        |\n",
      "| **cohort**         | The number of students in the cohort. (Integer)                                    |\n",
      "| **grad_cohort**    | The number of students who graduated from the cohort. (Integer)                    |\n",
      "| **grad_100**       | The number of students who graduated within 100% of normal time. (Integer)         |\n",
      "| **grad_150**       | The number of students who graduated within 150% of normal time. (Integer)         |\n",
      "| **grad_100_rate**  | The graduation rate within 100% of normal time. (Float)                            |\n",
      "| **grad_150_rate**  | The graduation rate within 150% of normal time. (Float)                            |\n",
      "| **grad_cohort_ct** | The total number of students in the cohort. (Integer)                              |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_institution_grads.csv**\n",
      "| Column name       | Description                                                                |\n",
      "|:------------------|:---------------------------------------------------------------------------|\n",
      "| **level**         | The type of institution (two-year or four-year). (String)                  |\n",
      "| **gender**        | The gender of the student. (String)                                        |\n",
      "| **race**          | The race/ethnicity of the student. (String)                                |\n",
      "| **cohort**        | The number of students in the cohort. (Integer)                            |\n",
      "| **grad_cohort**   | The number of students who graduated from the cohort. (Integer)            |\n",
      "| **grad_100**      | The number of students who graduated within 100% of normal time. (Integer) |\n",
      "| **grad_150**      | The number of students who graduated within 150% of normal time. (Integer) |\n",
      "| **grad_100_rate** | The graduation rate within 100% of normal time. (Float)                    |\n",
      "| **grad_150_rate** | The graduation rate within 150% of normal time. (Float)                    |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_institution_details.csv**\n",
      "| Column name                | Description                                                                                       |\n",
      "|:---------------------------|:--------------------------------------------------------------------------------------------------|\n",
      "| **state**                  | The state in which the college is located. (String)                                               |\n",
      "| **level**                  | The type of institution (two-year or four-year). (String)                                         |\n",
      "| **control**                | The type of college (public or private). (String)                                                 |\n",
      "| **chronname**              | The name of the college or university. (String)                                                   |\n",
      "| **city**                   | The city in which the college is located. (String)                                                |\n",
      "| **basic**                  | A flag indicating whether the college is a basic institution. (Boolean)                           |\n",
      "| **hbcu**                   | A flag indicating whether the college is a historically black college or university. (Boolean)    |\n",
      "| **flagship**               | A flag indicating whether the college is a flagship institution. (Boolean)                        |\n",
      "| **long_x**                 | The longitude of the college. (Float)                                                             |\n",
      "| **lat_y**                  | The latitude of the college. (Float)                                                              |\n",
      "| **site**                   | The website of the college. (String)                                                              |\n",
      "| **student_count**          | The number of students enrolled at the college. (Integer)                                         |\n",
      "| **awards_per_value**       | The number of awards per 100 full-time undergraduates. (Float)                                    |\n",
      "| **awards_per_state_value** | The number of awards per 100 full-time undergraduates compared to the state average. (Float)      |\n",
      "| **awards_per_natl_value**  | The number of awards per 100 full-time undergraduates compared to the national average. (Float)   |\n",
      "| **exp_award_value**        | The amount of money spent per award. (Float)                                                      |\n",
      "| **exp_award_state_value**  | The amount of money spent per award compared to the state average. (Float)                        |\n",
      "| **exp_award_natl_value**   | The amount of money spent per award compared to the national average. (Float)                     |\n",
      "| **exp_award_percentile**   | The percentile of the amount of money spent per award compared to other colleges. (Float)         |\n",
      "| **ft_pct**                 | The percentage of full-time students. (Float)                                                     |\n",
      "| **fte_value**              | The number of full-time equivalent students. (Float)                                              |\n",
      "| **fte_percentile**         | The percentile of the number of full-time equivalent students compared to other colleges. (Float) |\n",
      "| **med_sat_value**          | The median estimated SAT score. (Float)                                                           |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_state_sector_details.csv**\n",
      "| Column name                | Description                                                                                     |\n",
      "|:---------------------------|:------------------------------------------------------------------------------------------------|\n",
      "| **state**                  | The state in which the college is located. (String)                                             |\n",
      "| **state_abbr**             | The two-letter abbreviation of the state in which the college is located. (String)              |\n",
      "| **level**                  | The type of institution (two-year or four-year). (String)                                       |\n",
      "| **control**                | The type of college (public or private). (String)                                               |\n",
      "| **awards_per_state_value** | The number of awards per 100 full-time undergraduates compared to the state average. (Float)    |\n",
      "| **awards_per_natl_value**  | The number of awards per 100 full-time undergraduates compared to the national average. (Float) |\n",
      "| **exp_award_state_value**  | The amount of money spent per award compared to the state average. (Float)                      |\n",
      "| **exp_award_natl_value**   | The amount of money spent per award compared to the national average. (Float)                   |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Jonathan Ortiz](https://data.world/databeats).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: boost-student-success-with-college-completion-da\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: College Completion Dataset\n",
      "name 'detect' is not defined Language is not detected: Graduation Rates, Race, Efficiency Measures and More\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# College Completion Dataset\n",
      "### Graduation Rates, Race, Efficiency Measures and More\n",
      "By Jonathan Ortiz [[source]](https://data.world/databeats)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This College Completion dataset provides an invaluable insight into the success and progress of college students in the United States. It contains graduation rates, race and other data to offer a comprehensive view of college completion in America. The data is sourced from two primary sources – the National Center for Education Statistics (NCES)’ Integrated Postsecondary Education System (IPEDS) and Voluntary System of Accountability’s Student Success and Progress rate. \n",
      "&gt; \n",
      "&gt; At four-year institutions, the graduation figures come from IPEDS for first-time, full-time degree seeking students at the undergraduate level, who entered college six years earlier at four-year institutions or three years earlier at two-year institutions. Furthermore, colleges report how many students completed their program within 100 percent and 150 percent of normal time which corresponds with graduation within four years or six year respectively. Students reported as being of two or more races are included in totals but not shown separately \n",
      "&gt; \n",
      "&gt; When analyzing race and ethnicity data NCES have classified student demographics since 2009 into seven categories; White non-Hispanic; Black non Hispanic; American Indian/ Alaskan native ; Asian/ Pacific Islander ; Unknown race or ethnicity ; Non resident with two new categorize Native Hawaiian or Other Pacific Islander combined with Asian plus students belonging to several races. Also worth noting is that different classifications for graduate data stemming from 2008 could be due to variations in time frame examined & groupings used by particular colleges – those who can’t be identified from National Student Clearinghouse records won’t be subjected to penalty by these locations .  \n",
      "&gt; \n",
      "&gt; When it comes down to efficiency measures parameters like “Awards per 100 Full Time Undergraduate Students which includes all undergraduate completions reported by a particular institution including associate degrees & certificates less than 4 year programme will assist us here while we also take into consideration measures like expenditure categories , Pell grant percentage , endowment values , average student aid amounts & full time faculty members contributing outstandingly towards instructional research / public service initiatives . \n",
      "&gt; \n",
      "&gt;  When trying to quantify outcomes back up Median Estimated SAT score metric helps us when it is derived either on 25th percentile basis / 75th percentile basis with all these factors further qualified by identifying required criteria meeting 90% threshold when incoming students are considered for relevance .  Last but not least , Average Student Aid equalizes amount granted by institution dividing same over total sum received against what was allotted that particular year .   \n",
      "&gt; \n",
      "&gt;  All this analysis gives an opportunity get a holistic overview about performance , potential deficits &\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contains data on student success, graduation rates, race and gender demographics, an efficiency measure to compare colleges across states and more. It is a great source of information to help you better understand college completion and student success in the United States. \n",
      "&gt; \n",
      "&gt; In this guide we’ll explain how to use the data so that you can find out the best colleges for students with certain characteristics or focus on your target completion rate. We’ll also provide some useful tips for getting the most out of this dataset when seeking guidance on which institutions offer the highest graduation rates or have a good reputation for success in terms of completing programs within normal timeframes. \n",
      "&gt; \n",
      "&gt; Before getting into specifics about interpreting this dataset, it is important that you understand that each row represents information about a particular institution – such as its state affiliation, level (two-year vs four-year), control (public vs private), name and website. Each column contains various demographic information such as rate of awarding degrees compared to other institutions in its sector; race/ethnicity Makeup; full-time faculty percentage; median SAT score among first-time students; awards/grants comparison versus national average/state average - all applicable depending on institution location — and more! \n",
      "&gt; \n",
      "&gt; When using this dataset, our suggestion is that you begin by forming a hypothesis or research question concerning student completion at a given school based upon observable characteristics like financial resources invested per award or correlations between institutional control vs outcomes within five different sectors: public 2‑year schools, public 4‑year schools, private non‑profit 2‑year schools, private non‑profit 4‑year schools and for-profit postsecondary institutions. You may choose certain demographic groups as part of your hypothesis or study outcomes trends across entire population groups over time with these cohorts selected from 2000–2014 years listed in comparative chart below: \n",
      "&gt; \n",
      "&gt;     Cohort\tGroup\t        Numeric Equivalent Code\n",
      "&gt; \n",
      "&gt;         G12\tGraduates 12th grade   \t    1 \n",
      "&gt;         FW14 Freshman who will graduate 14th grade   2  \n",
      "&gt; FTUG14       First Time Undergraduate ‑ Graduates 14th year     3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
      "&gt; &nbsp; TTUG14 &nbsp; &nbsp; Transfer Undergraduates ‑ Graduates 14th year     4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
      "&gt; \n",
      "&gt;  Our recommendation would be using similar code method suggested above when combining “Group” & pressurizing corresponding numeric figure to specify cohort drop down menu under Statistical Filter tab formatting user experience accordingly). After forming initial\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing targeted programs to increase graduation rates and reduce the racial achievement gap: This dataset can be used to analyze college completion trends at both the state and institutional levels, focus on differential rates of student graduation that vary based on gender, race, or other factors, and measure performance against set benchmarks. This analysis could be used to create solutions tailored to individual institutions and states aimed at reducing disparities in college completion between different demographic groups.\n",
      "&gt; \n",
      "&gt; - Identifying financial efficiency measures for higher education institutions: Using this dataset, researchers can explore expenditure patterns among different types of schools (e.g., publics versus privates) and focus on total spending per award as a way of gauging financial efficiency among higher education institutions. By understanding the links between spending patterns and student outcomes such as graduation rates, policy makers can design strategies to optimize institutional resources while increasing overall collegiate success. \n",
      "&gt; \n",
      "&gt; - Developing a correlation model between SAT scores/test results and college completion: Researchers could use this dataset's estimated median SAT score information as well as its state-level comparison data to develop a model exploring the relationship between initial test scores (or other pre-college indications) with eventual college matriculation or graduation\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/databeats)\n",
      "&gt; \n",
      "&gt; \n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: cc_state_sector_grads.csv**\n",
      "| Column name        | Description                                                                        |\n",
      "|:-------------------|:-----------------------------------------------------------------------------------|\n",
      "| **state**          | The state in which the college is located. (String)                                |\n",
      "| **state_abbr**     | The two-letter abbreviation of the state in which the college is located. (String) |\n",
      "| **control**        | The type of college (public or private). (String)                                  |\n",
      "| **level**          | The type of institution (two-year or four-year). (String)                          |\n",
      "| **gender**         | The gender of the student. (String)                                                |\n",
      "| **race**           | The race/ethnicity of the student. (String)                                        |\n",
      "| **cohort**         | The number of students in the cohort. (Integer)                                    |\n",
      "| **grad_cohort**    | The number of students who graduated from the cohort. (Integer)                    |\n",
      "| **grad_100**       | The number of students who graduated within 100% of normal time. (Integer)         |\n",
      "| **grad_150**       | The number of students who graduated within 150% of normal time. (Integer)         |\n",
      "| **grad_100_rate**  | The graduation rate within 100% of normal time. (Float)                            |\n",
      "| **grad_150_rate**  | The graduation rate within 150% of normal time. (Float)                            |\n",
      "| **grad_cohort_ct** | The total number of students in the cohort. (Integer)                              |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_institution_grads.csv**\n",
      "| Column name       | Description                                                                |\n",
      "|:------------------|:---------------------------------------------------------------------------|\n",
      "| **level**         | The type of institution (two-year or four-year). (String)                  |\n",
      "| **gender**        | The gender of the student. (String)                                        |\n",
      "| **race**          | The race/ethnicity of the student. (String)                                |\n",
      "| **cohort**        | The number of students in the cohort. (Integer)                            |\n",
      "| **grad_cohort**   | The number of students who graduated from the cohort. (Integer)            |\n",
      "| **grad_100**      | The number of students who graduated within 100% of normal time. (Integer) |\n",
      "| **grad_150**      | The number of students who graduated within 150% of normal time. (Integer) |\n",
      "| **grad_100_rate** | The graduation rate within 100% of normal time. (Float)                    |\n",
      "| **grad_150_rate** | The graduation rate within 150% of normal time. (Float)                    |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_institution_details.csv**\n",
      "| Column name                | Description                                                                                       |\n",
      "|:---------------------------|:--------------------------------------------------------------------------------------------------|\n",
      "| **state**                  | The state in which the college is located. (String)                                               |\n",
      "| **level**                  | The type of institution (two-year or four-year). (String)                                         |\n",
      "| **control**                | The type of college (public or private). (String)                                                 |\n",
      "| **chronname**              | The name of the college or university. (String)                                                   |\n",
      "| **city**                   | The city in which the college is located. (String)                                                |\n",
      "| **basic**                  | A flag indicating whether the college is a basic institution. (Boolean)                           |\n",
      "| **hbcu**                   | A flag indicating whether the college is a historically black college or university. (Boolean)    |\n",
      "| **flagship**               | A flag indicating whether the college is a flagship institution. (Boolean)                        |\n",
      "| **long_x**                 | The longitude of the college. (Float)                                                             |\n",
      "| **lat_y**                  | The latitude of the college. (Float)                                                              |\n",
      "| **site**                   | The website of the college. (String)                                                              |\n",
      "| **student_count**          | The number of students enrolled at the college. (Integer)                                         |\n",
      "| **awards_per_value**       | The number of awards per 100 full-time undergraduates. (Float)                                    |\n",
      "| **awards_per_state_value** | The number of awards per 100 full-time undergraduates compared to the state average. (Float)      |\n",
      "| **awards_per_natl_value**  | The number of awards per 100 full-time undergraduates compared to the national average. (Float)   |\n",
      "| **exp_award_value**        | The amount of money spent per award. (Float)                                                      |\n",
      "| **exp_award_state_value**  | The amount of money spent per award compared to the state average. (Float)                        |\n",
      "| **exp_award_natl_value**   | The amount of money spent per award compared to the national average. (Float)                     |\n",
      "| **exp_award_percentile**   | The percentile of the amount of money spent per award compared to other colleges. (Float)         |\n",
      "| **ft_pct**                 | The percentage of full-time students. (Float)                                                     |\n",
      "| **fte_value**              | The number of full-time equivalent students. (Float)                                              |\n",
      "| **fte_percentile**         | The percentile of the number of full-time equivalent students compared to other colleges. (Float) |\n",
      "| **med_sat_value**          | The median estimated SAT score. (Float)                                                           |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: cc_state_sector_details.csv**\n",
      "| Column name                | Description                                                                                     |\n",
      "|:---------------------------|:------------------------------------------------------------------------------------------------|\n",
      "| **state**                  | The state in which the college is located. (String)                                             |\n",
      "| **state_abbr**             | The two-letter abbreviation of the state in which the college is located. (String)              |\n",
      "| **level**                  | The type of institution (two-year or four-year). (String)                                       |\n",
      "| **control**                | The type of college (public or private). (String)                                               |\n",
      "| **awards_per_state_value** | The number of awards per 100 full-time undergraduates compared to the state average. (Float)    |\n",
      "| **awards_per_natl_value**  | The number of awards per 100 full-time undergraduates compared to the national average. (Float) |\n",
      "| **exp_award_state_value**  | The amount of money spent per award compared to the state average. (Float)                      |\n",
      "| **exp_award_natl_value**   | The amount of money spent per award compared to the national average. (Float)                   |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Jonathan Ortiz](https://data.world/databeats).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/streaming-activity-dataset\n",
      "name 'detect' is not defined Language is not detected: streaming-activity-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Streaming Activity Dataset\n",
      "name 'detect' is not defined Language is not detected: 4 years of diverse music streaming data across platforms and ages\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Streaming Activity Dataset\n",
      "### 4 years of diverse music streaming data across platforms and ages\n",
      "By Sean Miller [[source]](https://data.world/kcmillersean)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> \n",
      "> The dataset consists of two main files: Scrobble_Features.csv and My Streaming Activity.csv. The Scrobble_Features.csv file contains detailed information about the music tracks, including genre, duration, popularity, and various audio features. On the other hand, the My Streaming Activity.csv file offers 4 years' worth of music streaming data from multiple platforms.\n",
      "> \n",
      "> Key columns in these files include:\n",
      "> - Performer: The name of the performer or artist.\n",
      "> - Song: The title of the song.\n",
      "> - Album: The name of the album that each song belongs to.\n",
      "> - spotify_genre: The genre(s) assigned to each song according to Spotify's classification.\n",
      "> - spotify_track_preview_url: URLs providing previews for each song on Spotify.\n",
      "> - spotify_track_duration_ms: The duration of each song in milliseconds.\n",
      "> - spotify_track_popularity: A popularity score indicating how popular each track is on Spotify.\n",
      "> - spotify_track_explicit: A boolean value indicating whether or not a track contains explicit content.\n",
      "> \n",
      "> Further musical attributes are also included:\n",
      "> - danceability: A measure determining how suitable a song is for dancing based on various musical elements.\n",
      "> - energy: An indicator measuring the intensity and activity level present in a song's composition.\n",
      "> - key: Identifies the key signature (e.g., C major) that each track is performed in\n",
      "> - loudness: Reveals how loud or soft a given track is overall in decibels (dB).\n",
      "> - mode : Indicates whether a given track is composed in major or minor scale/mode. \n",
      "> These attributes aim to provide insights into different aspects of a song's overall composition and impact.\n",
      "> \n",
      "> Additionally, this dataset offers information about the timestamps when streaming activities occurred in both Central Time Zone (TimeStamp_Central) and Coordinated Universal Time (UTC) (TimeStamp_UTC). \n",
      "> \n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> In this guide, we will walk you through how to effectively use this dataset for your analysis or projects. Let's get started!\n",
      "> \n",
      "> ## Understanding the Columns\n",
      "> \n",
      "> Before diving into analyzing the data, let's understand the meaning of each column in the dataset:\n",
      "> \n",
      "> - `Performer`: The name of the performer or artist of the song.\n",
      "> - `Song`: The title of the song.\n",
      "> - `spotify_genre`: The genre(s) of the song according to Spotify.\n",
      "> - `spotify_track_preview_url`: The URL of a preview of the song on Spotify.\n",
      "> - `spotify_track_duration_ms`: The duration of the song in milliseconds.\n",
      "> - `spotify_track_popularity`: The popularity score of the song on Spotify. (Numeric/Integer)\n",
      "> - `spotify_track_explicit`: Indicates whether the song contains explicit content. (Boolean)\n",
      "> - `danceability`: A measure of how suitable a song is for dancing based on a combination of musical elements. (Numeric/Float)\n",
      "> - `energy`: A measure o fthe intensity and activity level present in a track.(Alternatively it can also represent acoustic as well). (Numeric/Float)\n",
      ">  - 'key'- represents grouping.of songs based on keys found within that specific set pf songs\n",
      ">  - 'loundess' represents how loud or.silent that particular tract is usually defines by Clown Circle Diameter'.(diameter varies with loudness(sound pressure level).\n",
      ">  -'mode':defines what type/modeis represented(i.e If Major mode denoted by '1',If minor mood is denoted.by value '0')\n",
      ">  -'Speechiness':Detecting spoken words(actually presence/removal of spoken dialects.song verses).\n",
      ">  -Acousticness:Probability of track being acoustic,concerted,edt.\n",
      ">  -instrumentalness-instrumental.also calcylates effectively considering odds and ends ( for example; Intensity of beat.Solo drumming.\n",
      ">  -'liveness':a sentiment reflecting the probability that a song was performed since the recording being analysed\n",
      ">  'valence'-The musical positivity/cheerfulness conveyed by a track.'1'represents most positive ;'0'mostly one(most presumably sad)\n",
      ">  -tempo:'Rate at which particular beats re occur in.oncluding beats); BPM (\n",
      "\n",
      "### Research Ideas\n",
      "> - Music Recommendation System: This dataset can be used to develop a music recommendation system by analyzing the streaming activity and audio features of different songs. By understanding the preferences and listening habits of users, personalized music recommendations can be generated for individuals or households.\n",
      "> - Genre Analysis and Trends: The dataset provides information about the performer, genre, and popularity of songs. This data can be utilized to analyze trends in music genres over the years, identify popular artists in different genres, and understand the musical preferences of different age groups within a household.\n",
      "> - Impact of Explicit Content: With the presence of an explicit column indicating whether a song contains explicit content or not, this dataset can be used to examine how often explicit songs are streamed and if there are any patterns related to demographics or time. It can also provide insights into parental control measures by analyzing if certain age groups tend to stream explicit content more frequently compared to others\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/kcmillersean)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Scrobble_Features.csv**\n",
      "| Column name                   | Description                                                                                   |\n",
      "|:------------------------------|:----------------------------------------------------------------------------------------------|\n",
      "| **Performer**                 | The name of the artist or performer who created the song. (Text/String)                       |\n",
      "| **Song**                      | The title of the song. (Text/String)                                                          |\n",
      "| **spotify_genre**             | The genre(s) of the song according to Spotify. (Text/String)                                  |\n",
      "| **spotify_track_preview_url** | The URL to preview the song on Spotify. (Text/String)                                         |\n",
      "| **spotify_track_duration_ms** | The duration of the song in milliseconds. (Numeric/Integer)                                   |\n",
      "| **spotify_track_popularity**  | The popularity score of the song on Spotify. (Numeric/Integer)                                |\n",
      "| **spotify_track_explicit**    | Indicates whether the song contains explicit content (True/False). (Boolean)                  |\n",
      "| **danceability**              | A measure of how suitable a song is for dancing based on musical elements. (Numeric/Float)    |\n",
      "| **energy**                    | A measure indicating intensity and activity level in a song. (Numeric/Float)                  |\n",
      "| **key**                       | Represents which key (C, D, E...) in which the track is composed. (Text/String)               |\n",
      "| **loudness**                  | The loudness of the song in decibels (dB). (Numeric/Float)                                    |\n",
      "| **mode**                      | Indicates whether the song is in a major or minor key. (Text/String)                          |\n",
      "| **speechiness**               | A measure of the presence of spoken words in the song. (Numeric/Float)                        |\n",
      "| **acousticness**              | A measure of the acoustic quality of the song determined by musical elements. (Numeric/Float) |\n",
      "| **instrumentalness**          | A measure indicating the likelihood of the song being instrumental. (Numeric/Float)           |\n",
      "| **liveness**                  | A measure indicating the presence of a live audience in the song. (Numeric/Float)             |\n",
      "| **valence**                   | A measure indicating the positiveness conveyed through the song. (Numeric/Float)              |\n",
      "| **tempo**                     | The tempo of the song in beats per minute (BPM). (Numeric/Float)                              |\n",
      "| **time_signature**            | The time signature structure of the song. (Text/String)                                       |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: My Streaming Activity.csv**\n",
      "| Column name           | Description                                                                             |\n",
      "|:----------------------|:----------------------------------------------------------------------------------------|\n",
      "| **Performer**         | The name of the artist or performer who created the song. (Text/String)                 |\n",
      "| **Song**              | The title of the song. (Text/String)                                                    |\n",
      "| **TimeStamp_Central** | The timestamp of the streaming activity in the Central Time Zone. (DateTime)            |\n",
      "| **Album**             | The name of the album to which the song belongs. (Text/String)                          |\n",
      "| **TimeStamp_UTC**     | The timestamp of the streaming activity in Coordinated Universal Time (UTC). (DateTime) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Sean Miller](https://data.world/kcmillersean).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: streaming-activity-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Streaming Activity Dataset\n",
      "name 'detect' is not defined Language is not detected: 4 years of diverse music streaming data across platforms and ages\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Streaming Activity Dataset\n",
      "### 4 years of diverse music streaming data across platforms and ages\n",
      "By Sean Miller [[source]](https://data.world/kcmillersean)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> \n",
      "> The dataset consists of two main files: Scrobble_Features.csv and My Streaming Activity.csv. The Scrobble_Features.csv file contains detailed information about the music tracks, including genre, duration, popularity, and various audio features. On the other hand, the My Streaming Activity.csv file offers 4 years' worth of music streaming data from multiple platforms.\n",
      "> \n",
      "> Key columns in these files include:\n",
      "> - Performer: The name of the performer or artist.\n",
      "> - Song: The title of the song.\n",
      "> - Album: The name of the album that each song belongs to.\n",
      "> - spotify_genre: The genre(s) assigned to each song according to Spotify's classification.\n",
      "> - spotify_track_preview_url: URLs providing previews for each song on Spotify.\n",
      "> - spotify_track_duration_ms: The duration of each song in milliseconds.\n",
      "> - spotify_track_popularity: A popularity score indicating how popular each track is on Spotify.\n",
      "> - spotify_track_explicit: A boolean value indicating whether or not a track contains explicit content.\n",
      "> \n",
      "> Further musical attributes are also included:\n",
      "> - danceability: A measure determining how suitable a song is for dancing based on various musical elements.\n",
      "> - energy: An indicator measuring the intensity and activity level present in a song's composition.\n",
      "> - key: Identifies the key signature (e.g., C major) that each track is performed in\n",
      "> - loudness: Reveals how loud or soft a given track is overall in decibels (dB).\n",
      "> - mode : Indicates whether a given track is composed in major or minor scale/mode. \n",
      "> These attributes aim to provide insights into different aspects of a song's overall composition and impact.\n",
      "> \n",
      "> Additionally, this dataset offers information about the timestamps when streaming activities occurred in both Central Time Zone (TimeStamp_Central) and Coordinated Universal Time (UTC) (TimeStamp_UTC). \n",
      "> \n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> In this guide, we will walk you through how to effectively use this dataset for your analysis or projects. Let's get started!\n",
      "> \n",
      "> ## Understanding the Columns\n",
      "> \n",
      "> Before diving into analyzing the data, let's understand the meaning of each column in the dataset:\n",
      "> \n",
      "> - `Performer`: The name of the performer or artist of the song.\n",
      "> - `Song`: The title of the song.\n",
      "> - `spotify_genre`: The genre(s) of the song according to Spotify.\n",
      "> - `spotify_track_preview_url`: The URL of a preview of the song on Spotify.\n",
      "> - `spotify_track_duration_ms`: The duration of the song in milliseconds.\n",
      "> - `spotify_track_popularity`: The popularity score of the song on Spotify. (Numeric/Integer)\n",
      "> - `spotify_track_explicit`: Indicates whether the song contains explicit content. (Boolean)\n",
      "> - `danceability`: A measure of how suitable a song is for dancing based on a combination of musical elements. (Numeric/Float)\n",
      "> - `energy`: A measure o fthe intensity and activity level present in a track.(Alternatively it can also represent acoustic as well). (Numeric/Float)\n",
      ">  - 'key'- represents grouping.of songs based on keys found within that specific set pf songs\n",
      ">  - 'loundess' represents how loud or.silent that particular tract is usually defines by Clown Circle Diameter'.(diameter varies with loudness(sound pressure level).\n",
      ">  -'mode':defines what type/modeis represented(i.e If Major mode denoted by '1',If minor mood is denoted.by value '0')\n",
      ">  -'Speechiness':Detecting spoken words(actually presence/removal of spoken dialects.song verses).\n",
      ">  -Acousticness:Probability of track being acoustic,concerted,edt.\n",
      ">  -instrumentalness-instrumental.also calcylates effectively considering odds and ends ( for example; Intensity of beat.Solo drumming.\n",
      ">  -'liveness':a sentiment reflecting the probability that a song was performed since the recording being analysed\n",
      ">  'valence'-The musical positivity/cheerfulness conveyed by a track.'1'represents most positive ;'0'mostly one(most presumably sad)\n",
      ">  -tempo:'Rate at which particular beats re occur in.oncluding beats); BPM (\n",
      "\n",
      "### Research Ideas\n",
      "> - Music Recommendation System: This dataset can be used to develop a music recommendation system by analyzing the streaming activity and audio features of different songs. By understanding the preferences and listening habits of users, personalized music recommendations can be generated for individuals or households.\n",
      "> - Genre Analysis and Trends: The dataset provides information about the performer, genre, and popularity of songs. This data can be utilized to analyze trends in music genres over the years, identify popular artists in different genres, and understand the musical preferences of different age groups within a household.\n",
      "> - Impact of Explicit Content: With the presence of an explicit column indicating whether a song contains explicit content or not, this dataset can be used to examine how often explicit songs are streamed and if there are any patterns related to demographics or time. It can also provide insights into parental control measures by analyzing if certain age groups tend to stream explicit content more frequently compared to others\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/kcmillersean)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Scrobble_Features.csv**\n",
      "| Column name                   | Description                                                                                   |\n",
      "|:------------------------------|:----------------------------------------------------------------------------------------------|\n",
      "| **Performer**                 | The name of the artist or performer who created the song. (Text/String)                       |\n",
      "| **Song**                      | The title of the song. (Text/String)                                                          |\n",
      "| **spotify_genre**             | The genre(s) of the song according to Spotify. (Text/String)                                  |\n",
      "| **spotify_track_preview_url** | The URL to preview the song on Spotify. (Text/String)                                         |\n",
      "| **spotify_track_duration_ms** | The duration of the song in milliseconds. (Numeric/Integer)                                   |\n",
      "| **spotify_track_popularity**  | The popularity score of the song on Spotify. (Numeric/Integer)                                |\n",
      "| **spotify_track_explicit**    | Indicates whether the song contains explicit content (True/False). (Boolean)                  |\n",
      "| **danceability**              | A measure of how suitable a song is for dancing based on musical elements. (Numeric/Float)    |\n",
      "| **energy**                    | A measure indicating intensity and activity level in a song. (Numeric/Float)                  |\n",
      "| **key**                       | Represents which key (C, D, E...) in which the track is composed. (Text/String)               |\n",
      "| **loudness**                  | The loudness of the song in decibels (dB). (Numeric/Float)                                    |\n",
      "| **mode**                      | Indicates whether the song is in a major or minor key. (Text/String)                          |\n",
      "| **speechiness**               | A measure of the presence of spoken words in the song. (Numeric/Float)                        |\n",
      "| **acousticness**              | A measure of the acoustic quality of the song determined by musical elements. (Numeric/Float) |\n",
      "| **instrumentalness**          | A measure indicating the likelihood of the song being instrumental. (Numeric/Float)           |\n",
      "| **liveness**                  | A measure indicating the presence of a live audience in the song. (Numeric/Float)             |\n",
      "| **valence**                   | A measure indicating the positiveness conveyed through the song. (Numeric/Float)              |\n",
      "| **tempo**                     | The tempo of the song in beats per minute (BPM). (Numeric/Float)                              |\n",
      "| **time_signature**            | The time signature structure of the song. (Text/String)                                       |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: My Streaming Activity.csv**\n",
      "| Column name           | Description                                                                             |\n",
      "|:----------------------|:----------------------------------------------------------------------------------------|\n",
      "| **Performer**         | The name of the artist or performer who created the song. (Text/String)                 |\n",
      "| **Song**              | The title of the song. (Text/String)                                                    |\n",
      "| **TimeStamp_Central** | The timestamp of the streaming activity in the Central Time Zone. (DateTime)            |\n",
      "| **Album**             | The name of the album to which the song belongs. (Text/String)                          |\n",
      "| **TimeStamp_UTC**     | The timestamp of the streaming activity in Coordinated Universal Time (UTC). (DateTime) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Sean Miller](https://data.world/kcmillersean).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/video-game-ratings-and-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: video-game-ratings-and-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Video Game Ratings and Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: Critical and User Insights on Video Games Across Platforms\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Video Game Ratings and Reviews Dataset\n",
      "### Critical and User Insights on Video Games Across Platforms\n",
      "By Ian [[source]](https://data.world/yansian)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Video Game Reviews and Ratings Dataset is a robust, detailed, and meticulously compiled collection that contains in-depth information on an extensive range of video games. This all-embracing dataset caters to diverse research needs such as studying user behavior, analyzing industry trends, exploring the impact of critical opinions on public perception, or even understanding the progression of video game technology.\n",
      "> \n",
      "> This comprehensive dataset is contained within the file `all_games.csv`, which brings together a wealth of data points for each entry. It puts into context numerous aspects that encapsulate both qualitative and quantitative facets related to various video games. \n",
      "> \n",
      "> Each record in this collection comprises several key attributes pertaining to different dimensions of every game:\n",
      "> \n",
      "> - **name:** The name field reveals the title of the video game. It offers information about how these games are officially recognized in both marketplaces and gaming communities.\n",
      ">     \n",
      "> - **platform:** The platform attribute denotes what gaming console or system the particular video game can be played on. This could range from traditional major consoles like PlayStation, Xbox to modern platforms like iOS or Android.\n",
      "> \n",
      "> - **release_date:** Each entry also specifies when exactly each game was made publicly available by its developers with the release_date field detailing specific day-to-day releases.\n",
      "> \n",
      "> - **summary:** This column provides a brief description or synopsis about each game's storyline and gameplay mechanics. Ideal for context-setting purposes or bare-bones comprehension without delving into too many details.\n",
      "> \n",
      "> - **meta_score:** An amalgamation scores from professional critics is disclosed under meta_score column usually ranging from 0 to 100; offering valuable insights regarding viewpoints held by expert critics encompassing different aspects like graphics quality, storyline depth etc.\n",
      ">   \n",
      "> - **user_review:** Finally user_review throws light upon feedback shared by actual players users who have experienced the game first-hand thus incorporating genuine-player perspectives along with critical reviews forming an unbiased measure factoring holistic consumer sentiment towards specific games  \n",
      "> \n",
      "> With such granular level detail captured across so many attributes, this dataset enables a multi-faceted analysis of the gaming industry in microscopic detail. The potential research scope offered by this dataset is distinctly versatile and hones itself to be a valuable asset for researchers, analysts, game developers and gaming enthusiasts alike\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> ### 1. Understanding the Data Fields:\n",
      "> \n",
      "> Before you start diving into data analysis, it's crucial to understand what each column or field signifies:\n",
      "> \n",
      "> - **name**: This is the title of the video game.\n",
      "> - **platform**: This refers to the gaming console or system where you can play that particular game.\n",
      "> - **release_date**: It indicates when the game was officially released.\n",
      "> - **summary**: It gives a quick synopsis or description of what all antics does a particular game entail.\n",
      "> - **meta_score**: This is an aggregated score compiled from professional critic reviews, typically ranged between 0 - 100.\n",
      "> - **user_review**: These are actual comments given by players who have played these games.\n",
      "> \n",
      "> ### 2. How Can You Use this Data?\n",
      "> \n",
      "> Understanding these fields allows us several possibilities around types of analyses we could perform. Let's discuss a few potential use-cases:\n",
      "> \n",
      "> **Game Popularity & Sales Prediction Analysis:** Assess which games are popular based on their meta_scores and user_reviews. Improve sales prediction models by taking into account release_dates (e.g., predicting higher sales for releases around holidays) alongside track records of other titles within same platform type.\n",
      "> \n",
      "> **Trend Identification:** Discover trends in gamer preferences based on factors like meta scores (considering genre/platform differences), common themes within highly scored user reviews etc.\n",
      "> \n",
      "> **User Sentiment Analysis:** Using machine learning techniques can help analyze text-based user_reviews to gauge sentiment trends and produce insights related to customer satisfaction.\n",
      "> \n",
      "> **Competitor Analysis:** Compare high performing games across different criteria such as meta scores, user reviews and more. \n",
      "> \n",
      "> **Recommendation system:** Develop a video game suggestion model that provides recommendations based on factors like genre, platform type and user sentiment.\n",
      "> \n",
      "> So whether you're looking to identify leading games in the industry or trying to figure out what makes a game popular among users—this dataset is an incredibly valuable resource. Enjoy analysing!\n",
      "\n",
      "### Research Ideas\n",
      "> - Predicting Video Game Success: Analyzing the data to identify key factors (such as release timing, platform, and initial reviews) that contribute to the success or failure of a video game. This could be used by game developers to understand what elements they should focus on in order to make their games more successful and popular.\n",
      "> - Sentiment Analysis on User Reviews: Using natural language processing techniques on user_reviews for sentiment analysis. By doing this, we can get insights into prevailing sentiments among gamers about particular games or platforms.\n",
      "> - Trend Analysis: Studying patterns in release dates, platforms, and genres over time can allow us to spot trends in the gaming industry. For instance, we could track whether certain types of games become more popular at certain times or how new technologies/platforms affect gaming preferences\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/yansian)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "> - You are free to:\n",
      ">      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      ">      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "> - You must:\n",
      ">      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      ">      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      ">      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: all_games.csv**\n",
      "| Column name      | Description                                                                           |\n",
      "|:-----------------|:--------------------------------------------------------------------------------------|\n",
      "| **name**         | The specific title of each video game. (String)                                       |\n",
      "| **platform**     | The gaming console or system that supports the game. (String)                         |\n",
      "| **release_date** | The date when the game was officially released. (Date)                                |\n",
      "| **summary**      | A brief description or synopsis outlining what each video game is about. (String)     |\n",
      "| **meta_score**   | An aggregated score given by professional critics on a scale from 0 to 100. (Numeric) |\n",
      "| **user_review**  | Feedback provided by players/users who've played these games. (String)                |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Ian](https://data.world/yansian).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: video-game-ratings-and-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Video Game Ratings and Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: Critical and User Insights on Video Games Across Platforms\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Video Game Ratings and Reviews Dataset\n",
      "### Critical and User Insights on Video Games Across Platforms\n",
      "By Ian [[source]](https://data.world/yansian)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Video Game Reviews and Ratings Dataset is a robust, detailed, and meticulously compiled collection that contains in-depth information on an extensive range of video games. This all-embracing dataset caters to diverse research needs such as studying user behavior, analyzing industry trends, exploring the impact of critical opinions on public perception, or even understanding the progression of video game technology.\n",
      "> \n",
      "> This comprehensive dataset is contained within the file `all_games.csv`, which brings together a wealth of data points for each entry. It puts into context numerous aspects that encapsulate both qualitative and quantitative facets related to various video games. \n",
      "> \n",
      "> Each record in this collection comprises several key attributes pertaining to different dimensions of every game:\n",
      "> \n",
      "> - **name:** The name field reveals the title of the video game. It offers information about how these games are officially recognized in both marketplaces and gaming communities.\n",
      ">     \n",
      "> - **platform:** The platform attribute denotes what gaming console or system the particular video game can be played on. This could range from traditional major consoles like PlayStation, Xbox to modern platforms like iOS or Android.\n",
      "> \n",
      "> - **release_date:** Each entry also specifies when exactly each game was made publicly available by its developers with the release_date field detailing specific day-to-day releases.\n",
      "> \n",
      "> - **summary:** This column provides a brief description or synopsis about each game's storyline and gameplay mechanics. Ideal for context-setting purposes or bare-bones comprehension without delving into too many details.\n",
      "> \n",
      "> - **meta_score:** An amalgamation scores from professional critics is disclosed under meta_score column usually ranging from 0 to 100; offering valuable insights regarding viewpoints held by expert critics encompassing different aspects like graphics quality, storyline depth etc.\n",
      ">   \n",
      "> - **user_review:** Finally user_review throws light upon feedback shared by actual players users who have experienced the game first-hand thus incorporating genuine-player perspectives along with critical reviews forming an unbiased measure factoring holistic consumer sentiment towards specific games  \n",
      "> \n",
      "> With such granular level detail captured across so many attributes, this dataset enables a multi-faceted analysis of the gaming industry in microscopic detail. The potential research scope offered by this dataset is distinctly versatile and hones itself to be a valuable asset for researchers, analysts, game developers and gaming enthusiasts alike\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> ### 1. Understanding the Data Fields:\n",
      "> \n",
      "> Before you start diving into data analysis, it's crucial to understand what each column or field signifies:\n",
      "> \n",
      "> - **name**: This is the title of the video game.\n",
      "> - **platform**: This refers to the gaming console or system where you can play that particular game.\n",
      "> - **release_date**: It indicates when the game was officially released.\n",
      "> - **summary**: It gives a quick synopsis or description of what all antics does a particular game entail.\n",
      "> - **meta_score**: This is an aggregated score compiled from professional critic reviews, typically ranged between 0 - 100.\n",
      "> - **user_review**: These are actual comments given by players who have played these games.\n",
      "> \n",
      "> ### 2. How Can You Use this Data?\n",
      "> \n",
      "> Understanding these fields allows us several possibilities around types of analyses we could perform. Let's discuss a few potential use-cases:\n",
      "> \n",
      "> **Game Popularity & Sales Prediction Analysis:** Assess which games are popular based on their meta_scores and user_reviews. Improve sales prediction models by taking into account release_dates (e.g., predicting higher sales for releases around holidays) alongside track records of other titles within same platform type.\n",
      "> \n",
      "> **Trend Identification:** Discover trends in gamer preferences based on factors like meta scores (considering genre/platform differences), common themes within highly scored user reviews etc.\n",
      "> \n",
      "> **User Sentiment Analysis:** Using machine learning techniques can help analyze text-based user_reviews to gauge sentiment trends and produce insights related to customer satisfaction.\n",
      "> \n",
      "> **Competitor Analysis:** Compare high performing games across different criteria such as meta scores, user reviews and more. \n",
      "> \n",
      "> **Recommendation system:** Develop a video game suggestion model that provides recommendations based on factors like genre, platform type and user sentiment.\n",
      "> \n",
      "> So whether you're looking to identify leading games in the industry or trying to figure out what makes a game popular among users—this dataset is an incredibly valuable resource. Enjoy analysing!\n",
      "\n",
      "### Research Ideas\n",
      "> - Predicting Video Game Success: Analyzing the data to identify key factors (such as release timing, platform, and initial reviews) that contribute to the success or failure of a video game. This could be used by game developers to understand what elements they should focus on in order to make their games more successful and popular.\n",
      "> - Sentiment Analysis on User Reviews: Using natural language processing techniques on user_reviews for sentiment analysis. By doing this, we can get insights into prevailing sentiments among gamers about particular games or platforms.\n",
      "> - Trend Analysis: Studying patterns in release dates, platforms, and genres over time can allow us to spot trends in the gaming industry. For instance, we could track whether certain types of games become more popular at certain times or how new technologies/platforms affect gaming preferences\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/yansian)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "> - You are free to:\n",
      ">      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      ">      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "> - You must:\n",
      ">      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      ">      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      ">      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: all_games.csv**\n",
      "| Column name      | Description                                                                           |\n",
      "|:-----------------|:--------------------------------------------------------------------------------------|\n",
      "| **name**         | The specific title of each video game. (String)                                       |\n",
      "| **platform**     | The gaming console or system that supports the game. (String)                         |\n",
      "| **release_date** | The date when the game was officially released. (Date)                                |\n",
      "| **summary**      | A brief description or synopsis outlining what each video game is about. (String)     |\n",
      "| **meta_score**   | An aggregated score given by professional critics on a scale from 0 to 100. (Numeric) |\n",
      "| **user_review**  | Feedback provided by players/users who've played these games. (String)                |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Ian](https://data.world/yansian).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/unlock-college-performance-debt-and-earnings-out\n",
      "name 'detect' is not defined Language is not detected: unlock-college-performance-debt-and-earnings-out\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: College Performance, Debt and Earnings\n",
      "name 'detect' is not defined Language is not detected: Analyzing Student Completion and Loan Repayment Rates\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# College Performance, Debt and Earnings\n",
      "### Analyzing Student Completion and Loan Repayment Rates\n",
      "By Education [[source]](https://data.world/education)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This College Scorecard dataset offers a comprehensive look into the performance, cost, and outcomes of U.S. colleges and universities. It contains an extensive amount of data detailing information related to cost of attendance and various outcomes such as average salary after graduation, loan repayment rates, and gainful employment rates for graduates. The datasets also provides information on program level demographics such as gender breakdowns among enrolled students and faculty diversity in programs attended by students. This is an invaluable source of information for anyone who wants to make informed choices about their college education experience in terms of both costs and expected returns after graduation. Going beyond financial metrics, this dataset give insight into the cultural climate at each college or university so that users can analyze whether their unique backgrounds or experiences will fit into the campus ethos at those institutions. With this data set available to everyone interested in higher education options, individuals have a powerful tool to compare options from many perspectives including financial investment returns economics , educational quality measures ,  graduate success rate indices , faculty diversity break-ups etc \n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; This guide will provide guidance on how to utilize this powerful dataset to explore various stats related to college outcomes. \n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **Data Preparation:**\n",
      "&gt; Start by looking into the SQL query which is provided in order to get a better idea of what columns are available in this set. This will allow you to start thinking about what kind of information you want from your analysis and create an appropriate query for it (if desired). Additionally take a look at the column descriptions file which may help explain some variable names and their meanings for further data manipulations that may be required prior to analysis or plotting.\n",
      "&gt; \n",
      "&gt;  \n",
      "&gt; **Columns Selection/Data Cleaning:**\n",
      "&gt; Once you have a good idea of what variables are available in each column it's time to select what specific variables you want in your analysis using tools such as SELECT, WHERE etc... Furthermore, use a combination of cleaning methods like reordering columns, removing outliers or observations based on criteria like missing values or extreme values etc.. This process should allow us us refine our raw data depending upon our requirements before beginning exploration.\n",
      "&gt; \n",
      "&gt;   \n",
      "&gt; **Exploratory Analysis & Visualization:**\n",
      "&gt; Now we can start our exploratory analysis process by examining relationships between different independent & dependent variables- with charts & tables being used heavily when coming up with insights/inferences from our plots & tables. Tools such as pivot-tables can also be used here depending upon our requirements (generally these come handy when dealing with large datasets). Alongside exploratory analytics many statistical tests may also come useful if we're required identify statistically significant relationships like ANOVA Test for two or more groups , regression models etc.. Finally after generating interesting findings visualizing them using tools like ggplot2 is strongly advised- enabling easy presentation & understanding amongst readers even outside data science world .   \n",
      "&gt; \n",
      "&gt;   \n",
      "&gt; By following these simple steps we should now have a better understanding off how unlock valuable information using Unlock College Performance Debt & Earnings Outcomes dataset provided by Kaggle \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Analyzing the correlation between geographic region and performance outcomes of college students. \n",
      "&gt; - Investigating the cost-benefit analysis of tuition rates for colleges across different income brackets in order to make college more accessible for students from poorer economic backgrounds. \n",
      "&gt; - Studying the historical trends in student loan debt, with a focus on origination amounts and repayment periods, to gain insights into how debt impacts graduates' financial management strategies in life after college\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/education)\n",
      "&gt; \n",
      " \n",
      "### License \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Education](https://data.world/education).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: unlock-college-performance-debt-and-earnings-out\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: College Performance, Debt and Earnings\n",
      "name 'detect' is not defined Language is not detected: Analyzing Student Completion and Loan Repayment Rates\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# College Performance, Debt and Earnings\n",
      "### Analyzing Student Completion and Loan Repayment Rates\n",
      "By Education [[source]](https://data.world/education)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This College Scorecard dataset offers a comprehensive look into the performance, cost, and outcomes of U.S. colleges and universities. It contains an extensive amount of data detailing information related to cost of attendance and various outcomes such as average salary after graduation, loan repayment rates, and gainful employment rates for graduates. The datasets also provides information on program level demographics such as gender breakdowns among enrolled students and faculty diversity in programs attended by students. This is an invaluable source of information for anyone who wants to make informed choices about their college education experience in terms of both costs and expected returns after graduation. Going beyond financial metrics, this dataset give insight into the cultural climate at each college or university so that users can analyze whether their unique backgrounds or experiences will fit into the campus ethos at those institutions. With this data set available to everyone interested in higher education options, individuals have a powerful tool to compare options from many perspectives including financial investment returns economics , educational quality measures ,  graduate success rate indices , faculty diversity break-ups etc \n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; This guide will provide guidance on how to utilize this powerful dataset to explore various stats related to college outcomes. \n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **Data Preparation:**\n",
      "&gt; Start by looking into the SQL query which is provided in order to get a better idea of what columns are available in this set. This will allow you to start thinking about what kind of information you want from your analysis and create an appropriate query for it (if desired). Additionally take a look at the column descriptions file which may help explain some variable names and their meanings for further data manipulations that may be required prior to analysis or plotting.\n",
      "&gt; \n",
      "&gt;  \n",
      "&gt; **Columns Selection/Data Cleaning:**\n",
      "&gt; Once you have a good idea of what variables are available in each column it's time to select what specific variables you want in your analysis using tools such as SELECT, WHERE etc... Furthermore, use a combination of cleaning methods like reordering columns, removing outliers or observations based on criteria like missing values or extreme values etc.. This process should allow us us refine our raw data depending upon our requirements before beginning exploration.\n",
      "&gt; \n",
      "&gt;   \n",
      "&gt; **Exploratory Analysis & Visualization:**\n",
      "&gt; Now we can start our exploratory analysis process by examining relationships between different independent & dependent variables- with charts & tables being used heavily when coming up with insights/inferences from our plots & tables. Tools such as pivot-tables can also be used here depending upon our requirements (generally these come handy when dealing with large datasets). Alongside exploratory analytics many statistical tests may also come useful if we're required identify statistically significant relationships like ANOVA Test for two or more groups , regression models etc.. Finally after generating interesting findings visualizing them using tools like ggplot2 is strongly advised- enabling easy presentation & understanding amongst readers even outside data science world .   \n",
      "&gt; \n",
      "&gt;   \n",
      "&gt; By following these simple steps we should now have a better understanding off how unlock valuable information using Unlock College Performance Debt & Earnings Outcomes dataset provided by Kaggle \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Analyzing the correlation between geographic region and performance outcomes of college students. \n",
      "&gt; - Investigating the cost-benefit analysis of tuition rates for colleges across different income brackets in order to make college more accessible for students from poorer economic backgrounds. \n",
      "&gt; - Studying the historical trends in student loan debt, with a focus on origination amounts and repayment periods, to gain insights into how debt impacts graduates' financial management strategies in life after college\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/education)\n",
      "&gt; \n",
      " \n",
      "### License \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Education](https://data.world/education).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/movie-reviews-word2vec-embeddings-dataset\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-word2vec-embeddings-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Word2Vec Embeddings Dataset\n",
      "name 'detect' is not defined Language is not detected: Capturing Semantics in Textual Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Reviews Word2Vec Embeddings Dataset\n",
      "### Capturing Semantics in Textual Reviews\n",
      "By Jared Fernandez [[source]](https://data.world/jaredfern)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains a collection of Word2Vec embeddings for nearly 12,000 reviews from movies and other films. These embeddings allow the reviews to be represented in a meaningful way, providing insight into topics and trends present in the reviews. By utilizing this source of data, researchers can gain better understanding of language patterns that appear across various types of movie reviews. Additionally, models with these embeddings can be used to help create/improve models for sentiment analysis and other natural language processing tasks. Each row includes the reviewer's unique ID along with their review text and related word2vec embedding representing textual relationships found therein\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; How to Use this Dataset:\n",
      "&gt; \n",
      "&gt; - Download the dataset ‘Movie Reviews Word2Vec Embeddings’ from Kaggle. \n",
      "&gt; - This dataset contains an embedding type of word2vec, which is a type of neural network that creates high-dimensional vector representations of words based on their context in a training corpus. \n",
      "&gt; - Before making use of these embeddings, it’s important to understand what they are representing and how you can match them with other datasets for analysis purposes. The word2vec embeddings contain two columns – word (the specific word), and vec (the vector representation associated with that particular word).  \n",
      "&gt; - To leverage the data from this text corpus effectively, it is important to first extract meaningful information out of them such as sentiment ratings or determining various topics that appears more frequently in movie reviews etc.. Sorting through millions of reviews will require automated processing – either by leveraging machine learning algorithms or using natural language processing to determine sentiment polarities and extracting relevant keywords/topics for each review.   \n",
      "&gt; - You can also use the pre-processed Word Vectors (embeddings) along with supervised or unsupervised approaches available like Logistic Regression, BERT models etc.. to create features such as sentiment scoring or topic modelling - classifying texts into distinct categories etc.. That may be useful while doing some predictive analysis such as predicting movie ratings based on user reviews etc..  \n",
      "&gt; \n",
      "&gt; 6 Once you have made use of the pre-processed data from this dataset, you can extend your model's performance further by having better understanding about how those words relate one another using the vectors derived from thems (i.e., Cosine Similarity measurement) which shows relatedness between words thus providing additional insights about relationships among different text fragments or paragraphs in documents eventually helping your model understand better contextual relationships while performing analytics tasks on text corpora involving movie reviews data!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Automatically clustering movies with similar sentiment and themes. \n",
      "&gt; - Automatically generating movie plot summaries based on sentiment analysis of reviews. \n",
      "&gt; - Developing a movie recommendation system based on users’ preference in different genres or topics related to the movie in question\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/jaredfern)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "&gt; - You are free to:\n",
      "&gt;      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      "&gt;      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "&gt; - You must:\n",
      "&gt;      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      "&gt;      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      "&gt;      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Jared Fernandez](https://data.world/jaredfern).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-word2vec-embeddings-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Word2Vec Embeddings Dataset\n",
      "name 'detect' is not defined Language is not detected: Capturing Semantics in Textual Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Reviews Word2Vec Embeddings Dataset\n",
      "### Capturing Semantics in Textual Reviews\n",
      "By Jared Fernandez [[source]](https://data.world/jaredfern)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains a collection of Word2Vec embeddings for nearly 12,000 reviews from movies and other films. These embeddings allow the reviews to be represented in a meaningful way, providing insight into topics and trends present in the reviews. By utilizing this source of data, researchers can gain better understanding of language patterns that appear across various types of movie reviews. Additionally, models with these embeddings can be used to help create/improve models for sentiment analysis and other natural language processing tasks. Each row includes the reviewer's unique ID along with their review text and related word2vec embedding representing textual relationships found therein\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; How to Use this Dataset:\n",
      "&gt; \n",
      "&gt; - Download the dataset ‘Movie Reviews Word2Vec Embeddings’ from Kaggle. \n",
      "&gt; - This dataset contains an embedding type of word2vec, which is a type of neural network that creates high-dimensional vector representations of words based on their context in a training corpus. \n",
      "&gt; - Before making use of these embeddings, it’s important to understand what they are representing and how you can match them with other datasets for analysis purposes. The word2vec embeddings contain two columns – word (the specific word), and vec (the vector representation associated with that particular word).  \n",
      "&gt; - To leverage the data from this text corpus effectively, it is important to first extract meaningful information out of them such as sentiment ratings or determining various topics that appears more frequently in movie reviews etc.. Sorting through millions of reviews will require automated processing – either by leveraging machine learning algorithms or using natural language processing to determine sentiment polarities and extracting relevant keywords/topics for each review.   \n",
      "&gt; - You can also use the pre-processed Word Vectors (embeddings) along with supervised or unsupervised approaches available like Logistic Regression, BERT models etc.. to create features such as sentiment scoring or topic modelling - classifying texts into distinct categories etc.. That may be useful while doing some predictive analysis such as predicting movie ratings based on user reviews etc..  \n",
      "&gt; \n",
      "&gt; 6 Once you have made use of the pre-processed data from this dataset, you can extend your model's performance further by having better understanding about how those words relate one another using the vectors derived from thems (i.e., Cosine Similarity measurement) which shows relatedness between words thus providing additional insights about relationships among different text fragments or paragraphs in documents eventually helping your model understand better contextual relationships while performing analytics tasks on text corpora involving movie reviews data!\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Automatically clustering movies with similar sentiment and themes. \n",
      "&gt; - Automatically generating movie plot summaries based on sentiment analysis of reviews. \n",
      "&gt; - Developing a movie recommendation system based on users’ preference in different genres or topics related to the movie in question\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/jaredfern)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [Dataset copyright by authors](https://creativecommons.org/licenses/by/4.0/)**\n",
      "&gt; - You are free to:\n",
      "&gt;      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n",
      "&gt;      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n",
      "&gt; - You must:\n",
      "&gt;      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n",
      "&gt;      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n",
      "&gt;      - **Keep intact** - all notices that refer to this license, including copyright notices.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Jared Fernandez](https://data.world/jaredfern).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/hud-multifamily-housing-assisted-properties-data\n",
      "name 'detect' is not defined Language is not detected: hud-multifamily-housing-assisted-properties-data\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: HUD Multifamily Housing Assisted Properties\n",
      "name 'detect' is not defined Language is not detected: Detailed Overview of Subsidized Aid Programs and Population Characteristics\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# HUD Multifamily Housing Assisted Properties\n",
      "### Detailed Overview of Subsidized Aid Programs and Population Characteristics\n",
      "By Matthew Schnars [[source]](https://data.world/mschnars)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; HUD’s Multifamily Housing property portfolio is integral in providing secure, affordable rental units for low-income households - including seniors, those with special needs, and disabled individuals. Our coverage includes apartments and townhouses but can also include nursing homes, hospitals, elderly housing centers, mobile home parks and retirement service centers. Through subsidies and grants to property owners and developers we further our mission of promoting the construction and preservation of low cost housing opportunities. \n",
      "&gt; \n",
      "&gt; This dataset comprises such properties from across the United States; located via our enterprise geocoding service which uses latitude/longitude coordinates as well as associated attributes for those addresses that can be geocoded to an interpolated point along a street segment or a ZIP+4 centroid location. Ultimately this dataset provides key insights into multifamily housing availability throughout the U.S., providing valuable information regarding individual buildings associated with each property while also giving us direct insight into population serviced by such programs in terms of their developmental needs or disabilities which may exist. With these metrics in hand we are better equipped to identify areas where reduced prices on rental units may benefit local communities the most - aiding populations that require our assistance the most directly! Data Dictionary: DD_Multifamily Properties; Date of Coverage: 12/2017; Data Updated: Quarterly\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; First, let's talk about what kind of information you can get from this dataset. The HUD Multifamily Housing property portfolio consists primarily of rental housing properties with five or more dwelling units such as apartments or town houses. It includes subsidies and grants provided by HUD in order to promote the development and preservation of affordable rental units for low-income populations, and those with special needs such as the elderly, and disabled. \n",
      "&gt; \n",
      "&gt; The data contains geographic location information (latitude/longitude coordinates), city/state/ZIP code information as well as other internal identifiers for each property in the HUD portfolio. Additionally, there are columns related to different assistance programs which were previously noted above - Section 8 Project Based Assistance, Section 202 Supportive Housing for the Elderly and Section 811 Supportive Housing for Persons with Disabilities). \n",
      "&gt; \n",
      "&gt; In terms of how to use the data itself: due its size it is best explored by importing into an existing database solution like PostgreSQL or MongoDB where you can create custom views based on query keywords that bring back more meaningful results upon execution. Once you define what properties fit your criteria of interest then further analysis can commence - statistical trending on occupancy rates among much else given that next level drilling tends to open up many interesting possibilities like revenue forecasts creating new business models etc.. \n",
      "&gt; And speaking of statistics one area worth noting here specifically is that location data provided by HUD may not always be 100% accurate so caution should be exercised when running analytics queries because incorrect locations will lead astray any conclusions reached after running said queries - so feel free to double check your results before leveraging them in any form whatsoever!  \n",
      "&gt; \n",
      "&gt; Finally keep in mind that since there has only been one version of this dataset released thus far it would probably beneficial if you checked back every quarter or so just incase any changes were made particularity related to program eligibility requirements / reporting requirements etc..  \n",
      "&gt; \n",
      "&gt; Hope this helps and good luck on your journey considering using 'Hud Multifamily Properties Data' for whatever project at hand 😊\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - To identify potential opportunities for developers looking to add affordable housing in underserved areas. Depending on different factors such as the income of the local population, geography, and the type of funding method used to support these projects, developers could be more likely to invest in those areas that have previously been assisted by HUD programs. \n",
      "&gt; - To evaluate performance measures of existing affordable housing across different locations within a given region or state. This could help identify any discrepancies between properties or address any issues within a specified area or program by comparing various performance metrics including occupancy rates, rent collections, etc. \n",
      "&gt; - To implement a recommendation system for local governments who are looking for assistance from HUD’s programs related to Multi-Family Properties Assisted by evaluating factors such as number of occupants and availability of services in each area or region which can result in better implementation of funds allocated towards these projects\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/mschnars)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Matthew Schnars](https://data.world/mschnars).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hud-multifamily-housing-assisted-properties-data\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: HUD Multifamily Housing Assisted Properties\n",
      "name 'detect' is not defined Language is not detected: Detailed Overview of Subsidized Aid Programs and Population Characteristics\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# HUD Multifamily Housing Assisted Properties\n",
      "### Detailed Overview of Subsidized Aid Programs and Population Characteristics\n",
      "By Matthew Schnars [[source]](https://data.world/mschnars)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; HUD’s Multifamily Housing property portfolio is integral in providing secure, affordable rental units for low-income households - including seniors, those with special needs, and disabled individuals. Our coverage includes apartments and townhouses but can also include nursing homes, hospitals, elderly housing centers, mobile home parks and retirement service centers. Through subsidies and grants to property owners and developers we further our mission of promoting the construction and preservation of low cost housing opportunities. \n",
      "&gt; \n",
      "&gt; This dataset comprises such properties from across the United States; located via our enterprise geocoding service which uses latitude/longitude coordinates as well as associated attributes for those addresses that can be geocoded to an interpolated point along a street segment or a ZIP+4 centroid location. Ultimately this dataset provides key insights into multifamily housing availability throughout the U.S., providing valuable information regarding individual buildings associated with each property while also giving us direct insight into population serviced by such programs in terms of their developmental needs or disabilities which may exist. With these metrics in hand we are better equipped to identify areas where reduced prices on rental units may benefit local communities the most - aiding populations that require our assistance the most directly! Data Dictionary: DD_Multifamily Properties; Date of Coverage: 12/2017; Data Updated: Quarterly\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; First, let's talk about what kind of information you can get from this dataset. The HUD Multifamily Housing property portfolio consists primarily of rental housing properties with five or more dwelling units such as apartments or town houses. It includes subsidies and grants provided by HUD in order to promote the development and preservation of affordable rental units for low-income populations, and those with special needs such as the elderly, and disabled. \n",
      "&gt; \n",
      "&gt; The data contains geographic location information (latitude/longitude coordinates), city/state/ZIP code information as well as other internal identifiers for each property in the HUD portfolio. Additionally, there are columns related to different assistance programs which were previously noted above - Section 8 Project Based Assistance, Section 202 Supportive Housing for the Elderly and Section 811 Supportive Housing for Persons with Disabilities). \n",
      "&gt; \n",
      "&gt; In terms of how to use the data itself: due its size it is best explored by importing into an existing database solution like PostgreSQL or MongoDB where you can create custom views based on query keywords that bring back more meaningful results upon execution. Once you define what properties fit your criteria of interest then further analysis can commence - statistical trending on occupancy rates among much else given that next level drilling tends to open up many interesting possibilities like revenue forecasts creating new business models etc.. \n",
      "&gt; And speaking of statistics one area worth noting here specifically is that location data provided by HUD may not always be 100% accurate so caution should be exercised when running analytics queries because incorrect locations will lead astray any conclusions reached after running said queries - so feel free to double check your results before leveraging them in any form whatsoever!  \n",
      "&gt; \n",
      "&gt; Finally keep in mind that since there has only been one version of this dataset released thus far it would probably beneficial if you checked back every quarter or so just incase any changes were made particularity related to program eligibility requirements / reporting requirements etc..  \n",
      "&gt; \n",
      "&gt; Hope this helps and good luck on your journey considering using 'Hud Multifamily Properties Data' for whatever project at hand 😊\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - To identify potential opportunities for developers looking to add affordable housing in underserved areas. Depending on different factors such as the income of the local population, geography, and the type of funding method used to support these projects, developers could be more likely to invest in those areas that have previously been assisted by HUD programs. \n",
      "&gt; - To evaluate performance measures of existing affordable housing across different locations within a given region or state. This could help identify any discrepancies between properties or address any issues within a specified area or program by comparing various performance metrics including occupancy rates, rent collections, etc. \n",
      "&gt; - To implement a recommendation system for local governments who are looking for assistance from HUD’s programs related to Multi-Family Properties Assisted by evaluating factors such as number of occupants and availability of services in each area or region which can result in better implementation of funds allocated towards these projects\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/mschnars)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Matthew Schnars](https://data.world/mschnars).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/intel-orca-dialogue-pairs\n",
      "name 'detect' is not defined Language is not detected: intel-orca-dialogue-pairs\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Orca DPO Dialogue Pairs\n",
      "name 'detect' is not defined Language is not detected: Orca style for preference training (Intel's DPO dataset)\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Intel Orca Dialogue Pairs\n",
      "### Orca style for preference training (Intel's DPO dataset)\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The Intel/Orca/DPO Dialogue Pairs dataset is a unique resource for Natural language processing (NLP) research, combining AI and human conversations collected from online sources. This dataset is invaluable for exploring how human conversations can inform the development of conversational AI models. With columns such as System and Question extracted from chat logs, this dataset can help researchers understand more about how to better connect people with technology using meaningful dialogue. Furthermore, the data also includes columns for ChatGPT and Llama2–13b-Chat, two of the most widely used conversational AI models. By leveraging this data set, researchers have an exceptional opportunity to explore conversational techniques that enable humans and machines to communicate in natural languages\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This guide will provide an overview of how to use the Intel/Orca/DPO Dialogue Pairs dataset efficiently for human-centric natural language processing research. \n",
      "&gt; \n",
      "&gt; ##### Step 1: Understand the dataset \n",
      "&gt; The Intel/Orca/DPO Dialogue Pairs dataset is composed of two main columns: System and Question. The System column contains responses from AI systems, and the Question column contains questions asked by humans. Additionally, this dataset also contains columns for ChatGPT and Llama2–13b-Chat, two models used in developing conversational AI systems. \n",
      "&gt; \n",
      "&gt; ##### Step 2: Prepare your environment \n",
      "&gt; Before getting started with analyzing data from this dataset, you should first prepare your environment accordingly. Make sure that any necessary libraries or services are installed on your machine before attempting to work with the data from this dataset in order to avoid potential issues or errors during usage. \n",
      "&gt; \n",
      "&gt;  ##### Step 3: Access the data  \n",
      "&gt;  In order to access and start working with the data contained in this Dataset, you can either download it directly via a Kaggle account or alternatively access it through one of its REST Endpoints if available on other services (i.e Databricks).\n",
      "&gt; \n",
      "&gt;  ##### Step 4: Exploring & Analyzing the Data  \n",
      "&gt; \n",
      "&gt;  ##### Step 5 : Reporting Results    \n",
      "&gt;  Lastly ,once explorations and analyses have been completed its highly important that results are reported accurately especially when dealing with ethical datasets such as dialogue pairs since consequences could be dire if misinformation is disseminated .Reporting results should usually involve standard relevant indicators being declared while taking care conducting appropriate statistical tests ruling out incorrect anomalous outcomes \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing and improving natural language processing algorithms for AI-human conversation. \n",
      "&gt; - Building user-friendly chatbots that are better at recognizing and understanding human intent by training the model using this dataset. \n",
      "&gt; - Designing recommendation systems to predict user questions and generate more accurate responses based on previous conversations in the dataset\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name         | Description                                                                  |\n",
      "|:--------------------|:-----------------------------------------------------------------------------|\n",
      "| **system**          | Contains the AI system's response to the user's question. (Text)             |\n",
      "| **chatgpt**         | Contains the ChatGPT model's response to the user's question. (Text)         |\n",
      "| **llama2-13b-chat** | Contains the Llama2-13b-Chat model's response to the user's question. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/Intel/orca_dpo_pairs).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: intel-orca-dialogue-pairs\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Orca DPO Dialogue Pairs\n",
      "name 'detect' is not defined Language is not detected: Orca style for preference training (Intel's DPO dataset)\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Intel Orca Dialogue Pairs\n",
      "### Orca style for preference training (Intel's DPO dataset)\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; The Intel/Orca/DPO Dialogue Pairs dataset is a unique resource for Natural language processing (NLP) research, combining AI and human conversations collected from online sources. This dataset is invaluable for exploring how human conversations can inform the development of conversational AI models. With columns such as System and Question extracted from chat logs, this dataset can help researchers understand more about how to better connect people with technology using meaningful dialogue. Furthermore, the data also includes columns for ChatGPT and Llama2–13b-Chat, two of the most widely used conversational AI models. By leveraging this data set, researchers have an exceptional opportunity to explore conversational techniques that enable humans and machines to communicate in natural languages\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This guide will provide an overview of how to use the Intel/Orca/DPO Dialogue Pairs dataset efficiently for human-centric natural language processing research. \n",
      "&gt; \n",
      "&gt; ##### Step 1: Understand the dataset \n",
      "&gt; The Intel/Orca/DPO Dialogue Pairs dataset is composed of two main columns: System and Question. The System column contains responses from AI systems, and the Question column contains questions asked by humans. Additionally, this dataset also contains columns for ChatGPT and Llama2–13b-Chat, two models used in developing conversational AI systems. \n",
      "&gt; \n",
      "&gt; ##### Step 2: Prepare your environment \n",
      "&gt; Before getting started with analyzing data from this dataset, you should first prepare your environment accordingly. Make sure that any necessary libraries or services are installed on your machine before attempting to work with the data from this dataset in order to avoid potential issues or errors during usage. \n",
      "&gt; \n",
      "&gt;  ##### Step 3: Access the data  \n",
      "&gt;  In order to access and start working with the data contained in this Dataset, you can either download it directly via a Kaggle account or alternatively access it through one of its REST Endpoints if available on other services (i.e Databricks).\n",
      "&gt; \n",
      "&gt;  ##### Step 4: Exploring & Analyzing the Data  \n",
      "&gt; \n",
      "&gt;  ##### Step 5 : Reporting Results    \n",
      "&gt;  Lastly ,once explorations and analyses have been completed its highly important that results are reported accurately especially when dealing with ethical datasets such as dialogue pairs since consequences could be dire if misinformation is disseminated .Reporting results should usually involve standard relevant indicators being declared while taking care conducting appropriate statistical tests ruling out incorrect anomalous outcomes \n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing and improving natural language processing algorithms for AI-human conversation. \n",
      "&gt; - Building user-friendly chatbots that are better at recognizing and understanding human intent by training the model using this dataset. \n",
      "&gt; - Designing recommendation systems to predict user questions and generate more accurate responses based on previous conversations in the dataset\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name         | Description                                                                  |\n",
      "|:--------------------|:-----------------------------------------------------------------------------|\n",
      "| **system**          | Contains the AI system's response to the user's question. (Text)             |\n",
      "| **chatgpt**         | Contains the ChatGPT model's response to the user's question. (Text)         |\n",
      "| **llama2-13b-chat** | Contains the Llama2-13b-Chat model's response to the user's question. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/Intel/orca_dpo_pairs).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/unlocking-the-human-perspective-on-movie-reviews\n",
      "name 'detect' is not defined Language is not detected: unlocking-the-human-perspective-on-movie-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Rationales (Rationales For Movie Reviews)\n",
      "name 'detect' is not defined Language is not detected: Human annotated rationales for movie reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Rationales (Rationales For Movie Reviews)\n",
      "### Human annotated rationales for movie reviews\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/movie_rationales)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset was created to allow researchers to gain an in-depth understanding of the inner workings of human-generated movie reviews. With these train, test, and validation sets, researchers can explore different aspects of movie reviews, such as sentiment labels or rationales behind them. By analyzing this information and finding patterns and correlations, insightful ideas can be discovered that can lead to developing models powerful enough to uncover importance of the unique human perspectives when interpreting movie reviews. Any data scientist or researcher interested in AI applications is encouraged to take advantage of this dataset which may potentially provide useful insights into better understanding user intent when reviewing movies\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset is intended to enable researchers and developers to uncover the rationales behind movie reviews. To use it effectively, you must understand the data format and how each column in the dataset works. \n",
      "&gt; \n",
      "&gt; #### What does each column mean? \n",
      "&gt; - **review:** The text of the movie review. (String) \n",
      "&gt; - **label:** The sentiment label of the review (Positive, Negative, or Neutral). (String)  \n",
      "&gt; - **validation.csv:** The validation set which contains reviews, labels, and evidence which can be used to validate models developed for understanding human perspective on movie reviews.  \n",
      "&gt; - **train.csv:** The train set which contains reviews, labels as well as evidence used for training a model based on human annotations of movie reviews.   \n",
      "&gt; - **test.csv:** The test set which contains reviews, labels and evidence that can be used to evaluate models on unseen data related to understanding perspectives of humans when it comes to movie reviews.. \n",
      "&gt; \n",
      "&gt;  #### How do I use this dataset?  \n",
      "&gt; \n",
      "&gt;  To get started with this dataset you need a working environment such as Python or R where you have access library’s needed for natural language processing(NLP). After setting up an environment with libraries that support NLP tasks execute following steps :   \n",
      "&gt; \n",
      "&gt;  - Import csv files into your workspace using appropriate functions provided by specified language libraries e,.g., for Python use pandas read_csv() method .    \n",
      "&gt; \n",
      "&gt;  - Preprocess your text data in 'review' & 'label' columns by standardizing them like removing stopwords from sentences & converting words into lowercase etc .Following link [link](Preprocessing%20Python%20Libraries/) provides best possible preprocessing libraries available in Python .   \n",
      "&gt; \n",
      "&gt;  - Train&Test ML algorithms using appropriate feature extraction techniques related to NLP( Bag Of Words , TF-IDF , Word2Vec ) eines are some examples in many more are available Refer [link](Feature%20Extraction%20Techniques/)   \n",
      "&gt; \n",
      "&gt;  - Measure performance accuracy after running experiments on datasets provided validation & test sets we have also included precision recall curves along famous metrics like F1 score & accuracy score so you could easily analyze hyperparameter tuning & algorithm efficiency according their outputs values you get while testing your ML algorithm     \n",
      "&gt; \n",
      "&gt;  - Recommendation systems are always fun! build a simple machine learning reccomendation system by collecting user visits logs post hand writting new featuers might\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing an automated movie review summarizer based on user ratings, that can accurately capture the salient points of a review and summarize it for moviegoers.\n",
      "&gt; - Training a model to predict the sentiment of a review, by combining machine learning models with human-annotated rationales from this dataset. \n",
      "&gt; - Building an AI system that can detect linguistic markers of deception in reviews (e.g., 'fake news', thin reviews etc) and issue warnings on possible fraudulent purchases or online reviews\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/movie_rationales)\n",
      "&gt; \n",
      "\n",
      "### License\n",
      "&gt;  \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: validation.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: test.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/movie_rationales).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: unlocking-the-human-perspective-on-movie-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Rationales (Rationales For Movie Reviews)\n",
      "name 'detect' is not defined Language is not detected: Human annotated rationales for movie reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Rationales (Rationales For Movie Reviews)\n",
      "### Human annotated rationales for movie reviews\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/movie_rationales)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset was created to allow researchers to gain an in-depth understanding of the inner workings of human-generated movie reviews. With these train, test, and validation sets, researchers can explore different aspects of movie reviews, such as sentiment labels or rationales behind them. By analyzing this information and finding patterns and correlations, insightful ideas can be discovered that can lead to developing models powerful enough to uncover importance of the unique human perspectives when interpreting movie reviews. Any data scientist or researcher interested in AI applications is encouraged to take advantage of this dataset which may potentially provide useful insights into better understanding user intent when reviewing movies\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset is intended to enable researchers and developers to uncover the rationales behind movie reviews. To use it effectively, you must understand the data format and how each column in the dataset works. \n",
      "&gt; \n",
      "&gt; #### What does each column mean? \n",
      "&gt; - **review:** The text of the movie review. (String) \n",
      "&gt; - **label:** The sentiment label of the review (Positive, Negative, or Neutral). (String)  \n",
      "&gt; - **validation.csv:** The validation set which contains reviews, labels, and evidence which can be used to validate models developed for understanding human perspective on movie reviews.  \n",
      "&gt; - **train.csv:** The train set which contains reviews, labels as well as evidence used for training a model based on human annotations of movie reviews.   \n",
      "&gt; - **test.csv:** The test set which contains reviews, labels and evidence that can be used to evaluate models on unseen data related to understanding perspectives of humans when it comes to movie reviews.. \n",
      "&gt; \n",
      "&gt;  #### How do I use this dataset?  \n",
      "&gt; \n",
      "&gt;  To get started with this dataset you need a working environment such as Python or R where you have access library’s needed for natural language processing(NLP). After setting up an environment with libraries that support NLP tasks execute following steps :   \n",
      "&gt; \n",
      "&gt;  - Import csv files into your workspace using appropriate functions provided by specified language libraries e,.g., for Python use pandas read_csv() method .    \n",
      "&gt; \n",
      "&gt;  - Preprocess your text data in 'review' & 'label' columns by standardizing them like removing stopwords from sentences & converting words into lowercase etc .Following link [link](Preprocessing%20Python%20Libraries/) provides best possible preprocessing libraries available in Python .   \n",
      "&gt; \n",
      "&gt;  - Train&Test ML algorithms using appropriate feature extraction techniques related to NLP( Bag Of Words , TF-IDF , Word2Vec ) eines are some examples in many more are available Refer [link](Feature%20Extraction%20Techniques/)   \n",
      "&gt; \n",
      "&gt;  - Measure performance accuracy after running experiments on datasets provided validation & test sets we have also included precision recall curves along famous metrics like F1 score & accuracy score so you could easily analyze hyperparameter tuning & algorithm efficiency according their outputs values you get while testing your ML algorithm     \n",
      "&gt; \n",
      "&gt;  - Recommendation systems are always fun! build a simple machine learning reccomendation system by collecting user visits logs post hand writting new featuers might\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Developing an automated movie review summarizer based on user ratings, that can accurately capture the salient points of a review and summarize it for moviegoers.\n",
      "&gt; - Training a model to predict the sentiment of a review, by combining machine learning models with human-annotated rationales from this dataset. \n",
      "&gt; - Building an AI system that can detect linguistic markers of deception in reviews (e.g., 'fake news', thin reviews etc) and issue warnings on possible fraudulent purchases or online reviews\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/movie_rationales)\n",
      "&gt; \n",
      "\n",
      "### License\n",
      "&gt;  \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: validation.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: test.csv**\n",
      "| Column name   | Description                                                                                                                  |\n",
      "|:--------------|:-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review**    | Text from the movie review. (String)                                                                                         |\n",
      "| **label**     | Indicates whether a particular review’s sentiment can be classified as Positive (1), Negative (-1) or Neutral (0). (Integer) |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/movie_rationales).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/supermarket-ordering-invoicing-and-sales-analysi\n",
      "name 'detect' is not defined Language is not detected: supermarket-ordering-invoicing-and-sales-analysi\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Supermarket Ordering, Invoicing, and Sales\n",
      "name 'detect' is not defined Language is not detected: Measuring Consumer Behavior and Engagement\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Supermarket Ordering, Invoicing, and Sales Analysis\n",
      "### Measuring Consumer Behavior and Engagement\n",
      "By  [[source]](https://zenodo.org/record/4092667#.Y8OsBtJBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This data set provides an in-depth look into the ordering, invoicing and sales processes at a supermarket. With information ranging from customers' meal choices to the value of their orders and whether they were converted into sales, this dataset opens up endless possibilities to uncover consumer behavior trends and engagement within the business. From understanding who is exchanging with the company and when, to seeing what types of meals are most popular with consumers, this rich collection of data will allow us to gain priceless insights into consumer actions and habits that can inform strategic decisions. Dive deep into big data now by exploring Invoices.csv, OrderLeads.csv and SalesTeam.csv for invaluable knowledge about your customers!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset provides an in-depth look into the ordering and invoicing processes of a supermarket, as well as how consumers are engaging with it. This dataset can be used to analyze and gain insights into consumer purchasing behaviors and preferences at the store. \n",
      "> \n",
      "> The first step in analyzing this data set is to familiarize yourself with its content. The dataset contains three CSV files: Invoices.csv, OrderLeads.csv, and SalesTeam.csv have different features like date of meal, participants, Meal Price, Type of meal ,company Name ,Order Value etc .Each file contains a list of columns containing data related to each particular feature like Date ,Date Of Meal Participants etc . \n",
      "> \n",
      "> Once you understand what types of information is included in each table it’ll be easier for you to start drawing conclusions about customer preferences and trends from within the store's data set. You can use mathematical functions or statistical models such as regression analysis or cluster analysis in order to gain even further insight into customers’ behaviors within the store setting. Additionally you could use machine learning algorithms such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM) if your goal was improving targeting strategy or recognizing patterns between customer purchases over time.  \n",
      "> \n",
      "> All these techniques will help you determine what promotional tactics work best when trying to attract customers and promote sales through various marketing campaigns at this supermarket chain They will also help shed light on how customers engage with products within categories across different days/weeks/months according to their own individual purchasing habits which would ultimately contribute towards improved marketing strategies from management side . \n",
      "> \n",
      "> Overall this data set provides immense potential for advancing understanding retail behaviour by allowing us access specific transactions that occurred at a given time frame; ultimately providing us detailed insight into customer behavior trends along with tools such software packages that allow us manipulate these metrics however necessary for entertainment purposes that help us identify strategies designed for greater efficiency when increasing revenue \n",
      "\n",
      "### Research Ideas\n",
      "> - Identifying the most profitable customer segment based on order value and converted sales. \n",
      "> - Leveraging trends in participant size to suggest meal packages for different types of meals. \n",
      "> - Analyzing the conversion rate of orders over time to optimize promotional strategies and product offerings accordingly\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4092667#.Y8OsBtJBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Invoices.csv**\n",
      "| Column name      | Description                                                  |\n",
      "|:-----------------|:-------------------------------------------------------------|\n",
      "| **Date**         | The date the order was placed. (Date)                        |\n",
      "| **Date of Meal** | The date the meal was served. (Date)                         |\n",
      "| **Participants** | The number of people who participated in the meal. (Integer) |\n",
      "| **Meal Price**   | The cost of the meal. (Float)                                |\n",
      "| **Type of Meal** | The type of meal that was ordered. (String)                  |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: OrderLeads.csv**\n",
      "| Column name      | Description                                                   |\n",
      "|:-----------------|:--------------------------------------------------------------|\n",
      "| **Date**         | The date the order was placed. (Date)                         |\n",
      "| **Company Name** | The name of the company associated with the order. (String)   |\n",
      "| **Order Value**  | The total value of the order. (Float)                         |\n",
      "| **Converted**    | Whether or not the order was converted into a sale. (Boolean) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4092667#.Y8OsBtJBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: supermarket-ordering-invoicing-and-sales-analysi\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Supermarket Ordering, Invoicing, and Sales\n",
      "name 'detect' is not defined Language is not detected: Measuring Consumer Behavior and Engagement\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Supermarket Ordering, Invoicing, and Sales Analysis\n",
      "### Measuring Consumer Behavior and Engagement\n",
      "By  [[source]](https://zenodo.org/record/4092667#.Y8OsBtJBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This data set provides an in-depth look into the ordering, invoicing and sales processes at a supermarket. With information ranging from customers' meal choices to the value of their orders and whether they were converted into sales, this dataset opens up endless possibilities to uncover consumer behavior trends and engagement within the business. From understanding who is exchanging with the company and when, to seeing what types of meals are most popular with consumers, this rich collection of data will allow us to gain priceless insights into consumer actions and habits that can inform strategic decisions. Dive deep into big data now by exploring Invoices.csv, OrderLeads.csv and SalesTeam.csv for invaluable knowledge about your customers!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset provides an in-depth look into the ordering and invoicing processes of a supermarket, as well as how consumers are engaging with it. This dataset can be used to analyze and gain insights into consumer purchasing behaviors and preferences at the store. \n",
      "> \n",
      "> The first step in analyzing this data set is to familiarize yourself with its content. The dataset contains three CSV files: Invoices.csv, OrderLeads.csv, and SalesTeam.csv have different features like date of meal, participants, Meal Price, Type of meal ,company Name ,Order Value etc .Each file contains a list of columns containing data related to each particular feature like Date ,Date Of Meal Participants etc . \n",
      "> \n",
      "> Once you understand what types of information is included in each table it’ll be easier for you to start drawing conclusions about customer preferences and trends from within the store's data set. You can use mathematical functions or statistical models such as regression analysis or cluster analysis in order to gain even further insight into customers’ behaviors within the store setting. Additionally you could use machine learning algorithms such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM) if your goal was improving targeting strategy or recognizing patterns between customer purchases over time.  \n",
      "> \n",
      "> All these techniques will help you determine what promotional tactics work best when trying to attract customers and promote sales through various marketing campaigns at this supermarket chain They will also help shed light on how customers engage with products within categories across different days/weeks/months according to their own individual purchasing habits which would ultimately contribute towards improved marketing strategies from management side . \n",
      "> \n",
      "> Overall this data set provides immense potential for advancing understanding retail behaviour by allowing us access specific transactions that occurred at a given time frame; ultimately providing us detailed insight into customer behavior trends along with tools such software packages that allow us manipulate these metrics however necessary for entertainment purposes that help us identify strategies designed for greater efficiency when increasing revenue \n",
      "\n",
      "### Research Ideas\n",
      "> - Identifying the most profitable customer segment based on order value and converted sales. \n",
      "> - Leveraging trends in participant size to suggest meal packages for different types of meals. \n",
      "> - Analyzing the conversion rate of orders over time to optimize promotional strategies and product offerings accordingly\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4092667#.Y8OsBtJBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Invoices.csv**\n",
      "| Column name      | Description                                                  |\n",
      "|:-----------------|:-------------------------------------------------------------|\n",
      "| **Date**         | The date the order was placed. (Date)                        |\n",
      "| **Date of Meal** | The date the meal was served. (Date)                         |\n",
      "| **Participants** | The number of people who participated in the meal. (Integer) |\n",
      "| **Meal Price**   | The cost of the meal. (Float)                                |\n",
      "| **Type of Meal** | The type of meal that was ordered. (String)                  |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: OrderLeads.csv**\n",
      "| Column name      | Description                                                   |\n",
      "|:-----------------|:--------------------------------------------------------------|\n",
      "| **Date**         | The date the order was placed. (Date)                         |\n",
      "| **Company Name** | The name of the company associated with the order. (String)   |\n",
      "| **Order Value**  | The total value of the order. (Float)                         |\n",
      "| **Converted**    | Whether or not the order was converted into a sale. (Boolean) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4092667#.Y8OsBtJBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/spotify-tracks-genre-dataset\n",
      "name 'detect' is not defined Language is not detected: spotify-tracks-genre-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Spotify Tracks Genre \n",
      "name 'detect' is not defined Language is not detected: Audio features of tracks across diverse genres\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Spotify Tracks Genre \n",
      "### Audio features of tracks across diverse genres\n",
      "By maharshipandya (From Huggingface) [[source]](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset provides comprehensive information about Spotify tracks encompassing a diverse collection of 125 genres. It has been compiled and cleaned using Spotify's Web API and Python. Presented in CSV format, this dataset is easily accessible and amenable to analysis. The dataset comprises multiple columns, each representing distinctive audio features associated with individual tracks.\n",
      "&gt; \n",
      "&gt; The columns include: artists (the name of the artist or artists who performed the track), album_name (the title of the album to which the track belongs), track_name (the specific name of each track), popularity (a numerical score indicating the popularity of a song on Spotify ranging from 0 to 100), duration_ms (the duration of each track measured in milliseconds), explicit (a boolean value denoting whether a song contains explicit content or not).\n",
      "&gt; \n",
      "&gt; Furthermore, there are various audio features that provide deep insights into the musical characteristics of each track. These features include danceability, energy, key, loudness, mode, speechiness (indicating whether spoken words are present in a song), acousticness (measuring how much a song leans towards acoustic sounds rather than electric ones), instrumentalness (indicating how likely it is for a song to be instrumental rather than vocal-oriented).\n",
      "&gt; \n",
      "&gt; Additional audio attributes encompass liveness, reflecting the presence or absence of live audience elements within tracks; valence quantifying musical positiveness conveyed by a song; tempo denoting beats per minute; and time_signature revealing details about bar structures within tracks.\n",
      "&gt; \n",
      "&gt; The dataset enables users to discern patterns across multiple genres while also facilitating genre prediction based on perceptible audio nuances derived through machine learning models.\n",
      "&gt; \n",
      "&gt; Aspiring audiophiles, music enthusiasts,and data scientists can effectively harness this repository for research purposes—fostering extensive exploration into genre dynamics and comprehending nuanced relationships between various musical attributes featured in these Spotify masterpieces\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; - Introduction:\n",
      "&gt; \n",
      "&gt; - Download and Load the Dataset:\n",
      "&gt;    Start by downloading the dataset from Kaggle in CSV format. Once downloaded, load the dataset into your preferred programming environment or tool such as Python, R, or Excel.\n",
      "&gt; \n",
      "&gt; - Familiarize Yourself with the Columns:\n",
      "&gt;    Take some time to understand the meaning of each column in the dataset:\n",
      "&gt; \n",
      "&gt;    - artists: The name of the artist(s) who performed the track.\n",
      "&gt;    - album_name: The name of at album that contains a given track.\n",
      "&gt;    - track_name: The name of a specific track.\n",
      "&gt;    - popularity: A score indicating how popular a track is on Spotify (ranging from 0 to 100).\n",
      "&gt;    - duration_ms: The duration of a track in milliseconds.\n",
      "&gt;    - explicit: Indicates whether a track contains explicit content (True or False).\n",
      "&gt;    \n",
      "&gt; - Explore Audio Features:\n",
      "&gt;    This dataset includes various audio features associated with each track. Here are some notable ones:\n",
      "&gt; \n",
      "&gt;     A. Danceability:\n",
      "&gt;        Danceability measures how suitable a track is for dancing, ranging from 0 to 1. Tracks with high danceability scores are more energetic and rhythmic, making them ideal for dancing.\n",
      "&gt; \n",
      "&gt;     B. Energy:\n",
      "&gt;        Energy represents intensity and activity within a song on a scale from 0 to 1. Tracks with high energy tend to be more fast-paced and intense.\n",
      "&gt; \n",
      "&gt;     C.Loudness:\n",
      "&gt;       Loudness indicates how loud or quiet an entire song is in decibels (dB). Positive values represent louder songs while negative values suggest quieter ones.\n",
      "&gt; \n",
      "&gt;     D.Key:\n",
      "&gt;       Key refers to different musical keys assigned integers ranging from 0-11,\n",
      "&gt;       with each number representing a different key. Knowing the key can provide insights into the mood and tone of a song.\n",
      "&gt; \n",
      "&gt;     E.Valence:\n",
      "&gt;       Valence measures the musical positiveness conveyed by a track, ranging from 0 to 1. High valence values indicate more positive or happy tracks, while lower values suggest more negative or sad ones.\n",
      "&gt; \n",
      "&gt;     F.Tempo:\n",
      "&gt;       Tempo is the speed or pace of a song in beats per minute (BPM). It gives an idea about how fast or slow a track is.\n",
      "&gt; \n",
      "&gt; - Data Analysis and Visualization:\n",
      "&gt;    Utilize various data analysis techniques and visualization tools to gain insights into the\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Music Recommendation System: With multiple audio features such as danceability, energy, and valence, this dataset can be used to build a music recommendation system. By analyzing the preferences of users for certain genres and characteristics of tracks, the system can suggest similar tracks or even recommend new genres that users might enjoy.\n",
      "&gt; - Genre Classification: The dataset provides a wide range of genres across different musical characteristics. By training a machine learning model using the audio features as predictors and the genre as the target variable, it is possible to accurately classify tracks into their respective genres. This classification can be further used for organizing large music libraries or creating playlists based on specific genres.\n",
      "&gt; - Studying Genre Evolution: By analyzing trends in various audio features like tempo, danceability, and energy over time, researchers can gain insights into how different music genres have evolved over decades. This analysis could help in understanding cultural shifts and changes within specific musical styles and may shed light on the influence of different factors on genre development\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name          | Description                                                                                                                     |\n",
      "|:---------------------|:--------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **artists**          | The name(s) of the artist(s) associated with the track. (String)                                                                |\n",
      "| **album_name**       | The name of the album that the track belongs to. (String)                                                                       |\n",
      "| **track_name**       | The name of the track. (String)                                                                                                 |\n",
      "| **popularity**       | The popularity score of the track on Spotify, ranging from 0 to 100. (Integer)                                                  |\n",
      "| **duration_ms**      | The duration of the track in milliseconds. (Integer)                                                                            |\n",
      "| **explicit**         | A boolean value indicating whether the track contains explicit content. (Boolean)                                               |\n",
      "| **danceability**     | A score ranging from 0 to 1 that represents how suitable a track is for dancing based on various musical elements. (Float)      |\n",
      "| **energy**           | A measure of the intensity and activity of a track, ranging from 0 to 1. (Float)                                                |\n",
      "| **key**              | The key of the track represented by an integer value. (Integer)                                                                 |\n",
      "| **loudness**         | The loudness of the track in decibels (dB). (Float)                                                                             |\n",
      "| **mode**             | The tonal mode of the track, represented by an integer value (0 for minor, 1 for major). (Integer)                              |\n",
      "| **speechiness**      | A score ranging from 0 to 1 that represents the presence of spoken words in a track. (Float)                                    |\n",
      "| **acousticness**     | A score ranging from 0 to 1 that represents the extent to which a track possesses an acoustic quality. (Float)                  |\n",
      "| **instrumentalness** | A score ranging from 0 to 1 that represents the likelihood of a track being instrumental. (Float)                               |\n",
      "| **liveness**         | A score ranging from 0 to 1 that represents the presence of an audience during the recording or performance of a track. (Float) |\n",
      "| **valence**          | A score ranging from 0 to 1 that represents the musical positiveness conveyed by a track. (Float)                               |\n",
      "| **tempo**            | The tempo of the track in beats per minute (BPM). (Float)                                                                       |\n",
      "| **time_signature**   | The number of beats within each bar of the track. (Integer)                                                                     |\n",
      "| **track_genre**      | The genre of the track. (String)                                                                                                |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [maharshipandya (From Huggingface)](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: spotify-tracks-genre-dataset\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Spotify Tracks Genre \n",
      "name 'detect' is not defined Language is not detected: Audio features of tracks across diverse genres\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Spotify Tracks Genre \n",
      "### Audio features of tracks across diverse genres\n",
      "By maharshipandya (From Huggingface) [[source]](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset provides comprehensive information about Spotify tracks encompassing a diverse collection of 125 genres. It has been compiled and cleaned using Spotify's Web API and Python. Presented in CSV format, this dataset is easily accessible and amenable to analysis. The dataset comprises multiple columns, each representing distinctive audio features associated with individual tracks.\n",
      "&gt; \n",
      "&gt; The columns include: artists (the name of the artist or artists who performed the track), album_name (the title of the album to which the track belongs), track_name (the specific name of each track), popularity (a numerical score indicating the popularity of a song on Spotify ranging from 0 to 100), duration_ms (the duration of each track measured in milliseconds), explicit (a boolean value denoting whether a song contains explicit content or not).\n",
      "&gt; \n",
      "&gt; Furthermore, there are various audio features that provide deep insights into the musical characteristics of each track. These features include danceability, energy, key, loudness, mode, speechiness (indicating whether spoken words are present in a song), acousticness (measuring how much a song leans towards acoustic sounds rather than electric ones), instrumentalness (indicating how likely it is for a song to be instrumental rather than vocal-oriented).\n",
      "&gt; \n",
      "&gt; Additional audio attributes encompass liveness, reflecting the presence or absence of live audience elements within tracks; valence quantifying musical positiveness conveyed by a song; tempo denoting beats per minute; and time_signature revealing details about bar structures within tracks.\n",
      "&gt; \n",
      "&gt; The dataset enables users to discern patterns across multiple genres while also facilitating genre prediction based on perceptible audio nuances derived through machine learning models.\n",
      "&gt; \n",
      "&gt; Aspiring audiophiles, music enthusiasts,and data scientists can effectively harness this repository for research purposes—fostering extensive exploration into genre dynamics and comprehending nuanced relationships between various musical attributes featured in these Spotify masterpieces\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; - Introduction:\n",
      "&gt; \n",
      "&gt; - Download and Load the Dataset:\n",
      "&gt;    Start by downloading the dataset from Kaggle in CSV format. Once downloaded, load the dataset into your preferred programming environment or tool such as Python, R, or Excel.\n",
      "&gt; \n",
      "&gt; - Familiarize Yourself with the Columns:\n",
      "&gt;    Take some time to understand the meaning of each column in the dataset:\n",
      "&gt; \n",
      "&gt;    - artists: The name of the artist(s) who performed the track.\n",
      "&gt;    - album_name: The name of at album that contains a given track.\n",
      "&gt;    - track_name: The name of a specific track.\n",
      "&gt;    - popularity: A score indicating how popular a track is on Spotify (ranging from 0 to 100).\n",
      "&gt;    - duration_ms: The duration of a track in milliseconds.\n",
      "&gt;    - explicit: Indicates whether a track contains explicit content (True or False).\n",
      "&gt;    \n",
      "&gt; - Explore Audio Features:\n",
      "&gt;    This dataset includes various audio features associated with each track. Here are some notable ones:\n",
      "&gt; \n",
      "&gt;     A. Danceability:\n",
      "&gt;        Danceability measures how suitable a track is for dancing, ranging from 0 to 1. Tracks with high danceability scores are more energetic and rhythmic, making them ideal for dancing.\n",
      "&gt; \n",
      "&gt;     B. Energy:\n",
      "&gt;        Energy represents intensity and activity within a song on a scale from 0 to 1. Tracks with high energy tend to be more fast-paced and intense.\n",
      "&gt; \n",
      "&gt;     C.Loudness:\n",
      "&gt;       Loudness indicates how loud or quiet an entire song is in decibels (dB). Positive values represent louder songs while negative values suggest quieter ones.\n",
      "&gt; \n",
      "&gt;     D.Key:\n",
      "&gt;       Key refers to different musical keys assigned integers ranging from 0-11,\n",
      "&gt;       with each number representing a different key. Knowing the key can provide insights into the mood and tone of a song.\n",
      "&gt; \n",
      "&gt;     E.Valence:\n",
      "&gt;       Valence measures the musical positiveness conveyed by a track, ranging from 0 to 1. High valence values indicate more positive or happy tracks, while lower values suggest more negative or sad ones.\n",
      "&gt; \n",
      "&gt;     F.Tempo:\n",
      "&gt;       Tempo is the speed or pace of a song in beats per minute (BPM). It gives an idea about how fast or slow a track is.\n",
      "&gt; \n",
      "&gt; - Data Analysis and Visualization:\n",
      "&gt;    Utilize various data analysis techniques and visualization tools to gain insights into the\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Music Recommendation System: With multiple audio features such as danceability, energy, and valence, this dataset can be used to build a music recommendation system. By analyzing the preferences of users for certain genres and characteristics of tracks, the system can suggest similar tracks or even recommend new genres that users might enjoy.\n",
      "&gt; - Genre Classification: The dataset provides a wide range of genres across different musical characteristics. By training a machine learning model using the audio features as predictors and the genre as the target variable, it is possible to accurately classify tracks into their respective genres. This classification can be further used for organizing large music libraries or creating playlists based on specific genres.\n",
      "&gt; - Studying Genre Evolution: By analyzing trends in various audio features like tempo, danceability, and energy over time, researchers can gain insights into how different music genres have evolved over decades. This analysis could help in understanding cultural shifts and changes within specific musical styles and may shed light on the influence of different factors on genre development\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name          | Description                                                                                                                     |\n",
      "|:---------------------|:--------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **artists**          | The name(s) of the artist(s) associated with the track. (String)                                                                |\n",
      "| **album_name**       | The name of the album that the track belongs to. (String)                                                                       |\n",
      "| **track_name**       | The name of the track. (String)                                                                                                 |\n",
      "| **popularity**       | The popularity score of the track on Spotify, ranging from 0 to 100. (Integer)                                                  |\n",
      "| **duration_ms**      | The duration of the track in milliseconds. (Integer)                                                                            |\n",
      "| **explicit**         | A boolean value indicating whether the track contains explicit content. (Boolean)                                               |\n",
      "| **danceability**     | A score ranging from 0 to 1 that represents how suitable a track is for dancing based on various musical elements. (Float)      |\n",
      "| **energy**           | A measure of the intensity and activity of a track, ranging from 0 to 1. (Float)                                                |\n",
      "| **key**              | The key of the track represented by an integer value. (Integer)                                                                 |\n",
      "| **loudness**         | The loudness of the track in decibels (dB). (Float)                                                                             |\n",
      "| **mode**             | The tonal mode of the track, represented by an integer value (0 for minor, 1 for major). (Integer)                              |\n",
      "| **speechiness**      | A score ranging from 0 to 1 that represents the presence of spoken words in a track. (Float)                                    |\n",
      "| **acousticness**     | A score ranging from 0 to 1 that represents the extent to which a track possesses an acoustic quality. (Float)                  |\n",
      "| **instrumentalness** | A score ranging from 0 to 1 that represents the likelihood of a track being instrumental. (Float)                               |\n",
      "| **liveness**         | A score ranging from 0 to 1 that represents the presence of an audience during the recording or performance of a track. (Float) |\n",
      "| **valence**          | A score ranging from 0 to 1 that represents the musical positiveness conveyed by a track. (Float)                               |\n",
      "| **tempo**            | The tempo of the track in beats per minute (BPM). (Float)                                                                       |\n",
      "| **time_signature**   | The number of beats within each bar of the track. (Integer)                                                                     |\n",
      "| **track_genre**      | The genre of the track. (String)                                                                                                |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [maharshipandya (From Huggingface)](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/movie-reviews-dataset-with-sentiment-labels\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-dataset-with-sentiment-labels\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Dataset with Sentiment Labels\n",
      "name 'detect' is not defined Language is not detected: Analyze and Classify Feelings within Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Reviews Dataset with Sentiment Labels\n",
      "### Analyze and Classify Feelings within Reviews\n",
      "By Rajeev Sharma [[source]](https://data.world/rajeevsharma993)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains thousands of movie reviews from viewers across the world. Each review was given a sentiment label indicating whether the review is positive or negative. Use this dataset to get insights into general sentiment towards certain movies, and further explore any trends in sentiment throughout different types of genres, movie ratings, etc. Discover patterns in audience reactions to various films and gain valuable insights for your own projects –all with a few clicks! This Kaggle dataset can help you uncover a comprehensive picture of how viewers perceive movies within various segments and aid your decision-making processes in the entertainment industry\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset can be used for sentiment analysis to analyze the sentiment of movie reviews. The data includes reviews from multiple sources, each with a label indicating whether the review is positive or negative. \n",
      "> \n",
      "> To get started, you'll need to first read in the data into your workflow to perform any cleaning that may be necessary prior to analysis. Depending on the goal of your analysis, you may also want to remove duplicates, punctuation marks (e.g !), and emojis that are present within some reviews. Once this is complete, you can begin exploring the data using different machine learning algorithms such as KNN and logistic regression in order to classify the sentiment of each review accurately. It's important to note that these models should be tested against held out test datasets so that they can be evaluated on unseen data prior to deployment into any production systems where they will handle future incoming user-generated inputs from various sources around the world! \n",
      "> \n",
      "> By performing sentiment analysis on this dataset, researchers and practitioners will be able to gain a better understanding of how people rate certain movies in order gain insights about user preferences for certain genres or story themes - all with respect to their personal opinions rather than studio ratings or critics' scores which may not always reflect public opinion accurately! \n",
      "> \n",
      "> \n",
      "> Finally don't forget - depending on your interpretation and usage - it's important recognize when it could have neglected certain biases present within an unbalanced dataset (such as one with more negative reviews than positives ones) by taking caution when interpreting results accordingly! Who knows—maybe by using this dataset your insights could even inspire upcoming blockbuster hits in movie theaters near you!\n",
      "\n",
      "### Research Ideas\n",
      "> - Natural language processing (NLP) sentiment analysis to identify customer opinions and feedback on movies.\n",
      "> - Using predictive analytics to predict how a movie will be received by a specific market segment, based on its reviews from all regions of the world.\n",
      "> - Utilizing clustering algorithms for content-based recommendation systems for movies that are similar in tone and theme as those already enjoyed by users\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/rajeevsharma993)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: moviereviews2.csv**\n",
      "| Column name   | Description                                 |\n",
      "|:--------------|:--------------------------------------------|\n",
      "| **label**     | The sentiment label of the review. (String) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: moviereviews.csv**\n",
      "| Column name   | Description                                 |\n",
      "|:--------------|:--------------------------------------------|\n",
      "| **label**     | The sentiment label of the review. (String) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Rajeev Sharma](https://data.world/rajeevsharma993).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: movie-reviews-dataset-with-sentiment-labels\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movie Reviews Dataset with Sentiment Labels\n",
      "name 'detect' is not defined Language is not detected: Analyze and Classify Feelings within Reviews\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movie Reviews Dataset with Sentiment Labels\n",
      "### Analyze and Classify Feelings within Reviews\n",
      "By Rajeev Sharma [[source]](https://data.world/rajeevsharma993)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains thousands of movie reviews from viewers across the world. Each review was given a sentiment label indicating whether the review is positive or negative. Use this dataset to get insights into general sentiment towards certain movies, and further explore any trends in sentiment throughout different types of genres, movie ratings, etc. Discover patterns in audience reactions to various films and gain valuable insights for your own projects –all with a few clicks! This Kaggle dataset can help you uncover a comprehensive picture of how viewers perceive movies within various segments and aid your decision-making processes in the entertainment industry\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset can be used for sentiment analysis to analyze the sentiment of movie reviews. The data includes reviews from multiple sources, each with a label indicating whether the review is positive or negative. \n",
      "> \n",
      "> To get started, you'll need to first read in the data into your workflow to perform any cleaning that may be necessary prior to analysis. Depending on the goal of your analysis, you may also want to remove duplicates, punctuation marks (e.g !), and emojis that are present within some reviews. Once this is complete, you can begin exploring the data using different machine learning algorithms such as KNN and logistic regression in order to classify the sentiment of each review accurately. It's important to note that these models should be tested against held out test datasets so that they can be evaluated on unseen data prior to deployment into any production systems where they will handle future incoming user-generated inputs from various sources around the world! \n",
      "> \n",
      "> By performing sentiment analysis on this dataset, researchers and practitioners will be able to gain a better understanding of how people rate certain movies in order gain insights about user preferences for certain genres or story themes - all with respect to their personal opinions rather than studio ratings or critics' scores which may not always reflect public opinion accurately! \n",
      "> \n",
      "> \n",
      "> Finally don't forget - depending on your interpretation and usage - it's important recognize when it could have neglected certain biases present within an unbalanced dataset (such as one with more negative reviews than positives ones) by taking caution when interpreting results accordingly! Who knows—maybe by using this dataset your insights could even inspire upcoming blockbuster hits in movie theaters near you!\n",
      "\n",
      "### Research Ideas\n",
      "> - Natural language processing (NLP) sentiment analysis to identify customer opinions and feedback on movies.\n",
      "> - Using predictive analytics to predict how a movie will be received by a specific market segment, based on its reviews from all regions of the world.\n",
      "> - Utilizing clustering algorithms for content-based recommendation systems for movies that are similar in tone and theme as those already enjoyed by users\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/rajeevsharma993)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: moviereviews2.csv**\n",
      "| Column name   | Description                                 |\n",
      "|:--------------|:--------------------------------------------|\n",
      "| **label**     | The sentiment label of the review. (String) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: moviereviews.csv**\n",
      "| Column name   | Description                                 |\n",
      "|:--------------|:--------------------------------------------|\n",
      "| **label**     | The sentiment label of the review. (String) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Rajeev Sharma](https://data.world/rajeevsharma993).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/online-retail-transaction-data\n",
      "name 'detect' is not defined Language is not detected: online-retail-transaction-data\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Online Retail Transaction Data\n",
      "name 'detect' is not defined Language is not detected: UK Online Retail Sales and Customer Transaction Data\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Online Retail Transaction Data\n",
      "### UK Online Retail Sales and Customer Transaction Data\n",
      "By UCI [[source]](https://data.world/uci)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; # Comprehensive Dataset on Online Retail Sales and Customer Data\n",
      "&gt; \n",
      "&gt; Welcome to this comprehensive dataset offering a wide array of information related to online retail sales. This data set provides an in-depth look at transactions, product details, and customer information documented by an online retail company based in the UK. The scope of the data spans vastly, from granular details about each product sold to extensive customer data sets from different countries.\n",
      "&gt; \n",
      "&gt; This transnational data set is a treasure trove of vital business insights as it meticulously catalogues all the transactions that happened during its span. It houses rich transactional records curated by a renowned non-store online retail company based in the UK known for selling unique all-occasion gifts. A considerable portion of its clientele includes wholesalers; ergo, this dataset can prove instrumental for companies looking for patterns or studying purchasing trends among such businesses.\n",
      "&gt; \n",
      "&gt; The available attributes within this dataset offer valuable pieces of information:\n",
      "&gt; \n",
      "&gt; - **InvoiceNo:** This attribute refers to invoice numbers that are six-digit integral numbers uniquely assigned to every transaction logged in this system. Transactions marked with 'c' at the beginning signify cancellations - adding yet another dimension for purchase pattern analysis.\n",
      "&gt;   \n",
      "&gt; - **StockCode:** Stock Code corresponds with specific items as they're represented within the inventory system via 5-digit integral numbers; these allow easy identification and distinction between products.\n",
      "&gt;   \n",
      "&gt; - **Description:** This refers to product names, giving users qualitative knowledge about what kind of items are being bought and sold frequently.\n",
      "&gt;    \n",
      "&gt; - **Quantity:** These figures ascertain the volume of each product per transaction – important figures that can help understand buying trends better.\n",
      "&gt; \n",
      "&gt; - **InvoiceDate:** Invoice Dates detail when each transaction was generated down to precise timestamps – invaluable when conducting time-based trend analysis or segmentation studies.\n",
      "&gt; \n",
      "&gt; - **UnitPrice:** Unit prices represent how much each unit retails at — crucial for revenue calculations or cost-related analyses.\n",
      "&gt;    \n",
      "&gt; Finally,\n",
      "&gt; \n",
      "&gt; - **Country**: This locational attribute shows where each customer hails from, adding geographical segmentation to your data investigation toolkit.\n",
      "&gt; \n",
      "&gt; This dataset was originally collated by Dr Daqing Chen, Director of the Public Analytics group based at the School of Engineering, London South Bank University. His research studies and business cases with this dataset have been published in various papers contributing to establishing a solid theoretical basis for direct, data and digital marketing strategies.\n",
      "&gt; \n",
      "&gt; Access to such records can ensure enriching explorations or formulating insightful hypotheses about consumer behavior patterns among wholesalers. Whether it's managing inventory or studying transactional trends over time or spotting cancellation patterns - this dataset is apt for multiple forms of retail analysis\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ### 1. Sales Analysis:\n",
      "&gt; \n",
      "&gt; Sales data forms the backbone of this dataset, and it allows users to delve into various aspects of sales performance. \n",
      "&gt; You can use the **Quantity** and **UnitPrice** fields to calculate metrics like revenue, and further combine it with **InvoiceNo** information to understand sales over individual transactions.\n",
      "&gt; \n",
      "&gt; ### 2. Product Analysis:\n",
      "&gt; \n",
      "&gt; Each product in this dataset comes with its unique identifier (**StockCode**) and its name (**Description**). You could analyse which products are most popular based on Quantity sold or look at popularity per transaction by considering both Quantity and InvoiceNo.\n",
      "&gt; \n",
      "&gt; ### 3. Customer Segmentation:\n",
      "&gt; \n",
      "&gt; If you associated specific business logic onto the transactions (such as calculating total amounts), then you could use standard machine learning methods or even RFM (Recency, Frequency, Monetary) segmentation techniques combining it with 'CustomerID' for your customer base to understand customer behavior better.\n",
      "&gt; Concatenating invoice numbers (which stand for separate transactions) per client will give insights about your clients as well.\n",
      "&gt; \n",
      "&gt; ### 4. Geographical Analysis:\n",
      "&gt; \n",
      "&gt; The Country column enables analysts to study purchase patterns across different geographical locations.\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Practical applications\n",
      "&gt; \n",
      "&gt; Understand what products sell best where - It can help drive tailored marketing strategies.\n",
      "&gt; Anomalies detection – Identify unusual behaviors that might lead fraud investigations.\n",
      "&gt; Forecast Demand - Building time-series models can aid in predicting future sales.\n",
      "&gt; Promotional Strategy - Seeing what sells together frequently may help design product bundles or suggest products.\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Some useful tips\n",
      "&gt; - Understand meaning behind each field:\n",
      "&gt;    Each field describes an aspect of each transaction allowing us access analysis from multiple dimensions\n",
      "&gt; - Cleaning data:\n",
      "&gt;    Take care while using fields like Description since they are nominal fields and may require further cleaning to avoid any errors during an analysis.\n",
      "&gt; - Exploring correlations with classification and regression algorithms can be instrumental in revealing complex relations.\n",
      "&gt; \n",
      "&gt; In conclusion, this dataset is a Pandora's box - teeming with insights. Talented data analysts can use it to extract meaningful data stories that drive valuable business decisions\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Inventory Management: By analyzing the quantity and frequency of product sales, retailers can effectively manage their stock and predict future demand. This would help ensure that popular items are always available while less popular items aren't overstocked.\n",
      "&gt; - Customer Segmentation: Data from different countries can be used to understand buying habits across different geographical locations. This will allow the retail company to tailor its marketing strategy for each specific region or country, leading to more effective advertising campaigns.\n",
      "&gt; - Sales Trend Analysis: With data spanning almost a year, temporal patterns in purchasing behavior can be identified, including seasonality and other trends (like increase in sales during holidays). Techniques like time-series analysis could provide insights into peak shopping times or days of the week when sales are typically high.\n",
      "&gt;    \n",
      "&gt; - Predictive Analysis for Cross Selling & Upselling: Based on a customer's previous purchase history, predictive algorithms can be utilized to suggest related products which might interest the customer, enhancing upsell and cross-sell opportunities.\n",
      "&gt; - Detecting Fraud: Analysing sale returns (marked with 'c' in InvoiceNo) across customers or regions could help pinpoint fraudulent activities or operational issues leading to those returns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/uci)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: online_retail.csv**\n",
      "| Column name     | Description                                                                                                                        |\n",
      "|:----------------|:-----------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **InvoiceNo**   | A 6-digit number uniquely assigned to each transaction. If the number is prefixed with 'c', it indicates a cancellation. (Nominal) |\n",
      "| **StockCode**   | A unique identifier for each product sold by the retailer. (Nominal)                                                               |\n",
      "| **Description** | The name or a brief description of the product. (Nominal)                                                                          |\n",
      "| **Quantity**    | The number of units of the product sold in each transaction. (Numeric)                                                             |\n",
      "| **InvoiceDate** | The date and time when the transaction was made. (Datetime)                                                                        |\n",
      "| **UnitPrice**   | The price per unit of the product in sterling. (Numeric)                                                                           |\n",
      "| **Country**     | The country where the customer resides. (Nominal)                                                                                  |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [UCI](https://data.world/uci).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: online-retail-transaction-data\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Online Retail Transaction Data\n",
      "name 'detect' is not defined Language is not detected: UK Online Retail Sales and Customer Transaction Data\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Online Retail Transaction Data\n",
      "### UK Online Retail Sales and Customer Transaction Data\n",
      "By UCI [[source]](https://data.world/uci)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; # Comprehensive Dataset on Online Retail Sales and Customer Data\n",
      "&gt; \n",
      "&gt; Welcome to this comprehensive dataset offering a wide array of information related to online retail sales. This data set provides an in-depth look at transactions, product details, and customer information documented by an online retail company based in the UK. The scope of the data spans vastly, from granular details about each product sold to extensive customer data sets from different countries.\n",
      "&gt; \n",
      "&gt; This transnational data set is a treasure trove of vital business insights as it meticulously catalogues all the transactions that happened during its span. It houses rich transactional records curated by a renowned non-store online retail company based in the UK known for selling unique all-occasion gifts. A considerable portion of its clientele includes wholesalers; ergo, this dataset can prove instrumental for companies looking for patterns or studying purchasing trends among such businesses.\n",
      "&gt; \n",
      "&gt; The available attributes within this dataset offer valuable pieces of information:\n",
      "&gt; \n",
      "&gt; - **InvoiceNo:** This attribute refers to invoice numbers that are six-digit integral numbers uniquely assigned to every transaction logged in this system. Transactions marked with 'c' at the beginning signify cancellations - adding yet another dimension for purchase pattern analysis.\n",
      "&gt;   \n",
      "&gt; - **StockCode:** Stock Code corresponds with specific items as they're represented within the inventory system via 5-digit integral numbers; these allow easy identification and distinction between products.\n",
      "&gt;   \n",
      "&gt; - **Description:** This refers to product names, giving users qualitative knowledge about what kind of items are being bought and sold frequently.\n",
      "&gt;    \n",
      "&gt; - **Quantity:** These figures ascertain the volume of each product per transaction – important figures that can help understand buying trends better.\n",
      "&gt; \n",
      "&gt; - **InvoiceDate:** Invoice Dates detail when each transaction was generated down to precise timestamps – invaluable when conducting time-based trend analysis or segmentation studies.\n",
      "&gt; \n",
      "&gt; - **UnitPrice:** Unit prices represent how much each unit retails at — crucial for revenue calculations or cost-related analyses.\n",
      "&gt;    \n",
      "&gt; Finally,\n",
      "&gt; \n",
      "&gt; - **Country**: This locational attribute shows where each customer hails from, adding geographical segmentation to your data investigation toolkit.\n",
      "&gt; \n",
      "&gt; This dataset was originally collated by Dr Daqing Chen, Director of the Public Analytics group based at the School of Engineering, London South Bank University. His research studies and business cases with this dataset have been published in various papers contributing to establishing a solid theoretical basis for direct, data and digital marketing strategies.\n",
      "&gt; \n",
      "&gt; Access to such records can ensure enriching explorations or formulating insightful hypotheses about consumer behavior patterns among wholesalers. Whether it's managing inventory or studying transactional trends over time or spotting cancellation patterns - this dataset is apt for multiple forms of retail analysis\n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ### 1. Sales Analysis:\n",
      "&gt; \n",
      "&gt; Sales data forms the backbone of this dataset, and it allows users to delve into various aspects of sales performance. \n",
      "&gt; You can use the **Quantity** and **UnitPrice** fields to calculate metrics like revenue, and further combine it with **InvoiceNo** information to understand sales over individual transactions.\n",
      "&gt; \n",
      "&gt; ### 2. Product Analysis:\n",
      "&gt; \n",
      "&gt; Each product in this dataset comes with its unique identifier (**StockCode**) and its name (**Description**). You could analyse which products are most popular based on Quantity sold or look at popularity per transaction by considering both Quantity and InvoiceNo.\n",
      "&gt; \n",
      "&gt; ### 3. Customer Segmentation:\n",
      "&gt; \n",
      "&gt; If you associated specific business logic onto the transactions (such as calculating total amounts), then you could use standard machine learning methods or even RFM (Recency, Frequency, Monetary) segmentation techniques combining it with 'CustomerID' for your customer base to understand customer behavior better.\n",
      "&gt; Concatenating invoice numbers (which stand for separate transactions) per client will give insights about your clients as well.\n",
      "&gt; \n",
      "&gt; ### 4. Geographical Analysis:\n",
      "&gt; \n",
      "&gt; The Country column enables analysts to study purchase patterns across different geographical locations.\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Practical applications\n",
      "&gt; \n",
      "&gt; Understand what products sell best where - It can help drive tailored marketing strategies.\n",
      "&gt; Anomalies detection – Identify unusual behaviors that might lead fraud investigations.\n",
      "&gt; Forecast Demand - Building time-series models can aid in predicting future sales.\n",
      "&gt; Promotional Strategy - Seeing what sells together frequently may help design product bundles or suggest products.\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; ## Some useful tips\n",
      "&gt; - Understand meaning behind each field:\n",
      "&gt;    Each field describes an aspect of each transaction allowing us access analysis from multiple dimensions\n",
      "&gt; - Cleaning data:\n",
      "&gt;    Take care while using fields like Description since they are nominal fields and may require further cleaning to avoid any errors during an analysis.\n",
      "&gt; - Exploring correlations with classification and regression algorithms can be instrumental in revealing complex relations.\n",
      "&gt; \n",
      "&gt; In conclusion, this dataset is a Pandora's box - teeming with insights. Talented data analysts can use it to extract meaningful data stories that drive valuable business decisions\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Inventory Management: By analyzing the quantity and frequency of product sales, retailers can effectively manage their stock and predict future demand. This would help ensure that popular items are always available while less popular items aren't overstocked.\n",
      "&gt; - Customer Segmentation: Data from different countries can be used to understand buying habits across different geographical locations. This will allow the retail company to tailor its marketing strategy for each specific region or country, leading to more effective advertising campaigns.\n",
      "&gt; - Sales Trend Analysis: With data spanning almost a year, temporal patterns in purchasing behavior can be identified, including seasonality and other trends (like increase in sales during holidays). Techniques like time-series analysis could provide insights into peak shopping times or days of the week when sales are typically high.\n",
      "&gt;    \n",
      "&gt; - Predictive Analysis for Cross Selling & Upselling: Based on a customer's previous purchase history, predictive algorithms can be utilized to suggest related products which might interest the customer, enhancing upsell and cross-sell opportunities.\n",
      "&gt; - Detecting Fraud: Analysing sale returns (marked with 'c' in InvoiceNo) across customers or regions could help pinpoint fraudulent activities or operational issues leading to those returns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/uci)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: online_retail.csv**\n",
      "| Column name     | Description                                                                                                                        |\n",
      "|:----------------|:-----------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **InvoiceNo**   | A 6-digit number uniquely assigned to each transaction. If the number is prefixed with 'c', it indicates a cancellation. (Nominal) |\n",
      "| **StockCode**   | A unique identifier for each product sold by the retailer. (Nominal)                                                               |\n",
      "| **Description** | The name or a brief description of the product. (Nominal)                                                                          |\n",
      "| **Quantity**    | The number of units of the product sold in each transaction. (Numeric)                                                             |\n",
      "| **InvoiceDate** | The date and time when the transaction was made. (Datetime)                                                                        |\n",
      "| **UnitPrice**   | The price per unit of the product in sterling. (Numeric)                                                                           |\n",
      "| **Country**     | The country where the customer resides. (Nominal)                                                                                  |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [UCI](https://data.world/uci).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/metamathqa-performance-with-mistral-7b\n",
      "name 'detect' is not defined Language is not detected: metamathqa-performance-with-mistral-7b\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: MetaMath QA\n",
      "name 'detect' is not defined Language is not detected: Mathematical Questions for Large Language Models\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# MetaMath QA\n",
      "### Mathematical Questions for Large Language Models\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/meta-math/MetaMathQA)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains meta-mathematics questions and answers collected from the Mistral-7B question-answering system. The responses, types, and queries are all provided in order to help boost the performance of MetaMathQA while maintaining high accuracy. With its well-structured design, this dataset provides users with an efficient way to investigate various aspects of question answering models and further understand how they function. Whether you are a professional or beginner, this dataset is sure to offer invaluable insights into the development of more powerful QA systems!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; ###### Data Dictionary  \n",
      "&gt; The MetaMathQA dataset contains three columns: response, type, and query. \n",
      "&gt; - **Response:** the response to the query given by the question answering system. (String) \n",
      "&gt; - **Type:** the type of query provided as input to the system. (String) \n",
      "&gt; - **Query:**the question posed to the system for which a response is required. (String) \n",
      "&gt; \n",
      "&gt;     \n",
      "&gt; ##### Preparing data for analysis  \n",
      "&gt; \n",
      "&gt;  It’s important that before you dive into analysis, you first familiarize yourself with what kind data values are present in each column and also check if any preprocessing needs to be done on them such as removing unwanted characters or filling in missing values etc., so that it can be used without any issue while training or testing your model further down in your process flow. \n",
      "&gt; \n",
      "&gt;  ##### Training Models using Mistral 7B   \n",
      "&gt; \n",
      "&gt;  Mistral 7B is an open source framework designed for building machine learning models quickly and easily from tabular (csv) datasets such as those found in this dataset 'MetaMathQA ' . After collecting and preprocessing your dataset accordingly Mistral 7B provides with support for various Machine Learning algorithms like Support Vector Machines (SVM), Logistic Regression , Decision trees etc , allowing one to select from various popular libraries these offered algorithms with powerful overall hyperparameter optimization techniques so soon after selecting algorithm configuration its good practice that one use GridSearchCV & RandomSearchCV methods further tune both optimizations during model building stages . Post selection process one can then go ahead validate performances of selected models through metrics like accuracy score , F1 Metric , Precision Score & Recall Scores .   \n",
      "&gt; \n",
      "&gt;  ##### Testing phosphors :  \n",
      "&gt; \n",
      "&gt;  After successful completion building phase right way would be robustly testing phosphors on different evaluation metrics mentioned above Model infusion stage helps here immediately make predictions based on earlier trained model OK auto back new test cases presented by domain experts could hey run quality assurance check again base score metrics mentioned above know asses confidence value post execution HHO updating baseline scores running experiments better preferred methodology AI workflows because Core advantage finally being have relevancy inexactness induced errors altogether impact low\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Generating natural language processing (NLP) models to better identify patterns and connections between questions, answers, and types. \n",
      "&gt; - Developing understandings on the efficiency of certain language features in producing successful question-answering results for different types of queries. \n",
      "&gt; - Optimizing search algorithms that surface relevant answer results based on types of queries\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/meta-math/MetaMathQA)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                         |\n",
      "|:--------------|:------------------------------------|\n",
      "| **response**  | The response to the query. (String) |\n",
      "| **type**      | The type of query. (String)         |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/meta-math/MetaMathQA).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: metamathqa-performance-with-mistral-7b\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: MetaMath QA\n",
      "name 'detect' is not defined Language is not detected: Mathematical Questions for Large Language Models\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# MetaMath QA\n",
      "### Mathematical Questions for Large Language Models\n",
      "By Huggingface Hub [[source]](https://huggingface.co/datasets/meta-math/MetaMathQA)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This dataset contains meta-mathematics questions and answers collected from the Mistral-7B question-answering system. The responses, types, and queries are all provided in order to help boost the performance of MetaMathQA while maintaining high accuracy. With its well-structured design, this dataset provides users with an efficient way to investigate various aspects of question answering models and further understand how they function. Whether you are a professional or beginner, this dataset is sure to offer invaluable insights into the development of more powerful QA systems!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; \n",
      "&gt; ###### Data Dictionary  \n",
      "&gt; The MetaMathQA dataset contains three columns: response, type, and query. \n",
      "&gt; - **Response:** the response to the query given by the question answering system. (String) \n",
      "&gt; - **Type:** the type of query provided as input to the system. (String) \n",
      "&gt; - **Query:**the question posed to the system for which a response is required. (String) \n",
      "&gt; \n",
      "&gt;     \n",
      "&gt; ##### Preparing data for analysis  \n",
      "&gt; \n",
      "&gt;  It’s important that before you dive into analysis, you first familiarize yourself with what kind data values are present in each column and also check if any preprocessing needs to be done on them such as removing unwanted characters or filling in missing values etc., so that it can be used without any issue while training or testing your model further down in your process flow. \n",
      "&gt; \n",
      "&gt;  ##### Training Models using Mistral 7B   \n",
      "&gt; \n",
      "&gt;  Mistral 7B is an open source framework designed for building machine learning models quickly and easily from tabular (csv) datasets such as those found in this dataset 'MetaMathQA ' . After collecting and preprocessing your dataset accordingly Mistral 7B provides with support for various Machine Learning algorithms like Support Vector Machines (SVM), Logistic Regression , Decision trees etc , allowing one to select from various popular libraries these offered algorithms with powerful overall hyperparameter optimization techniques so soon after selecting algorithm configuration its good practice that one use GridSearchCV & RandomSearchCV methods further tune both optimizations during model building stages . Post selection process one can then go ahead validate performances of selected models through metrics like accuracy score , F1 Metric , Precision Score & Recall Scores .   \n",
      "&gt; \n",
      "&gt;  ##### Testing phosphors :  \n",
      "&gt; \n",
      "&gt;  After successful completion building phase right way would be robustly testing phosphors on different evaluation metrics mentioned above Model infusion stage helps here immediately make predictions based on earlier trained model OK auto back new test cases presented by domain experts could hey run quality assurance check again base score metrics mentioned above know asses confidence value post execution HHO updating baseline scores running experiments better preferred methodology AI workflows because Core advantage finally being have relevancy inexactness induced errors altogether impact low\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Generating natural language processing (NLP) models to better identify patterns and connections between questions, answers, and types. \n",
      "&gt; - Developing understandings on the efficiency of certain language features in producing successful question-answering results for different types of queries. \n",
      "&gt; - Optimizing search algorithms that surface relevant answer results based on types of queries\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://huggingface.co/datasets/meta-math/MetaMathQA)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: train.csv**\n",
      "| Column name   | Description                         |\n",
      "|:--------------|:------------------------------------|\n",
      "| **response**  | The response to the query. (String) |\n",
      "| **type**      | The type of query. (String)         |\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/meta-math/MetaMathQA).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/paintings-collection\n",
      "name 'detect' is not defined Language is not detected: paintings-collection\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Paintings Collection\n",
      "name 'detect' is not defined Language is not detected: A diverse collection of paintings from artists throughout history\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Paintings Collection\n",
      "### A diverse collection of paintings from artists throughout history\n",
      "By Gove Allen [[source]](https://data.world/atlas-query)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Paintings Dataset is a rich and diverse collection of various paintings from different artists spanning across multiple time periods. It includes a wide range of art styles, techniques, and subjects, providing an extensive resource for art enthusiasts, historians, researchers, and anyone interested in exploring the world of visual arts.\n",
      "> \n",
      "> This dataset aims to capture the essence of artistic expression through its vast array of paintings. From classical masterpieces to contemporary works, it offers a comprehensive perspective on the evolution of artistic creativity throughout history.\n",
      "> \n",
      "> Each record in this dataset represents an individual painting with detailed information such as artist's name, artwork title (if applicable), genre/style classification (e.g., landscape, portrait), medium (e.g., oil on canvas), dimensions (height and width), and provenance details if available. Additionally, some records may include additional metadata like the year or era in which the artwork was created.\n",
      "> \n",
      "> By providing such comprehensive data about each painting included within this dataset, it enables users to study various aspects of art history. Researchers can analyze trends across different time periods or explore specific artistic movements by filtering the dataset based on genre or style categories. Art enthusiasts can also use this dataset to discover new artists or artworks that align with their interests.\n",
      "> \n",
      "> This valuable collection appeals not only to those seeking knowledge or inspiration from renowned artworks but also encourages exploration into lesser-known pieces that may have been overlooked in mainstream discourse. It fosters engagement with cultural heritage while promoting diversity and inclusivity within the realm of visual arts.\n",
      "> \n",
      "> Whether you are interested in studying classical works by universally acclaimed painters like Leonardo da Vinci or exploring modern expressions by emerging contemporary artists—this Paintings Dataset has something for everyone who appreciates aesthetics and enjoys unraveling stories through brushstrokes on canvas\n",
      "\n",
      "### How to use the dataset\n",
      "> # How to Use the Paintings Dataset\n",
      "> \n",
      "> Welcome to the Paintings Dataset! This dataset is a comprehensive collection of various paintings from different artists and time periods. It contains information about the artist, title, genre, style, and medium of each painting. Whether you are an art enthusiast, researcher, or just curious about paintings, this guide will help you navigate through this dataset easily.\n",
      "> \n",
      "> ## 1. Understanding the Columns\n",
      "> \n",
      "> This dataset consists of several columns that provide detailed information about each painting. Here is a brief description of each column:\n",
      "> \n",
      "> - **Artist**: The name of the artist who created the painting.\n",
      "> - **Title**: The title or name given to the artwork by the artist.\n",
      "> - **Genre**: The artistic category or subject matter depicted in the painting.\n",
      "> - **Style**: The specific artistic style or movement associated with the painting.\n",
      "> - **Medium**: The materials and techniques used by the artist to create the artwork.\n",
      "> \n",
      "> ## 2. Exploring Artists and Their Paintings\n",
      "> \n",
      "> One interesting way to use this dataset is to explore individual artists and their artworks. You can filter by a specific artist's name in order to retrieve all their paintings included in this collection.\n",
      "> \n",
      "> For example: If you are interested in exploring all paintings by Leonardo da Vinci, simply filter using Leonardo da Vinci in Artist column using your preferred data analysis tool.\n",
      "> \n",
      "> ## 3. Analyzing Painting Genres\n",
      "> \n",
      "> The genre column allows you to analyze different categories within this collection of paintings. You can examine popular genres or compare them across different eras.\n",
      "> \n",
      "> To analyze genres:\n",
      "> - Get unique values for Genre column.\n",
      "> - Count frequency for each genre value.\n",
      "> - Visualize results using bar charts or other graphical representations.\n",
      "> \n",
      "> You might discover which genres were more predominant during certain periods or which artists were known for specific subjects!\n",
      "> \n",
      "> ## 4. Investigating Artistic Styles\n",
      "> \n",
      "> Similar to genres, artistic styles also play an essential role in the world of painting. This dataset includes various styles like Impressionism, Cubism, Realism, etc. By analyzing the artistic styles column, you can explore trends and shifts in artistic movements.\n",
      "> \n",
      "> To investigate styles:\n",
      "> - Get unique values for Style column.\n",
      "> - Count frequency for each style value.\n",
      "> - Visualize results using bar charts or other graphical representations.\n",
      "> \n",
      "> This will enable you to understand how different artists preferred different styles during various time periods.\n",
      "> \n",
      "> ## 5. Analyzing Painting Mediums\n",
      "> \n",
      "> The medium column provides insights into the materials and techniques employed by artists to create their paintings. You can uncover which mediums were\n",
      "\n",
      "### Research Ideas\n",
      "> - Artistic analysis: This dataset can be used to analyze different artistic styles, techniques, and themes across various time periods and artists. It can provide insights into the evolution of art over time and the influence of different artists on each other.\n",
      "> - Machine learning training: This dataset can be used to train machine learning models in order to classify paintings based on their style, artist, or time period. This can help in automating tasks such as identifying a painting's origins or classifying a painting into its respective art movement.\n",
      "> - Recommendation systems: By analyzing the characteristics of paintings in this dataset, it is possible to develop recommendation systems for art enthusiasts based on their preferences. This could involve suggesting similar paintings based on style, theme, or artist, providing personalized recommendations for those interested in exploring new works of art\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/atlas-query)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Gove Allen](https://data.world/atlas-query).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: paintings-collection\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Paintings Collection\n",
      "name 'detect' is not defined Language is not detected: A diverse collection of paintings from artists throughout history\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Paintings Collection\n",
      "### A diverse collection of paintings from artists throughout history\n",
      "By Gove Allen [[source]](https://data.world/atlas-query)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Paintings Dataset is a rich and diverse collection of various paintings from different artists spanning across multiple time periods. It includes a wide range of art styles, techniques, and subjects, providing an extensive resource for art enthusiasts, historians, researchers, and anyone interested in exploring the world of visual arts.\n",
      "> \n",
      "> This dataset aims to capture the essence of artistic expression through its vast array of paintings. From classical masterpieces to contemporary works, it offers a comprehensive perspective on the evolution of artistic creativity throughout history.\n",
      "> \n",
      "> Each record in this dataset represents an individual painting with detailed information such as artist's name, artwork title (if applicable), genre/style classification (e.g., landscape, portrait), medium (e.g., oil on canvas), dimensions (height and width), and provenance details if available. Additionally, some records may include additional metadata like the year or era in which the artwork was created.\n",
      "> \n",
      "> By providing such comprehensive data about each painting included within this dataset, it enables users to study various aspects of art history. Researchers can analyze trends across different time periods or explore specific artistic movements by filtering the dataset based on genre or style categories. Art enthusiasts can also use this dataset to discover new artists or artworks that align with their interests.\n",
      "> \n",
      "> This valuable collection appeals not only to those seeking knowledge or inspiration from renowned artworks but also encourages exploration into lesser-known pieces that may have been overlooked in mainstream discourse. It fosters engagement with cultural heritage while promoting diversity and inclusivity within the realm of visual arts.\n",
      "> \n",
      "> Whether you are interested in studying classical works by universally acclaimed painters like Leonardo da Vinci or exploring modern expressions by emerging contemporary artists—this Paintings Dataset has something for everyone who appreciates aesthetics and enjoys unraveling stories through brushstrokes on canvas\n",
      "\n",
      "### How to use the dataset\n",
      "> # How to Use the Paintings Dataset\n",
      "> \n",
      "> Welcome to the Paintings Dataset! This dataset is a comprehensive collection of various paintings from different artists and time periods. It contains information about the artist, title, genre, style, and medium of each painting. Whether you are an art enthusiast, researcher, or just curious about paintings, this guide will help you navigate through this dataset easily.\n",
      "> \n",
      "> ## 1. Understanding the Columns\n",
      "> \n",
      "> This dataset consists of several columns that provide detailed information about each painting. Here is a brief description of each column:\n",
      "> \n",
      "> - **Artist**: The name of the artist who created the painting.\n",
      "> - **Title**: The title or name given to the artwork by the artist.\n",
      "> - **Genre**: The artistic category or subject matter depicted in the painting.\n",
      "> - **Style**: The specific artistic style or movement associated with the painting.\n",
      "> - **Medium**: The materials and techniques used by the artist to create the artwork.\n",
      "> \n",
      "> ## 2. Exploring Artists and Their Paintings\n",
      "> \n",
      "> One interesting way to use this dataset is to explore individual artists and their artworks. You can filter by a specific artist's name in order to retrieve all their paintings included in this collection.\n",
      "> \n",
      "> For example: If you are interested in exploring all paintings by Leonardo da Vinci, simply filter using Leonardo da Vinci in Artist column using your preferred data analysis tool.\n",
      "> \n",
      "> ## 3. Analyzing Painting Genres\n",
      "> \n",
      "> The genre column allows you to analyze different categories within this collection of paintings. You can examine popular genres or compare them across different eras.\n",
      "> \n",
      "> To analyze genres:\n",
      "> - Get unique values for Genre column.\n",
      "> - Count frequency for each genre value.\n",
      "> - Visualize results using bar charts or other graphical representations.\n",
      "> \n",
      "> You might discover which genres were more predominant during certain periods or which artists were known for specific subjects!\n",
      "> \n",
      "> ## 4. Investigating Artistic Styles\n",
      "> \n",
      "> Similar to genres, artistic styles also play an essential role in the world of painting. This dataset includes various styles like Impressionism, Cubism, Realism, etc. By analyzing the artistic styles column, you can explore trends and shifts in artistic movements.\n",
      "> \n",
      "> To investigate styles:\n",
      "> - Get unique values for Style column.\n",
      "> - Count frequency for each style value.\n",
      "> - Visualize results using bar charts or other graphical representations.\n",
      "> \n",
      "> This will enable you to understand how different artists preferred different styles during various time periods.\n",
      "> \n",
      "> ## 5. Analyzing Painting Mediums\n",
      "> \n",
      "> The medium column provides insights into the materials and techniques employed by artists to create their paintings. You can uncover which mediums were\n",
      "\n",
      "### Research Ideas\n",
      "> - Artistic analysis: This dataset can be used to analyze different artistic styles, techniques, and themes across various time periods and artists. It can provide insights into the evolution of art over time and the influence of different artists on each other.\n",
      "> - Machine learning training: This dataset can be used to train machine learning models in order to classify paintings based on their style, artist, or time period. This can help in automating tasks such as identifying a painting's origins or classifying a painting into its respective art movement.\n",
      "> - Recommendation systems: By analyzing the characteristics of paintings in this dataset, it is possible to develop recommendation systems for art enthusiasts based on their preferences. This could involve suggesting similar paintings based on style, theme, or artist, providing personalized recommendations for those interested in exploring new works of art\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/atlas-query)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Gove Allen](https://data.world/atlas-query).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/hydra-movies-dataset-directors-writers-cast-and\n",
      "name 'detect' is not defined Language is not detected: hydra-movies-dataset-directors-writers-cast-and\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Hydra Movies Dataset: Directors, Writers, Cast..\n",
      "name 'detect' is not defined Language is not detected: A Rich Source for Movie Research\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Hydra Movies Dataset: Directors, Writers, Cast, and Metadata\n",
      "### A Rich Source for Movie Research\n",
      "By kai tanyu [[source]](https://data.world/iliketurtles)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset comprises of a comprehensive collection of movie metadata scraped from the Hydra Movies database - weighing in at an impressive 3,940 movies total. Webmasters and movie enthusiasts alike will be able to appreciate the wealth of data available, with unprecedented access to directors, writers, cast and short summaries which previously were not possible to view via Hydra's publicly available datasets. Each movie offers up valuable insights into title, year of release, full summary/short summary, genres, unique IMDB identifier along with runtime plus ratings according to age or content restrictions and lastly a URL path which provides direct access to an official poster for the respective film. Unleash your inner cinephile by utilizing this dataset for advanced analysis beyond what streaming platforms provide - along with gaining valuable insights into upcoming films!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset is a perfect resource for webmasters, movie enthusiasts, and other researchers interested in movie metadata. With over 3,940 movies in the collection, this dataset is an excellent source for discovering insights about the film industry as well as receiving additional details on movies that are not easily accessible through APIs or public datasets. \n",
      "> \n",
      "> - Understand what each column means and familiarize yourself with their data types. Every row in this dataset includes information such as title of the movie (string), year released (integer), genres (string), runtime (integer), youtube trailer(string), rating (string) etc. Experiment by filtering out each column to understand how it changes your results. \n",
      "> - Try creating a visualization with the data - this could be easily done through Kaggle's python notebook tool with seaborn / matplotlib libraries or simply downloading it in CSV format to work offline within Tableau to explore further relationships between datasets such as finding which directors had most success within certain genres over years etc... . \n",
      "> - Connect it up to other publicly available API's or datasets - such as IMDB or Rotten Tomatoes API's where you can enrich your results with more comprehensive details about certain movies that may have been missing from our scraped dataset originally eg Box office Grossing.. \n",
      "> \n",
      "> With these tips you should be able to start exploring quickly and uncover deeper insights into how the Movie Industry has shifted over time!\n",
      "\n",
      "### Research Ideas\n",
      "> - Utilizing movie ratings to suggest similar movies in a personalized recommendation system. \n",
      "> - Using summary data for text mining analysis to acquire insights about common themes, topics and genres of different wide-release films over the years. \n",
      "> - Create interactive visualizations to represent relationships between casts, directors, and writers on whether their works were generally positively or negatively reviewed by critics or audiences alike\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/iliketurtles)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Hydra-Movie-Scrape.csv**\n",
      "| Column name         | Description                                    |\n",
      "|:--------------------|:-----------------------------------------------|\n",
      "| **Title**           | The title of the movie. (String)               |\n",
      "| **Year**            | The year the movie was released. (Integer)     |\n",
      "| **Summary**         | A full summary of the movie. (String)          |\n",
      "| **Short Summary**   | A brief summary of the movie. (String)         |\n",
      "| **Genres**          | The genres the movie falls into. (String)      |\n",
      "| **Runtime**         | The length of the movie in minutes. (Integer)  |\n",
      "| **YouTube Trailer** | The YouTube trailer ID for the movie. (String) |\n",
      "| **Rating**          | The rating of the movie. (String)              |\n",
      "| **Movie Poster**    | The URL path for the movie poster. (String)    |\n",
      "| **Director**        | The director of the movie. (String)            |\n",
      "| **Writers**         | The writers of the movie. (String)             |\n",
      "| **Cast**            | The cast of the movie. (String)                |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [kai tanyu](https://data.world/iliketurtles).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hydra-movies-dataset-directors-writers-cast-and\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Hydra Movies Dataset: Directors, Writers, Cast..\n",
      "name 'detect' is not defined Language is not detected: A Rich Source for Movie Research\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Hydra Movies Dataset: Directors, Writers, Cast, and Metadata\n",
      "### A Rich Source for Movie Research\n",
      "By kai tanyu [[source]](https://data.world/iliketurtles)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset comprises of a comprehensive collection of movie metadata scraped from the Hydra Movies database - weighing in at an impressive 3,940 movies total. Webmasters and movie enthusiasts alike will be able to appreciate the wealth of data available, with unprecedented access to directors, writers, cast and short summaries which previously were not possible to view via Hydra's publicly available datasets. Each movie offers up valuable insights into title, year of release, full summary/short summary, genres, unique IMDB identifier along with runtime plus ratings according to age or content restrictions and lastly a URL path which provides direct access to an official poster for the respective film. Unleash your inner cinephile by utilizing this dataset for advanced analysis beyond what streaming platforms provide - along with gaining valuable insights into upcoming films!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset is a perfect resource for webmasters, movie enthusiasts, and other researchers interested in movie metadata. With over 3,940 movies in the collection, this dataset is an excellent source for discovering insights about the film industry as well as receiving additional details on movies that are not easily accessible through APIs or public datasets. \n",
      "> \n",
      "> - Understand what each column means and familiarize yourself with their data types. Every row in this dataset includes information such as title of the movie (string), year released (integer), genres (string), runtime (integer), youtube trailer(string), rating (string) etc. Experiment by filtering out each column to understand how it changes your results. \n",
      "> - Try creating a visualization with the data - this could be easily done through Kaggle's python notebook tool with seaborn / matplotlib libraries or simply downloading it in CSV format to work offline within Tableau to explore further relationships between datasets such as finding which directors had most success within certain genres over years etc... . \n",
      "> - Connect it up to other publicly available API's or datasets - such as IMDB or Rotten Tomatoes API's where you can enrich your results with more comprehensive details about certain movies that may have been missing from our scraped dataset originally eg Box office Grossing.. \n",
      "> \n",
      "> With these tips you should be able to start exploring quickly and uncover deeper insights into how the Movie Industry has shifted over time!\n",
      "\n",
      "### Research Ideas\n",
      "> - Utilizing movie ratings to suggest similar movies in a personalized recommendation system. \n",
      "> - Using summary data for text mining analysis to acquire insights about common themes, topics and genres of different wide-release films over the years. \n",
      "> - Create interactive visualizations to represent relationships between casts, directors, and writers on whether their works were generally positively or negatively reviewed by critics or audiences alike\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/iliketurtles)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Hydra-Movie-Scrape.csv**\n",
      "| Column name         | Description                                    |\n",
      "|:--------------------|:-----------------------------------------------|\n",
      "| **Title**           | The title of the movie. (String)               |\n",
      "| **Year**            | The year the movie was released. (Integer)     |\n",
      "| **Summary**         | A full summary of the movie. (String)          |\n",
      "| **Short Summary**   | A brief summary of the movie. (String)         |\n",
      "| **Genres**          | The genres the movie falls into. (String)      |\n",
      "| **Runtime**         | The length of the movie in minutes. (Integer)  |\n",
      "| **YouTube Trailer** | The YouTube trailer ID for the movie. (String) |\n",
      "| **Rating**          | The rating of the movie. (String)              |\n",
      "| **Movie Poster**    | The URL path for the movie poster. (String)    |\n",
      "| **Director**        | The director of the movie. (String)            |\n",
      "| **Writers**         | The writers of the movie. (String)             |\n",
      "| **Cast**            | The cast of the movie. (String)                |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [kai tanyu](https://data.world/iliketurtles).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/movies-from-allmovie-com-a-comprehensive-430000\n",
      "name 'detect' is not defined Language is not detected: movies-from-allmovie-com-a-comprehensive-430000\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movies from Allmovie.com\n",
      "name 'detect' is not defined Language is not detected: Analyzing Movie Director, Cast, Genre, Language, Rating, and Synopsis\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movies from Allmovie.com: A Comprehensive 430000+ Records Dataset\n",
      "### Analyzing Movie Director, Cast, Genre, Language, Rating, and Synopsis\n",
      "By Crawl Feeds [[source]](https://data.world/opensnippets)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains 430000+ movies listed in allmovie.com, offering a comprehensive look at the world of film and filmmakers. With data fields such as title, genre, released_at, poster, language director domain duration synopsis trailer average_rating cast and crawled_at url and id you can examine everything from trends among specific genres to international successes or gather specific information about any particular movie or filmmaker included in this mighty collection of cinematic knowledge. Which culture is currently predicted to produce the most successful blockbusters? How does the release date correlate with awards won? What type of scripts are being produced for higher-revenue-generating entities? All these questions can be answered with this comprehensive source. See what trends stand out from this incredibly diverse collection!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset contains more than 430000+ records of movies from allmovie.com. Each record includes valuable information about the movie such as its name, genre, released_at, posterimage, language, director, domain (ex. Netflix), duration, synopsis (brief description), trailer (video), average_rating, cast list and crawled_at date and time. \n",
      "> \n",
      "> This dataset is perfect for anyone who wants to explore movie data or do a deep dive into individual films in the database. Here are some of the potential uses you can get out of this dataset: \n",
      "> - Create custom visualizations on movie trends over time \n",
      "> - Analyze what genres are most popular by language or country \n",
      "> - Recommend similar movies based on shared attributes like cast/crew/genre etc \n",
      "> - Learn more about a specific film by viewing its trailer video or read its synopsis  \n",
      "> \n",
      ">  To properly use this dataset to compare different films and analyze trends over time: \n",
      "> \n",
      ">  1) Group elements according to their categories / genres and sort them accordingly. When thinking about analyzing trends it helps to group each element into one uniform category like Drama for example - you can then analyze which genre is most trending in your desired area. It also helps when sorting through data for specific film titles by grouping them alphabetically which will help organize records quickly should there be many entries under one title .  \n",
      "> \n",
      ">  2) Analyze detailed descriptions separately with other elements associated with elements being categorized as ‘synopsis’ such as trailers and length of films etc.. A lot of useful information can be gained from exploring detailed descriptions associated with data points such as exploring themes within certain movies posters measuring audience engagement through views in trailers etc..  \n",
      "> \n",
      ">  3) Utilize movie rating columns in making predictions on which genres might experience an upswing or drop off on popularity amongst audiences following analysis spanning across different countries / language demographics; You may also want to extract these columns onto separate sheets if they mostly remain static across different categories because these ratings often prove instrumental in viewers decision making process when selecting what they would watch next .   \n",
      "> \n",
      ">  4 } Union queries between tables like comparing how actors may have worked across multiple projects , exploring how directors styles might evolve over time; One example could be joining Foreign Language Film category with Average Rating column to find highest rated international releases per year which would serve as great starting point when choosing where watch foreign cinema outside streaming services catalogues \n",
      "\n",
      "### Research Ideas\n",
      "> - Creating a movie recommendation engine - The dataset can be used to analyze user preferences from their past watch history and generate movie recommendations based on genre, director, average rating, language, etc. \n",
      "> - Predicting box office earnings of movies - By analyzing the data points present in the dataset such as release date, duration of the movie, cast members and budget of the movie among other factors a machine learning model could be trained to predict box office collection for upcoming movies. \n",
      "> - Conducting Sentiment Analysis on Movie Reviews - Using Natural Language Processing techniques along with sentiment analysis model on reviews present in this dataset will enable us to extract overall sentiment surrounding a particular movie and provide insights into where the strong positive/negative sentiment is originating from among viewers\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/opensnippets)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: movies_dataset_from_allmovie.csv**\n",
      "| Column name        | Description                                |\n",
      "|:-------------------|:-------------------------------------------|\n",
      "| **name**           | The name of the movie. (String)            |\n",
      "| **genre**          | The genre of the movie. (String)           |\n",
      "| **released_at**    | The date the movie was released. (Date)    |\n",
      "| **poster**         | The URL of the movie poster. (String)      |\n",
      "| **language**       | The language of the movie. (String)        |\n",
      "| **director**       | The director of the movie. (String)        |\n",
      "| **domain**         | The domain of the movie. (String)          |\n",
      "| **duration**       | The length of the movie. (Integer)         |\n",
      "| **synopsis**       | A brief description of the movie. (String) |\n",
      "| **trailer**        | The URL of the movie trailer. (String)     |\n",
      "| **average_rating** | The average rating of the movie. (Float)   |\n",
      "| **cast**           | The cast of the movie. (String)            |\n",
      "| **crawled_at**     | The date the movie was crawled. (Date)     |\n",
      "| **url**            | The URL of the movie. (String)             |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Crawl Feeds](https://data.world/opensnippets).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: movies-from-allmovie-com-a-comprehensive-430000\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Movies from Allmovie.com\n",
      "name 'detect' is not defined Language is not detected: Analyzing Movie Director, Cast, Genre, Language, Rating, and Synopsis\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Movies from Allmovie.com: A Comprehensive 430000+ Records Dataset\n",
      "### Analyzing Movie Director, Cast, Genre, Language, Rating, and Synopsis\n",
      "By Crawl Feeds [[source]](https://data.world/opensnippets)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains 430000+ movies listed in allmovie.com, offering a comprehensive look at the world of film and filmmakers. With data fields such as title, genre, released_at, poster, language director domain duration synopsis trailer average_rating cast and crawled_at url and id you can examine everything from trends among specific genres to international successes or gather specific information about any particular movie or filmmaker included in this mighty collection of cinematic knowledge. Which culture is currently predicted to produce the most successful blockbusters? How does the release date correlate with awards won? What type of scripts are being produced for higher-revenue-generating entities? All these questions can be answered with this comprehensive source. See what trends stand out from this incredibly diverse collection!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> This dataset contains more than 430000+ records of movies from allmovie.com. Each record includes valuable information about the movie such as its name, genre, released_at, posterimage, language, director, domain (ex. Netflix), duration, synopsis (brief description), trailer (video), average_rating, cast list and crawled_at date and time. \n",
      "> \n",
      "> This dataset is perfect for anyone who wants to explore movie data or do a deep dive into individual films in the database. Here are some of the potential uses you can get out of this dataset: \n",
      "> - Create custom visualizations on movie trends over time \n",
      "> - Analyze what genres are most popular by language or country \n",
      "> - Recommend similar movies based on shared attributes like cast/crew/genre etc \n",
      "> - Learn more about a specific film by viewing its trailer video or read its synopsis  \n",
      "> \n",
      ">  To properly use this dataset to compare different films and analyze trends over time: \n",
      "> \n",
      ">  1) Group elements according to their categories / genres and sort them accordingly. When thinking about analyzing trends it helps to group each element into one uniform category like Drama for example - you can then analyze which genre is most trending in your desired area. It also helps when sorting through data for specific film titles by grouping them alphabetically which will help organize records quickly should there be many entries under one title .  \n",
      "> \n",
      ">  2) Analyze detailed descriptions separately with other elements associated with elements being categorized as ‘synopsis’ such as trailers and length of films etc.. A lot of useful information can be gained from exploring detailed descriptions associated with data points such as exploring themes within certain movies posters measuring audience engagement through views in trailers etc..  \n",
      "> \n",
      ">  3) Utilize movie rating columns in making predictions on which genres might experience an upswing or drop off on popularity amongst audiences following analysis spanning across different countries / language demographics; You may also want to extract these columns onto separate sheets if they mostly remain static across different categories because these ratings often prove instrumental in viewers decision making process when selecting what they would watch next .   \n",
      "> \n",
      ">  4 } Union queries between tables like comparing how actors may have worked across multiple projects , exploring how directors styles might evolve over time; One example could be joining Foreign Language Film category with Average Rating column to find highest rated international releases per year which would serve as great starting point when choosing where watch foreign cinema outside streaming services catalogues \n",
      "\n",
      "### Research Ideas\n",
      "> - Creating a movie recommendation engine - The dataset can be used to analyze user preferences from their past watch history and generate movie recommendations based on genre, director, average rating, language, etc. \n",
      "> - Predicting box office earnings of movies - By analyzing the data points present in the dataset such as release date, duration of the movie, cast members and budget of the movie among other factors a machine learning model could be trained to predict box office collection for upcoming movies. \n",
      "> - Conducting Sentiment Analysis on Movie Reviews - Using Natural Language Processing techniques along with sentiment analysis model on reviews present in this dataset will enable us to extract overall sentiment surrounding a particular movie and provide insights into where the strong positive/negative sentiment is originating from among viewers\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/opensnippets)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: movies_dataset_from_allmovie.csv**\n",
      "| Column name        | Description                                |\n",
      "|:-------------------|:-------------------------------------------|\n",
      "| **name**           | The name of the movie. (String)            |\n",
      "| **genre**          | The genre of the movie. (String)           |\n",
      "| **released_at**    | The date the movie was released. (Date)    |\n",
      "| **poster**         | The URL of the movie poster. (String)      |\n",
      "| **language**       | The language of the movie. (String)        |\n",
      "| **director**       | The director of the movie. (String)        |\n",
      "| **domain**         | The domain of the movie. (String)          |\n",
      "| **duration**       | The length of the movie. (Integer)         |\n",
      "| **synopsis**       | A brief description of the movie. (String) |\n",
      "| **trailer**        | The URL of the movie trailer. (String)     |\n",
      "| **average_rating** | The average rating of the movie. (Float)   |\n",
      "| **cast**           | The cast of the movie. (String)            |\n",
      "| **crawled_at**     | The date the movie was crawled. (Date)     |\n",
      "| **url**            | The URL of the movie. (String)             |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Crawl Feeds](https://data.world/opensnippets).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/imdb-movie-metadata-multivariate-analysis\n",
      "name 'detect' is not defined Language is not detected: imdb-movie-metadata-multivariate-analysis\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: IMDB Movie Metadata Multivariate Analysis\n",
      "name 'detect' is not defined Language is not detected: Exploring the Impact of Types, Ratings, and Genres on Audience Engagement\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# IMDB Movie Metadata Multivariate Analysis\n",
      "### Exploring the Impact of Types, Ratings, and Genres on Audience Engagement\n",
      "By Addi Ait-Mlouk [[source]](https://data.world/aitmlouk)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This amazing IMDB movie metadata dataset has everything you need to uncover the secrets of cinema! Featuring a comprehensive set of variables that range from budget, revenue, and cast size to title types and genres - it holds the keys for unlocking lucrative insights about your favorite films. This dataset contains an array of information on over 10,000 film titles from 6 different countries (USA, UK, Germany, Canada, India and Japan). Its detailed columns include Movie ID (unique identifier for each film), Title Type (TV Series/Movie/Video), Production Budget & Revenue figures in USD along with a breakdown into Country-specific Currency Units such as EUR or GBP. Additionally, each entry features the Primary Genre Category it was classified under along with secondary Genres if applicable. Finally this collection includes text fields giving insight into plot keywords as well as Cast & Crew credits including names of Actors/Actresses in main roles plus other important personnel working on set such as Directors or Writers. Together all these features create an invaluable resource suitable for detailed analysis aiming to understand movie trends over time – How budgets compare across nations? What genres are doing better internationally all these fascinating questions addressed by this incredible dataset!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contain 2000 films from IMDb, containing basic metadata on the films such as genre, budget and gross box office performance. It has a variety of columns which provide valuable insights when attempting to do multivariate analysis on this data. The data is divided into metacritic score (which is a measure of critics ratings), budget (in dollars), revenue (in dollars) and eight other stats related to movies \n",
      "&gt;  \n",
      "&gt; - Metacritic Score: A metric based on critical reviews ranging from 0 - 100 indicating how well the movie was reviewed by critics.\n",
      "&gt; - Budget: Total budget in US dollars for each movie. This can be used to better understand why certain films are successful or not so successful when analyzing against other metrics in the dataset \n",
      "&gt; - Revenue: Total revenue earned in US dollars for each movie at Box Office and outside digital downloads/streams & TV airplay etc… we could use this to calculate revenue per budget for movies as well as success rates of certain genres over others given higher budgets  \n",
      "&gt; - Release Date: The release date given in string format mainly used for understanding seasonality effects between various releases in different times throughout the year  e.g summer vs winter releases  \n",
      "&gt; 5 .Runtime: Length of time that a film plays for expressed in minutes  could be useful measuring watchability/ engagement potential or correlations with average ticket prices at theatre pricing timeframes(matinees vs evening shows)  \n",
      "&gt; \n",
      "&gt; 6 Genres : Genres associated with a particular film e.g comedy ,drama , horror etc … Value information taken from freedb collection created by IMDB,these categories can also help further narrow down what people like and disliked about certain films or companies producing them     \n",
      "&gt; \n",
      "&gt; 7 Directors : Each directors name usually one director per film which might give us some insight into directing decisions made which have an impact on box-office performance    \n",
      "&gt; \n",
      "&gt;  8 Writers : Names of writers associated with a particular project same reasoning applies here that making different decisions around who write content reflects audience response   \n",
      "&gt; \n",
      "&gt;  9 Actors/Actresses : Names of some key actors associate with major roles like leads/support\t takes who worked on stories will give more insight into why viewers preferred them OVER OTHER PROJECTS   THEY MAY HAVE BEEN ASSOCIATED WITH along their career paths   \n",
      "&gt; \n",
      "&gt;  10 Countries apart from USA origin countries are identified Here since many american productions try new approaches keeping an eye out may point us towards effective techniques being implemented abroad that have potential application to American markets\t          ADR\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Creating movie recommendation systems based on user preferences: This dataset can be used to uncover patterns in user movie watching habits and preferences by factoring in multiple variables such as genre, release year, director, cast, popularity/ratings etc., and recommending similar movies accordingly. \n",
      "&gt; - Cost vs Profit predictive models: This dataset can be utilized to construct a predictive model that helps film studios identify which genres of movies are more profitable based on factors like budget size, popularity ratings and award nominations. The model would help studio executives better allocate their resources for more profitable films in a cost effective way. \n",
      "&gt; - Improving film production techniques via Analytics : Patterns derived from this dataset can also be used to help filmmakers understand what works best for audiences and how to optimally structure their productions so that it maximizes profits for the studio (e.g Learning what elements should be included in the first 10 minutes of a film when capturing viewers’ attention)\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/aitmlouk)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Addi Ait-Mlouk](https://data.world/aitmlouk).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: imdb-movie-metadata-multivariate-analysis\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: IMDB Movie Metadata Multivariate Analysis\n",
      "name 'detect' is not defined Language is not detected: Exploring the Impact of Types, Ratings, and Genres on Audience Engagement\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# IMDB Movie Metadata Multivariate Analysis\n",
      "### Exploring the Impact of Types, Ratings, and Genres on Audience Engagement\n",
      "By Addi Ait-Mlouk [[source]](https://data.world/aitmlouk)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "&gt; This amazing IMDB movie metadata dataset has everything you need to uncover the secrets of cinema! Featuring a comprehensive set of variables that range from budget, revenue, and cast size to title types and genres - it holds the keys for unlocking lucrative insights about your favorite films. This dataset contains an array of information on over 10,000 film titles from 6 different countries (USA, UK, Germany, Canada, India and Japan). Its detailed columns include Movie ID (unique identifier for each film), Title Type (TV Series/Movie/Video), Production Budget & Revenue figures in USD along with a breakdown into Country-specific Currency Units such as EUR or GBP. Additionally, each entry features the Primary Genre Category it was classified under along with secondary Genres if applicable. Finally this collection includes text fields giving insight into plot keywords as well as Cast & Crew credits including names of Actors/Actresses in main roles plus other important personnel working on set such as Directors or Writers. Together all these features create an invaluable resource suitable for detailed analysis aiming to understand movie trends over time – How budgets compare across nations? What genres are doing better internationally all these fascinating questions addressed by this incredible dataset!\n",
      "\n",
      "### More Datasets\n",
      "&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "&gt; - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "&gt; This dataset contain 2000 films from IMDb, containing basic metadata on the films such as genre, budget and gross box office performance. It has a variety of columns which provide valuable insights when attempting to do multivariate analysis on this data. The data is divided into metacritic score (which is a measure of critics ratings), budget (in dollars), revenue (in dollars) and eight other stats related to movies \n",
      "&gt;  \n",
      "&gt; - Metacritic Score: A metric based on critical reviews ranging from 0 - 100 indicating how well the movie was reviewed by critics.\n",
      "&gt; - Budget: Total budget in US dollars for each movie. This can be used to better understand why certain films are successful or not so successful when analyzing against other metrics in the dataset \n",
      "&gt; - Revenue: Total revenue earned in US dollars for each movie at Box Office and outside digital downloads/streams & TV airplay etc… we could use this to calculate revenue per budget for movies as well as success rates of certain genres over others given higher budgets  \n",
      "&gt; - Release Date: The release date given in string format mainly used for understanding seasonality effects between various releases in different times throughout the year  e.g summer vs winter releases  \n",
      "&gt; 5 .Runtime: Length of time that a film plays for expressed in minutes  could be useful measuring watchability/ engagement potential or correlations with average ticket prices at theatre pricing timeframes(matinees vs evening shows)  \n",
      "&gt; \n",
      "&gt; 6 Genres : Genres associated with a particular film e.g comedy ,drama , horror etc … Value information taken from freedb collection created by IMDB,these categories can also help further narrow down what people like and disliked about certain films or companies producing them     \n",
      "&gt; \n",
      "&gt; 7 Directors : Each directors name usually one director per film which might give us some insight into directing decisions made which have an impact on box-office performance    \n",
      "&gt; \n",
      "&gt;  8 Writers : Names of writers associated with a particular project same reasoning applies here that making different decisions around who write content reflects audience response   \n",
      "&gt; \n",
      "&gt;  9 Actors/Actresses : Names of some key actors associate with major roles like leads/support\t takes who worked on stories will give more insight into why viewers preferred them OVER OTHER PROJECTS   THEY MAY HAVE BEEN ASSOCIATED WITH along their career paths   \n",
      "&gt; \n",
      "&gt;  10 Countries apart from USA origin countries are identified Here since many american productions try new approaches keeping an eye out may point us towards effective techniques being implemented abroad that have potential application to American markets\t          ADR\n",
      "\n",
      "### Research Ideas\n",
      "&gt; - Creating movie recommendation systems based on user preferences: This dataset can be used to uncover patterns in user movie watching habits and preferences by factoring in multiple variables such as genre, release year, director, cast, popularity/ratings etc., and recommending similar movies accordingly. \n",
      "&gt; - Cost vs Profit predictive models: This dataset can be utilized to construct a predictive model that helps film studios identify which genres of movies are more profitable based on factors like budget size, popularity ratings and award nominations. The model would help studio executives better allocate their resources for more profitable films in a cost effective way. \n",
      "&gt; - Improving film production techniques via Analytics : Patterns derived from this dataset can also be used to help filmmakers understand what works best for audiences and how to optimally structure their productions so that it maximizes profits for the studio (e.g Learning what elements should be included in the first 10 minutes of a film when capturing viewers’ attention)\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; [Data Source](https://data.world/aitmlouk)\n",
      "&gt; \n",
      "&gt;\n",
      "\n",
      "\n",
      "### License\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "### Acknowledgements\n",
      "&gt; If you use this dataset in your research, please credit the original authors.\n",
      "&gt; If you use this dataset in your research, please credit [Addi Ait-Mlouk](https://data.world/aitmlouk).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/booking-com-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: booking-com-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Booking.com Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Booking.com Hotel Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Booking.com Hotel Reviews\n",
      "### Booking.com Hotel Reviews Dataset\n",
      "By Crawl Feeds [[source]](https://data.world/opensnippets)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Booking.com Reviews Dataset is a comprehensive collection of hotel reviews and ratings gathered from the popular travel booking website, Booking.com. This dataset provides valuable insights into the experiences and opinions of customers who have stayed at various hotels across different locations. \n",
      "> \n",
      "> With over 700K records available, this dataset offers immense potential for analysis and research in the field of hospitality and tourism. Each review includes key information such as the title given by the reviewer, raw text content without any processing, reviewer's name or username, tags or labels associated with the review, average rating of the hotel, number of images attached to the review, URL of the review page as well as additional metadata.\n",
      "> \n",
      "> The dataset captures details about both domestic and international travelers' nationalities, providing a diverse perspective on people's experiences with different hotels. Furthermore,rating distribution can be observed through average ratings provided by reviewers.\n",
      "> \n",
      "> In addition to retrieving insightful customer feedback on specific hotels, this dataset also allows for understanding trends in customer preferences, satisfaction levels,and sentiments towards various amenities or services provided by hotels. Researchers can explore correlations between variables like average rating and nationality to gain valuable insights into cultural differences in customer expectations.\n",
      "> \n",
      "> This data has been crawled from Booking.com's website along with relevant time stamps indicating when each review was given as well as when it was captured from their site. The availability of both reviewed_at (date & time) and crawled_at (date & time) stamp provides an opportunity for temporal analysis studies.\n",
      "> \n",
      "> Researchers interested in analyzing this dataset can conveniently access it through Crawl Feeds platform or choose to download individual datasets consisting of 20 million+ reviews. This comprehensive dataset serves as an excellent resource for researchers studying topics related to customer satisfaction in the hospitality industry while providing a deeper understanding through extensive textual information combined with necessary metadata\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> - Dataset Overview:\n",
      ">    - The dataset contains various columns that provide information about each review, including review title, reviewed by (name/username of the reviewer), tags or labels associated with the review, average rating of the hotel, number of images attached to the review, URL of the review page, text content of the review, nationality of the reviewer, crawled date and time (when reviewed data was obtained), hotel name and its URL.\n",
      ">    - The dataset is available in CSV format and can be downloaded from Crawl Feeds website ([Download link](https://crawlfeeds.com/datasets/booking-dot-com-reviews-datasets)).\n",
      ">    - It consists of over 700K records.\n",
      "> \n",
      "> - Dataset Columns:\n",
      ">    - `review_title`: The title given by a reviewer for their review.\n",
      ">    - `reviewed_by`: Name or username of the person who gave a particular review.\n",
      ">    - `images`: Number oimagesizontallyf images attached to a specific review.\n",
      ">    - `avg_rating`: Average rating given to a hotel based on multiple reviews.\n",
      ">    - `url`: URL link leading to a particular review page.\n",
      ">    - `hotel_name`: The name attributed to each hotel being reviewed\n",
      ">     . \n",
      "> - Tips on Using this Dataset:\n",
      ">    \n",
      ">     i) Understand Review Text: Analyzing raw_review_text column can provide insights into customer experiences at different hotels as they are direct personal accounts shared by previous guests. Natural Language Processing techniques can be applied in order to extract sentiments from these textual descriptions.\n",
      "> \n",
      ">     ii) Explore Average Ratings: By examining avg_rating column across different hotels or categories (if available) of hotels, it is possible to identify trends and patterns. This information can be helpful while recommending hotels to potential customers or understanding customer satisfaction levels.\n",
      "> \n",
      ">     iii) Analyze Tags: Utilize the tags column which provides labels or keywords associated with each review. By grouping reviews using these tags, you can extract themes or common topics that appear frequently in customer feedback.\n",
      "> \n",
      ">     iv) Visualize Images: The images column denotes the number of images attached to each review. You can explore this data by visualizing the images if available, providing additional insights into hotel facilities and amenities.\n",
      "> \n",
      "> - Data Cleaning and Preprocessing:\n",
      ">    - As with any dataset\n",
      "\n",
      "### Research Ideas\n",
      "> - Sentiment Analysis: The dataset can be used for sentiment analysis to analyze the overall sentiment and opinions of hotel guests. By analyzing the review text and ratings, one can determine whether guests had a positive or negative experience at a particular hotel.\n",
      "> - Trend Analysis: The dataset can be used to analyze trends in hotel reviews over time. By examining the reviewed_at column, one can identify patterns or changes in customer satisfaction levels and identify areas of improvement for hotels.\n",
      "> - Recommendation System: The dataset can be utilized to build recommendation systems for hotels. By considering factors such as average rating, tags, and review text, personalized recommendations can be generated for users based on their preferences and requirements\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/opensnippets)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: booking_reviews copy.csv**\n",
      "| Column name         | Description                                                                                                                |\n",
      "|:--------------------|:---------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review_title**    | The title of the review given by the reviewer. (Text)                                                                      |\n",
      "| **reviewed_at**     | The date and time when the review was given by the reviewer. (Datetime)                                                    |\n",
      "| **reviewed_by**     | The name or username of the person who gave the review. (Text)                                                             |\n",
      "| **images**          | The number of images attached to the review. (Numeric)                                                                     |\n",
      "| **crawled_at**      | The date and time when the review was crawled from the website. (Datetime)                                                 |\n",
      "| **url**             | The URL of the review page. (Text)                                                                                         |\n",
      "| **hotel_name**      | The name of the hotel being reviewed. (Text)                                                                               |\n",
      "| **hotel_url**       | The URL of the hotel's page on Booking.com. (Text)                                                                         |\n",
      "| **avg_rating**      | The average rating of hotels given by reviewers. (Numeric)                                                                 |\n",
      "| **nationality**     | The nationality of reviewers. (Text)                                                                                       |\n",
      "| **review_text**     | The text content of each review without any processing or modifications. (Text)                                            |\n",
      "| **raw_review_text** | The potentially pre-processed version of the review text for further analysis. (Text)                                      |\n",
      "| **tags**            | The tags or labels that categorize and classify the content of the review. (Text)                                          |\n",
      "| **meta**            | Supplementary information associated with certain reviews that could provide extra insights into guest experiences. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Crawl Feeds](https://data.world/opensnippets).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: booking-com-hotel-reviews\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Booking.com Hotel Reviews\n",
      "name 'detect' is not defined Language is not detected: Booking.com Hotel Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Booking.com Hotel Reviews\n",
      "### Booking.com Hotel Reviews Dataset\n",
      "By Crawl Feeds [[source]](https://data.world/opensnippets)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> The Booking.com Reviews Dataset is a comprehensive collection of hotel reviews and ratings gathered from the popular travel booking website, Booking.com. This dataset provides valuable insights into the experiences and opinions of customers who have stayed at various hotels across different locations. \n",
      "> \n",
      "> With over 700K records available, this dataset offers immense potential for analysis and research in the field of hospitality and tourism. Each review includes key information such as the title given by the reviewer, raw text content without any processing, reviewer's name or username, tags or labels associated with the review, average rating of the hotel, number of images attached to the review, URL of the review page as well as additional metadata.\n",
      "> \n",
      "> The dataset captures details about both domestic and international travelers' nationalities, providing a diverse perspective on people's experiences with different hotels. Furthermore,rating distribution can be observed through average ratings provided by reviewers.\n",
      "> \n",
      "> In addition to retrieving insightful customer feedback on specific hotels, this dataset also allows for understanding trends in customer preferences, satisfaction levels,and sentiments towards various amenities or services provided by hotels. Researchers can explore correlations between variables like average rating and nationality to gain valuable insights into cultural differences in customer expectations.\n",
      "> \n",
      "> This data has been crawled from Booking.com's website along with relevant time stamps indicating when each review was given as well as when it was captured from their site. The availability of both reviewed_at (date & time) and crawled_at (date & time) stamp provides an opportunity for temporal analysis studies.\n",
      "> \n",
      "> Researchers interested in analyzing this dataset can conveniently access it through Crawl Feeds platform or choose to download individual datasets consisting of 20 million+ reviews. This comprehensive dataset serves as an excellent resource for researchers studying topics related to customer satisfaction in the hospitality industry while providing a deeper understanding through extensive textual information combined with necessary metadata\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> \n",
      "> - Dataset Overview:\n",
      ">    - The dataset contains various columns that provide information about each review, including review title, reviewed by (name/username of the reviewer), tags or labels associated with the review, average rating of the hotel, number of images attached to the review, URL of the review page, text content of the review, nationality of the reviewer, crawled date and time (when reviewed data was obtained), hotel name and its URL.\n",
      ">    - The dataset is available in CSV format and can be downloaded from Crawl Feeds website ([Download link](https://crawlfeeds.com/datasets/booking-dot-com-reviews-datasets)).\n",
      ">    - It consists of over 700K records.\n",
      "> \n",
      "> - Dataset Columns:\n",
      ">    - `review_title`: The title given by a reviewer for their review.\n",
      ">    - `reviewed_by`: Name or username of the person who gave a particular review.\n",
      ">    - `images`: Number oimagesizontallyf images attached to a specific review.\n",
      ">    - `avg_rating`: Average rating given to a hotel based on multiple reviews.\n",
      ">    - `url`: URL link leading to a particular review page.\n",
      ">    - `hotel_name`: The name attributed to each hotel being reviewed\n",
      ">     . \n",
      "> - Tips on Using this Dataset:\n",
      ">    \n",
      ">     i) Understand Review Text: Analyzing raw_review_text column can provide insights into customer experiences at different hotels as they are direct personal accounts shared by previous guests. Natural Language Processing techniques can be applied in order to extract sentiments from these textual descriptions.\n",
      "> \n",
      ">     ii) Explore Average Ratings: By examining avg_rating column across different hotels or categories (if available) of hotels, it is possible to identify trends and patterns. This information can be helpful while recommending hotels to potential customers or understanding customer satisfaction levels.\n",
      "> \n",
      ">     iii) Analyze Tags: Utilize the tags column which provides labels or keywords associated with each review. By grouping reviews using these tags, you can extract themes or common topics that appear frequently in customer feedback.\n",
      "> \n",
      ">     iv) Visualize Images: The images column denotes the number of images attached to each review. You can explore this data by visualizing the images if available, providing additional insights into hotel facilities and amenities.\n",
      "> \n",
      "> - Data Cleaning and Preprocessing:\n",
      ">    - As with any dataset\n",
      "\n",
      "### Research Ideas\n",
      "> - Sentiment Analysis: The dataset can be used for sentiment analysis to analyze the overall sentiment and opinions of hotel guests. By analyzing the review text and ratings, one can determine whether guests had a positive or negative experience at a particular hotel.\n",
      "> - Trend Analysis: The dataset can be used to analyze trends in hotel reviews over time. By examining the reviewed_at column, one can identify patterns or changes in customer satisfaction levels and identify areas of improvement for hotels.\n",
      "> - Recommendation System: The dataset can be utilized to build recommendation systems for hotels. By considering factors such as average rating, tags, and review text, personalized recommendations can be generated for users based on their preferences and requirements\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/opensnippets)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **Unknown License - Please check the dataset description for more information.**\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: booking_reviews copy.csv**\n",
      "| Column name         | Description                                                                                                                |\n",
      "|:--------------------|:---------------------------------------------------------------------------------------------------------------------------|\n",
      "| **review_title**    | The title of the review given by the reviewer. (Text)                                                                      |\n",
      "| **reviewed_at**     | The date and time when the review was given by the reviewer. (Datetime)                                                    |\n",
      "| **reviewed_by**     | The name or username of the person who gave the review. (Text)                                                             |\n",
      "| **images**          | The number of images attached to the review. (Numeric)                                                                     |\n",
      "| **crawled_at**      | The date and time when the review was crawled from the website. (Datetime)                                                 |\n",
      "| **url**             | The URL of the review page. (Text)                                                                                         |\n",
      "| **hotel_name**      | The name of the hotel being reviewed. (Text)                                                                               |\n",
      "| **hotel_url**       | The URL of the hotel's page on Booking.com. (Text)                                                                         |\n",
      "| **avg_rating**      | The average rating of hotels given by reviewers. (Numeric)                                                                 |\n",
      "| **nationality**     | The nationality of reviewers. (Text)                                                                                       |\n",
      "| **review_text**     | The text content of each review without any processing or modifications. (Text)                                            |\n",
      "| **raw_review_text** | The potentially pre-processed version of the review text for further analysis. (Text)                                      |\n",
      "| **tags**            | The tags or labels that categorize and classify the content of the review. (Text)                                          |\n",
      "| **meta**            | Supplementary information associated with certain reviews that could provide extra insights into guest experiences. (Text) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Crawl Feeds](https://data.world/opensnippets).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/tweet-sentiment-s-impact-on-stock-returns\n",
      "name 'detect' is not defined Language is not detected: tweet-sentiment-s-impact-on-stock-returns\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Tweet Sentiment's Impact on Stock Returns\n",
      "name 'detect' is not defined Language is not detected: 862,231 Labeled Instances\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Tweet Sentiment's Impact on Stock Returns\n",
      "### 862,231 Labeled Instances\n",
      "By  [[source]](https://zenodo.org/record/4662780#.Y8Os69JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains 862,231 labeled tweets and associated stock returns, providing a comprehensive look into the impact of social media on company-level stock market performance. For each tweet, researchers have extracted data such as the date of the tweet and its associated stock symbol, along with metrics such as last price and various returns (1-day return, 2-day return, 3-day return, 7-day return). Also recorded are volatility scores for both 10 day intervals and 30 day intervals. Finally, sentiment scores from both Long Short - Term Memory (LSTM) and TextBlob models have been included to quantify the overall tone in which these messages were delivered. With this dataset you will be able to explore how tweets can affect a company's share prices both short term and long term by leveraging all of these data points for analysis!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> In order to use this dataset, users can utilize descriptive statistics such as histograms or regression techniques  to establish relationships between tweet content & sentiment with corresponding stock return data points such as 1-day & 7-day returns measurements.\n",
      "> \n",
      "> The primary fields used for analysis include Tweet Text (TWEET), Stock symbol (STOCK), Date (DATE), Closing Price at the time of Tweet (LAST_PRICE) a range of Volatility measures  10 day Volatility(VOLATILITY_10D)and 30 day Volatility(VOLATILITY_30D ) for each Stock which capture changes in market fluctuation during different periods around when Twitter reactions occur. Additionally Sentiment Polarity analysis undertaken via two Machine learning algorithms LSTM Polarity(LSTM_POLARITY)and Textblob polarity provide insight into whether people are expressing positive or negative sentiments about each company at given times which again could influence thereby potentially influence Stock Prices over shorter term periods like 1-Day Returns(1_DAY_RETURN),2-Day Returns(2_DAY_RETURN)or longer term horizon like 7 Day Returns*7DAY RETURNS*.Finally MENTION field indicates if names/acronyms associated with Companies were specifically mentioned in each Tweet or not which gives extra insight into whether company specific contexts were present within individual Tweets aka “Company Relevancy”\n",
      "\n",
      "### Research Ideas\n",
      "> - Analyzing the degree to which tweets can influence stock prices. By analyzing relationships between variables such as tweet sentiment and stock returns, correlations can be identified that could be used to inform investment decisions. \n",
      "> - Exploring natural language processing (NLP) models for predicting future market trends based on textual data such as tweets. Through testing and evaluating different text-based models using this dataset, better predictive models may emerge that can give investors advance warning of upcoming market shifts due to news or other events.\n",
      "> - Investigating the impact of different types of tweets (positive/negative, factual/opinionated) on stock prices over specific time frames. By studying correlations between the sentiment or nature of a tweet and its effect on stocks, insights may be gained into what sort of news or events have a greater impact on markets in general\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4662780#.Y8Os69JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: reduced_dataset-release.csv**\n",
      "| Column name           | Description                                                                                            |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------------|\n",
      "| **TWEET**             | Text of the tweet. (String)                                                                            |\n",
      "| **STOCK**             | Company's stock mentioned in the tweet. (String)                                                       |\n",
      "| **DATE**              | Date the tweet was posted. (Date)                                                                      |\n",
      "| **LAST_PRICE**        | Company's last price at the time of tweeting. (Float)                                                  |\n",
      "| **1_DAY_RETURN**      | Amount the stock returned or lost over the course of the next day after being tweeted about. (Float)   |\n",
      "| **2_DAY_RETURN**      | Amount the stock returned or lost over the course of the two days after being tweeted about. (Float)   |\n",
      "| **3_DAY_RETURN**      | Amount the stock returned or lost over the course of the three days after being tweeted about. (Float) |\n",
      "| **7_DAY_RETURN**      | Amount the stock returned or lost over the course of the seven days after being tweeted about. (Float) |\n",
      "| **PX_VOLUME**         | Volume traded at the time of tweeting. (Integer)                                                       |\n",
      "| **VOLATILITY_10D**    | Volatility measure across 10 day window. (Float)                                                       |\n",
      "| **VOLATILITY_30D**    | Volatility measure across 30 day window. (Float)                                                       |\n",
      "| **LSTM_POLARITY**     | Labeled sentiment from LSTM. (Float)                                                                   |\n",
      "| **TEXTBLOB_POLARITY** | Labeled sentiment from TextBlob. (Float)                                                               |\n",
      "| **MENTION**           | Number of times the stock was mentioned in the tweet. (Integer)                                        |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: full_dataset-release.csv**\n",
      "| Column name           | Description                                                                                            |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------------|\n",
      "| **TWEET**             | Text of the tweet. (String)                                                                            |\n",
      "| **STOCK**             | Company's stock mentioned in the tweet. (String)                                                       |\n",
      "| **DATE**              | Date the tweet was posted. (Date)                                                                      |\n",
      "| **LAST_PRICE**        | Company's last price at the time of tweeting. (Float)                                                  |\n",
      "| **1_DAY_RETURN**      | Amount the stock returned or lost over the course of the next day after being tweeted about. (Float)   |\n",
      "| **2_DAY_RETURN**      | Amount the stock returned or lost over the course of the two days after being tweeted about. (Float)   |\n",
      "| **3_DAY_RETURN**      | Amount the stock returned or lost over the course of the three days after being tweeted about. (Float) |\n",
      "| **7_DAY_RETURN**      | Amount the stock returned or lost over the course of the seven days after being tweeted about. (Float) |\n",
      "| **PX_VOLUME**         | Volume traded at the time of tweeting. (Integer)                                                       |\n",
      "| **VOLATILITY_10D**    | Volatility measure across 10 day window. (Float)                                                       |\n",
      "| **VOLATILITY_30D**    | Volatility measure across 30 day window. (Float)                                                       |\n",
      "| **LSTM_POLARITY**     | Labeled sentiment from LSTM. (Float)                                                                   |\n",
      "| **TEXTBLOB_POLARITY** | Labeled sentiment from TextBlob. (Float)                                                               |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4662780#.Y8Os69JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tweet-sentiment-s-impact-on-stock-returns\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Tweet Sentiment's Impact on Stock Returns\n",
      "name 'detect' is not defined Language is not detected: 862,231 Labeled Instances\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Tweet Sentiment's Impact on Stock Returns\n",
      "### 862,231 Labeled Instances\n",
      "By  [[source]](https://zenodo.org/record/4662780#.Y8Os69JBwUE)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset contains 862,231 labeled tweets and associated stock returns, providing a comprehensive look into the impact of social media on company-level stock market performance. For each tweet, researchers have extracted data such as the date of the tweet and its associated stock symbol, along with metrics such as last price and various returns (1-day return, 2-day return, 3-day return, 7-day return). Also recorded are volatility scores for both 10 day intervals and 30 day intervals. Finally, sentiment scores from both Long Short - Term Memory (LSTM) and TextBlob models have been included to quantify the overall tone in which these messages were delivered. With this dataset you will be able to explore how tweets can affect a company's share prices both short term and long term by leveraging all of these data points for analysis!\n",
      "\n",
      "### More Datasets\n",
      "> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n",
      "\n",
      "### Featured Notebooks\n",
      "> - 🚨 **Your notebook can be here!** 🚨! \n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> In order to use this dataset, users can utilize descriptive statistics such as histograms or regression techniques  to establish relationships between tweet content & sentiment with corresponding stock return data points such as 1-day & 7-day returns measurements.\n",
      "> \n",
      "> The primary fields used for analysis include Tweet Text (TWEET), Stock symbol (STOCK), Date (DATE), Closing Price at the time of Tweet (LAST_PRICE) a range of Volatility measures  10 day Volatility(VOLATILITY_10D)and 30 day Volatility(VOLATILITY_30D ) for each Stock which capture changes in market fluctuation during different periods around when Twitter reactions occur. Additionally Sentiment Polarity analysis undertaken via two Machine learning algorithms LSTM Polarity(LSTM_POLARITY)and Textblob polarity provide insight into whether people are expressing positive or negative sentiments about each company at given times which again could influence thereby potentially influence Stock Prices over shorter term periods like 1-Day Returns(1_DAY_RETURN),2-Day Returns(2_DAY_RETURN)or longer term horizon like 7 Day Returns*7DAY RETURNS*.Finally MENTION field indicates if names/acronyms associated with Companies were specifically mentioned in each Tweet or not which gives extra insight into whether company specific contexts were present within individual Tweets aka “Company Relevancy”\n",
      "\n",
      "### Research Ideas\n",
      "> - Analyzing the degree to which tweets can influence stock prices. By analyzing relationships between variables such as tweet sentiment and stock returns, correlations can be identified that could be used to inform investment decisions. \n",
      "> - Exploring natural language processing (NLP) models for predicting future market trends based on textual data such as tweets. Through testing and evaluating different text-based models using this dataset, better predictive models may emerge that can give investors advance warning of upcoming market shifts due to news or other events.\n",
      "> - Investigating the impact of different types of tweets (positive/negative, factual/opinionated) on stock prices over specific time frames. By studying correlations between the sentiment or nature of a tweet and its effect on stocks, insights may be gained into what sort of news or events have a greater impact on markets in general\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://zenodo.org/record/4662780#.Y8Os69JBwUE)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n",
      "> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: reduced_dataset-release.csv**\n",
      "| Column name           | Description                                                                                            |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------------|\n",
      "| **TWEET**             | Text of the tweet. (String)                                                                            |\n",
      "| **STOCK**             | Company's stock mentioned in the tweet. (String)                                                       |\n",
      "| **DATE**              | Date the tweet was posted. (Date)                                                                      |\n",
      "| **LAST_PRICE**        | Company's last price at the time of tweeting. (Float)                                                  |\n",
      "| **1_DAY_RETURN**      | Amount the stock returned or lost over the course of the next day after being tweeted about. (Float)   |\n",
      "| **2_DAY_RETURN**      | Amount the stock returned or lost over the course of the two days after being tweeted about. (Float)   |\n",
      "| **3_DAY_RETURN**      | Amount the stock returned or lost over the course of the three days after being tweeted about. (Float) |\n",
      "| **7_DAY_RETURN**      | Amount the stock returned or lost over the course of the seven days after being tweeted about. (Float) |\n",
      "| **PX_VOLUME**         | Volume traded at the time of tweeting. (Integer)                                                       |\n",
      "| **VOLATILITY_10D**    | Volatility measure across 10 day window. (Float)                                                       |\n",
      "| **VOLATILITY_30D**    | Volatility measure across 30 day window. (Float)                                                       |\n",
      "| **LSTM_POLARITY**     | Labeled sentiment from LSTM. (Float)                                                                   |\n",
      "| **TEXTBLOB_POLARITY** | Labeled sentiment from TextBlob. (Float)                                                               |\n",
      "| **MENTION**           | Number of times the stock was mentioned in the tweet. (Integer)                                        |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: full_dataset-release.csv**\n",
      "| Column name           | Description                                                                                            |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------------|\n",
      "| **TWEET**             | Text of the tweet. (String)                                                                            |\n",
      "| **STOCK**             | Company's stock mentioned in the tweet. (String)                                                       |\n",
      "| **DATE**              | Date the tweet was posted. (Date)                                                                      |\n",
      "| **LAST_PRICE**        | Company's last price at the time of tweeting. (Float)                                                  |\n",
      "| **1_DAY_RETURN**      | Amount the stock returned or lost over the course of the next day after being tweeted about. (Float)   |\n",
      "| **2_DAY_RETURN**      | Amount the stock returned or lost over the course of the two days after being tweeted about. (Float)   |\n",
      "| **3_DAY_RETURN**      | Amount the stock returned or lost over the course of the three days after being tweeted about. (Float) |\n",
      "| **7_DAY_RETURN**      | Amount the stock returned or lost over the course of the seven days after being tweeted about. (Float) |\n",
      "| **PX_VOLUME**         | Volume traded at the time of tweeting. (Integer)                                                       |\n",
      "| **VOLATILITY_10D**    | Volatility measure across 10 day window. (Float)                                                       |\n",
      "| **VOLATILITY_30D**    | Volatility measure across 30 day window. (Float)                                                       |\n",
      "| **LSTM_POLARITY**     | Labeled sentiment from LSTM. (Float)                                                                   |\n",
      "| **TEXTBLOB_POLARITY** | Labeled sentiment from TextBlob. (Float)                                                               |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [](https://zenodo.org/record/4662780#.Y8Os69JBwUE).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: thedevastator/streaming-activity-and-scrobble-features\n",
      "name 'detect' is not defined Language is not detected: streaming-activity-and-scrobble-features\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Streaming Activity and Scrobble Features\n",
      "name 'detect' is not defined Language is not detected: Diverse streaming data with song features and scrobble activities\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Streaming Activity and Scrobble Features\n",
      "### Diverse streaming data with song features and scrobble activities\n",
      "By Sean Miller [[source]](https://data.world/kcmillersean)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset provides a comprehensive and diverse collection of internet streaming data, covering a span of four years. It includes streaming activities and scrobble features from various platforms and devices, such as YouTube, Spotify, local iTunes library, and Amazon Echo. The data is sourced from a household with a wide age range, encompassing individuals from birth to 38 years old.\n",
      "> \n",
      "> With an extensive range of columns, this dataset offers in-depth insights into music consumption patterns and preferences. Key attributes captured include the performer or artist name (**Performer**), song title (**Song**), Spotify genre tags (**spotify_genre**), URL for the Spotify track preview (**spotify_track_preview_url**), duration of the song in milliseconds (**spotify_track_duration_ms**), popularity score on Spotify (measured by **spotify_track_popularity**), explicit content indicator (represented by **spotify_track_explicit** flag), danceability rating (**danceability**), energy level (**energy**), key of the song's composition (**key**) in musical notation format (e.g., C sharp or D flat major/minor scale).\n",
      "> \n",
      "> Additionally, other valuable metrics include sound loudness measured in decibels (dB) (**loudness**) that defines the overall volume intensity present within each song. The mode attribute indicates whether the song is composed primarily in a major or minor key tonality system (`major` or `minor` values respectively). Speechiness measure quantifies the presence of spoken words within a given track on a numerical scale(**speechiness**) while acousticness score determines how much acoustic quality is exhibited with larger values indicating more prominent acoustic elements(**acousticness)**.\n",
      "> \n",
      "> The dataset also profiles instrumentalness score which implies whether a particular track leans more towards instrumental music without significant vocal components. Liveness metric assesses live audience presence during musical performances based on recorded sound characteristics(*liveness*). Valence measures the positiveness conveyed through melodic aspects inducing emotional response(**valence**), and tempo indicates the speed or pace of the song in beats per minute(BPM)(**tempo**). Time signature represents the rhythmic structure of a song, defined by numerator indicating beats per bar and denominator indicating note value that represents one beat(**time_signature**).\n",
      "> \n",
      "> The dataset includes information about scrobble timestamps in Central Time Zone (**TimeStamp_Central**) as well as Coordinated Universal Time (UTC)(**TimeStamp_UTC**) to provide comprehensive insights into streaming trends with a geographic perspective.\n",
      "> \n",
      "> In summary, this rich dataset offers valuable information on music streaming preferences, ranging from detailed song attributes to user activity timestamps. It\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> - Introduction:\n",
      "> \n",
      "> - Dataset Overview:\n",
      ">    The dataset is available in two files: Scrobble_Features.csv and My Streaming Activity.csv. \n",
      ">    - Scrobble_Features.csv: This file includes detailed information about song features, such as danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness,valence,tempo,time_signature etc.\n",
      ">    - My Streaming Activity.csv: This file contains information about the streaming activities like performer,song,time stamps(album,data time utc/central) etc.\n",
      "> \n",
      "> - Columns Description:\n",
      ">    Each column in the dataset provides specific information related to the songs or streaming activities. Here are some important columns:\n",
      "> \n",
      "> - Performer: This column represents the name of the artist or performer of each song.\n",
      "> - Song: This column provides the title of each song.\n",
      "> - spotify_genre: It indicates genre(s) of each song according to Spotify's classification.\n",
      "> - spotify_track_preview_url : an url indicating where can you find a preview od te fragmentof ech songs,\n",
      "> - spotify_track_duration_ms : duration indication in miliseconds,\n",
      ">   \n",
      "> Important Columns for analyzing Songs/Trends:\n",
      "> \n",
      "> \n",
      "> *spotify_track_popularity:* Popularity score measures on Spotify for each individual songs\n",
      "> *danceability:* A measure representing how suitable a song is for dancing based on musical elements\n",
      "> *energy:* An indicator that reflects how intense or active a particular track is\n",
      "> *loudness:* The overall volume level (loudness) represented by decibels (dB)\n",
      "> *mode:* It represents whether the song is in a major or minor key\n",
      "> *speechiness:* Represents the presence of spoken words in a song\n",
      "> *acousticness:* This metric measures the acoustic quality of each song\n",
      "> *instrumentalness:* Represents the likelihood that a particular song is instrumental\n",
      "> *liveness:* Reflects on how much presence of live audience there is in each song. It shows a measure where higher values indicate more likeliness of representing live performances.\n",
      "> valence: A measure that represents musical positiveness conveyed by each song\n",
      "> \n",
      "> \n",
      "> - Analyzing Song Features:\n",
      ">    The dataset provides a variety of numerical and categorical features to\n",
      "\n",
      "### Research Ideas\n",
      "> - Music recommendation system: With information on song features such as danceability, energy, and valence, this dataset can be used to develop a music recommendation system that suggests songs based on the user's preferences and mood.\n",
      "> - Analyzing streaming trends: By analyzing the streaming activity data over time, it is possible to identify popular songs and genres, peak listening hours, and the impact of external factors (e.g., events, holidays) on music consumption.\n",
      "> - Exploring musical preferences across different age groups: Since this dataset includes streaming data from a household with a wide age range, it can be used to analyze and compare musical preferences across different demographics. This analysis can provide insights into how music tastes vary based on factors like age or generation\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/kcmillersean)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Scrobble_Features.csv**\n",
      "| Column name                   | Description                                                                                               |\n",
      "|:------------------------------|:----------------------------------------------------------------------------------------------------------|\n",
      "| **Performer**                 | The name of the artist or performer of the song. (Text)                                                   |\n",
      "| **Song**                      | The title of the song. (Text)                                                                             |\n",
      "| **spotify_genre**             | The genre(s) of the song according to Spotify. (Text)                                                     |\n",
      "| **spotify_track_preview_url** | The URL of a preview of the song on Spotify. (Text)                                                       |\n",
      "| **spotify_track_duration_ms** | The duration of the song in milliseconds. (Numeric)                                                       |\n",
      "| **spotify_track_popularity**  | The popularity score of the song on Spotify. (Numeric)                                                    |\n",
      "| **spotify_track_explicit**    | Indicates whether the song contains explicit content (Boolean)                                            |\n",
      "| **danceability**              | A measure indicating how suitable a song is for dancing based on musical elements. (Numeric)              |\n",
      "| **energy**                    | A measure determining the intensity and activity level within a song. (Numeric)                           |\n",
      "| **key**                       | The key in which a particular track is composed (e.g., C, D, E). (Text)                                   |\n",
      "| **loudness**                  | The overall volume level of the song expressed in decibels. (Numeric)                                     |\n",
      "| **mode**                      | Indicates whether the song is in a major or minor key. (Text)                                             |\n",
      "| **speechiness**               | Gauges the presence of spoken words within a song. (Numeric)                                              |\n",
      "| **acousticness**              | A measure indicating the extent of acoustic qualities within a track. (Numeric)                           |\n",
      "| **instrumentalness**          | Estimates the likelihood that a song is instrumental in nature. (Numeric)                                 |\n",
      "| **liveness**                  | Assesses if live audience participation can be detected within tracks. (Numeric)                          |\n",
      "| **valence**                   | A measure quantifying musical positiveness conveyed by each track. (Numeric)                              |\n",
      "| **tempo**                     | The beat per minute (BPM) providing insight into pacing or rhythm. (Numeric)                              |\n",
      "| **time_signature**            | Detailed key and time signature attributes offering further insights into musical characteristics. (Text) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: My Streaming Activity.csv**\n",
      "| Column name           | Description                                                                                      |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------|\n",
      "| **Performer**         | The name of the artist or performer of the song. (Text)                                          |\n",
      "| **Song**              | The title of the song. (Text)                                                                    |\n",
      "| **TimeStamp_Central** | The timestamp of the streaming activity recorded in the Central Time Zone. (DateTime)            |\n",
      "| **Album**             | The name of the album to which the song belongs. (Text)                                          |\n",
      "| **TimeStamp_UTC**     | The timestamp of the streaming activity recorded in Coordinated Universal Time (UTC). (DateTime) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Sean Miller](https://data.world/kcmillersean).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: streaming-activity-and-scrobble-features\n",
      "name 'detect' is not defined Language is not detected: thedevastator\n",
      "name 'detect' is not defined Language is not detected: Streaming Activity and Scrobble Features\n",
      "name 'detect' is not defined Language is not detected: Diverse streaming data with song features and scrobble activities\n",
      "name 'detect' is not defined Language is not detected: _____\n",
      "# Streaming Activity and Scrobble Features\n",
      "### Diverse streaming data with song features and scrobble activities\n",
      "By Sean Miller [[source]](https://data.world/kcmillersean)\n",
      "_____\n",
      "\n",
      "### About this dataset\n",
      "> This dataset provides a comprehensive and diverse collection of internet streaming data, covering a span of four years. It includes streaming activities and scrobble features from various platforms and devices, such as YouTube, Spotify, local iTunes library, and Amazon Echo. The data is sourced from a household with a wide age range, encompassing individuals from birth to 38 years old.\n",
      "> \n",
      "> With an extensive range of columns, this dataset offers in-depth insights into music consumption patterns and preferences. Key attributes captured include the performer or artist name (**Performer**), song title (**Song**), Spotify genre tags (**spotify_genre**), URL for the Spotify track preview (**spotify_track_preview_url**), duration of the song in milliseconds (**spotify_track_duration_ms**), popularity score on Spotify (measured by **spotify_track_popularity**), explicit content indicator (represented by **spotify_track_explicit** flag), danceability rating (**danceability**), energy level (**energy**), key of the song's composition (**key**) in musical notation format (e.g., C sharp or D flat major/minor scale).\n",
      "> \n",
      "> Additionally, other valuable metrics include sound loudness measured in decibels (dB) (**loudness**) that defines the overall volume intensity present within each song. The mode attribute indicates whether the song is composed primarily in a major or minor key tonality system (`major` or `minor` values respectively). Speechiness measure quantifies the presence of spoken words within a given track on a numerical scale(**speechiness**) while acousticness score determines how much acoustic quality is exhibited with larger values indicating more prominent acoustic elements(**acousticness)**.\n",
      "> \n",
      "> The dataset also profiles instrumentalness score which implies whether a particular track leans more towards instrumental music without significant vocal components. Liveness metric assesses live audience presence during musical performances based on recorded sound characteristics(*liveness*). Valence measures the positiveness conveyed through melodic aspects inducing emotional response(**valence**), and tempo indicates the speed or pace of the song in beats per minute(BPM)(**tempo**). Time signature represents the rhythmic structure of a song, defined by numerator indicating beats per bar and denominator indicating note value that represents one beat(**time_signature**).\n",
      "> \n",
      "> The dataset includes information about scrobble timestamps in Central Time Zone (**TimeStamp_Central**) as well as Coordinated Universal Time (UTC)(**TimeStamp_UTC**) to provide comprehensive insights into streaming trends with a geographic perspective.\n",
      "> \n",
      "> In summary, this rich dataset offers valuable information on music streaming preferences, ranging from detailed song attributes to user activity timestamps. It\n",
      "\n",
      "### How to use the dataset\n",
      "> \n",
      "> - Introduction:\n",
      "> \n",
      "> - Dataset Overview:\n",
      ">    The dataset is available in two files: Scrobble_Features.csv and My Streaming Activity.csv. \n",
      ">    - Scrobble_Features.csv: This file includes detailed information about song features, such as danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness,valence,tempo,time_signature etc.\n",
      ">    - My Streaming Activity.csv: This file contains information about the streaming activities like performer,song,time stamps(album,data time utc/central) etc.\n",
      "> \n",
      "> - Columns Description:\n",
      ">    Each column in the dataset provides specific information related to the songs or streaming activities. Here are some important columns:\n",
      "> \n",
      "> - Performer: This column represents the name of the artist or performer of each song.\n",
      "> - Song: This column provides the title of each song.\n",
      "> - spotify_genre: It indicates genre(s) of each song according to Spotify's classification.\n",
      "> - spotify_track_preview_url : an url indicating where can you find a preview od te fragmentof ech songs,\n",
      "> - spotify_track_duration_ms : duration indication in miliseconds,\n",
      ">   \n",
      "> Important Columns for analyzing Songs/Trends:\n",
      "> \n",
      "> \n",
      "> *spotify_track_popularity:* Popularity score measures on Spotify for each individual songs\n",
      "> *danceability:* A measure representing how suitable a song is for dancing based on musical elements\n",
      "> *energy:* An indicator that reflects how intense or active a particular track is\n",
      "> *loudness:* The overall volume level (loudness) represented by decibels (dB)\n",
      "> *mode:* It represents whether the song is in a major or minor key\n",
      "> *speechiness:* Represents the presence of spoken words in a song\n",
      "> *acousticness:* This metric measures the acoustic quality of each song\n",
      "> *instrumentalness:* Represents the likelihood that a particular song is instrumental\n",
      "> *liveness:* Reflects on how much presence of live audience there is in each song. It shows a measure where higher values indicate more likeliness of representing live performances.\n",
      "> valence: A measure that represents musical positiveness conveyed by each song\n",
      "> \n",
      "> \n",
      "> - Analyzing Song Features:\n",
      ">    The dataset provides a variety of numerical and categorical features to\n",
      "\n",
      "### Research Ideas\n",
      "> - Music recommendation system: With information on song features such as danceability, energy, and valence, this dataset can be used to develop a music recommendation system that suggests songs based on the user's preferences and mood.\n",
      "> - Analyzing streaming trends: By analyzing the streaming activity data over time, it is possible to identify popular songs and genres, peak listening hours, and the impact of external factors (e.g., events, holidays) on music consumption.\n",
      "> - Exploring musical preferences across different age groups: Since this dataset includes streaming data from a household with a wide age range, it can be used to analyze and compare musical preferences across different demographics. This analysis can provide insights into how music tastes vary based on factors like age or generation\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> [Data Source](https://data.world/kcmillersean)\n",
      "> \n",
      ">\n",
      "\n",
      "\n",
      "### License\n",
      "> \n",
      "> \n",
      "> See the dataset description for more information.\n",
      "\n",
      "### Columns\n",
      "\n",
      "**File: Scrobble_Features.csv**\n",
      "| Column name                   | Description                                                                                               |\n",
      "|:------------------------------|:----------------------------------------------------------------------------------------------------------|\n",
      "| **Performer**                 | The name of the artist or performer of the song. (Text)                                                   |\n",
      "| **Song**                      | The title of the song. (Text)                                                                             |\n",
      "| **spotify_genre**             | The genre(s) of the song according to Spotify. (Text)                                                     |\n",
      "| **spotify_track_preview_url** | The URL of a preview of the song on Spotify. (Text)                                                       |\n",
      "| **spotify_track_duration_ms** | The duration of the song in milliseconds. (Numeric)                                                       |\n",
      "| **spotify_track_popularity**  | The popularity score of the song on Spotify. (Numeric)                                                    |\n",
      "| **spotify_track_explicit**    | Indicates whether the song contains explicit content (Boolean)                                            |\n",
      "| **danceability**              | A measure indicating how suitable a song is for dancing based on musical elements. (Numeric)              |\n",
      "| **energy**                    | A measure determining the intensity and activity level within a song. (Numeric)                           |\n",
      "| **key**                       | The key in which a particular track is composed (e.g., C, D, E). (Text)                                   |\n",
      "| **loudness**                  | The overall volume level of the song expressed in decibels. (Numeric)                                     |\n",
      "| **mode**                      | Indicates whether the song is in a major or minor key. (Text)                                             |\n",
      "| **speechiness**               | Gauges the presence of spoken words within a song. (Numeric)                                              |\n",
      "| **acousticness**              | A measure indicating the extent of acoustic qualities within a track. (Numeric)                           |\n",
      "| **instrumentalness**          | Estimates the likelihood that a song is instrumental in nature. (Numeric)                                 |\n",
      "| **liveness**                  | Assesses if live audience participation can be detected within tracks. (Numeric)                          |\n",
      "| **valence**                   | A measure quantifying musical positiveness conveyed by each track. (Numeric)                              |\n",
      "| **tempo**                     | The beat per minute (BPM) providing insight into pacing or rhythm. (Numeric)                              |\n",
      "| **time_signature**            | Detailed key and time signature attributes offering further insights into musical characteristics. (Text) |\n",
      "\n",
      "_____\n",
      "\n",
      "**File: My Streaming Activity.csv**\n",
      "| Column name           | Description                                                                                      |\n",
      "|:----------------------|:-------------------------------------------------------------------------------------------------|\n",
      "| **Performer**         | The name of the artist or performer of the song. (Text)                                          |\n",
      "| **Song**              | The title of the song. (Text)                                                                    |\n",
      "| **TimeStamp_Central** | The timestamp of the streaming activity recorded in the Central Time Zone. (DateTime)            |\n",
      "| **Album**             | The name of the album to which the song belongs. (Text)                                          |\n",
      "| **TimeStamp_UTC**     | The timestamp of the streaming activity recorded in Coordinated Universal Time (UTC). (DateTime) |\n",
      "\n",
      "### Acknowledgements\n",
      "> If you use this dataset in your research, please credit the original authors.\n",
      "> If you use this dataset in your research, please credit [Sean Miller](https://data.world/kcmillersean).\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: khanhdnguyen/goodreadss-books\n",
      "name 'detect' is not defined Language is not detected: goodreadss-books\n",
      "name 'detect' is not defined Language is not detected: khanhdnguyen\n",
      "name 'detect' is not defined Language is not detected: Goodreads's Books\n",
      "name 'detect' is not defined Language is not detected: The collection of books listed on Goodreads.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset consists of over 20,000 books available on Goodreads. The dataset is collected by crawling information directly from official [Goodreads website](https://www.goodreads.com/).<br>\n",
      "Since December 2020, Goodreads no longer issues new developer keys for public developer API. Therefore, goodreads crawler project has been created to retrieve raw data from the website.\n",
      "\n",
      "*Note - Raw data alert:*\n",
      "- Duplicates\n",
      "- Missing values\n",
      "- Invalid values\n",
      "- Multi-values columns\n",
      "- Datetime formats\n",
      "\n",
      "### Content\n",
      "\n",
      "Features:\n",
      "- bookID: book's ID\n",
      "- title: book's title\n",
      "- authors: list of authors\n",
      "- description: summary description\n",
      "- num_ratings: total number of ratings\n",
      "- num_reviews: total number of reviews\n",
      "- avg_rating: average rating\n",
      "- language: languages\n",
      "- publish date: current book's published date\n",
      "- first_publish_date: The published date of first version\n",
      "- series: book's series\n",
      "- characters: list of characters \n",
      "- places: book's places\n",
      "- awards: List of winning awards\n",
      "- genres: list of book's genres\n",
      "- isbn: International Standard Book Number\n",
      "- isbn13: International Standard Book Number (13 digits)\n",
      "- rated 5, 4, 3, 2, 1: Number of rated reviews\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Data wrangling\n",
      "* Data visualisation\n",
      "* Book classification\n",
      "* Book recommendation\n",
      "* Predict book popularity/ratings\n",
      "\n",
      "*Please upvote if you found this dataset is useful*\n",
      "name 'detect' is not defined Language is not detected: goodreadss-books\n",
      "name 'detect' is not defined Language is not detected: khanhdnguyen\n",
      "name 'detect' is not defined Language is not detected: Goodreads's Books\n",
      "name 'detect' is not defined Language is not detected: The collection of books listed on Goodreads.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset consists of over 20,000 books available on Goodreads. The dataset is collected by crawling information directly from official [Goodreads website](https://www.goodreads.com/).<br>\n",
      "Since December 2020, Goodreads no longer issues new developer keys for public developer API. Therefore, goodreads crawler project has been created to retrieve raw data from the website.\n",
      "\n",
      "*Note - Raw data alert:*\n",
      "- Duplicates\n",
      "- Missing values\n",
      "- Invalid values\n",
      "- Multi-values columns\n",
      "- Datetime formats\n",
      "\n",
      "### Content\n",
      "\n",
      "Features:\n",
      "- bookID: book's ID\n",
      "- title: book's title\n",
      "- authors: list of authors\n",
      "- description: summary description\n",
      "- num_ratings: total number of ratings\n",
      "- num_reviews: total number of reviews\n",
      "- avg_rating: average rating\n",
      "- language: languages\n",
      "- publish date: current book's published date\n",
      "- first_publish_date: The published date of first version\n",
      "- series: book's series\n",
      "- characters: list of characters \n",
      "- places: book's places\n",
      "- awards: List of winning awards\n",
      "- genres: list of book's genres\n",
      "- isbn: International Standard Book Number\n",
      "- isbn13: International Standard Book Number (13 digits)\n",
      "- rated 5, 4, 3, 2, 1: Number of rated reviews\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "* Data wrangling\n",
      "* Data visualisation\n",
      "* Book classification\n",
      "* Book recommendation\n",
      "* Predict book popularity/ratings\n",
      "\n",
      "*Please upvote if you found this dataset is useful*\n",
      "name 'detect' is not defined Language is not detected: tanvirullash/food-affection\n",
      "name 'detect' is not defined Language is not detected: food-affection\n",
      "name 'detect' is not defined Language is not detected: tanvirullash\n",
      "name 'detect' is not defined Language is not detected: FoodAffect\n",
      "name 'detect' is not defined Language is not detected: A dataset of food affectivity\n",
      "name 'detect' is not defined Language is not detected: At a sampling rate of 128 Hz, we collected EEG data from 25 volunteers (18 male and 7 female) ranging in age from 25 to 40.\n",
      "A 14-channel EMOTIV Epoc+ was used to collect data. The experiment and the significance of the various scales used in the evaluation were explained to each participant. The following methods were used to create a presentation featuring photos of 40 different cuisines: \n",
      "- Displaying each food for 10 seconds\n",
      "- Display of tranquil scenario following each food image for 17 seconds to counteract the influence of one image leading to the next\n",
      "- In the meantime, self-assessment of Likeness, Valence and Arousal\n",
      "\n",
      "To collect EEG data, we put a headset directly on the participant's scalp. To attract the participants' entire attention, the entire environment was designed to be peaceful and distraction-free, and they were also instructed to use as little muscle movements as possible.\n",
      "Foods were chosen and sequenced with consideration based on their nutritive value, acceptance across different age ranges, and accessibility. The food presentation featured both large feasts and little appetizers. Everything has been covered, from basic staples like rice and chicken curry to popular fast-food options like burgers and pizza. Slides offered a number of drink alternatives, such as soft drinks, Laban (watered-down yoghurt drink), Milk-Saffron drink, and so on, to evoke an overall better reaction.\n",
      "Participants had to rate their affective information for an individual food, for *Likeness* ranging between 0 and 5 (0= Least Like, 5= Most Like), **Valance** as *Excitement* ranging between 0 and 5 (0= Least Excitement, 5= Most Excitement), and **Arousal** as *Feelings* containing labels: 0 = Disgusting and 1 = Pleasant. There is also a generalized version of *Likeness* and *Excitement* where the range is between *1* and *2*. *1* means Least and *2* means Most.\n",
      "\n",
      "**Please cite the following papers if the dataset is used in a publication:**\n",
      "\n",
      "T. Islam, A. R. Joyita, M. G. R. Alam, M. M. Hassan, M. R. Hassan and R. Gravina, \"Human Behavior-Based Personalized Meal Recommendation and Menu Planning Social System,\" in IEEE Transactions on Computational Social Systems, 2022, doi: 10.1109/TCSS.2022.3213506.\n",
      "name 'detect' is not defined Language is not detected: food-affection\n",
      "name 'detect' is not defined Language is not detected: tanvirullash\n",
      "name 'detect' is not defined Language is not detected: FoodAffect\n",
      "name 'detect' is not defined Language is not detected: A dataset of food affectivity\n",
      "name 'detect' is not defined Language is not detected: At a sampling rate of 128 Hz, we collected EEG data from 25 volunteers (18 male and 7 female) ranging in age from 25 to 40.\n",
      "A 14-channel EMOTIV Epoc+ was used to collect data. The experiment and the significance of the various scales used in the evaluation were explained to each participant. The following methods were used to create a presentation featuring photos of 40 different cuisines: \n",
      "- Displaying each food for 10 seconds\n",
      "- Display of tranquil scenario following each food image for 17 seconds to counteract the influence of one image leading to the next\n",
      "- In the meantime, self-assessment of Likeness, Valence and Arousal\n",
      "\n",
      "To collect EEG data, we put a headset directly on the participant's scalp. To attract the participants' entire attention, the entire environment was designed to be peaceful and distraction-free, and they were also instructed to use as little muscle movements as possible.\n",
      "Foods were chosen and sequenced with consideration based on their nutritive value, acceptance across different age ranges, and accessibility. The food presentation featured both large feasts and little appetizers. Everything has been covered, from basic staples like rice and chicken curry to popular fast-food options like burgers and pizza. Slides offered a number of drink alternatives, such as soft drinks, Laban (watered-down yoghurt drink), Milk-Saffron drink, and so on, to evoke an overall better reaction.\n",
      "Participants had to rate their affective information for an individual food, for *Likeness* ranging between 0 and 5 (0= Least Like, 5= Most Like), **Valance** as *Excitement* ranging between 0 and 5 (0= Least Excitement, 5= Most Excitement), and **Arousal** as *Feelings* containing labels: 0 = Disgusting and 1 = Pleasant. There is also a generalized version of *Likeness* and *Excitement* where the range is between *1* and *2*. *1* means Least and *2* means Most.\n",
      "\n",
      "**Please cite the following papers if the dataset is used in a publication:**\n",
      "\n",
      "T. Islam, A. R. Joyita, M. G. R. Alam, M. M. Hassan, M. R. Hassan and R. Gravina, \"Human Behavior-Based Personalized Meal Recommendation and Menu Planning Social System,\" in IEEE Transactions on Computational Social Systems, 2022, doi: 10.1109/TCSS.2022.3213506.\n",
      "name 'detect' is not defined Language is not detected: ngalenal1004/tmdb-movies-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: ngalenal1004\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset 2024\n",
      "name 'detect' is not defined Language is not detected: A collection of 10000 movies sourced from The Movie Database (TMDb)\n",
      "name 'detect' is not defined Language is not detected: ### Title: TMDB Movies Dataset 2024\n",
      "\n",
      "TMDB Movies Dataset 2024\n",
      "\n",
      "### Description:\n",
      "\n",
      "This dataset contains information about a collection of 10000 movies sourced from The Movie Database (TMDb). The data includes details about the movies such as titles, genres, actors, directors, average ratings, release years, and other relevant information.\n",
      "\n",
      "### Content:\n",
      "\n",
      "- **Included Columns:**\n",
      "  - MovieID: Unique identifier for the movie\n",
      "  - Title: Movie title\n",
      "  - Genres: Movie genres\n",
      "  - Actors: List of actors in the movie\n",
      "  - Director: Movie director\n",
      "  - VoteAverage: Average rating of the movie\n",
      "  - ReleaseYear: Year of movie release\n",
      "\n",
      "\n",
      "### Usage:\n",
      "\n",
      "This dataset can be utilized for creating movie recommendation systems, analyzing movie trends, practicing data cleaning and feature engineering, and other movie-related projects. It can inspire in-depth analyses on user preferences in movies, the evolution of movie genres over time, or exploration of top-rated movies by year.\n",
      "\n",
      "### Source:\n",
      "\n",
      "The data has been collected from The Movie Database (TMDb) API.\n",
      "\n",
      "### Additional Notes:\n",
      "\n",
      "- **Notes:** This dataset may require initial cleaning and manipulation for more advanced analyses. Some data might be missing or need additional processing.\n",
      "\n",
      "### Author:\n",
      "\n",
      "- **Author:** NGARI LENDOYE Alix\n",
      "- **Contact Email:** ngarilenal@gmail.com\n",
      "\n",
      "### License:\n",
      "\n",
      "The data is provided under the MIT license. Ensure compliance with the license terms and provide appropriate attribution to the data source (TMDb) when using the dataset.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: ngalenal1004\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset 2024\n",
      "name 'detect' is not defined Language is not detected: A collection of 10000 movies sourced from The Movie Database (TMDb)\n",
      "name 'detect' is not defined Language is not detected: ### Title: TMDB Movies Dataset 2024\n",
      "\n",
      "TMDB Movies Dataset 2024\n",
      "\n",
      "### Description:\n",
      "\n",
      "This dataset contains information about a collection of 10000 movies sourced from The Movie Database (TMDb). The data includes details about the movies such as titles, genres, actors, directors, average ratings, release years, and other relevant information.\n",
      "\n",
      "### Content:\n",
      "\n",
      "- **Included Columns:**\n",
      "  - MovieID: Unique identifier for the movie\n",
      "  - Title: Movie title\n",
      "  - Genres: Movie genres\n",
      "  - Actors: List of actors in the movie\n",
      "  - Director: Movie director\n",
      "  - VoteAverage: Average rating of the movie\n",
      "  - ReleaseYear: Year of movie release\n",
      "\n",
      "\n",
      "### Usage:\n",
      "\n",
      "This dataset can be utilized for creating movie recommendation systems, analyzing movie trends, practicing data cleaning and feature engineering, and other movie-related projects. It can inspire in-depth analyses on user preferences in movies, the evolution of movie genres over time, or exploration of top-rated movies by year.\n",
      "\n",
      "### Source:\n",
      "\n",
      "The data has been collected from The Movie Database (TMDb) API.\n",
      "\n",
      "### Additional Notes:\n",
      "\n",
      "- **Notes:** This dataset may require initial cleaning and manipulation for more advanced analyses. Some data might be missing or need additional processing.\n",
      "\n",
      "### Author:\n",
      "\n",
      "- **Author:** NGARI LENDOYE Alix\n",
      "- **Contact Email:** ngarilenal@gmail.com\n",
      "\n",
      "### License:\n",
      "\n",
      "The data is provided under the MIT license. Ensure compliance with the license terms and provide appropriate attribution to the data source (TMDb) when using the dataset.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: skhalili/iraniancarnumberplate\n",
      "name 'detect' is not defined Language is not detected: iraniancarnumberplate\n",
      "name 'detect' is not defined Language is not detected: skhalili\n",
      "name 'detect' is not defined Language is not detected: IranianCarsNumberPlate\n",
      "name 'detect' is not defined Language is not detected: number plate detection\n",
      "name 'detect' is not defined Language is not detected: This dataset is for number plate detection collected on the Sheypoor site(https://www.sheypoor.com/). We used from LabelImg(https://github.com/tzutalin/labelImg) program for labeling. The number of images in the dataset is about 442. All images have only one number plate and labels are saved in an XML file.\n",
      "name 'detect' is not defined Language is not detected: iraniancarnumberplate\n",
      "name 'detect' is not defined Language is not detected: skhalili\n",
      "name 'detect' is not defined Language is not detected: IranianCarsNumberPlate\n",
      "name 'detect' is not defined Language is not detected: number plate detection\n",
      "name 'detect' is not defined Language is not detected: This dataset is for number plate detection collected on the Sheypoor site(https://www.sheypoor.com/). We used from LabelImg(https://github.com/tzutalin/labelImg) program for labeling. The number of images in the dataset is about 442. All images have only one number plate and labels are saved in an XML file.\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj/news-popularity-in-multiple-social-media-data-set\n",
      "name 'detect' is not defined Language is not detected: news-popularity-in-multiple-social-media-data-set\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj\n",
      "name 'detect' is not defined Language is not detected: News Popularity in Multiple Social Media data set\n",
      "name 'detect' is not defined Language is not detected: Large data set of news items and their respective social feedback\n",
      "name 'detect' is not defined Language is not detected: This is a large data set of news items and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn. \n",
      "The collected data relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine. \n",
      "This data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.\n",
      "\n",
      "Further details on the process of building the data set are provided in the article mentioned in the 'Relevant Papers' section.\n",
      "\n",
      "An .R file is provided to provide a simple introduction to handling the data set.\n",
      "name 'detect' is not defined Language is not detected: news-popularity-in-multiple-social-media-data-set\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj\n",
      "name 'detect' is not defined Language is not detected: News Popularity in Multiple Social Media data set\n",
      "name 'detect' is not defined Language is not detected: Large data set of news items and their respective social feedback\n",
      "name 'detect' is not defined Language is not detected: This is a large data set of news items and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn. \n",
      "The collected data relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine. \n",
      "This data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.\n",
      "\n",
      "Further details on the process of building the data set are provided in the article mentioned in the 'Relevant Papers' section.\n",
      "\n",
      "An .R file is provided to provide a simple introduction to handling the data set.\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj/recipe-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: recipe-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews and User Feedback Dataset\n",
      "name 'detect' is not defined Language is not detected: The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in sentiment analysis, user behavior analysis, recipe recommendation systems, and more. It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain.\n",
      "name 'detect' is not defined Language is not detected: recipe-reviews-dataset\n",
      "name 'detect' is not defined Language is not detected: satyajeetkumarraj\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews Dataset\n",
      "name 'detect' is not defined Language is not detected: Recipe Reviews and User Feedback Dataset\n",
      "name 'detect' is not defined Language is not detected: The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in sentiment analysis, user behavior analysis, recipe recommendation systems, and more. It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain.\n",
      "name 'detect' is not defined Language is not detected: ayushnitb/song-features-dataset-regressing-popularity\n",
      "name 'detect' is not defined Language is not detected: song-features-dataset-regressing-popularity\n",
      "name 'detect' is not defined Language is not detected: ayushnitb\n",
      "name 'detect' is not defined Language is not detected: Song Features Dataset - Regressing Popularity \n",
      "name 'detect' is not defined Language is not detected: Spotify Song features.\n",
      "name 'detect' is not defined Language is not detected: **Introduction**\n",
      "Spotify for Developers offers a wide range of possibilities to utilize the extensive catalog of Spotify data. One of them are the audio features calculated for each song and made available via the official Spotify Web API.\n",
      "\n",
      "This is an attempt to retrieve the spotify data post the last extracted data. Haven't fully tested if this spotify allowed any other API full request post 2019\n",
      "\n",
      "**About**\n",
      "Each song (row) has values for artist name, track name, track id and the audio features itself (for more information about the audio features check out this doc from Spotify).\n",
      "\n",
      "Additionally, there is also a popularity feature included in this dataset. Please note that Spotify recalculates this value based on the number of plays the track receives so it might not be correct value anymore when you access the data.\n",
      "\n",
      "**Key Questions/Hypothesis that can be Answered**\n",
      "1.  ARE SONGS IN MAJOR MODE ARE MORE POPULAR THAN ONES IN MINOR?\n",
      "2. ARE SONGS WITH HIGH LOUDNESS ARE MOST POPULAR?\n",
      "3. MOST PEOPLE LIKE LISTENING TO SONGS WITH SHORTER DURATION?\n",
      "\n",
      "In addition more detailed analysis can be done to see what causes a song to be popular.\n",
      "\n",
      "**Credit**\n",
      "Entire Credit goes to Spotify for providing this data via their Web API.\n",
      "\n",
      "https://developer.spotify.com/documentation/web-api/reference/tracks/get-track/\n",
      "name 'detect' is not defined Language is not detected: song-features-dataset-regressing-popularity\n",
      "name 'detect' is not defined Language is not detected: ayushnitb\n",
      "name 'detect' is not defined Language is not detected: Song Features Dataset - Regressing Popularity \n",
      "name 'detect' is not defined Language is not detected: Spotify Song features.\n",
      "name 'detect' is not defined Language is not detected: **Introduction**\n",
      "Spotify for Developers offers a wide range of possibilities to utilize the extensive catalog of Spotify data. One of them are the audio features calculated for each song and made available via the official Spotify Web API.\n",
      "\n",
      "This is an attempt to retrieve the spotify data post the last extracted data. Haven't fully tested if this spotify allowed any other API full request post 2019\n",
      "\n",
      "**About**\n",
      "Each song (row) has values for artist name, track name, track id and the audio features itself (for more information about the audio features check out this doc from Spotify).\n",
      "\n",
      "Additionally, there is also a popularity feature included in this dataset. Please note that Spotify recalculates this value based on the number of plays the track receives so it might not be correct value anymore when you access the data.\n",
      "\n",
      "**Key Questions/Hypothesis that can be Answered**\n",
      "1.  ARE SONGS IN MAJOR MODE ARE MORE POPULAR THAN ONES IN MINOR?\n",
      "2. ARE SONGS WITH HIGH LOUDNESS ARE MOST POPULAR?\n",
      "3. MOST PEOPLE LIKE LISTENING TO SONGS WITH SHORTER DURATION?\n",
      "\n",
      "In addition more detailed analysis can be done to see what causes a song to be popular.\n",
      "\n",
      "**Credit**\n",
      "Entire Credit goes to Spotify for providing this data via their Web API.\n",
      "\n",
      "https://developer.spotify.com/documentation/web-api/reference/tracks/get-track/\n",
      "name 'detect' is not defined Language is not detected: adityadeshpande23/amsterdam-airbnb\n",
      "name 'detect' is not defined Language is not detected: amsterdam-airbnb\n",
      "name 'detect' is not defined Language is not detected: adityadeshpande23\n",
      "name 'detect' is not defined Language is not detected: Amsterdam - AirBnb\n",
      "name 'detect' is not defined Language is not detected: Rooms information for AirBnb sites in Amsterdam\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "Includes details of the total number of bedrooms, bathrooms, guests, latitude, longitude, reviews, if the place is instantly bookable, room_type in the hotel\n",
      "name 'detect' is not defined Language is not detected: amsterdam-airbnb\n",
      "name 'detect' is not defined Language is not detected: adityadeshpande23\n",
      "name 'detect' is not defined Language is not detected: Amsterdam - AirBnb\n",
      "name 'detect' is not defined Language is not detected: Rooms information for AirBnb sites in Amsterdam\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "Includes details of the total number of bedrooms, bathrooms, guests, latitude, longitude, reviews, if the place is instantly bookable, room_type in the hotel\n",
      "name 'detect' is not defined Language is not detected: kaggleprollc/nba-forecast-data\n",
      "name 'detect' is not defined Language is not detected: nba-forecast-data\n",
      "name 'detect' is not defined Language is not detected: kaggleprollc\n",
      "name 'detect' is not defined Language is not detected: NBA Forecast Data\n",
      "name 'detect' is not defined Language is not detected: Statistical Forecasts for NBA Team Performance and Playoff Outcomes\n",
      "name 'detect' is not defined Language is not detected: The FiveThirtyEight NBA Forecasts Dataset is a comprehensive collection of data that has been carefully selected by the statistical analysis website FiveThirtyEight. It is accessible on GitHub. The National Basketball Association (NBA) team performance forecasts are the center of the collection, which offers useful data for researchers, analysts, and sports fans interested in investigating statistical predictions and trends within the NBA.\n",
      "\n",
      "The complex forecasting methods used by FiveThirtyEight are used to produce a variety of features and metrics in this dataset. It includes predictions for team win-loss records, playoff probabilities, and even NBA championship probabilities at the season-level. Users can track team performance over time, acquire insights into predicted NBA season outcomes, and analyze success-related aspects by looking at these forecasts.\n",
      "\n",
      "[GitHub Link](https://github.com/fivethirtyeight/data/tree/master/nba-forecasts)\n",
      "\n",
      "[FiveThirtyEight](https://projects.fivethirtyeight.com/2020-nba-predictions/)\n",
      "\n",
      "Note: The dataset is publicly accessible on GitHub but may need to be handled and preprocessed properly before being used in particular analytical applications.\n",
      "name 'detect' is not defined Language is not detected: nba-forecast-data\n",
      "name 'detect' is not defined Language is not detected: kaggleprollc\n",
      "name 'detect' is not defined Language is not detected: NBA Forecast Data\n",
      "name 'detect' is not defined Language is not detected: Statistical Forecasts for NBA Team Performance and Playoff Outcomes\n",
      "name 'detect' is not defined Language is not detected: The FiveThirtyEight NBA Forecasts Dataset is a comprehensive collection of data that has been carefully selected by the statistical analysis website FiveThirtyEight. It is accessible on GitHub. The National Basketball Association (NBA) team performance forecasts are the center of the collection, which offers useful data for researchers, analysts, and sports fans interested in investigating statistical predictions and trends within the NBA.\n",
      "\n",
      "The complex forecasting methods used by FiveThirtyEight are used to produce a variety of features and metrics in this dataset. It includes predictions for team win-loss records, playoff probabilities, and even NBA championship probabilities at the season-level. Users can track team performance over time, acquire insights into predicted NBA season outcomes, and analyze success-related aspects by looking at these forecasts.\n",
      "\n",
      "[GitHub Link](https://github.com/fivethirtyeight/data/tree/master/nba-forecasts)\n",
      "\n",
      "[FiveThirtyEight](https://projects.fivethirtyeight.com/2020-nba-predictions/)\n",
      "\n",
      "Note: The dataset is publicly accessible on GitHub but may need to be handled and preprocessed properly before being used in particular analytical applications.\n",
      "name 'detect' is not defined Language is not detected: ishikajohari/imdb-data-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: imdb-data-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: ishikajohari\n",
      "name 'detect' is not defined Language is not detected: IMDb Movies/Shows with Descriptions\n",
      "name 'detect' is not defined Language is not detected: Top 8k movies and/or tv shows with descriptions from the 90s and onwards\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "This dataset contains IMDb data in conjunction with descriptions for the titles taken from Rotten Tomatoes.\n",
      "\n",
      "This data was collected in an attempt to aid my [Books dataset](https://www.kaggle.com/datasets/ishikajohari/best-books-10k-multi-genre-data) to help with projects concerning cross-content analysis/recommendations for instance.\n",
      "\n",
      "### *Please Upvote if this helps you!*\n",
      "\n",
      "# Content\n",
      "The most (Total 21 features) prominent features are:\n",
      "1. **Title Type** \n",
      "2. **Primary Title**\n",
      "3. **Original Title**\n",
      "4. **Is Adult?** \n",
      "5. **Year** \n",
      "6. **Run-time Minutes** \n",
      "7. **Genres (Multiple)**\n",
      "8. **Average Rating (as on IMDb)** \n",
      "9. **Num. of Votes**\n",
      "10. **Region**\n",
      "11. **Genres** \n",
      "12. **Average Rating** \n",
      "13. **Number of Ratings** \n",
      "14. **Types**\n",
      "15. **Attributes** \n",
      "16. **Description**\n",
      "\n",
      "# Important Data Context\n",
      "The initial dataset being too large has been filtered. The following are the criteria for it:\n",
      "* The data is from the 90s and onwards\n",
      "* Only 'en' (English) language titles have been retained\n",
      "* The regions range from Canada, Greater Britain, India and USA Only\n",
      "* Movie/shows from the 90s-00s with ratings greater than or equal to 7.9 have been retained\n",
      "* Movie/shows from the 2000s and onwards with ratings greater than or equal to 6.5 have been retained\n",
      "* Only titles with num of rating votes greater than 3000 have been retained for Canada and India \n",
      "\n",
      "# Inspiration\n",
      "- Cluster movies/shows based on Descriptions and Genres\n",
      "- Content-based recommendation system using Genre, Description, and Ratings\n",
      "- Genre prediction from Description data (Multi-label classification)\n",
      "name 'detect' is not defined Language is not detected: imdb-data-with-descriptions\n",
      "name 'detect' is not defined Language is not detected: ishikajohari\n",
      "name 'detect' is not defined Language is not detected: IMDb Movies/Shows with Descriptions\n",
      "name 'detect' is not defined Language is not detected: Top 8k movies and/or tv shows with descriptions from the 90s and onwards\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "This dataset contains IMDb data in conjunction with descriptions for the titles taken from Rotten Tomatoes.\n",
      "\n",
      "This data was collected in an attempt to aid my [Books dataset](https://www.kaggle.com/datasets/ishikajohari/best-books-10k-multi-genre-data) to help with projects concerning cross-content analysis/recommendations for instance.\n",
      "\n",
      "### *Please Upvote if this helps you!*\n",
      "\n",
      "# Content\n",
      "The most (Total 21 features) prominent features are:\n",
      "1. **Title Type** \n",
      "2. **Primary Title**\n",
      "3. **Original Title**\n",
      "4. **Is Adult?** \n",
      "5. **Year** \n",
      "6. **Run-time Minutes** \n",
      "7. **Genres (Multiple)**\n",
      "8. **Average Rating (as on IMDb)** \n",
      "9. **Num. of Votes**\n",
      "10. **Region**\n",
      "11. **Genres** \n",
      "12. **Average Rating** \n",
      "13. **Number of Ratings** \n",
      "14. **Types**\n",
      "15. **Attributes** \n",
      "16. **Description**\n",
      "\n",
      "# Important Data Context\n",
      "The initial dataset being too large has been filtered. The following are the criteria for it:\n",
      "* The data is from the 90s and onwards\n",
      "* Only 'en' (English) language titles have been retained\n",
      "* The regions range from Canada, Greater Britain, India and USA Only\n",
      "* Movie/shows from the 90s-00s with ratings greater than or equal to 7.9 have been retained\n",
      "* Movie/shows from the 2000s and onwards with ratings greater than or equal to 6.5 have been retained\n",
      "* Only titles with num of rating votes greater than 3000 have been retained for Canada and India \n",
      "\n",
      "# Inspiration\n",
      "- Cluster movies/shows based on Descriptions and Genres\n",
      "- Content-based recommendation system using Genre, Description, and Ratings\n",
      "- Genre prediction from Description data (Multi-label classification)\n",
      "name 'detect' is not defined Language is not detected: prathamsaraf1389/spotify-global-top-50-daily-update\n",
      "name 'detect' is not defined Language is not detected: spotify-global-top-50-daily-update\n",
      "name 'detect' is not defined Language is not detected: prathamsaraf1389\n",
      "name 'detect' is not defined Language is not detected: Spotify Global Top 50 (Daily Update)\n",
      "name 'detect' is not defined Language is not detected: Discover Daily-updated Spotify Global Top 50 -World's Most Popular Songs Dataset\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "Spotify Global Top 50 (Daily Update)\n",
      "\n",
      "**Context**\n",
      "Spotify is one of the most popular music streaming platforms in the world. Every day, the platform curates a list of the top 50 most streamed songs from around the globe. This dataset provides information about these top 50 songs, including their names, the artists who performed them, the album name, release date, track popularity, track duration, and more.\n",
      "\n",
      "\n",
      "***Disclaimer***\n",
      "This dataset is only used for research purposes, users must abide by the relevant laws and regulations of their location, please do not use it for illegal purposes. The user shall bear all the consequences caused by illegal use.\n",
      "\n",
      "**Content**\n",
      "The dataset contains information about the top 50 most streamed songs on Spotify, including their track name, artist name, album name, release date, duration, popularity, explicit lyrics, and images of the album cover art.\n",
      "\n",
      "The data is available in CSV format and is updated daily. The dataset can be used to analyze trends in global music streaming, track the popularity of individual songs and artists, and gain insights into the preferences of Spotify users around the world.\n",
      "\n",
      "Apart from this the images folder contains the cover art in 640 * 640 image size with the naming convention {trackname}_{name_of_artist[0]}\n",
      "\n",
      "**Inspiration**\n",
      "This dataset can be used by music enthusiasts, researchers, and data scientists to gain insights into the music industry, track trends, and discover new and popular artists. It can also be used to develop recommendation systems for music streaming platforms, understand the preferences of users, and predict the success of new songs and albums.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: spotify-global-top-50-daily-update\n",
      "name 'detect' is not defined Language is not detected: prathamsaraf1389\n",
      "name 'detect' is not defined Language is not detected: Spotify Global Top 50 (Daily Update)\n",
      "name 'detect' is not defined Language is not detected: Discover Daily-updated Spotify Global Top 50 -World's Most Popular Songs Dataset\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "Spotify Global Top 50 (Daily Update)\n",
      "\n",
      "**Context**\n",
      "Spotify is one of the most popular music streaming platforms in the world. Every day, the platform curates a list of the top 50 most streamed songs from around the globe. This dataset provides information about these top 50 songs, including their names, the artists who performed them, the album name, release date, track popularity, track duration, and more.\n",
      "\n",
      "\n",
      "***Disclaimer***\n",
      "This dataset is only used for research purposes, users must abide by the relevant laws and regulations of their location, please do not use it for illegal purposes. The user shall bear all the consequences caused by illegal use.\n",
      "\n",
      "**Content**\n",
      "The dataset contains information about the top 50 most streamed songs on Spotify, including their track name, artist name, album name, release date, duration, popularity, explicit lyrics, and images of the album cover art.\n",
      "\n",
      "The data is available in CSV format and is updated daily. The dataset can be used to analyze trends in global music streaming, track the popularity of individual songs and artists, and gain insights into the preferences of Spotify users around the world.\n",
      "\n",
      "Apart from this the images folder contains the cover art in 640 * 640 image size with the naming convention {trackname}_{name_of_artist[0]}\n",
      "\n",
      "**Inspiration**\n",
      "This dataset can be used by music enthusiasts, researchers, and data scientists to gain insights into the music industry, track trends, and discover new and popular artists. It can also be used to develop recommendation systems for music streaming platforms, understand the preferences of users, and predict the success of new songs and albums.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sudhanshuy17/popular-tv-shows-data-set\n",
      "name 'detect' is not defined Language is not detected: popular-tv-shows-data-set\n",
      "name 'detect' is not defined Language is not detected: sudhanshuy17\n",
      "name 'detect' is not defined Language is not detected: Popular TV Shows data set\n",
      "name 'detect' is not defined Language is not detected: This is a popular tv shows data set, fetched using the data from the TMDB API   \n",
      "name 'detect' is not defined Language is not detected: \"This dataset contains information on a selection of popular TV shows from various networks and countries. It includes information on the show's title, year of release, genre, main cast, and a \"This dataset contains information on a selection of popular TV shows from various networks and countries. It includes information on the show's title, year of release, genre, main cast, and a brief summary of the plot. With this dataset, one can analyze trends in TV show popularity and content over time, as well as investigate the characteristics of successful TV shows. The data is suitable for a variety of applications, such as content recommendation systems, media research, and entertainment industry analysis.\"\n",
      "You can find the dataset in the Tv_shows.csv file.\n",
      "name 'detect' is not defined Language is not detected: popular-tv-shows-data-set\n",
      "name 'detect' is not defined Language is not detected: sudhanshuy17\n",
      "name 'detect' is not defined Language is not detected: Popular TV Shows data set\n",
      "name 'detect' is not defined Language is not detected: This is a popular tv shows data set, fetched using the data from the TMDB API   \n",
      "name 'detect' is not defined Language is not detected: \"This dataset contains information on a selection of popular TV shows from various networks and countries. It includes information on the show's title, year of release, genre, main cast, and a \"This dataset contains information on a selection of popular TV shows from various networks and countries. It includes information on the show's title, year of release, genre, main cast, and a brief summary of the plot. With this dataset, one can analyze trends in TV show popularity and content over time, as well as investigate the characteristics of successful TV shows. The data is suitable for a variety of applications, such as content recommendation systems, media research, and entertainment industry analysis.\"\n",
      "You can find the dataset in the Tv_shows.csv file.\n",
      "name 'detect' is not defined Language is not detected: muhammedsal98/bank-marketing\n",
      "name 'detect' is not defined Language is not detected: bank-marketing\n",
      "name 'detect' is not defined Language is not detected: muhammedsal98\n",
      "name 'detect' is not defined Language is not detected: Bank Marketing\n",
      "name 'detect' is not defined Language is not detected: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012\n",
      "name 'detect' is not defined Language is not detected:  1. Title: Bank Marketing\n",
      "\n",
      "[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October 2011. EUROSIS.\n",
      "\n",
      "  Available at: [pdf] http://hdl.handle.net/1822/14838\n",
      "                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt\n",
      "\n",
      "2. Sources\n",
      "   Created by: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012\n",
      "   \n",
      "3. Past Usage:\n",
      "\n",
      "  The full dataset was described and analyzed in:\n",
      "\n",
      "  S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, \n",
      "  Portugal, October 2011. EUROSIS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 4. Relevant Information:\n",
      "\n",
      "   The data is related to direct marketing campaigns of a Portuguese banking institution. \n",
      "   The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, \n",
      "   in order to access if the product (bank term deposit) would be (or not) subscribed. \n",
      "\n",
      " \n",
      "\n",
      "   The classification goal is to predict if the client will subscribe to a term deposit (variable y).\n",
      "\n",
      "5. Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)\n",
      "\n",
      "6. Number of Attributes: 16 + output attribute.\n",
      "\n",
      " 7. There are two datasets: \n",
      "      1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010).\n",
      "      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.\n",
      "   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).\n",
      "\n",
      "\n",
      " Objective\n",
      "\n",
      "The objective is to predict if the contacted client will accept joining the deposit or not.\n",
      "name 'detect' is not defined Language is not detected: bank-marketing\n",
      "name 'detect' is not defined Language is not detected: muhammedsal98\n",
      "name 'detect' is not defined Language is not detected: Bank Marketing\n",
      "name 'detect' is not defined Language is not detected: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012\n",
      "name 'detect' is not defined Language is not detected:  1. Title: Bank Marketing\n",
      "\n",
      "[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October 2011. EUROSIS.\n",
      "\n",
      "  Available at: [pdf] http://hdl.handle.net/1822/14838\n",
      "                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt\n",
      "\n",
      "2. Sources\n",
      "   Created by: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012\n",
      "   \n",
      "3. Past Usage:\n",
      "\n",
      "  The full dataset was described and analyzed in:\n",
      "\n",
      "  S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, \n",
      "  Portugal, October 2011. EUROSIS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 4. Relevant Information:\n",
      "\n",
      "   The data is related to direct marketing campaigns of a Portuguese banking institution. \n",
      "   The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, \n",
      "   in order to access if the product (bank term deposit) would be (or not) subscribed. \n",
      "\n",
      " \n",
      "\n",
      "   The classification goal is to predict if the client will subscribe to a term deposit (variable y).\n",
      "\n",
      "5. Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)\n",
      "\n",
      "6. Number of Attributes: 16 + output attribute.\n",
      "\n",
      " 7. There are two datasets: \n",
      "      1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010).\n",
      "      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.\n",
      "   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).\n",
      "\n",
      "\n",
      " Objective\n",
      "\n",
      "The objective is to predict if the contacted client will accept joining the deposit or not.\n",
      "name 'detect' is not defined Language is not detected: mohdrehanhaider/chrono-analyzer\n",
      "name 'detect' is not defined Language is not detected: chrono-analyzer\n",
      "name 'detect' is not defined Language is not detected: mohdrehanhaider\n",
      "name 'detect' is not defined Language is not detected: Luxury Chrono Watches\n",
      "name 'detect' is not defined Language is not detected: Luxury Watch Images and Prices\n",
      "name 'detect' is not defined Language is not detected: This Chronic Watches Manufacturer and Price Prediction Dataset, consisting of images of various watch styles, provides valuable information for training a neural network model to classify images and predict the watch's manufacturer and price in INR. Beyond the primary application of training a model for classification and regression tasks, this dataset opens up several other exciting possibilities and applications:\n",
      "\n",
      "1. **Product Recommendation System**: The dataset can be utilized to build a product recommendation system for customers interested in buying watches. By understanding the brand preferences and price ranges of customers, the recommendation system can suggest the most suitable Chronic watch options.\n",
      "\n",
      "2. **Image-based Search**: E-commerce platforms can use this dataset to develop an image-based search feature. Users can upload a picture of a watch they like, and the system will use image recognition to find similar watches from the dataset.\n",
      "\n",
      "3. **Price Trend Analysis**: By analyzing the relationship between the serial number and the price in INR, retailers and manufacturers can identify price trends over time. This information can help them make pricing decisions and understand market demand for different watch models.\n",
      "\n",
      "4. **Market Segmentation**: The dataset can be used for market segmentation analysis to understand the popularity of different watch brands in specific regions or demographics. This information can guide marketing and distribution strategies.\n",
      "\n",
      "5. **Competitor Analysis**: Retailers and manufacturers can use the dataset to compare their watch models with competitors in terms of pricing and popularity. This analysis can lead to better competitive strategies.\n",
      "\n",
      "6. **Visual Merchandising**: Retail stores can utilize this dataset to design visually appealing displays that showcase watches of various brands. By highlighting specific watch styles, they can attract customers with diverse preferences.\n",
      "\n",
      "7. **Customer Behavior Analysis**: Analyzing customer preferences based on image classification can provide valuable insights into what design elements or brand aesthetics appeal to different customer segments.\n",
      "\n",
      "8. **Brand Perception Analysis**: Image recognition techniques can help analyze how customers perceive different brands based on their watch designs, leading to improvements in branding and marketing strategies.\n",
      "\n",
      "9. **Fake Watch Detection**: By training a model to identify authentic Chronic watch images, the dataset can be used to create a tool for detecting counterfeit watches, ensuring customer trust and safety.\n",
      "\n",
      "10. **Market Insights**: Researchers and analysts can use this dataset to gain insights into the popularity and price distribution of Chronic watches in the market, contributing to market reports and industry analysis.\n",
      "\n",
      "11. **Virtual Try-On**: Using image recognition, augmented reality (AR) technology can enable customers to virtually try on Chronic watches, helping them visualize how different styles look on their wrists before making a purchase.\n",
      "\n",
      "12. **Brand Monitoring**: Brands can monitor online platforms for images of their watches to gauge consumer sentiment, identify brand ambassadors, and address potential issues or counterfeit products.\n",
      "\n",
      "By harnessing the power of neural networks, researchers and industry professionals can unlock invaluable insights into watch brand classification and price prediction. Beyond its primary applications, this dataset opens up a world of possibilities, from revolutionizing e-commerce with image-based search to empowering brand monitoring and counterfeit detection. \n",
      "name 'detect' is not defined Language is not detected: chrono-analyzer\n",
      "name 'detect' is not defined Language is not detected: mohdrehanhaider\n",
      "name 'detect' is not defined Language is not detected: Luxury Chrono Watches\n",
      "name 'detect' is not defined Language is not detected: Luxury Watch Images and Prices\n",
      "name 'detect' is not defined Language is not detected: This Chronic Watches Manufacturer and Price Prediction Dataset, consisting of images of various watch styles, provides valuable information for training a neural network model to classify images and predict the watch's manufacturer and price in INR. Beyond the primary application of training a model for classification and regression tasks, this dataset opens up several other exciting possibilities and applications:\n",
      "\n",
      "1. **Product Recommendation System**: The dataset can be utilized to build a product recommendation system for customers interested in buying watches. By understanding the brand preferences and price ranges of customers, the recommendation system can suggest the most suitable Chronic watch options.\n",
      "\n",
      "2. **Image-based Search**: E-commerce platforms can use this dataset to develop an image-based search feature. Users can upload a picture of a watch they like, and the system will use image recognition to find similar watches from the dataset.\n",
      "\n",
      "3. **Price Trend Analysis**: By analyzing the relationship between the serial number and the price in INR, retailers and manufacturers can identify price trends over time. This information can help them make pricing decisions and understand market demand for different watch models.\n",
      "\n",
      "4. **Market Segmentation**: The dataset can be used for market segmentation analysis to understand the popularity of different watch brands in specific regions or demographics. This information can guide marketing and distribution strategies.\n",
      "\n",
      "5. **Competitor Analysis**: Retailers and manufacturers can use the dataset to compare their watch models with competitors in terms of pricing and popularity. This analysis can lead to better competitive strategies.\n",
      "\n",
      "6. **Visual Merchandising**: Retail stores can utilize this dataset to design visually appealing displays that showcase watches of various brands. By highlighting specific watch styles, they can attract customers with diverse preferences.\n",
      "\n",
      "7. **Customer Behavior Analysis**: Analyzing customer preferences based on image classification can provide valuable insights into what design elements or brand aesthetics appeal to different customer segments.\n",
      "\n",
      "8. **Brand Perception Analysis**: Image recognition techniques can help analyze how customers perceive different brands based on their watch designs, leading to improvements in branding and marketing strategies.\n",
      "\n",
      "9. **Fake Watch Detection**: By training a model to identify authentic Chronic watch images, the dataset can be used to create a tool for detecting counterfeit watches, ensuring customer trust and safety.\n",
      "\n",
      "10. **Market Insights**: Researchers and analysts can use this dataset to gain insights into the popularity and price distribution of Chronic watches in the market, contributing to market reports and industry analysis.\n",
      "\n",
      "11. **Virtual Try-On**: Using image recognition, augmented reality (AR) technology can enable customers to virtually try on Chronic watches, helping them visualize how different styles look on their wrists before making a purchase.\n",
      "\n",
      "12. **Brand Monitoring**: Brands can monitor online platforms for images of their watches to gauge consumer sentiment, identify brand ambassadors, and address potential issues or counterfeit products.\n",
      "\n",
      "By harnessing the power of neural networks, researchers and industry professionals can unlock invaluable insights into watch brand classification and price prediction. Beyond its primary applications, this dataset opens up a world of possibilities, from revolutionizing e-commerce with image-based search to empowering brand monitoring and counterfeit detection. \n",
      "name 'detect' is not defined Language is not detected: sohommajumder21/appliances-energy-prediction-data-set\n",
      "name 'detect' is not defined Language is not detected: appliances-energy-prediction-data-set\n",
      "name 'detect' is not defined Language is not detected: sohommajumder21\n",
      "name 'detect' is not defined Language is not detected: Appliances energy prediction Data Set\n",
      "name 'detect' is not defined Language is not detected: https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Experimental data used to create regression models of appliances energy use in a low energy building.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Data Set Characteristics:  \n",
      "\n",
      "Multivariate, Time-Series, Regression\n",
      "\n",
      "Number of Instances(Rows):\n",
      "\n",
      "19735\n",
      "\n",
      "Number of Attributes(Columns):\n",
      "\n",
      "29\n",
      "\n",
      "\n",
      "Associated Tasks:\n",
      "\n",
      "Regression\n",
      "\n",
      "\n",
      "Source:\n",
      "\n",
      "Luis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS).\n",
      "\n",
      "\n",
      "Data Set Information:\n",
      "Given in Metadata tab about the sources and collection methodology.\n",
      "\n",
      "\n",
      "## Attribute Information:\n",
      "\n",
      "date time year-month-day hour:minute:second\n",
      "\n",
      "Appliances, energy use in Wh (**target variable for prediction**)\n",
      "\n",
      "lights, energy use of light fixtures in the house in Wh\n",
      "\n",
      "T1, Temperature in kitchen area, in Celsius\n",
      "\n",
      "RH_1, Humidity in kitchen area, in %\n",
      "\n",
      "T2, Temperature in living room area, in Celsius\n",
      "\n",
      "RH_2, Humidity in living room area, in %\n",
      "\n",
      "T3, Temperature in laundry room area\n",
      "\n",
      "RH_3, Humidity in laundry room area, in %\n",
      "\n",
      "T4, Temperature in office room, in Celsius\n",
      "\n",
      "RH_4, Humidity in office room, in %\n",
      "\n",
      "T5, Temperature in bathroom, in Celsius\n",
      "\n",
      "RH_5, Humidity in bathroom, in %\n",
      "\n",
      "T6, Temperature outside the building (north side), in Celsius\n",
      "\n",
      "RH_6, Humidity outside the building (north side), in %\n",
      "\n",
      "T7, Temperature in ironing room , in Celsius\n",
      "\n",
      "RH_7, Humidity in ironing room, in %\n",
      "\n",
      "T8, Temperature in teenager room 2, in Celsius\n",
      "\n",
      "RH_8, Humidity in teenager room 2, in %\n",
      "\n",
      "T9, Temperature in parents room, in Celsius\n",
      "\n",
      "RH_9, Humidity in parents room, in %\n",
      "\n",
      "\n",
      "To, Temperature outside (from Chievres weather station), in Celsius\n",
      "\n",
      "Pressure (from Chievres weather station), in mm Hg\n",
      "\n",
      "RH_out, Humidity outside (from Chievres weather station), in %\n",
      "\n",
      "Wind speed (from Chievres weather station), in m/s\n",
      "\n",
      "Visibility (from Chievres weather station), in km\n",
      "\n",
      "Tdewpoint (from Chievres weather station), Â°C\n",
      "\n",
      "rv1, Random variable 1, nondimensional\n",
      "\n",
      "rv2, Random variable 2, nondimensional\n",
      "\n",
      "Where indicated, hourly data (then interpolated) from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis, rp5.ru. Permission was obtained from Reliable Prognosis for the distribution of the 4.5 months of weather data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link](https://www.sciencedirect.com/science/article/abs/pii/S0378778816308970?via%3Dihub).\n",
      "\n",
      "### Citation\n",
      "\n",
      "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "1) This is a regression task, You should predict the \"appliances\" column. Column descriptions are given above. Please read them before proceeding.\n",
      "2) Appropriate time series analysis with regression is preferred more.\n",
      "3) Exploratory data analysis with charts and plots.\n",
      "\n",
      "Have fun!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: appliances-energy-prediction-data-set\n",
      "name 'detect' is not defined Language is not detected: sohommajumder21\n",
      "name 'detect' is not defined Language is not detected: Appliances energy prediction Data Set\n",
      "name 'detect' is not defined Language is not detected: https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Experimental data used to create regression models of appliances energy use in a low energy building.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Data Set Characteristics:  \n",
      "\n",
      "Multivariate, Time-Series, Regression\n",
      "\n",
      "Number of Instances(Rows):\n",
      "\n",
      "19735\n",
      "\n",
      "Number of Attributes(Columns):\n",
      "\n",
      "29\n",
      "\n",
      "\n",
      "Associated Tasks:\n",
      "\n",
      "Regression\n",
      "\n",
      "\n",
      "Source:\n",
      "\n",
      "Luis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS).\n",
      "\n",
      "\n",
      "Data Set Information:\n",
      "Given in Metadata tab about the sources and collection methodology.\n",
      "\n",
      "\n",
      "## Attribute Information:\n",
      "\n",
      "date time year-month-day hour:minute:second\n",
      "\n",
      "Appliances, energy use in Wh (**target variable for prediction**)\n",
      "\n",
      "lights, energy use of light fixtures in the house in Wh\n",
      "\n",
      "T1, Temperature in kitchen area, in Celsius\n",
      "\n",
      "RH_1, Humidity in kitchen area, in %\n",
      "\n",
      "T2, Temperature in living room area, in Celsius\n",
      "\n",
      "RH_2, Humidity in living room area, in %\n",
      "\n",
      "T3, Temperature in laundry room area\n",
      "\n",
      "RH_3, Humidity in laundry room area, in %\n",
      "\n",
      "T4, Temperature in office room, in Celsius\n",
      "\n",
      "RH_4, Humidity in office room, in %\n",
      "\n",
      "T5, Temperature in bathroom, in Celsius\n",
      "\n",
      "RH_5, Humidity in bathroom, in %\n",
      "\n",
      "T6, Temperature outside the building (north side), in Celsius\n",
      "\n",
      "RH_6, Humidity outside the building (north side), in %\n",
      "\n",
      "T7, Temperature in ironing room , in Celsius\n",
      "\n",
      "RH_7, Humidity in ironing room, in %\n",
      "\n",
      "T8, Temperature in teenager room 2, in Celsius\n",
      "\n",
      "RH_8, Humidity in teenager room 2, in %\n",
      "\n",
      "T9, Temperature in parents room, in Celsius\n",
      "\n",
      "RH_9, Humidity in parents room, in %\n",
      "\n",
      "\n",
      "To, Temperature outside (from Chievres weather station), in Celsius\n",
      "\n",
      "Pressure (from Chievres weather station), in mm Hg\n",
      "\n",
      "RH_out, Humidity outside (from Chievres weather station), in %\n",
      "\n",
      "Wind speed (from Chievres weather station), in m/s\n",
      "\n",
      "Visibility (from Chievres weather station), in km\n",
      "\n",
      "Tdewpoint (from Chievres weather station), Â°C\n",
      "\n",
      "rv1, Random variable 1, nondimensional\n",
      "\n",
      "rv2, Random variable 2, nondimensional\n",
      "\n",
      "Where indicated, hourly data (then interpolated) from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis, rp5.ru. Permission was obtained from Reliable Prognosis for the distribution of the 4.5 months of weather data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link](https://www.sciencedirect.com/science/article/abs/pii/S0378778816308970?via%3Dihub).\n",
      "\n",
      "### Citation\n",
      "\n",
      "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "1) This is a regression task, You should predict the \"appliances\" column. Column descriptions are given above. Please read them before proceeding.\n",
      "2) Appropriate time series analysis with regression is preferred more.\n",
      "3) Exploratory data analysis with charts and plots.\n",
      "\n",
      "Have fun!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: mohamed044hamedy/superstoredata\n",
      "name 'detect' is not defined Language is not detected: superstoredata\n",
      "name 'detect' is not defined Language is not detected: mohamed044hamedy\n",
      "name 'detect' is not defined Language is not detected: Superstore Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset containing Sales & Expenses of a Superstore\n",
      "name 'detect' is not defined Language is not detected: ### **Context**\n",
      "With growing demands and cut-throat competitions in the market, a Superstore Giant is seeking your knowledge in understanding what works best for them. They would like to understand which products, regions, categories and customer segments they should target or avoid.\n",
      "\n",
      "Retail dataset of a global superstore for 4 years.\n",
      "\n",
      "You can even take this a step further and try and build a Regression model to predict Sales or Profit.\n",
      "\n",
      "Go crazy with the dataset, but also make sure to provide some business insights to improve.\n",
      "\n",
      "### **Metadata**\n",
      "\n",
      "Order ID =&gt; Unique Order ID for each Customer.\n",
      "\n",
      "Order Date =&gt; Order Date of the product.\n",
      "\n",
      "Ship Date =&gt; Shipping Date of the Product.\n",
      "\n",
      "Ship Mode=&gt; Shipping Mode specified by the Customer.\n",
      "\n",
      "Customer Name =&gt; Name of the Customer.\n",
      "\n",
      "Segment =&gt; The segment where the Customer belongs.\n",
      "\n",
      "State =&gt; State of residence of the Customer.\n",
      "\n",
      "Country =&gt; Country of residence of the Customer.\n",
      "\n",
      "Market =&gt; The market place of the product.\n",
      "\n",
      "Region =&gt; Region where the Customer belong.\n",
      "\n",
      "Product ID =&gt; Unique ID of the Product.\n",
      "\n",
      "Category =&gt; Category of the product ordered.\n",
      "\n",
      "Sub-Category =&gt; Sub-Category of the product ordered.\n",
      "\n",
      "Product Name =&gt; Name of the Product\n",
      "\n",
      "Unit Price =&gt; The price for one unit.\n",
      "\n",
      "Quantity =&gt; Quantity of the Product.\n",
      "\n",
      "Discount =&gt; Discount provided.\n",
      "\n",
      "Shipping Cost =&gt; The cost for shipping\n",
      "\n",
      "Order Priority =&gt; Items shipped via priority are shipped by air which results in faster delivery times.\n",
      "\n",
      "Sales =&gt; Sales of the Product.\n",
      "\n",
      "Expenses =&gt; The expense is the cost of operations that a company incurs to generate revenue. \n",
      "\n",
      "Revenue =&gt; The Revenue refers to the total earnings.\n",
      "\n",
      "Year =&gt; Year of the Sales.\n",
      "\n",
      "### **Acknowledgements**\n",
      "\n",
      "I do not own this data. I merely found it from the Tableau website and add some row. All credits to the original authors/creators. For educational purposes only.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: superstoredata\n",
      "name 'detect' is not defined Language is not detected: mohamed044hamedy\n",
      "name 'detect' is not defined Language is not detected: Superstore Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset containing Sales & Expenses of a Superstore\n",
      "name 'detect' is not defined Language is not detected: ### **Context**\n",
      "With growing demands and cut-throat competitions in the market, a Superstore Giant is seeking your knowledge in understanding what works best for them. They would like to understand which products, regions, categories and customer segments they should target or avoid.\n",
      "\n",
      "Retail dataset of a global superstore for 4 years.\n",
      "\n",
      "You can even take this a step further and try and build a Regression model to predict Sales or Profit.\n",
      "\n",
      "Go crazy with the dataset, but also make sure to provide some business insights to improve.\n",
      "\n",
      "### **Metadata**\n",
      "\n",
      "Order ID =&gt; Unique Order ID for each Customer.\n",
      "\n",
      "Order Date =&gt; Order Date of the product.\n",
      "\n",
      "Ship Date =&gt; Shipping Date of the Product.\n",
      "\n",
      "Ship Mode=&gt; Shipping Mode specified by the Customer.\n",
      "\n",
      "Customer Name =&gt; Name of the Customer.\n",
      "\n",
      "Segment =&gt; The segment where the Customer belongs.\n",
      "\n",
      "State =&gt; State of residence of the Customer.\n",
      "\n",
      "Country =&gt; Country of residence of the Customer.\n",
      "\n",
      "Market =&gt; The market place of the product.\n",
      "\n",
      "Region =&gt; Region where the Customer belong.\n",
      "\n",
      "Product ID =&gt; Unique ID of the Product.\n",
      "\n",
      "Category =&gt; Category of the product ordered.\n",
      "\n",
      "Sub-Category =&gt; Sub-Category of the product ordered.\n",
      "\n",
      "Product Name =&gt; Name of the Product\n",
      "\n",
      "Unit Price =&gt; The price for one unit.\n",
      "\n",
      "Quantity =&gt; Quantity of the Product.\n",
      "\n",
      "Discount =&gt; Discount provided.\n",
      "\n",
      "Shipping Cost =&gt; The cost for shipping\n",
      "\n",
      "Order Priority =&gt; Items shipped via priority are shipped by air which results in faster delivery times.\n",
      "\n",
      "Sales =&gt; Sales of the Product.\n",
      "\n",
      "Expenses =&gt; The expense is the cost of operations that a company incurs to generate revenue. \n",
      "\n",
      "Revenue =&gt; The Revenue refers to the total earnings.\n",
      "\n",
      "Year =&gt; Year of the Sales.\n",
      "\n",
      "### **Acknowledgements**\n",
      "\n",
      "I do not own this data. I merely found it from the Tableau website and add some row. All credits to the original authors/creators. For educational purposes only.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hernan4444/animeplanet-character-recommendation\n",
      "name 'detect' is not defined Language is not detected: animeplanet-character-recommendation\n",
      "name 'detect' is not defined Language is not detected: hernan4444\n",
      "name 'detect' is not defined Language is not detected: Anime-Planet Character Recommendation\n",
      "name 'detect' is not defined Language is not detected: Waifus and husbando dataset\n",
      "name 'detect' is not defined Language is not detected: # Anime-Planet Character Recommendation\n",
      "\n",
      "&gt; Waifus and husbando dataset\n",
      "\n",
      "\n",
      "\n",
      "This dataset contains information about 132.028 characters and the preference from 72.629 different users of characters scrapped from anime-planet. In particular, this dataset contain:\n",
      "\n",
      "- Information about the character like Tags, description, genre, etc.\n",
      "- HTML with character information to do data scrapping. These files contain information such as name, alias, hate rank, tags, etc.\n",
      "- The character list per user. Include characters that love and hate.\n",
      "\n",
      "\n",
      "### Warning: this dataset includes information on adult anime characters.\n",
      "\n",
      "\n",
      "## Content\n",
      "\n",
      "**The anime data was scrapped between June 29th and August 14th.**\n",
      "\n",
      "* The \"html\" folder contain 1 html per character (132.028 different characters). I uploaded 2 files as example to don't increase the size of this dataset.  All HTML files are in this link: [https://drive.google.com/drive/folders/1Kg0OZ6dEsQuJZVqj1CcTGwDnwp4sNOnW?usp=sharing](https://drive.google.com/drive/folders/1Kg0OZ6dEsQuJZVqj1CcTGwDnwp4sNOnW?usp=sharing)\n",
      "\n",
      "* `user_characters.csv` have the list of all character register by the user with the respective love boolean (means if the user love or hate the character). This dataset contains 12 Million row, 72.629 different animes and 132.028 different characters. The file have the following columns:\n",
      "\n",
      "1. user_id:  non identifiable randomly generated user id.\n",
      "2. character_id:  non identifiable randomly generated character id.\n",
      "3. loved: True if the user loves the character, False if he hates it.\n",
      "\n",
      "\n",
      "* `characters_metadata.csv` contain general information of every character (132.028 different character) like Tags, alias, name, gender, etc. This file have the following columns:\n",
      "\n",
      "1.  ID:  non identifiable randomly generated character id.\n",
      "2.  Name: full name of this character.\n",
      "3.  Alias: another way to call the character.\n",
      "4.  Gender:  gender of the this character.\n",
      "5. Hair Color: hair color of this character.\n",
      "6. Love Rank: love rank based in users preference.\n",
      "7. Hate Rank: hate rank based in users preference.\n",
      "8. Eye color: eye color of the character.\n",
      "9. Birthday: date of his birthday.\n",
      "10. Blood Type: blood type of the character.\n",
      "11. Tags: comma separated list of tags for this character.\n",
      "12. Love Count: how many users love this character.\n",
      "13. Hate Count: how many users hate this character.\n",
      "14. Description: short text with description if this character.\n",
      "15. url: url to the main page of character in [Anime Planet](https://www.anime-planet.com/).\n",
      "\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "Thanks to:\n",
      "1. [Anime Planet](https://www.anime-planet.com/) for providing anime data.\n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "1. Experiment with different types of recommended. For instance, collaborative filtering or based on context like Tags, description, etc.\n",
      "\n",
      "2. Use this information to build a character recommended system.\n",
      "\n",
      "3. Build another dataset with anime topic.\n",
      "\n",
      "4. Try to Improve [Anime Recommendation Database 2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020) with more data of characters from the anime. This need to extract the anime id from every html.\n",
      "name 'detect' is not defined Language is not detected: animeplanet-character-recommendation\n",
      "name 'detect' is not defined Language is not detected: hernan4444\n",
      "name 'detect' is not defined Language is not detected: Anime-Planet Character Recommendation\n",
      "name 'detect' is not defined Language is not detected: Waifus and husbando dataset\n",
      "name 'detect' is not defined Language is not detected: # Anime-Planet Character Recommendation\n",
      "\n",
      "&gt; Waifus and husbando dataset\n",
      "\n",
      "\n",
      "\n",
      "This dataset contains information about 132.028 characters and the preference from 72.629 different users of characters scrapped from anime-planet. In particular, this dataset contain:\n",
      "\n",
      "- Information about the character like Tags, description, genre, etc.\n",
      "- HTML with character information to do data scrapping. These files contain information such as name, alias, hate rank, tags, etc.\n",
      "- The character list per user. Include characters that love and hate.\n",
      "\n",
      "\n",
      "### Warning: this dataset includes information on adult anime characters.\n",
      "\n",
      "\n",
      "## Content\n",
      "\n",
      "**The anime data was scrapped between June 29th and August 14th.**\n",
      "\n",
      "* The \"html\" folder contain 1 html per character (132.028 different characters). I uploaded 2 files as example to don't increase the size of this dataset.  All HTML files are in this link: [https://drive.google.com/drive/folders/1Kg0OZ6dEsQuJZVqj1CcTGwDnwp4sNOnW?usp=sharing](https://drive.google.com/drive/folders/1Kg0OZ6dEsQuJZVqj1CcTGwDnwp4sNOnW?usp=sharing)\n",
      "\n",
      "* `user_characters.csv` have the list of all character register by the user with the respective love boolean (means if the user love or hate the character). This dataset contains 12 Million row, 72.629 different animes and 132.028 different characters. The file have the following columns:\n",
      "\n",
      "1. user_id:  non identifiable randomly generated user id.\n",
      "2. character_id:  non identifiable randomly generated character id.\n",
      "3. loved: True if the user loves the character, False if he hates it.\n",
      "\n",
      "\n",
      "* `characters_metadata.csv` contain general information of every character (132.028 different character) like Tags, alias, name, gender, etc. This file have the following columns:\n",
      "\n",
      "1.  ID:  non identifiable randomly generated character id.\n",
      "2.  Name: full name of this character.\n",
      "3.  Alias: another way to call the character.\n",
      "4.  Gender:  gender of the this character.\n",
      "5. Hair Color: hair color of this character.\n",
      "6. Love Rank: love rank based in users preference.\n",
      "7. Hate Rank: hate rank based in users preference.\n",
      "8. Eye color: eye color of the character.\n",
      "9. Birthday: date of his birthday.\n",
      "10. Blood Type: blood type of the character.\n",
      "11. Tags: comma separated list of tags for this character.\n",
      "12. Love Count: how many users love this character.\n",
      "13. Hate Count: how many users hate this character.\n",
      "14. Description: short text with description if this character.\n",
      "15. url: url to the main page of character in [Anime Planet](https://www.anime-planet.com/).\n",
      "\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "Thanks to:\n",
      "1. [Anime Planet](https://www.anime-planet.com/) for providing anime data.\n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "1. Experiment with different types of recommended. For instance, collaborative filtering or based on context like Tags, description, etc.\n",
      "\n",
      "2. Use this information to build a character recommended system.\n",
      "\n",
      "3. Build another dataset with anime topic.\n",
      "\n",
      "4. Try to Improve [Anime Recommendation Database 2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020) with more data of characters from the anime. This need to extract the anime id from every html.\n",
      "name 'detect' is not defined Language is not detected: cerolacia/black-friday-sales-prediction\n",
      "name 'detect' is not defined Language is not detected: black-friday-sales-prediction\n",
      "name 'detect' is not defined Language is not detected: cerolacia\n",
      "name 'detect' is not defined Language is not detected: BLACK FRIDAY SALES PREDICTION\n",
      "name 'detect' is not defined Language is not detected: Create personalized Offer By Predicting the Purchase Amount of the customer\n",
      "name 'detect' is not defined Language is not detected: A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.\n",
      "The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
      "\n",
      "Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products.\n",
      "\n",
      "Data\n",
      "| Variable | Definition |\n",
      "|--- |--- |\n",
      "| User_ID | User ID |\n",
      "| Product_ID | Product ID |\n",
      "| Gender | Sex of User |\n",
      "| Age  |Age in bins|\n",
      "| Occupation | Occupation(Masked) |\n",
      "|City_Category|Category of the City (A,B,C)|\n",
      "|Stay_In_Current_City_Years|Number of years stay in current city|\n",
      "|Marital_Status|Marital Status|\n",
      "|Product_Category_1|Product Category (Masked)|\n",
      "|Product_Category_2|Product may belongs to other category also (Masked)|\n",
      "|Product_Category_3|\tProduct may belongs to other category also (Masked)|\n",
      "|Purchase|\tPurchase Amount (Target Variable)|\n",
      "\n",
      "Your model performance will be evaluated on the basis of your prediction of the purchase amount for the test data (test.csv), which contains similar data-points as train except for their purchase amount. Your submission needs to be in the format as shown in \"SampleSubmission.csv\".\n",
      "\n",
      "We at our end, have the actual purchase amount for the test dataset, against which your predictions will be evaluated. Submissions are scored on the root mean squared error (RMSE). RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:\n",
      "\n",
      "\n",
      "Where y hat is the predicted value and y is the original value.\n",
      "name 'detect' is not defined Language is not detected: black-friday-sales-prediction\n",
      "name 'detect' is not defined Language is not detected: cerolacia\n",
      "name 'detect' is not defined Language is not detected: BLACK FRIDAY SALES PREDICTION\n",
      "name 'detect' is not defined Language is not detected: Create personalized Offer By Predicting the Purchase Amount of the customer\n",
      "name 'detect' is not defined Language is not detected: A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.\n",
      "The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
      "\n",
      "Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products.\n",
      "\n",
      "Data\n",
      "| Variable | Definition |\n",
      "|--- |--- |\n",
      "| User_ID | User ID |\n",
      "| Product_ID | Product ID |\n",
      "| Gender | Sex of User |\n",
      "| Age  |Age in bins|\n",
      "| Occupation | Occupation(Masked) |\n",
      "|City_Category|Category of the City (A,B,C)|\n",
      "|Stay_In_Current_City_Years|Number of years stay in current city|\n",
      "|Marital_Status|Marital Status|\n",
      "|Product_Category_1|Product Category (Masked)|\n",
      "|Product_Category_2|Product may belongs to other category also (Masked)|\n",
      "|Product_Category_3|\tProduct may belongs to other category also (Masked)|\n",
      "|Purchase|\tPurchase Amount (Target Variable)|\n",
      "\n",
      "Your model performance will be evaluated on the basis of your prediction of the purchase amount for the test data (test.csv), which contains similar data-points as train except for their purchase amount. Your submission needs to be in the format as shown in \"SampleSubmission.csv\".\n",
      "\n",
      "We at our end, have the actual purchase amount for the test dataset, against which your predictions will be evaluated. Submissions are scored on the root mean squared error (RMSE). RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:\n",
      "\n",
      "\n",
      "Where y hat is the predicted value and y is the original value.\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja/customer-rating-data-by-amazon\n",
      "name 'detect' is not defined Language is not detected: customer-rating-data-by-amazon\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja\n",
      "name 'detect' is not defined Language is not detected: Customer Rating Data By Amazon\n",
      "name 'detect' is not defined Language is not detected: Amazon Customer rating for recommendation system\n",
      "name 'detect' is not defined Language is not detected: **### ****! Upvote will help me alot Thank You!😗******\n",
      "\n",
      "**Context:**\n",
      "In the ever-evolving world of e-commerce, Amazon.com stands as a pioneering giant. Known for its innovative spirit and remarkable journey, Amazon has not only experienced glorious heights but also faced intriguing challenges along the way. Here are some fascinating insights:\n",
      "\n",
      "**Dataset Overview:**\n",
      "\n",
      "Now, let's delve into the dataset at hand. It comprises an extensive collection of over 2 million customer reviews and ratings of beauty-related products available on Amazon's platform. The dataset includes valuable information such as:\n",
      "\n",
      "Unique User IDs for customer identification.\n",
      "Product ASIN (Amazon's distinctive product identifier).\n",
      "Ratings, which reflect customer satisfaction on a scale from 1 to 5.\n",
      "Timestamps, recorded in UNIX time, indicating when the ratings were submitted.\n",
      "Acknowledgments:\n",
      "This dataset is just a fragment of the extensive Amazon product dataset, encompassing a staggering 142.8 million reviews spanning the period from May 1996 to July 2014. The complete dataset provides a wealth of information, including detailed product reviews, metadata, category information, pricing data, brand details, and even image features.\n",
      "\n",
      "**A Costly Downtime:**\n",
      "\n",
      "In August 2013, Amazon encountered a 40-minute website downtime, causing a notable loss of $4.8 million. This incident highlights the critical importance of maintaining a seamless online presence.\n",
      "The 1-Click Innovation:\n",
      "\n",
      "Amazon's inventive prowess is exemplified by its patent on the \"1-Click\" buying feature. This technology was not only a game-changer for Amazon but is also licensed to other tech giants, including Apple.\n",
      "Warehouses on Steroids:\n",
      "\n",
      "Amazon's Phoenix fulfillment center is a colossal structure, spanning a jaw-dropping 1.2 million square feet. It serves as a testament to the logistics marvel that powers the company's global operations.\n",
      "The Power of Recommendations:\n",
      "\n",
      "Amazon leverages a robust recommendation engine that relies on customer ratings and purchase history to provide personalized product suggestions. This engine is pivotal in enhancing customer satisfaction and driving sales.\n",
      "\n",
      "**Inspiration:**\n",
      "Now, the challenge lies in leveraging this condensed dataset to build a powerful recommendation engine. Can we tap into this data to create a recommendation system that mirrors the capabilities of Amazon's own engine? It's an exciting endeavor, and your innovative ideas and solutions are the driving force behind this exploration.\n",
      "name 'detect' is not defined Language is not detected: customer-rating-data-by-amazon\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja\n",
      "name 'detect' is not defined Language is not detected: Customer Rating Data By Amazon\n",
      "name 'detect' is not defined Language is not detected: Amazon Customer rating for recommendation system\n",
      "name 'detect' is not defined Language is not detected: **### ****! Upvote will help me alot Thank You!😗******\n",
      "\n",
      "**Context:**\n",
      "In the ever-evolving world of e-commerce, Amazon.com stands as a pioneering giant. Known for its innovative spirit and remarkable journey, Amazon has not only experienced glorious heights but also faced intriguing challenges along the way. Here are some fascinating insights:\n",
      "\n",
      "**Dataset Overview:**\n",
      "\n",
      "Now, let's delve into the dataset at hand. It comprises an extensive collection of over 2 million customer reviews and ratings of beauty-related products available on Amazon's platform. The dataset includes valuable information such as:\n",
      "\n",
      "Unique User IDs for customer identification.\n",
      "Product ASIN (Amazon's distinctive product identifier).\n",
      "Ratings, which reflect customer satisfaction on a scale from 1 to 5.\n",
      "Timestamps, recorded in UNIX time, indicating when the ratings were submitted.\n",
      "Acknowledgments:\n",
      "This dataset is just a fragment of the extensive Amazon product dataset, encompassing a staggering 142.8 million reviews spanning the period from May 1996 to July 2014. The complete dataset provides a wealth of information, including detailed product reviews, metadata, category information, pricing data, brand details, and even image features.\n",
      "\n",
      "**A Costly Downtime:**\n",
      "\n",
      "In August 2013, Amazon encountered a 40-minute website downtime, causing a notable loss of $4.8 million. This incident highlights the critical importance of maintaining a seamless online presence.\n",
      "The 1-Click Innovation:\n",
      "\n",
      "Amazon's inventive prowess is exemplified by its patent on the \"1-Click\" buying feature. This technology was not only a game-changer for Amazon but is also licensed to other tech giants, including Apple.\n",
      "Warehouses on Steroids:\n",
      "\n",
      "Amazon's Phoenix fulfillment center is a colossal structure, spanning a jaw-dropping 1.2 million square feet. It serves as a testament to the logistics marvel that powers the company's global operations.\n",
      "The Power of Recommendations:\n",
      "\n",
      "Amazon leverages a robust recommendation engine that relies on customer ratings and purchase history to provide personalized product suggestions. This engine is pivotal in enhancing customer satisfaction and driving sales.\n",
      "\n",
      "**Inspiration:**\n",
      "Now, the challenge lies in leveraging this condensed dataset to build a powerful recommendation engine. Can we tap into this data to create a recommendation system that mirrors the capabilities of Amazon's own engine? It's an exciting endeavor, and your innovative ideas and solutions are the driving force behind this exploration.\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja/e-commerece-sales-data-2023-24\n",
      "name 'detect' is not defined Language is not detected: e-commerece-sales-data-2023-24\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja\n",
      "name 'detect' is not defined Language is not detected: E-commerece Sales Data 2023-24\n",
      "name 'detect' is not defined Language is not detected: A  E-commerce Sales Dataset: User Profiles, Product Details, and interactions\n",
      "name 'detect' is not defined Language is not detected: ***😍Upvote and share this would help me alot Thank You!***\n",
      "\n",
      "**Description:**\n",
      "The E-commerce Sales Data dataset provides a comprehensive collection of information related to user profiles, product details, and user-product interactions. It is a valuable resource for understanding customer behavior, preferences, and purchasing trends on an e-commerce platform.\n",
      "\n",
      "**Dataset Structure:**\n",
      "\n",
      "User Sheet: This sheet contains user profiles, including details such as user ID, name, age, location, and other relevant information. It helps in understanding the demographics and characteristics of the platform's users.\n",
      "\n",
      "**Product Sheet:** The product sheet offers insights into the various products available on the e-commerce platform. It includes product IDs, names, categories, prices, descriptions, and other product-specific attributes.\n",
      "\n",
      "**Interactions Sheet:** The interactions sheet is a crucial component of the dataset, capturing the interactions between users and products. It records details of user actions, such as product views, purchases, reviews, and ratings. This data is essential for building recommendation systems and understanding user preferences.\n",
      "\n",
      "***Potential Use Cases:***\n",
      "\n",
      "Recommendation Systems: With the user-product interaction data, this dataset is ideal for building recommendation systems. It allows the development of personalized product recommendations to enhance the user experience.\n",
      "\n",
      "**Market Basket Analysis**: The dataset can be used for market basket analysis to understand which products are frequently purchased together, aiding in inventory management and targeted marketing.\n",
      "\n",
      "**User Behavior Analysis:** By analyzing user interactions, you can gain insights into user behavior, such as popular product categories, browsing patterns, and the impact of user reviews and ratings on purchasing decisions.\n",
      "\n",
      "**Targeted Marketing:** The dataset can inform marketing strategies, enabling businesses to tailor promotions and advertisements to specific user segments and product categories.\n",
      "\n",
      "This E-commerce Sales Data dataset is a valuable resource for e-commerce platforms and data scientists seeking to optimize the shopping experience, enhance customer satisfaction, and drive business growth through data-driven insights.\n",
      "name 'detect' is not defined Language is not detected: e-commerece-sales-data-2023-24\n",
      "name 'detect' is not defined Language is not detected: ahmedaliraja\n",
      "name 'detect' is not defined Language is not detected: E-commerece Sales Data 2023-24\n",
      "name 'detect' is not defined Language is not detected: A  E-commerce Sales Dataset: User Profiles, Product Details, and interactions\n",
      "name 'detect' is not defined Language is not detected: ***😍Upvote and share this would help me alot Thank You!***\n",
      "\n",
      "**Description:**\n",
      "The E-commerce Sales Data dataset provides a comprehensive collection of information related to user profiles, product details, and user-product interactions. It is a valuable resource for understanding customer behavior, preferences, and purchasing trends on an e-commerce platform.\n",
      "\n",
      "**Dataset Structure:**\n",
      "\n",
      "User Sheet: This sheet contains user profiles, including details such as user ID, name, age, location, and other relevant information. It helps in understanding the demographics and characteristics of the platform's users.\n",
      "\n",
      "**Product Sheet:** The product sheet offers insights into the various products available on the e-commerce platform. It includes product IDs, names, categories, prices, descriptions, and other product-specific attributes.\n",
      "\n",
      "**Interactions Sheet:** The interactions sheet is a crucial component of the dataset, capturing the interactions between users and products. It records details of user actions, such as product views, purchases, reviews, and ratings. This data is essential for building recommendation systems and understanding user preferences.\n",
      "\n",
      "***Potential Use Cases:***\n",
      "\n",
      "Recommendation Systems: With the user-product interaction data, this dataset is ideal for building recommendation systems. It allows the development of personalized product recommendations to enhance the user experience.\n",
      "\n",
      "**Market Basket Analysis**: The dataset can be used for market basket analysis to understand which products are frequently purchased together, aiding in inventory management and targeted marketing.\n",
      "\n",
      "**User Behavior Analysis:** By analyzing user interactions, you can gain insights into user behavior, such as popular product categories, browsing patterns, and the impact of user reviews and ratings on purchasing decisions.\n",
      "\n",
      "**Targeted Marketing:** The dataset can inform marketing strategies, enabling businesses to tailor promotions and advertisements to specific user segments and product categories.\n",
      "\n",
      "This E-commerce Sales Data dataset is a valuable resource for e-commerce platforms and data scientists seeking to optimize the shopping experience, enhance customer satisfaction, and drive business growth through data-driven insights.\n",
      "name 'detect' is not defined Language is not detected: govindlodhi/cradit-card-froud\n",
      "name 'detect' is not defined Language is not detected: cradit-card-froud\n",
      "name 'detect' is not defined Language is not detected: govindlodhi\n",
      "name 'detect' is not defined Language is not detected: cradit_card_froud\n",
      "name 'detect' is not defined Language is not detected: Credit Card Fraud Detection\n",
      "name 'detect' is not defined Language is not detected: cradit-card-froud\n",
      "name 'detect' is not defined Language is not detected: govindlodhi\n",
      "name 'detect' is not defined Language is not detected: cradit_card_froud\n",
      "name 'detect' is not defined Language is not detected: Credit Card Fraud Detection\n",
      "name 'detect' is not defined Language is not detected: astronautelvis/anime-recommendation\n",
      "name 'detect' is not defined Language is not detected: anime-recommendation\n",
      "name 'detect' is not defined Language is not detected: astronautelvis\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendation\n",
      "name 'detect' is not defined Language is not detected: 10000 anime and their correlated anime based on user descriptions\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset contains information about anime and recommendation of a similar anime based on their perceived similarity by users. There are similar datasets on Kaggle but this is more comprehensive. It serves to show a basic correlation in terms of plot, character designs, etc among the animes. \n",
      "\n",
      "The database is expected to be updated every 120 days. Upcoming additions :\n",
      "\n",
      "10,000 more anime entries. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains 10015x3 data fields. Column names are self-explanatory.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to:\n",
      "\n",
      "MyAnimeList for providing manga data.\n",
      "Jikan API for providing user's preference.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: anime-recommendation\n",
      "name 'detect' is not defined Language is not detected: astronautelvis\n",
      "name 'detect' is not defined Language is not detected: Anime Recommendation\n",
      "name 'detect' is not defined Language is not detected: 10000 anime and their correlated anime based on user descriptions\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset contains information about anime and recommendation of a similar anime based on their perceived similarity by users. There are similar datasets on Kaggle but this is more comprehensive. It serves to show a basic correlation in terms of plot, character designs, etc among the animes. \n",
      "\n",
      "The database is expected to be updated every 120 days. Upcoming additions :\n",
      "\n",
      "10,000 more anime entries. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains 10015x3 data fields. Column names are self-explanatory.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to:\n",
      "\n",
      "MyAnimeList for providing manga data.\n",
      "Jikan API for providing user's preference.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hemanthhari/vlsi-data\n",
      "name 'detect' is not defined Language is not detected: vlsi-data\n",
      "name 'detect' is not defined Language is not detected: hemanthhari\n",
      "name 'detect' is not defined Language is not detected: VLSI_data\n",
      "name 'detect' is not defined Language is not detected: Data Containing parameters for a Mosfet \n",
      "name 'detect' is not defined Language is not detected: The dataset contains various MOSFET properties data. The end goal is to predict the threshold characteristics and i/o characteristics for the provided parameters. \n",
      "The dataset was collected a part of multiple experiments. With the advancement in technology, it would be great if we can deploy AI to predict the characteristics.\n",
      "name 'detect' is not defined Language is not detected: vlsi-data\n",
      "name 'detect' is not defined Language is not detected: hemanthhari\n",
      "name 'detect' is not defined Language is not detected: VLSI_data\n",
      "name 'detect' is not defined Language is not detected: Data Containing parameters for a Mosfet \n",
      "name 'detect' is not defined Language is not detected: The dataset contains various MOSFET properties data. The end goal is to predict the threshold characteristics and i/o characteristics for the provided parameters. \n",
      "The dataset was collected a part of multiple experiments. With the advancement in technology, it would be great if we can deploy AI to predict the characteristics.\n",
      "name 'detect' is not defined Language is not detected: parisrohan/nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: parisrohan\n",
      "name 'detect' is not defined Language is not detected: NYC taxi trip duration\n",
      "name 'detect' is not defined Language is not detected: Taxi data like pickup/drop location, time & duration by a NYC taxi company\n",
      "name 'detect' is not defined Language is not detected: Problem Statement:\n",
      "Predict the estimated time a taxi takes to reach the entered location in New York City from the given data. \n",
      "\n",
      "Dataset shape:\n",
      "729322 rows and 11 columns\n",
      "\n",
      "Variable categorization:\n",
      "1. Continous numerical: id, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, trip_duration\n",
      "2. Discrete numerical: vendor_id, passenger_count\n",
      "3. Datetime: pickup_datetime, dropoff_datetime\n",
      "4. Nominal categorical: store_and_fwd_flag\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: parisrohan\n",
      "name 'detect' is not defined Language is not detected: NYC taxi trip duration\n",
      "name 'detect' is not defined Language is not detected: Taxi data like pickup/drop location, time & duration by a NYC taxi company\n",
      "name 'detect' is not defined Language is not detected: Problem Statement:\n",
      "Predict the estimated time a taxi takes to reach the entered location in New York City from the given data. \n",
      "\n",
      "Dataset shape:\n",
      "729322 rows and 11 columns\n",
      "\n",
      "Variable categorization:\n",
      "1. Continous numerical: id, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, trip_duration\n",
      "2. Discrete numerical: vendor_id, passenger_count\n",
      "3. Datetime: pickup_datetime, dropoff_datetime\n",
      "4. Nominal categorical: store_and_fwd_flag\n",
      "name 'detect' is not defined Language is not detected: akash14/house-price-dataset\n",
      "name 'detect' is not defined Language is not detected: house-price-dataset\n",
      "name 'detect' is not defined Language is not detected: akash14\n",
      "name 'detect' is not defined Language is not detected: House Price Dataset\n",
      "name 'detect' is not defined Language is not detected: House prices in India\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "House Price Prediction Challenge\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Welcome to the House Price Prediction Challenge, you will test your regression skills by designing an algorithm to accurately predict the house prices in India. Accurately predicting house prices can be a daunting task. The buyers are just not concerned about the size(square feet) of the house and there are various other factors that play a key role to decide the price of a house/property. It can be extremely difficult to figure out the right set of attributes that are contributing to understanding the buyer's behavior as such. This dataset has been collected across various property aggregators across India.  In this competition, provided the 12 influencing factors your role as a data scientist is to predict the prices as accurately as possible.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "From MachineHack\n",
      "\n",
      "\n",
      "### Attributes Description:\n",
      "POSTED_BY - Category marking who has listed the property\n",
      "UNDER_CONSTRUCTION - Under Construction or Not\n",
      "RERA - Rera approved or Not\n",
      "BHK_NO - Number of Rooms\n",
      "BHK_OR_RK - Type of property\n",
      "SQUARE_FT - Total area of the house in square feet\n",
      "READY_TO_MOVE - Category marking Ready to move or Not\n",
      "RESALE - Category marking Resale or not\n",
      "ADDRESS - Address of the property\n",
      "LONGITUDE - Longitude of the property\n",
      "LATITUDE - Latitude of the property\n",
      "name 'detect' is not defined Language is not detected: house-price-dataset\n",
      "name 'detect' is not defined Language is not detected: akash14\n",
      "name 'detect' is not defined Language is not detected: House Price Dataset\n",
      "name 'detect' is not defined Language is not detected: House prices in India\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "House Price Prediction Challenge\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Welcome to the House Price Prediction Challenge, you will test your regression skills by designing an algorithm to accurately predict the house prices in India. Accurately predicting house prices can be a daunting task. The buyers are just not concerned about the size(square feet) of the house and there are various other factors that play a key role to decide the price of a house/property. It can be extremely difficult to figure out the right set of attributes that are contributing to understanding the buyer's behavior as such. This dataset has been collected across various property aggregators across India.  In this competition, provided the 12 influencing factors your role as a data scientist is to predict the prices as accurately as possible.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "From MachineHack\n",
      "\n",
      "\n",
      "### Attributes Description:\n",
      "POSTED_BY - Category marking who has listed the property\n",
      "UNDER_CONSTRUCTION - Under Construction or Not\n",
      "RERA - Rera approved or Not\n",
      "BHK_NO - Number of Rooms\n",
      "BHK_OR_RK - Type of property\n",
      "SQUARE_FT - Total area of the house in square feet\n",
      "READY_TO_MOVE - Category marking Ready to move or Not\n",
      "RESALE - Category marking Resale or not\n",
      "ADDRESS - Address of the property\n",
      "LONGITUDE - Longitude of the property\n",
      "LATITUDE - Latitude of the property\n",
      "name 'detect' is not defined Language is not detected: priyanshuganwani09/electric-vehicle-population-dataset\n",
      "name 'detect' is not defined Language is not detected: electric-vehicle-population-dataset\n",
      "name 'detect' is not defined Language is not detected: priyanshuganwani09\n",
      "name 'detect' is not defined Language is not detected: Electric Vehicle Population Dataset\n",
      "name 'detect' is not defined Language is not detected: Exploring the Landscape: Insights from the Electric Vehicle Population Dataset\n",
      "name 'detect' is not defined Language is not detected: This dataset shows the Battery Electric Vehicles (BEVs) and Plug-in Hybrid Electric Vehicles (PHEVs) that are currently registered through Washington State Department of Licensing (DOL).\n",
      "The Electric Vehicle Population Dataset encompasses a wealth of features that encapsulate the multifaceted landscape of electric vehicle (EV) adoption. This comprehensive dataset includes key attributes such as vehicle types, charging infrastructure details, geographical distribution, and temporal trends, providing a holistic view of the dynamic evolution of sustainable transportation.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F9525730%2Fd62a7c302e04f78ca38db161769960f3%2FScreenshot%202024-01-29%20171537.png?generation=1706532815145212&alt=media)\n",
      "\n",
      "**Inspiration**\n",
      "\n",
      "The Electric Vehicle Population Dataset inspires sustainable transportation practices, informed policymaking, optimized infrastructure, and innovation in the automotive industry. It empowers stakeholders to understand adoption patterns, influence regulatory policies, and collaborate for a cleaner, more sustainable future.\n",
      "\n",
      "**Acknowledgment**\n",
      "\n",
      "https://catalog.data.gov/dataset/electric-vehicle-population-data\n",
      "name 'detect' is not defined Language is not detected: electric-vehicle-population-dataset\n",
      "name 'detect' is not defined Language is not detected: priyanshuganwani09\n",
      "name 'detect' is not defined Language is not detected: Electric Vehicle Population Dataset\n",
      "name 'detect' is not defined Language is not detected: Exploring the Landscape: Insights from the Electric Vehicle Population Dataset\n",
      "name 'detect' is not defined Language is not detected: This dataset shows the Battery Electric Vehicles (BEVs) and Plug-in Hybrid Electric Vehicles (PHEVs) that are currently registered through Washington State Department of Licensing (DOL).\n",
      "The Electric Vehicle Population Dataset encompasses a wealth of features that encapsulate the multifaceted landscape of electric vehicle (EV) adoption. This comprehensive dataset includes key attributes such as vehicle types, charging infrastructure details, geographical distribution, and temporal trends, providing a holistic view of the dynamic evolution of sustainable transportation.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F9525730%2Fd62a7c302e04f78ca38db161769960f3%2FScreenshot%202024-01-29%20171537.png?generation=1706532815145212&alt=media)\n",
      "\n",
      "**Inspiration**\n",
      "\n",
      "The Electric Vehicle Population Dataset inspires sustainable transportation practices, informed policymaking, optimized infrastructure, and innovation in the automotive industry. It empowers stakeholders to understand adoption patterns, influence regulatory policies, and collaborate for a cleaner, more sustainable future.\n",
      "\n",
      "**Acknowledgment**\n",
      "\n",
      "https://catalog.data.gov/dataset/electric-vehicle-population-data\n",
      "name 'detect' is not defined Language is not detected: anmolkumar/janatahack-healthcare-analytics-part-2\n",
      "name 'detect' is not defined Language is not detected: janatahack-healthcare-analytics-part-2\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Healthcare Analytics Part 2\n",
      "name 'detect' is not defined Language is not detected: Predict length of stay for the patient in Covid crisis\n",
      "name 'detect' is not defined Language is not detected: [Janatahack: Healthcare Analytics II](https://datahack.analyticsvidhya.com/contest/janatahack-healthcare-analytics-ii/#ProblemStatement)\n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "Recent Covid-19 Pandemic has raised alarms over one of the most overlooked area to focus: Healthcare Management. While healthcare management has various use cases for using data science, patient length of stay is one critical parameter to observe and predict if one wants to improve the efficiency of the healthcare management in a hospital. \n",
      "\n",
      "This parameter helps hospitals to identify patients of high LOS risk (patients who will stay longer) at the time of admission. Once identified, patients with high LOS risk can have their treatment plan optimized to miminize LOS and lower the chance of staff/visitor infection. Also, prior knowledge of LOS can aid in logistics such as room and bed allocation planning.\n",
      "\n",
      "Suppose you have been hired as Data Scientist of HealthMan – a not for profit organization dedicated to manage the functioning of Hospitals in a professional and optimal manner.\n",
      "The task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.\n",
      "\n",
      " \n",
      "\n",
      "## Data Description\n",
      "\n",
      "Train.zip contains 1 csv alongside the data dictionary that contains definitions for each variable\n",
      "\n",
      "train.csv – File containing features related to patient, hospital and Length of stay on case basis\n",
      "\n",
      "train_data_dict.csv – File containing the information of the features in train file\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "\n",
      "test.csv – File containing features related to patient, hospital. Need to predict the Length of stay for each case_id\n",
      "\n",
      "\n",
      "\n",
      "## Sample Submission:\n",
      "\n",
      "case_id: Unique id for each case\n",
      "\n",
      "Stay: Length of stay for the patient w.r.t each case id in test data\n",
      "name 'detect' is not defined Language is not detected: janatahack-healthcare-analytics-part-2\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Healthcare Analytics Part 2\n",
      "name 'detect' is not defined Language is not detected: Predict length of stay for the patient in Covid crisis\n",
      "name 'detect' is not defined Language is not detected: [Janatahack: Healthcare Analytics II](https://datahack.analyticsvidhya.com/contest/janatahack-healthcare-analytics-ii/#ProblemStatement)\n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "Recent Covid-19 Pandemic has raised alarms over one of the most overlooked area to focus: Healthcare Management. While healthcare management has various use cases for using data science, patient length of stay is one critical parameter to observe and predict if one wants to improve the efficiency of the healthcare management in a hospital. \n",
      "\n",
      "This parameter helps hospitals to identify patients of high LOS risk (patients who will stay longer) at the time of admission. Once identified, patients with high LOS risk can have their treatment plan optimized to miminize LOS and lower the chance of staff/visitor infection. Also, prior knowledge of LOS can aid in logistics such as room and bed allocation planning.\n",
      "\n",
      "Suppose you have been hired as Data Scientist of HealthMan – a not for profit organization dedicated to manage the functioning of Hospitals in a professional and optimal manner.\n",
      "The task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.\n",
      "\n",
      " \n",
      "\n",
      "## Data Description\n",
      "\n",
      "Train.zip contains 1 csv alongside the data dictionary that contains definitions for each variable\n",
      "\n",
      "train.csv – File containing features related to patient, hospital and Length of stay on case basis\n",
      "\n",
      "train_data_dict.csv – File containing the information of the features in train file\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "\n",
      "test.csv – File containing features related to patient, hospital. Need to predict the Length of stay for each case_id\n",
      "\n",
      "\n",
      "\n",
      "## Sample Submission:\n",
      "\n",
      "case_id: Unique id for each case\n",
      "\n",
      "Stay: Length of stay for the patient w.r.t each case id in test data\n",
      "name 'detect' is not defined Language is not detected: anmolkumar/house-price-prediction-challenge\n",
      "name 'detect' is not defined Language is not detected: house-price-prediction-challenge\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: House Price Prediction Challenge\n",
      "name 'detect' is not defined Language is not detected: Predict the house prices in India\n",
      "name 'detect' is not defined Language is not detected: # House Price Prediction Challenge\n",
      "\n",
      "## Overview\n",
      "\n",
      "Welcome to the House Price Prediction Challenge, you will test your regression skills by designing an algorithm to accurately predict the house prices in India. Accurately predicting house prices can be a daunting task. The buyers are just not concerned about the size(square feet) of the house and there are various other factors that play a key role to decide the price of a house/property. It can be extremely difficult to figure out the right set of attributes that are contributing to understanding the buyer's behavior as such. This dataset has been collected across various property aggregators across India.  In this competition, provided the 12 influencing factors your role as a data scientist is to predict the prices as accurately as possible.\n",
      "\n",
      "Also, in this competition, you will get a lot of room for feature engineering and mastering advanced regression techniques such as Random Forest, Deep Neural Nets, and various other ensembling techniques. \n",
      "\n",
      "## Data Description:\n",
      "Train.csv - 29451 rows x 12 columns\n",
      "Test.csv - 68720 rows x 11 columns\n",
      "Sample Submission - Acceptable submission format. (.csv/.xlsx file with 68720 rows) \n",
      "\n",
      "## Attributes Description:\n",
      "|Column  |  Description|\n",
      "| --- | --- |\n",
      "|POSTED_BY| Category marking who has listed the property|\n",
      "|UNDER_CONSTRUCTION | Under Construction or Not|\n",
      "|RERA | Rera approved or Not|\n",
      "|BHK_NO | Number of Rooms|\n",
      "|BHK_OR_RK | Type of property|\n",
      "|SQUARE_FT | Total area of the house in square feet|\n",
      "|READY_TO_MOVE|  Category marking Ready to move or Not|\n",
      "|RESALE | Category marking Resale or not|\n",
      "|ADDRESS | Address of the property|\n",
      "|LONGITUDE | Longitude of the property|\n",
      "|LATITUDE | Latitude of the property|\n",
      " \n",
      "## ACKNOWLEDGMENT:\n",
      "The dataset for this hackathon was contributed by [Devrup Banerjee](https://www.machinehack.com/user/profile/5ef0a238b7efcc325e390297) . We would like to appreciate his efforts for this contribution to the Machinehack community. \n",
      "name 'detect' is not defined Language is not detected: house-price-prediction-challenge\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: House Price Prediction Challenge\n",
      "name 'detect' is not defined Language is not detected: Predict the house prices in India\n",
      "name 'detect' is not defined Language is not detected: # House Price Prediction Challenge\n",
      "\n",
      "## Overview\n",
      "\n",
      "Welcome to the House Price Prediction Challenge, you will test your regression skills by designing an algorithm to accurately predict the house prices in India. Accurately predicting house prices can be a daunting task. The buyers are just not concerned about the size(square feet) of the house and there are various other factors that play a key role to decide the price of a house/property. It can be extremely difficult to figure out the right set of attributes that are contributing to understanding the buyer's behavior as such. This dataset has been collected across various property aggregators across India.  In this competition, provided the 12 influencing factors your role as a data scientist is to predict the prices as accurately as possible.\n",
      "\n",
      "Also, in this competition, you will get a lot of room for feature engineering and mastering advanced regression techniques such as Random Forest, Deep Neural Nets, and various other ensembling techniques. \n",
      "\n",
      "## Data Description:\n",
      "Train.csv - 29451 rows x 12 columns\n",
      "Test.csv - 68720 rows x 11 columns\n",
      "Sample Submission - Acceptable submission format. (.csv/.xlsx file with 68720 rows) \n",
      "\n",
      "## Attributes Description:\n",
      "|Column  |  Description|\n",
      "| --- | --- |\n",
      "|POSTED_BY| Category marking who has listed the property|\n",
      "|UNDER_CONSTRUCTION | Under Construction or Not|\n",
      "|RERA | Rera approved or Not|\n",
      "|BHK_NO | Number of Rooms|\n",
      "|BHK_OR_RK | Type of property|\n",
      "|SQUARE_FT | Total area of the house in square feet|\n",
      "|READY_TO_MOVE|  Category marking Ready to move or Not|\n",
      "|RESALE | Category marking Resale or not|\n",
      "|ADDRESS | Address of the property|\n",
      "|LONGITUDE | Longitude of the property|\n",
      "|LATITUDE | Latitude of the property|\n",
      " \n",
      "## ACKNOWLEDGMENT:\n",
      "The dataset for this hackathon was contributed by [Devrup Banerjee](https://www.machinehack.com/user/profile/5ef0a238b7efcc325e390297) . We would like to appreciate his efforts for this contribution to the Machinehack community. \n",
      "name 'detect' is not defined Language is not detected: anmolkumar/machine-hack-melanoma-tumor-size-prediction\n",
      "name 'detect' is not defined Language is not detected: machine-hack-melanoma-tumor-size-prediction\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Machine Hack: Melanoma Tumor Size Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict the melanoma cancer tumor size using machine learning\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Welcome to our regular closed dataset based weekend hackathon. In this weekend hackathon, we are challenging all the machinehackers to predict the melanoma tumor size based on various attributes. Melanomas present in many different shapes, sizes, and colors. That’s why it’s tricky to provide a comprehensive set of warning signs. Melanoma, also known as malignant melanoma, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. \n",
      "\n",
      "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, there were 3.1 million people with active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. There are also high rates in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States melanoma occurs about 1.6 times more often in men than women.\n",
      "\n",
      "### Content\n",
      "\n",
      "Train.csv - 9146 rows x 9 columns\n",
      "Test.csv - 36584 rows x 8 columns\n",
      "Sample Submission - Acceptable submission format\n",
      "\n",
      "### Attributes\n",
      "\n",
      "| Attributes |Description  |\n",
      "| --- | --- |\n",
      "|mass_npea|  the mass of the area understudy for melanoma tumor|\n",
      "|size_npear| the size of the area understudy for melanoma tumor|\n",
      "|malign_ratio| ration of normal to malign surface understudy|\n",
      "|damage_size| unrecoverable area of skin damaged by the tumor|\n",
      "|exposed_area| total area exposed to the tumor|\n",
      "|std_dev_malign| standard deviation of malign skin measurements|\n",
      "|err_malign| error in malign skin measurements|\n",
      "|malign_penalty| penalty applied due to measurement error in the lab|\n",
      "|damage_ratio| the ratio of damage to total spread on the skin|\n",
      "|tumor_size| size of melanoma_tumor|\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "[Machine Hack: Melanoma Tumor Size Prediction](https://www.machinehack.com/hackathons/melanoma_tumor_size_prediction_weekend_hackathon_15)\n",
      "name 'detect' is not defined Language is not detected: machine-hack-melanoma-tumor-size-prediction\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Machine Hack: Melanoma Tumor Size Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict the melanoma cancer tumor size using machine learning\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Welcome to our regular closed dataset based weekend hackathon. In this weekend hackathon, we are challenging all the machinehackers to predict the melanoma tumor size based on various attributes. Melanomas present in many different shapes, sizes, and colors. That’s why it’s tricky to provide a comprehensive set of warning signs. Melanoma, also known as malignant melanoma, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. \n",
      "\n",
      "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, there were 3.1 million people with active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. There are also high rates in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States melanoma occurs about 1.6 times more often in men than women.\n",
      "\n",
      "### Content\n",
      "\n",
      "Train.csv - 9146 rows x 9 columns\n",
      "Test.csv - 36584 rows x 8 columns\n",
      "Sample Submission - Acceptable submission format\n",
      "\n",
      "### Attributes\n",
      "\n",
      "| Attributes |Description  |\n",
      "| --- | --- |\n",
      "|mass_npea|  the mass of the area understudy for melanoma tumor|\n",
      "|size_npear| the size of the area understudy for melanoma tumor|\n",
      "|malign_ratio| ration of normal to malign surface understudy|\n",
      "|damage_size| unrecoverable area of skin damaged by the tumor|\n",
      "|exposed_area| total area exposed to the tumor|\n",
      "|std_dev_malign| standard deviation of malign skin measurements|\n",
      "|err_malign| error in malign skin measurements|\n",
      "|malign_penalty| penalty applied due to measurement error in the lab|\n",
      "|damage_ratio| the ratio of damage to total spread on the skin|\n",
      "|tumor_size| size of melanoma_tumor|\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "[Machine Hack: Melanoma Tumor Size Prediction](https://www.machinehack.com/hackathons/melanoma_tumor_size_prediction_weekend_hackathon_15)\n",
      "name 'detect' is not defined Language is not detected: anmolkumar/topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles 2.0\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya - HackLive 3: Guided Hackathon - NLP\n",
      "name 'detect' is not defined Language is not detected: # Analytics Vidhya - Topic Modeling for Research Articles 2.0\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "2. Mathematics\n",
      "3. Physics\n",
      "4. Statistics\n",
      "\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      " \n",
      "# Data Dictionary \n",
      "\n",
      "- train.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|ABSTRACT\t|Abstract of the research article|\n",
      "|Computer Science|Whether article belongs to topic computer science (1/0)|\n",
      "|Mathematics\t|Whether article belongs to topic Mathematics (1/0)|\n",
      "|Physics\t|Whether article belongs to topic physics (1/0)|\n",
      "|Statistics|\tWhether article belongs to topic Statistics (1/0)|\n",
      "|Tags\t(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|\n",
      "\n",
      "- test.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|ABSTRACT\t|Abstract of the research article|\n",
      "|Computer Science|Whether article belongs to topic computer science (1/0)|\n",
      "|Mathematics\t|Whether article belongs to topic Mathematics (1/0)|\n",
      "|Physics\t|Whether article belongs to topic physics (1/0)|\n",
      "|Statistics|\tWhether article belongs to topic Statistics (1/0)|\n",
      "\n",
      "- sample_submission.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|Tags\t(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|\n",
      "\n",
      "# Evaluation Metric\n",
      "Submissions are evaluated on micro F1 Score between the predicted and observed tags for each article in the test set\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles-20\n",
      "name 'detect' is not defined Language is not detected: anmolkumar\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles 2.0\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya - HackLive 3: Guided Hackathon - NLP\n",
      "name 'detect' is not defined Language is not detected: # Analytics Vidhya - Topic Modeling for Research Articles 2.0\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "2. Mathematics\n",
      "3. Physics\n",
      "4. Statistics\n",
      "\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      " \n",
      "# Data Dictionary \n",
      "\n",
      "- train.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|ABSTRACT\t|Abstract of the research article|\n",
      "|Computer Science|Whether article belongs to topic computer science (1/0)|\n",
      "|Mathematics\t|Whether article belongs to topic Mathematics (1/0)|\n",
      "|Physics\t|Whether article belongs to topic physics (1/0)|\n",
      "|Statistics|\tWhether article belongs to topic Statistics (1/0)|\n",
      "|Tags\t(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|\n",
      "\n",
      "- test.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|ABSTRACT\t|Abstract of the research article|\n",
      "|Computer Science|Whether article belongs to topic computer science (1/0)|\n",
      "|Mathematics\t|Whether article belongs to topic Mathematics (1/0)|\n",
      "|Physics\t|Whether article belongs to topic physics (1/0)|\n",
      "|Statistics|\tWhether article belongs to topic Statistics (1/0)|\n",
      "\n",
      "- sample_submission.csv\n",
      "\n",
      "| Column |Description  |\n",
      "| --- | --- |\n",
      "|id| Unique ID for each article|\n",
      "|Tags\t(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|\n",
      "\n",
      "# Evaluation Metric\n",
      "Submissions are evaluated on micro F1 Score between the predicted and observed tags for each article in the test set\n",
      "name 'detect' is not defined Language is not detected: mcandocia/running-heart-rate-recovery\n",
      "name 'detect' is not defined Language is not detected: running-heart-rate-recovery\n",
      "name 'detect' is not defined Language is not detected: mcandocia\n",
      "name 'detect' is not defined Language is not detected: Running and Heart Rate Data\n",
      "name 'detect' is not defined Language is not detected: Running and Heart rate summary, stop, start and lap event data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I have been collecting GPS, heart rate, and temperature data from my runs over the past year. This data corresponds to the stopwatch stop & start events.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Each row contains data from when I stopped my watch and started it again, as well as some summarized data over various time windows before the stop, as well as a few variables describing the overall run (e.g., average temperature, average longitude/latitude).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I wanted to see how my heart rate dropped based on various variables. I've used nonlinear regression to get a decent model in terms of heart rate, rest time, and temperature, but it seems to overestimate heart rate recovery for higher temperatures in more intense runs.\n",
      "name 'detect' is not defined Language is not detected: running-heart-rate-recovery\n",
      "name 'detect' is not defined Language is not detected: mcandocia\n",
      "name 'detect' is not defined Language is not detected: Running and Heart Rate Data\n",
      "name 'detect' is not defined Language is not detected: Running and Heart rate summary, stop, start and lap event data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "I have been collecting GPS, heart rate, and temperature data from my runs over the past year. This data corresponds to the stopwatch stop & start events.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Each row contains data from when I stopped my watch and started it again, as well as some summarized data over various time windows before the stop, as well as a few variables describing the overall run (e.g., average temperature, average longitude/latitude).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "I wanted to see how my heart rate dropped based on various variables. I've used nonlinear regression to get a decent model in terms of heart rate, rest time, and temperature, but it seems to overestimate heart rate recovery for higher temperatures in more intense runs.\n",
      "name 'detect' is not defined Language is not detected: sumuduchamika/movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sumuduchamika\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation System\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: sumuduchamika\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation System\n",
      "name 'detect' is not defined Language is not detected: meastanmay/imdb-dataset\n",
      "name 'detect' is not defined Language is not detected: imdb-dataset\n",
      "name 'detect' is not defined Language is not detected: meastanmay\n",
      "name 'detect' is not defined Language is not detected: IMDB Dataset\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation System\n",
      "name 'detect' is not defined Language is not detected: Recommender System is a system that seeks to predict or filter preferences according to the user’s choices. Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. \n",
      "\n",
      "Recommender systems produce a list of recommendations in any of the two ways: \n",
      " \n",
      "\n",
      "**1. Collaborative filtering:** Collaborative filtering approaches build a model from the user’s past behavior (i.e. items purchased or searched by the user) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that users may have an interest in.\n",
      "**2. Content-based filtering:** Content-based filtering approaches uses a series of discrete characteristics of an item in order to recommend additional items with similar properties. Content-based filtering methods are totally based on a description of the item and a profile of the user’s preferences. It recommends items based on the user’s past preferences.\n",
      "\n",
      "Let’s develop a basic recommendation system by suggesting items that are most similar to a particular item, in this case, movies. It just tells what movies/items are most similar to the user’s movie choice.\n",
      "name 'detect' is not defined Language is not detected: imdb-dataset\n",
      "name 'detect' is not defined Language is not detected: meastanmay\n",
      "name 'detect' is not defined Language is not detected: IMDB Dataset\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation System\n",
      "name 'detect' is not defined Language is not detected: Recommender System is a system that seeks to predict or filter preferences according to the user’s choices. Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. \n",
      "\n",
      "Recommender systems produce a list of recommendations in any of the two ways: \n",
      " \n",
      "\n",
      "**1. Collaborative filtering:** Collaborative filtering approaches build a model from the user’s past behavior (i.e. items purchased or searched by the user) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that users may have an interest in.\n",
      "**2. Content-based filtering:** Content-based filtering approaches uses a series of discrete characteristics of an item in order to recommend additional items with similar properties. Content-based filtering methods are totally based on a description of the item and a profile of the user’s preferences. It recommends items based on the user’s past preferences.\n",
      "\n",
      "Let’s develop a basic recommendation system by suggesting items that are most similar to a particular item, in this case, movies. It just tells what movies/items are most similar to the user’s movie choice.\n",
      "name 'detect' is not defined Language is not detected: bhavyarajdev/indian-railways-schedule-prices-availability-data\n",
      "name 'detect' is not defined Language is not detected: indian-railways-schedule-prices-availability-data\n",
      "name 'detect' is not defined Language is not detected: bhavyarajdev\n",
      "name 'detect' is not defined Language is not detected: Indian Railways Schedule-Prices-Availability Data\n",
      "name 'detect' is not defined Language is not detected: Comprehensive Dataset Featuring Public Trains Scrapped from IRCTC in 10-2023\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "This dataset originates from my project that aims to improve the accessibility of train travel in India. The project helps travelers find and book indirect train journeys, which can be useful for long or remote trips where direct trains are scarce. The dataset is based on this project and covers the gap left by other platforms that only show direct trains, which may not suit some travelers' needs.\n",
      "\n",
      "**Sources**\n",
      "\n",
      "The primary source of this dataset is the Indian Railways Catering and Tourism Corporation (IRCTC) website during October-2023. Through web scraping techniques, data was collected, including public train schedules, pricing details, and availability information.\n",
      "\n",
      "**Project on GitHub**\n",
      "\n",
      "The Project Report on my [GitHub Repository](https://github.com/bhavya2403/TrainTripper/) explains how I analyzed the prices and their dependencies, and why I chose decision tree regression as the best ML model for this case. You can find all the code regarding the data collection, model building and final output generation in the repository. Users here can perform time-series analysis on the prices and availability data, improve the accuracy of price prediction and create a model for availability prediction.\n",
      "name 'detect' is not defined Language is not detected: indian-railways-schedule-prices-availability-data\n",
      "name 'detect' is not defined Language is not detected: bhavyarajdev\n",
      "name 'detect' is not defined Language is not detected: Indian Railways Schedule-Prices-Availability Data\n",
      "name 'detect' is not defined Language is not detected: Comprehensive Dataset Featuring Public Trains Scrapped from IRCTC in 10-2023\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "This dataset originates from my project that aims to improve the accessibility of train travel in India. The project helps travelers find and book indirect train journeys, which can be useful for long or remote trips where direct trains are scarce. The dataset is based on this project and covers the gap left by other platforms that only show direct trains, which may not suit some travelers' needs.\n",
      "\n",
      "**Sources**\n",
      "\n",
      "The primary source of this dataset is the Indian Railways Catering and Tourism Corporation (IRCTC) website during October-2023. Through web scraping techniques, data was collected, including public train schedules, pricing details, and availability information.\n",
      "\n",
      "**Project on GitHub**\n",
      "\n",
      "The Project Report on my [GitHub Repository](https://github.com/bhavya2403/TrainTripper/) explains how I analyzed the prices and their dependencies, and why I chose decision tree regression as the best ML model for this case. You can find all the code regarding the data collection, model building and final output generation in the repository. Users here can perform time-series analysis on the prices and availability data, improve the accuracy of price prediction and create a model for availability prediction.\n",
      "name 'detect' is not defined Language is not detected: qasimhassan/reducing-the-number-of-high-fatality-accidents\n",
      "name 'detect' is not defined Language is not detected: reducing-the-number-of-high-fatality-accidents\n",
      "name 'detect' is not defined Language is not detected: qasimhassan\n",
      "name 'detect' is not defined Language is not detected: Road Satefy Data\n",
      "name 'detect' is not defined Language is not detected: Reducing the number of high fatality accidents\n",
      "name 'detect' is not defined Language is not detected: ## 📖 Background\n",
      "You work for the road safety team within the department of transport and are looking into how they can reduce the number of major incidents. The safety team classes major incidents as fatal accidents involving 3+ casualties. They are trying to learn more about the characteristics of these major incidents so they can brainstorm interventions that could lower the number of deaths. They have asked for your assistance with answering a number of questions.\n",
      "\n",
      "## 💪 Competition challenge\n",
      "\n",
      "Create a report that covers the following:\n",
      "\n",
      "1. What time of day and day of the week do most major incidents happen?\n",
      "2. Are there any patterns in the time of day/ day of the week when major incidents occur?\n",
      "3. What characteristics stand out in major incidents compared with other accidents?\n",
      "4. On what areas would you recommend the planning team focus their brainstorming efforts to reduce major incidents?\n",
      "\n",
      "## 🧑‍⚖️ Judging criteria\n",
      "\n",
      "| CATEGORY | WEIGHTING | DETAILS                                                              |\n",
      "|:---------|:----------|:---------------------------------------------------------------------|\n",
      "| **Recommendations** | 35%       | <ul><li>Clarity of recommendations - how clear and well presented the recommendation is.</li><li>Quality of recommendations - are appropriate analytical techniques used & are the conclusions valid?</li><li>Number of relevant insights found for the target audience.</li></ul>       |\n",
      "| **Storytelling**  | 30%       | <ul><li>How well the data and insights are connected to the recommendation.</li><li>How the narrative and whole report connects together.</li><li>Balancing making the report in depth enough but also concise.</li></ul> |\n",
      "| **Visualizations** | 25% | <ul><li>Appropriateness of visualization used.</li><li>Clarity of insight from visualization.</li></ul> |\n",
      "| **Votes** | 10% | <ul><li>Up voting - most upvoted entries get the most points.</li></ul> |\n",
      "\n",
      "## ✅ Checklist before publishing into the competition\n",
      "- Remove redundant cells like the judging criteria so the workbook is focused on your story.\n",
      "- Make sure the workbook reads well and explains how you found your insights.\n",
      "- Check that all the cells run without error.\n",
      "\n",
      "## ⌛️ Time is ticking. Good luck!\n",
      "name 'detect' is not defined Language is not detected: reducing-the-number-of-high-fatality-accidents\n",
      "name 'detect' is not defined Language is not detected: qasimhassan\n",
      "name 'detect' is not defined Language is not detected: Road Satefy Data\n",
      "name 'detect' is not defined Language is not detected: Reducing the number of high fatality accidents\n",
      "name 'detect' is not defined Language is not detected: ## 📖 Background\n",
      "You work for the road safety team within the department of transport and are looking into how they can reduce the number of major incidents. The safety team classes major incidents as fatal accidents involving 3+ casualties. They are trying to learn more about the characteristics of these major incidents so they can brainstorm interventions that could lower the number of deaths. They have asked for your assistance with answering a number of questions.\n",
      "\n",
      "## 💪 Competition challenge\n",
      "\n",
      "Create a report that covers the following:\n",
      "\n",
      "1. What time of day and day of the week do most major incidents happen?\n",
      "2. Are there any patterns in the time of day/ day of the week when major incidents occur?\n",
      "3. What characteristics stand out in major incidents compared with other accidents?\n",
      "4. On what areas would you recommend the planning team focus their brainstorming efforts to reduce major incidents?\n",
      "\n",
      "## 🧑‍⚖️ Judging criteria\n",
      "\n",
      "| CATEGORY | WEIGHTING | DETAILS                                                              |\n",
      "|:---------|:----------|:---------------------------------------------------------------------|\n",
      "| **Recommendations** | 35%       | <ul><li>Clarity of recommendations - how clear and well presented the recommendation is.</li><li>Quality of recommendations - are appropriate analytical techniques used & are the conclusions valid?</li><li>Number of relevant insights found for the target audience.</li></ul>       |\n",
      "| **Storytelling**  | 30%       | <ul><li>How well the data and insights are connected to the recommendation.</li><li>How the narrative and whole report connects together.</li><li>Balancing making the report in depth enough but also concise.</li></ul> |\n",
      "| **Visualizations** | 25% | <ul><li>Appropriateness of visualization used.</li><li>Clarity of insight from visualization.</li></ul> |\n",
      "| **Votes** | 10% | <ul><li>Up voting - most upvoted entries get the most points.</li></ul> |\n",
      "\n",
      "## ✅ Checklist before publishing into the competition\n",
      "- Remove redundant cells like the judging criteria so the workbook is focused on your story.\n",
      "- Make sure the workbook reads well and explains how you found your insights.\n",
      "- Check that all the cells run without error.\n",
      "\n",
      "## ⌛️ Time is ticking. Good luck!\n",
      "name 'detect' is not defined Language is not detected: rutvijesdiya/social-media\n",
      "name 'detect' is not defined Language is not detected: social-media\n",
      "name 'detect' is not defined Language is not detected: rutvijesdiya\n",
      "name 'detect' is not defined Language is not detected: Social_Media\n",
      "name 'detect' is not defined Language is not detected: A robust recommendation system analyzes user preferences to suggest post to user\n",
      "name 'detect' is not defined Language is not detected: # **Recommendation System**\n",
      "**Identify users who might be interested in a particular post by analyzing the likes of similar users.**\n",
      "\n",
      "\n",
      "This dataset contain **likes_challenge, posts_challenge, users_challenge** and **testing _users** csv files.\n",
      "\n",
      "\n",
      "**1) post_challenge:** posts_challenge: This file contains information about posts, with columns for user_id (the user who posted the post) and post_id (the ID of the posted post).\n",
      "\n",
      "**2) users_challenge:**  This file contains information about users, with columns for user_name (the name of the user who posted a post) and user_id (the ID corresponding to the user's name).\n",
      "\n",
      "**3) likes_challange:**This file contains data about likes on posts, with columns for user_id (the ID of the user who liked a post) and post_id (the ID of the post that was liked by the user).\n",
      "\n",
      "**4) testing_users:**This file contains data about users, with columns for user_id and user_name, presumably for testing purposes. \n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: social-media\n",
      "name 'detect' is not defined Language is not detected: rutvijesdiya\n",
      "name 'detect' is not defined Language is not detected: Social_Media\n",
      "name 'detect' is not defined Language is not detected: A robust recommendation system analyzes user preferences to suggest post to user\n",
      "name 'detect' is not defined Language is not detected: # **Recommendation System**\n",
      "**Identify users who might be interested in a particular post by analyzing the likes of similar users.**\n",
      "\n",
      "\n",
      "This dataset contain **likes_challenge, posts_challenge, users_challenge** and **testing _users** csv files.\n",
      "\n",
      "\n",
      "**1) post_challenge:** posts_challenge: This file contains information about posts, with columns for user_id (the user who posted the post) and post_id (the ID of the posted post).\n",
      "\n",
      "**2) users_challenge:**  This file contains information about users, with columns for user_name (the name of the user who posted a post) and user_id (the ID corresponding to the user's name).\n",
      "\n",
      "**3) likes_challange:**This file contains data about likes on posts, with columns for user_id (the ID of the user who liked a post) and post_id (the ID of the post that was liked by the user).\n",
      "\n",
      "**4) testing_users:**This file contains data about users, with columns for user_id and user_name, presumably for testing purposes. \n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: simaydl1294/movielense-100k\n",
      "name 'detect' is not defined Language is not detected: movielense-100k\n",
      "name 'detect' is not defined Language is not detected: simaydl1294\n",
      "name 'detect' is not defined Language is not detected: movielens_100k\n",
      "name 'detect' is not defined Language is not detected: This dataset is a classic benchmark for collaborative filtering and recommendation systems, containing user ratings for a collection of movies.\n",
      "    Number of users: 943\n",
      "    Number of movies: 1682\n",
      "    Number of ratings: 100,000\n",
      "    Number of genres: 19\n",
      "name 'detect' is not defined Language is not detected: movielense-100k\n",
      "name 'detect' is not defined Language is not detected: simaydl1294\n",
      "name 'detect' is not defined Language is not detected: movielens_100k\n",
      "name 'detect' is not defined Language is not detected: This dataset is a classic benchmark for collaborative filtering and recommendation systems, containing user ratings for a collection of movies.\n",
      "    Number of users: 943\n",
      "    Number of movies: 1682\n",
      "    Number of ratings: 100,000\n",
      "    Number of genres: 19\n",
      "name 'detect' is not defined Language is not detected: rafpast/rob-burbea-all-teachings-embedded\n",
      "name 'detect' is not defined Language is not detected: rob-burbea-all-teachings-embedded\n",
      "name 'detect' is not defined Language is not detected: rafpast\n",
      "name 'detect' is not defined Language is not detected: Rob Burbea all teachings embedded\n",
      "name 'detect' is not defined Language is not detected: All teachings from 2005-2012, 2013-2020 embedded with multi-qa-MiniLM-L6-cos-v1\n",
      "name 'detect' is not defined Language is not detected: **Background**\n",
      "This dataset contains transcriptions and embeddings of Rob Burbea's teachings, courtesy of the Hermes Amara Foundation. The source text is available in the following Airtable: [Hermes Amara Foundation - Rob Burbea Transcripts](https://airtable.com/shr9OS6jqmWvWTG5g/tblHlCKWIIhZzEFMk/viw3k0IfSo0Dve9ZJ).\n",
      "\n",
      "Information about Rob's biography can be found on [dharmaseed.org](https://dharmaseed.org/teacher/210/).\n",
      "\n",
      "More about Hermes Amara Foundation [here](https://hermesamara.org/).\n",
      "\n",
      "**Technical details**\n",
      "The dataset was produced with [ALP](https://github.com/rpast/ALP) - a knowledge-grounded conversational system. It is a Python Flask-based chatbot app that I use to 'talk with' Rob's teachings.\n",
      "Teaching transcripts got embedded with _multi-qa-MiniLM-L6-cos-v1_. Here you can find details of the model: https://www.sbert.net/docs/pretrained_models.html\n",
      "Each source page got split by token count so each text snippet does not exceed 500 token count so they easily fit BERT based token limit.\n",
      "Embeddings are serialized with Python's pickle module. Use pickle.load to deserialize.\n",
      "\n",
      "**Usage**\n",
      "You can use this dataset for following tasks:\n",
      "* Semantic search over Rob's teachings: find relevant sources to your specific question.\n",
      "* Chatbot Development: Use the teachings as a knowledge base to build a chatbot that can provide responses based on Rob's philosophy and teachings. See implementation example in Python: [ALP](https://github.com/rpast/ALP).\n",
      "* Knowledge Graphs: Generate a knowledge graph of Rob's teachings, linking various topics, themes, and entities together to create a comprehensive visual depiction of his discourse.\n",
      "* Teaching Recommendation Systems: Based on user's past reading and interest, recommend similar teachings by Rob.\n",
      "etc.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: rob-burbea-all-teachings-embedded\n",
      "name 'detect' is not defined Language is not detected: rafpast\n",
      "name 'detect' is not defined Language is not detected: Rob Burbea all teachings embedded\n",
      "name 'detect' is not defined Language is not detected: All teachings from 2005-2012, 2013-2020 embedded with multi-qa-MiniLM-L6-cos-v1\n",
      "name 'detect' is not defined Language is not detected: **Background**\n",
      "This dataset contains transcriptions and embeddings of Rob Burbea's teachings, courtesy of the Hermes Amara Foundation. The source text is available in the following Airtable: [Hermes Amara Foundation - Rob Burbea Transcripts](https://airtable.com/shr9OS6jqmWvWTG5g/tblHlCKWIIhZzEFMk/viw3k0IfSo0Dve9ZJ).\n",
      "\n",
      "Information about Rob's biography can be found on [dharmaseed.org](https://dharmaseed.org/teacher/210/).\n",
      "\n",
      "More about Hermes Amara Foundation [here](https://hermesamara.org/).\n",
      "\n",
      "**Technical details**\n",
      "The dataset was produced with [ALP](https://github.com/rpast/ALP) - a knowledge-grounded conversational system. It is a Python Flask-based chatbot app that I use to 'talk with' Rob's teachings.\n",
      "Teaching transcripts got embedded with _multi-qa-MiniLM-L6-cos-v1_. Here you can find details of the model: https://www.sbert.net/docs/pretrained_models.html\n",
      "Each source page got split by token count so each text snippet does not exceed 500 token count so they easily fit BERT based token limit.\n",
      "Embeddings are serialized with Python's pickle module. Use pickle.load to deserialize.\n",
      "\n",
      "**Usage**\n",
      "You can use this dataset for following tasks:\n",
      "* Semantic search over Rob's teachings: find relevant sources to your specific question.\n",
      "* Chatbot Development: Use the teachings as a knowledge base to build a chatbot that can provide responses based on Rob's philosophy and teachings. See implementation example in Python: [ALP](https://github.com/rpast/ALP).\n",
      "* Knowledge Graphs: Generate a knowledge graph of Rob's teachings, linking various topics, themes, and entities together to create a comprehensive visual depiction of his discourse.\n",
      "* Teaching Recommendation Systems: Based on user's past reading and interest, recommend similar teachings by Rob.\n",
      "etc.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ahmedashrafahmed/household-power-consumption\n",
      "name 'detect' is not defined Language is not detected: household-power-consumption\n",
      "name 'detect' is not defined Language is not detected: ahmedashrafahmed\n",
      "name 'detect' is not defined Language is not detected: Household_power_consumption\n",
      "name 'detect' is not defined Language is not detected: I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file.\n",
      "Thanks in advance :)\n",
      "\n",
      "Context: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "Data Set Characteristics:\n",
      "Multivariate, Time-Series\n",
      "\n",
      "Associated Tasks:\n",
      "Regression, Clustering\n",
      "\n",
      "Data Set Information:\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\n",
      "Notes:\n",
      "1.(globalactivepower*1000/60 - submetering1 - submetering2 - submetering3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "Attribute Information:\n",
      "1.date: Date in format dd/mm/yyyy\n",
      "\n",
      "2.time: time in format hh:mm:ss\n",
      "\n",
      "3.globalactivepower: household global minute-averaged active power (in kilowatt)\n",
      "\n",
      "4.globalreactivepower: household global minute-averaged reactive power (in kilowatt)\n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt)\n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
      "\n",
      "7.submetering1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
      "\n",
      "8.submetering2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
      "\n",
      "9.submetering3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: household-power-consumption\n",
      "name 'detect' is not defined Language is not detected: ahmedashrafahmed\n",
      "name 'detect' is not defined Language is not detected: Household_power_consumption\n",
      "name 'detect' is not defined Language is not detected: I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data.\n",
      "Sorry about the format, it's in text file.\n",
      "Thanks in advance :)\n",
      "\n",
      "Context: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
      "\n",
      "Data Set Characteristics:\n",
      "Multivariate, Time-Series\n",
      "\n",
      "Associated Tasks:\n",
      "Regression, Clustering\n",
      "\n",
      "Data Set Information:\n",
      "\n",
      "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months).\n",
      "Notes:\n",
      "1.(globalactivepower*1000/60 - submetering1 - submetering2 - submetering3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
      "\n",
      "2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
      "\n",
      "Attribute Information:\n",
      "1.date: Date in format dd/mm/yyyy\n",
      "\n",
      "2.time: time in format hh:mm:ss\n",
      "\n",
      "3.globalactivepower: household global minute-averaged active power (in kilowatt)\n",
      "\n",
      "4.globalreactivepower: household global minute-averaged reactive power (in kilowatt)\n",
      "\n",
      "5.voltage: minute-averaged voltage (in volt)\n",
      "\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
      "\n",
      "7.submetering1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
      "\n",
      "8.submetering2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
      "\n",
      "9.submetering3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
      "name 'detect' is not defined Language is not detected: sachinprabhu007/amazon-ml-engineer-hiring-challenge\n",
      "name 'detect' is not defined Language is not detected: amazon-ml-engineer-hiring-challenge\n",
      "name 'detect' is not defined Language is not detected: sachinprabhu007\n",
      "name 'detect' is not defined Language is not detected: Amazon ML Engineer Hiring Challenge\n",
      "name 'detect' is not defined Language is not detected: #Predict the Category of the Customer\n",
      "\n",
      "#Problem Statement\n",
      "We all know that when we visit an e-commerce or TV series website or even YouTube we see a separate suggestion box, where in they show some content which you might like. These are mainly based on the content that you have consumed on their website previously. These are called as Recommendation engine. \n",
      "\n",
      "Now consider you have been running a start up since last one year and now you have been able to gather some customer data and you want to build a recommendation engine. Based on certain features you have to cluster the customers into two different groups so that you can recommend the correct products based on the customer’s cluster.\n",
      "\n",
      "#Challenge\n",
      "1. Your work is to build a predictive model to predict the category of the customer. You have to predict the column : “customer_category”\n",
      "2. You need to upload a presentation proposal addressing the following questions:\n",
      "     - What are the pros & cons of recommendation by this approach?\n",
      "     - Propose an architecture that will work more efficiently when building a recommendation engine for an e-commerce platform\n",
      "\n",
      "\n",
      "\n",
      "Data Description\n",
      "Data Files:\n",
      "\n",
      "Train.csv : 10378 x 10\n",
      "\n",
      "Test.csv : 7161 x 9\n",
      "\n",
      "Results.csv : 7161 x 2\n",
      "\n",
      "Data Description:\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F35f3efb9d1b54e9bc2f8a90cee016898%2F2.PNG?generation=1606564415878423&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "Submission Format \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F10907448e8f46490709f3204b48cf829%2F3.PNG?generation=1606564446898722&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "Instructions\n",
      "Upload your predictions in  a .csv format under ‘Upload your Prediction File’\n",
      "\n",
      "Upload your source code along with the presentation in a .zip format under ‘Upload your Source File’\n",
      "\n",
      "#Evaluation Criteria\n",
      "Please note the evaluation metric for score generation will be macro precision-score\n",
      "\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F54b5468950a9873331161518686bce32%2F1.PNG?generation=1606564475111802&alt=media)\n",
      "\n",
      "\n",
      "Evaluation metric for the source code & presentation submitted is as follows:\n",
      "\n",
      "\n",
      "Approach: How well is the problem statement understood and attended to?\n",
      "\n",
      "Creativity: Depth of Analysis\n",
      "\n",
      "Visualisations: What features are being presented and how?\n",
      "\n",
      "\n",
      "Total Score (10+10+10 = 30)\n",
      "name 'detect' is not defined Language is not detected: amazon-ml-engineer-hiring-challenge\n",
      "name 'detect' is not defined Language is not detected: sachinprabhu007\n",
      "name 'detect' is not defined Language is not detected: Amazon ML Engineer Hiring Challenge\n",
      "name 'detect' is not defined Language is not detected: #Predict the Category of the Customer\n",
      "\n",
      "#Problem Statement\n",
      "We all know that when we visit an e-commerce or TV series website or even YouTube we see a separate suggestion box, where in they show some content which you might like. These are mainly based on the content that you have consumed on their website previously. These are called as Recommendation engine. \n",
      "\n",
      "Now consider you have been running a start up since last one year and now you have been able to gather some customer data and you want to build a recommendation engine. Based on certain features you have to cluster the customers into two different groups so that you can recommend the correct products based on the customer’s cluster.\n",
      "\n",
      "#Challenge\n",
      "1. Your work is to build a predictive model to predict the category of the customer. You have to predict the column : “customer_category”\n",
      "2. You need to upload a presentation proposal addressing the following questions:\n",
      "     - What are the pros & cons of recommendation by this approach?\n",
      "     - Propose an architecture that will work more efficiently when building a recommendation engine for an e-commerce platform\n",
      "\n",
      "\n",
      "\n",
      "Data Description\n",
      "Data Files:\n",
      "\n",
      "Train.csv : 10378 x 10\n",
      "\n",
      "Test.csv : 7161 x 9\n",
      "\n",
      "Results.csv : 7161 x 2\n",
      "\n",
      "Data Description:\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F35f3efb9d1b54e9bc2f8a90cee016898%2F2.PNG?generation=1606564415878423&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "Submission Format \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F10907448e8f46490709f3204b48cf829%2F3.PNG?generation=1606564446898722&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "Instructions\n",
      "Upload your predictions in  a .csv format under ‘Upload your Prediction File’\n",
      "\n",
      "Upload your source code along with the presentation in a .zip format under ‘Upload your Source File’\n",
      "\n",
      "#Evaluation Criteria\n",
      "Please note the evaluation metric for score generation will be macro precision-score\n",
      "\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1623540%2F54b5468950a9873331161518686bce32%2F1.PNG?generation=1606564475111802&alt=media)\n",
      "\n",
      "\n",
      "Evaluation metric for the source code & presentation submitted is as follows:\n",
      "\n",
      "\n",
      "Approach: How well is the problem statement understood and attended to?\n",
      "\n",
      "Creativity: Depth of Analysis\n",
      "\n",
      "Visualisations: What features are being presented and how?\n",
      "\n",
      "\n",
      "Total Score (10+10+10 = 30)\n",
      "name 'detect' is not defined Language is not detected: ue153011/authentic-dataset-for-movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: authentic-dataset-for-movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: ue153011\n",
      "name 'detect' is not defined Language is not detected: Authentic Dataset for Movie recommendation system\n",
      "name 'detect' is not defined Language is not detected: Dataset for BEGINNERS in ml to try out a movie recommendation system.\n",
      "name 'detect' is not defined Language is not detected: This is a dataset for all those BEGINNERS  who are new to machine learning and wants to make a Movie Recommendation System.\n",
      "name 'detect' is not defined Language is not detected: authentic-dataset-for-movie-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: ue153011\n",
      "name 'detect' is not defined Language is not detected: Authentic Dataset for Movie recommendation system\n",
      "name 'detect' is not defined Language is not detected: Dataset for BEGINNERS in ml to try out a movie recommendation system.\n",
      "name 'detect' is not defined Language is not detected: This is a dataset for all those BEGINNERS  who are new to machine learning and wants to make a Movie Recommendation System.\n",
      "name 'detect' is not defined Language is not detected: annecool37/museum-data\n",
      "name 'detect' is not defined Language is not detected: museum-data\n",
      "name 'detect' is not defined Language is not detected: annecool37\n",
      "name 'detect' is not defined Language is not detected: Museum Reviews Collected from TripAdvisor\n",
      "name 'detect' is not defined Language is not detected: Museum name, type, location, review, rating, rank, etc.\n",
      "name 'detect' is not defined Language is not detected: Visit my [GitHub][1] for detailed description and code.  \n",
      "\n",
      "Also, here's an App, a Recommendation System for Museum Selection, I created with this data set:\n",
      "http://216.230.228.88:3838/bootcamp006_project/Project5-Capstone/Museo/app/\n",
      "\n",
      "### Inspiration for Further Analysis\n",
      "- Predict if a museum will be featured or not\n",
      "- Discover the important factors that results in a higher rating \n",
      "- Apply natural language processing to discover insights from review/quote/tag data\n",
      "- Cluster museums based on review/quote polarity or subjectivity (sentiment analysis)\n",
      "- Apply association rules to uncover relationship of different combinations of review tags\n",
      "\n",
      "### Processed and Merged Data\n",
      "tripadvisor_merged.csv: A file containing museum data collected from TripAdvisor including tag/ type/ review/ quote features\n",
      "\n",
      "![Data Scraped][2]\n",
      "![Features Added][3]\n",
      "\n",
      "### Raw Data Scraped from TripAdvisor (US/World)\n",
      "- tripadvisor_museum: general museum data scraped from TripAdvisor\n",
      "- traveler_type: {'museum': ['Families','Couples','Solo','Business','Friends']}\n",
      "- traveler_rating: {'museum': ['Excellent','Very good','Average','Poor','Terrible']}\n",
      "- tag_clouds: {'museum': ['tag 1', 'tag 2', 'tag 3' ...]}\n",
      "- review_quote: {'museum': ['quote 1', 'quote 2', ... ,'quote 10']}\n",
      "- review_content: {'museum': ['review 1', 'review 2', ... ,'review 10']}\n",
      "- museum_categories: {'museum': ['museum type 1','museum type 2', ...]} \n",
      "\n",
      "\n",
      "  [1]: https://github.com/annecool37/MuseumRecommendation\n",
      "  [2]: http://blog.nycdatascience.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-19-at-11.30.36-PM.png\n",
      "  [3]: http://blog.nycdatascience.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-19-at-11.30.47-PM.png\n",
      "name 'detect' is not defined Language is not detected: museum-data\n",
      "name 'detect' is not defined Language is not detected: annecool37\n",
      "name 'detect' is not defined Language is not detected: Museum Reviews Collected from TripAdvisor\n",
      "name 'detect' is not defined Language is not detected: Museum name, type, location, review, rating, rank, etc.\n",
      "name 'detect' is not defined Language is not detected: Visit my [GitHub][1] for detailed description and code.  \n",
      "\n",
      "Also, here's an App, a Recommendation System for Museum Selection, I created with this data set:\n",
      "http://216.230.228.88:3838/bootcamp006_project/Project5-Capstone/Museo/app/\n",
      "\n",
      "### Inspiration for Further Analysis\n",
      "- Predict if a museum will be featured or not\n",
      "- Discover the important factors that results in a higher rating \n",
      "- Apply natural language processing to discover insights from review/quote/tag data\n",
      "- Cluster museums based on review/quote polarity or subjectivity (sentiment analysis)\n",
      "- Apply association rules to uncover relationship of different combinations of review tags\n",
      "\n",
      "### Processed and Merged Data\n",
      "tripadvisor_merged.csv: A file containing museum data collected from TripAdvisor including tag/ type/ review/ quote features\n",
      "\n",
      "![Data Scraped][2]\n",
      "![Features Added][3]\n",
      "\n",
      "### Raw Data Scraped from TripAdvisor (US/World)\n",
      "- tripadvisor_museum: general museum data scraped from TripAdvisor\n",
      "- traveler_type: {'museum': ['Families','Couples','Solo','Business','Friends']}\n",
      "- traveler_rating: {'museum': ['Excellent','Very good','Average','Poor','Terrible']}\n",
      "- tag_clouds: {'museum': ['tag 1', 'tag 2', 'tag 3' ...]}\n",
      "- review_quote: {'museum': ['quote 1', 'quote 2', ... ,'quote 10']}\n",
      "- review_content: {'museum': ['review 1', 'review 2', ... ,'review 10']}\n",
      "- museum_categories: {'museum': ['museum type 1','museum type 2', ...]} \n",
      "\n",
      "\n",
      "  [1]: https://github.com/annecool37/MuseumRecommendation\n",
      "  [2]: http://blog.nycdatascience.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-19-at-11.30.36-PM.png\n",
      "  [3]: http://blog.nycdatascience.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-19-at-11.30.47-PM.png\n",
      "name 'detect' is not defined Language is not detected: jiashenliu/515k-hotel-reviews-data-in-europe\n",
      "name 'detect' is not defined Language is not detected: 515k-hotel-reviews-data-in-europe\n",
      "name 'detect' is not defined Language is not detected: jiashenliu\n",
      "name 'detect' is not defined Language is not detected: 515K Hotel Reviews Data in Europe\n",
      "name 'detect' is not defined Language is not detected: Can you make your trip more cozy by using data science?\n",
      "name 'detect' is not defined Language is not detected: ### Acknowledgements\n",
      "\n",
      "The data was scraped from [Booking.com][1]. All data in the file is publicly available to everyone already. Please be noted that data is originally owned by [Booking.com][2]. \n",
      "### Data Context\n",
      "\n",
      "This dataset contains 515,000 customer reviews and scoring of 1493 luxury hotels across Europe. Meanwhile, the geographical location of hotels are also provided for further analysis.\n",
      "\n",
      "### Data Content\n",
      "\n",
      "The csv file contains 17 fields. The description of each field is as below:\n",
      "\n",
      " - Hotel_Address: Address of hotel. \n",
      " - Review_Date: Date when reviewer posted the corresponding review.\n",
      " - Average_Score: Average Score of the hotel, calculated based on the latest comment in the last year.\n",
      " - Hotel_Name: Name of Hotel\n",
      " - Reviewer_Nationality: Nationality of Reviewer\n",
      " - Negative_Review: Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Negative'\n",
      " - Review_Total_Negative_Word_Counts: Total number of words in the negative review.\n",
      " - Positive_Review: Positive Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Positive'\n",
      " - Review_Total_Positive_Word_Counts: Total number of words in the positive review.\n",
      " - Reviewer_Score: Score the reviewer has given to the hotel, based on his/her experience\n",
      " - Total_Number_of_Reviews_Reviewer_Has_Given: Number of Reviews the reviewers has given in the past.\n",
      " - Total_Number_of_Reviews: Total number of valid reviews the hotel has.\n",
      " - Tags: Tags reviewer gave the hotel.\n",
      " - days_since_review: Duration between the review date and scrape date.\n",
      " - Additional_Number_of_Scoring: There are also some guests who just made a scoring on the service rather than a review. This number indicates how many valid scores without review in there.\n",
      " - lat: Latitude of the hotel\n",
      " - lng: longtitude of the hotel\n",
      "\n",
      "*In order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.*\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!\n",
      "\n",
      " - Fit a regression model on reviews and score to see which words are more indicative to a higher/lower score\n",
      " - Perform a sentiment analysis on the reviews\n",
      " - Find correlation between reviewer's nationality and scores.\n",
      " - Beautiful and informative visualization on the dataset.\n",
      " - Clustering hotels based on reviews\n",
      " - Simple recommendation engine to the guest who is fond of a special characteristic of hotel.\n",
      "\n",
      "The idea is unlimited! Please, have a look into data, generate some ideas and leave a master kernel here! I am ready to upvote your ideas and kernels! Cheers!\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  [1]: http://www.booking.com\n",
      "  [2]: http://www.booking.com\n",
      "  [3]: https://www.kaggle.com/jiashenliu\n",
      "name 'detect' is not defined Language is not detected: 515k-hotel-reviews-data-in-europe\n",
      "name 'detect' is not defined Language is not detected: jiashenliu\n",
      "name 'detect' is not defined Language is not detected: 515K Hotel Reviews Data in Europe\n",
      "name 'detect' is not defined Language is not detected: Can you make your trip more cozy by using data science?\n",
      "name 'detect' is not defined Language is not detected: ### Acknowledgements\n",
      "\n",
      "The data was scraped from [Booking.com][1]. All data in the file is publicly available to everyone already. Please be noted that data is originally owned by [Booking.com][2]. \n",
      "### Data Context\n",
      "\n",
      "This dataset contains 515,000 customer reviews and scoring of 1493 luxury hotels across Europe. Meanwhile, the geographical location of hotels are also provided for further analysis.\n",
      "\n",
      "### Data Content\n",
      "\n",
      "The csv file contains 17 fields. The description of each field is as below:\n",
      "\n",
      " - Hotel_Address: Address of hotel. \n",
      " - Review_Date: Date when reviewer posted the corresponding review.\n",
      " - Average_Score: Average Score of the hotel, calculated based on the latest comment in the last year.\n",
      " - Hotel_Name: Name of Hotel\n",
      " - Reviewer_Nationality: Nationality of Reviewer\n",
      " - Negative_Review: Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Negative'\n",
      " - Review_Total_Negative_Word_Counts: Total number of words in the negative review.\n",
      " - Positive_Review: Positive Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Positive'\n",
      " - Review_Total_Positive_Word_Counts: Total number of words in the positive review.\n",
      " - Reviewer_Score: Score the reviewer has given to the hotel, based on his/her experience\n",
      " - Total_Number_of_Reviews_Reviewer_Has_Given: Number of Reviews the reviewers has given in the past.\n",
      " - Total_Number_of_Reviews: Total number of valid reviews the hotel has.\n",
      " - Tags: Tags reviewer gave the hotel.\n",
      " - days_since_review: Duration between the review date and scrape date.\n",
      " - Additional_Number_of_Scoring: There are also some guests who just made a scoring on the service rather than a review. This number indicates how many valid scores without review in there.\n",
      " - lat: Latitude of the hotel\n",
      " - lng: longtitude of the hotel\n",
      "\n",
      "*In order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.*\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!\n",
      "\n",
      " - Fit a regression model on reviews and score to see which words are more indicative to a higher/lower score\n",
      " - Perform a sentiment analysis on the reviews\n",
      " - Find correlation between reviewer's nationality and scores.\n",
      " - Beautiful and informative visualization on the dataset.\n",
      " - Clustering hotels based on reviews\n",
      " - Simple recommendation engine to the guest who is fond of a special characteristic of hotel.\n",
      "\n",
      "The idea is unlimited! Please, have a look into data, generate some ideas and leave a master kernel here! I am ready to upvote your ideas and kernels! Cheers!\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  [1]: http://www.booking.com\n",
      "  [2]: http://www.booking.com\n",
      "  [3]: https://www.kaggle.com/jiashenliu\n",
      "name 'detect' is not defined Language is not detected: ayush12nagar/laptop-price-predictor-regression\n",
      "name 'detect' is not defined Language is not detected: laptop-price-predictor-regression\n",
      "name 'detect' is not defined Language is not detected: ayush12nagar\n",
      "name 'detect' is not defined Language is not detected: laptop-price-predictor-regression-\n",
      "name 'detect' is not defined Language is not detected: laptop-price-predictor-regression\n",
      "name 'detect' is not defined Language is not detected: ayush12nagar\n",
      "name 'detect' is not defined Language is not detected: laptop-price-predictor-regression-\n",
      "name 'detect' is not defined Language is not detected: hunter0007/ecommerce-dataset-for-predictive-marketing-2023\n",
      "name 'detect' is not defined Language is not detected: ecommerce-dataset-for-predictive-marketing-2023\n",
      "name 'detect' is not defined Language is not detected: hunter0007\n",
      "name 'detect' is not defined Language is not detected: Supermarket dataset for predictive marketing 2023\n",
      "name 'detect' is not defined Language is not detected: Foresee your customers' needs in order to create successful campaigns\n",
      "name 'detect' is not defined Language is not detected: ### *Please feel free to share it with others and consider supporting me if you find it helpful ⭐️.*\n",
      "\n",
      "### Inspiration:\n",
      "- [K-Means Clustering & PCA](https://www.kaggle.com/code/hunter0007/predictive-marketing-k-means-clustering-pca)\n",
      "- [🎖Product Recommendation - Apriori & causal ML\n",
      "](https://www.kaggle.com/code/hunter0007/product-recommendation-apriori-causal-ml)\n",
      "\n",
      "### Context\n",
      "\n",
      "The dataset consists of over 2 million purchase records at a renowned Hunter's supermarket :)\n",
      "\n",
      "### Content\n",
      "\n",
      "#### *Supermarket consumer behaviour dataset consists of 2019501 Rows & 12 Columns :*\n",
      "\n",
      "- order_id – (A unique number to identity the order)\n",
      "- user_id - (A unique number to identify the user)\n",
      "- order_number – (Number of the order)\n",
      "- order_dow – (Day of the Week the order was made)\n",
      "- order_hour_of_day – (Time of the order)\n",
      "- days_since_prior_order - (History of the order)\n",
      "- product_id – (Id of the product)\n",
      "- add_to_cart_order – (Number of items added to cart)\n",
      "- reordered – (If the reorder took place)\n",
      "- department_id - (Unique number allocated to each department)\n",
      "- department – (Names of the departments)\n",
      "- product_name – (Name of the products)\n",
      "name 'detect' is not defined Language is not detected: ecommerce-dataset-for-predictive-marketing-2023\n",
      "name 'detect' is not defined Language is not detected: hunter0007\n",
      "name 'detect' is not defined Language is not detected: Supermarket dataset for predictive marketing 2023\n",
      "name 'detect' is not defined Language is not detected: Foresee your customers' needs in order to create successful campaigns\n",
      "name 'detect' is not defined Language is not detected: ### *Please feel free to share it with others and consider supporting me if you find it helpful ⭐️.*\n",
      "\n",
      "### Inspiration:\n",
      "- [K-Means Clustering & PCA](https://www.kaggle.com/code/hunter0007/predictive-marketing-k-means-clustering-pca)\n",
      "- [🎖Product Recommendation - Apriori & causal ML\n",
      "](https://www.kaggle.com/code/hunter0007/product-recommendation-apriori-causal-ml)\n",
      "\n",
      "### Context\n",
      "\n",
      "The dataset consists of over 2 million purchase records at a renowned Hunter's supermarket :)\n",
      "\n",
      "### Content\n",
      "\n",
      "#### *Supermarket consumer behaviour dataset consists of 2019501 Rows & 12 Columns :*\n",
      "\n",
      "- order_id – (A unique number to identity the order)\n",
      "- user_id - (A unique number to identify the user)\n",
      "- order_number – (Number of the order)\n",
      "- order_dow – (Day of the Week the order was made)\n",
      "- order_hour_of_day – (Time of the order)\n",
      "- days_since_prior_order - (History of the order)\n",
      "- product_id – (Id of the product)\n",
      "- add_to_cart_order – (Number of items added to cart)\n",
      "- reordered – (If the reorder took place)\n",
      "- department_id - (Unique number allocated to each department)\n",
      "- department – (Names of the departments)\n",
      "- product_name – (Name of the products)\n",
      "name 'detect' is not defined Language is not detected: banaankiamanesh/imu-for-ai\n",
      "name 'detect' is not defined Language is not detected: imu-for-ai\n",
      "name 'detect' is not defined Language is not detected: banaankiamanesh\n",
      "name 'detect' is not defined Language is not detected: IMU Data for AI\n",
      "name 'detect' is not defined Language is not detected: IMU data for Replacement of non-linear filters with Regression Models.\n",
      "name 'detect' is not defined Language is not detected: ### What's all of this about?\n",
      "This dataset was created to implement a paper in how to estimate Euler angles or quaternions with AI, instead of using non-linear filters, like **Kalman Filters or Madgwick filters**...\n",
      "\n",
      "### What's Inside?\n",
      "The Data Contains of 3-axis GYRO + 3-axis Accelerometer + 3-axis Magnetometer data, along with the labels(containing pitch, roll, yaw and quaternions).\n",
      "So any supervised multivariate regression approach can be implemented on it.\n",
      "\n",
      "Sampling frequency of the data is EXACTLY equal to 100 Hz.\n",
      "IMU used for the data collection procedure is: BNO055 9-axis IMU by BOSCH.\n",
      "![BNO055 IMU](https://aws1.discourse-cdn.com/arduino/original/4X/3/3/7/3379bb032889edd383ef64658624a5e835bd78cf.jpeg)\n",
      "\n",
      "Everything is clearly labeled when you open the CSV files.\n",
      "\n",
      "***Note:*** the Files having \"(No Mag)\" in their names, doesn't contain magnetometer data.\n",
      "For more information, Contact me from: [My Email](mailto:kiamaneshbanaan@gmail.com)\n",
      "\n",
      "[\"GitHub Repo\"](https://github.com/BanaanKiamanesh/DL_for_Navigation)\n",
      "name 'detect' is not defined Language is not detected: imu-for-ai\n",
      "name 'detect' is not defined Language is not detected: banaankiamanesh\n",
      "name 'detect' is not defined Language is not detected: IMU Data for AI\n",
      "name 'detect' is not defined Language is not detected: IMU data for Replacement of non-linear filters with Regression Models.\n",
      "name 'detect' is not defined Language is not detected: ### What's all of this about?\n",
      "This dataset was created to implement a paper in how to estimate Euler angles or quaternions with AI, instead of using non-linear filters, like **Kalman Filters or Madgwick filters**...\n",
      "\n",
      "### What's Inside?\n",
      "The Data Contains of 3-axis GYRO + 3-axis Accelerometer + 3-axis Magnetometer data, along with the labels(containing pitch, roll, yaw and quaternions).\n",
      "So any supervised multivariate regression approach can be implemented on it.\n",
      "\n",
      "Sampling frequency of the data is EXACTLY equal to 100 Hz.\n",
      "IMU used for the data collection procedure is: BNO055 9-axis IMU by BOSCH.\n",
      "![BNO055 IMU](https://aws1.discourse-cdn.com/arduino/original/4X/3/3/7/3379bb032889edd383ef64658624a5e835bd78cf.jpeg)\n",
      "\n",
      "Everything is clearly labeled when you open the CSV files.\n",
      "\n",
      "***Note:*** the Files having \"(No Mag)\" in their names, doesn't contain magnetometer data.\n",
      "For more information, Contact me from: [My Email](mailto:kiamaneshbanaan@gmail.com)\n",
      "\n",
      "[\"GitHub Repo\"](https://github.com/BanaanKiamanesh/DL_for_Navigation)\n",
      "name 'detect' is not defined Language is not detected: keshavramaiah/hotel-recommendation\n",
      "name 'detect' is not defined Language is not detected: hotel-recommendation\n",
      "name 'detect' is not defined Language is not detected: keshavramaiah\n",
      "name 'detect' is not defined Language is not detected: hotel recommendation\n",
      "name 'detect' is not defined Language is not detected: This dataset is useful for building your own hotel recommender systems.\n",
      "name 'detect' is not defined Language is not detected: Hello Guys!\n",
      "This was a dataset collected for building hotel based recommender systems based on geo-tagging,prices and other features\n",
      "the dataset is collected from various resources.This could only be used for academic and research purpose,could not be sold\n",
      "or distributed for commercial purposes.\n",
      "name 'detect' is not defined Language is not detected: hotel-recommendation\n",
      "name 'detect' is not defined Language is not detected: keshavramaiah\n",
      "name 'detect' is not defined Language is not detected: hotel recommendation\n",
      "name 'detect' is not defined Language is not detected: This dataset is useful for building your own hotel recommender systems.\n",
      "name 'detect' is not defined Language is not detected: Hello Guys!\n",
      "This was a dataset collected for building hotel based recommender systems based on geo-tagging,prices and other features\n",
      "the dataset is collected from various resources.This could only be used for academic and research purpose,could not be sold\n",
      "or distributed for commercial purposes.\n",
      "name 'detect' is not defined Language is not detected: guru001/movie-genre-prediction\n",
      "name 'detect' is not defined Language is not detected: movie-genre-prediction\n",
      "name 'detect' is not defined Language is not detected: guru001\n",
      "name 'detect' is not defined Language is not detected: movie-genre-prediction\n",
      "name 'detect' is not defined Language is not detected: data-driven science huggingface competition | movie genre prediction competition\n",
      "name 'detect' is not defined Language is not detected: **Objective**\n",
      "\n",
      "The goal of this competition is to design a predictive model that accurately classifies movies into their respective genres based on their titles and synopses.\n",
      "\n",
      "The challenge lies not just in achieving high accuracy, but also in ensuring that the model is efficient and interpretable.\n",
      "\n",
      "**Why This is Interesting and Relevant**\n",
      "\n",
      "Understanding movie genres based on titles and synopses is a fascinating problem for multiple reasons.\n",
      "\n",
      "From a recommendation system perspective, an effective genre classifier can help build more personalized user recommendations, increasing user engagement on streaming platforms.\n",
      "\n",
      "In the context of box office performance, understanding the relationship between genres and how they are perceived in synopses can provide insight into patterns of commercial success or failure.\n",
      "\n",
      "Furthermore, this challenge can facilitate a deeper comprehension of movie themes and trends in the industry, contributing to cultural and societal studies.\n",
      "\n",
      "**Dataset**\n",
      "\n",
      "Participants will be provided with a comprehensive dataset comprising ~100,000 movies. Each entry includes the original title, the genre(s), and the synopsis of the movie.\n",
      "\n",
      "The dataset contains a mix of both original and AI-generated titles, genres, and synopses to test the robustness of the models.\n",
      "\n",
      "The 10 genres include action, adventure, crime, family, fantasy, horror, mystery, romance, scifi, and thriller.\n",
      "name 'detect' is not defined Language is not detected: movie-genre-prediction\n",
      "name 'detect' is not defined Language is not detected: guru001\n",
      "name 'detect' is not defined Language is not detected: movie-genre-prediction\n",
      "name 'detect' is not defined Language is not detected: data-driven science huggingface competition | movie genre prediction competition\n",
      "name 'detect' is not defined Language is not detected: **Objective**\n",
      "\n",
      "The goal of this competition is to design a predictive model that accurately classifies movies into their respective genres based on their titles and synopses.\n",
      "\n",
      "The challenge lies not just in achieving high accuracy, but also in ensuring that the model is efficient and interpretable.\n",
      "\n",
      "**Why This is Interesting and Relevant**\n",
      "\n",
      "Understanding movie genres based on titles and synopses is a fascinating problem for multiple reasons.\n",
      "\n",
      "From a recommendation system perspective, an effective genre classifier can help build more personalized user recommendations, increasing user engagement on streaming platforms.\n",
      "\n",
      "In the context of box office performance, understanding the relationship between genres and how they are perceived in synopses can provide insight into patterns of commercial success or failure.\n",
      "\n",
      "Furthermore, this challenge can facilitate a deeper comprehension of movie themes and trends in the industry, contributing to cultural and societal studies.\n",
      "\n",
      "**Dataset**\n",
      "\n",
      "Participants will be provided with a comprehensive dataset comprising ~100,000 movies. Each entry includes the original title, the genre(s), and the synopsis of the movie.\n",
      "\n",
      "The dataset contains a mix of both original and AI-generated titles, genres, and synopses to test the robustness of the models.\n",
      "\n",
      "The 10 genres include action, adventure, crime, family, fantasy, horror, mystery, romance, scifi, and thriller.\n",
      "name 'detect' is not defined Language is not detected: eutiagovski/imdb-movies-and-tv-series-dataset\n",
      "name 'detect' is not defined Language is not detected: imdb-movies-and-tv-series-dataset\n",
      "name 'detect' is not defined Language is not detected: eutiagovski\n",
      "name 'detect' is not defined Language is not detected: IMDB Movies and TV Series Dataset\n",
      "name 'detect' is not defined Language is not detected: More then 18k movies and TV series collected from IMDB website\n",
      "name 'detect' is not defined Language is not detected: **Description:**\n",
      "Welcome to the IMDb Movie and TV Series Titles Dataset! This dataset comprises over 18,000 titles extracted from the IMDb website, representing a diverse array of movies and TV series. Each title included in this dataset has been meticulously filtered based on two criteria: an average rating higher than 7 and a minimum of 1,000 user votes. \n",
      "\n",
      "**Key Features:**\n",
      "- **Title Information:** Explore a vast collection of movie and TV series titles, ranging across various genres, release years, and production types.\n",
      "- **Ratings and Popularity:** Discover titles with exceptionally high average ratings, indicative of quality content that has garnered significant attention from viewers.\n",
      "\n",
      "**Potential Applications:**\n",
      "- **Content Recommendation Systems:** Utilize this dataset to develop recommendation algorithms for movie and TV series enthusiasts, helping them discover high-rated titles.\n",
      "- **Market Analysis:** Conduct market research and analysis within the entertainment industry, identifying trends and preferences among audiences.\n",
      "\n",
      "**Data Source:**\n",
      "The data in this dataset has been collected directly from the IMDb website, a renowned platform for movie and TV series information, ratings, and reviews.\n",
      "\n",
      "**License:**\n",
      "This dataset is provided under an open access license, allowing users to freely access, analyze, and utilize the data for various purposes. Please refer to the dataset's documentation for specific terms of use.\n",
      "\n",
      "**Acknowledgments:**\n",
      "We extend our gratitude to IMDb for providing access to their platform and allowing us to collect and share this dataset with the community.\n",
      "\n",
      "**Feedback:**\n",
      "We encourage feedback, suggestions, and contributions from users to enhance the quality and relevance of this dataset. Your input is invaluable in improving the utility and usability of this resource.\n",
      "\n",
      "**Explore and Analyze:**\n",
      "Dive into the IMDb Movie and TV Series Titles Dataset to uncover valuable insights, trends, and patterns within the world of entertainment. Whether you're a data scientist, a movie buff, or a researcher, this dataset offers a wealth of opportunities for exploration and analysis.\n",
      "\n",
      "**Note:**\n",
      "Please note that while this dataset contains a curated selection of titles with high ratings and popularity, individual preferences and tastes may vary. Additionally, certain titles may be subject to availability based on regional restrictions or licensing agreements.\n",
      "name 'detect' is not defined Language is not detected: imdb-movies-and-tv-series-dataset\n",
      "name 'detect' is not defined Language is not detected: eutiagovski\n",
      "name 'detect' is not defined Language is not detected: IMDB Movies and TV Series Dataset\n",
      "name 'detect' is not defined Language is not detected: More then 18k movies and TV series collected from IMDB website\n",
      "name 'detect' is not defined Language is not detected: **Description:**\n",
      "Welcome to the IMDb Movie and TV Series Titles Dataset! This dataset comprises over 18,000 titles extracted from the IMDb website, representing a diverse array of movies and TV series. Each title included in this dataset has been meticulously filtered based on two criteria: an average rating higher than 7 and a minimum of 1,000 user votes. \n",
      "\n",
      "**Key Features:**\n",
      "- **Title Information:** Explore a vast collection of movie and TV series titles, ranging across various genres, release years, and production types.\n",
      "- **Ratings and Popularity:** Discover titles with exceptionally high average ratings, indicative of quality content that has garnered significant attention from viewers.\n",
      "\n",
      "**Potential Applications:**\n",
      "- **Content Recommendation Systems:** Utilize this dataset to develop recommendation algorithms for movie and TV series enthusiasts, helping them discover high-rated titles.\n",
      "- **Market Analysis:** Conduct market research and analysis within the entertainment industry, identifying trends and preferences among audiences.\n",
      "\n",
      "**Data Source:**\n",
      "The data in this dataset has been collected directly from the IMDb website, a renowned platform for movie and TV series information, ratings, and reviews.\n",
      "\n",
      "**License:**\n",
      "This dataset is provided under an open access license, allowing users to freely access, analyze, and utilize the data for various purposes. Please refer to the dataset's documentation for specific terms of use.\n",
      "\n",
      "**Acknowledgments:**\n",
      "We extend our gratitude to IMDb for providing access to their platform and allowing us to collect and share this dataset with the community.\n",
      "\n",
      "**Feedback:**\n",
      "We encourage feedback, suggestions, and contributions from users to enhance the quality and relevance of this dataset. Your input is invaluable in improving the utility and usability of this resource.\n",
      "\n",
      "**Explore and Analyze:**\n",
      "Dive into the IMDb Movie and TV Series Titles Dataset to uncover valuable insights, trends, and patterns within the world of entertainment. Whether you're a data scientist, a movie buff, or a researcher, this dataset offers a wealth of opportunities for exploration and analysis.\n",
      "\n",
      "**Note:**\n",
      "Please note that while this dataset contains a curated selection of titles with high ratings and popularity, individual preferences and tastes may vary. Additionally, certain titles may be subject to availability based on regional restrictions or licensing agreements.\n",
      "name 'detect' is not defined Language is not detected: whigmalwhim/steam-releases\n",
      "name 'detect' is not defined Language is not detected: steam-releases\n",
      "name 'detect' is not defined Language is not detected: whigmalwhim\n",
      "name 'detect' is not defined Language is not detected: Steam Releases\n",
      "name 'detect' is not defined Language is not detected: All Steam listing, peak players, and scores from (2006-2023)\n",
      "name 'detect' is not defined Language is not detected: A dataset containing information about Steam games ranging from 2006-2023. Information includes publisher, technologies used, tags, developers, players, and rating metrics. A public open-source alternative to Steamdb data that is viable for use in regression tasks and applications. \n",
      "\"games-release-all\" contains all of the information about the 65958 games on Steam, including the last time the game was updated on the Steam store. \n",
      "name 'detect' is not defined Language is not detected: steam-releases\n",
      "name 'detect' is not defined Language is not detected: whigmalwhim\n",
      "name 'detect' is not defined Language is not detected: Steam Releases\n",
      "name 'detect' is not defined Language is not detected: All Steam listing, peak players, and scores from (2006-2023)\n",
      "name 'detect' is not defined Language is not detected: A dataset containing information about Steam games ranging from 2006-2023. Information includes publisher, technologies used, tags, developers, players, and rating metrics. A public open-source alternative to Steamdb data that is viable for use in regression tasks and applications. \n",
      "\"games-release-all\" contains all of the information about the 65958 games on Steam, including the last time the game was updated on the Steam store. \n",
      "name 'detect' is not defined Language is not detected: patrickgendotti/steam-achievementstatscom-rankings\n",
      "name 'detect' is not defined Language is not detected: steam-achievementstatscom-rankings\n",
      "name 'detect' is not defined Language is not detected: patrickgendotti\n",
      "name 'detect' is not defined Language is not detected: Steam AchievementStats.com Rankings\n",
      "name 'detect' is not defined Language is not detected: Are you ranked on Steam?\n",
      "name 'detect' is not defined Language is not detected: This data was scraped from Achievement Stats (https://achievementstats.com/index.php) and provides info for the first 200k ranked players and the aggregated country data for all players with public profiles on Steam. \n",
      "\n",
      "My EDA notebook has some basic data visualizations and calculated columns that might be useful to you, as well as a couple of example linear regression models for the country data.\n",
      "\n",
      "If you want to learn more about the scraping methods or code behind the data presented here, check out my Github project here: https://github.com/Gendo90/Achievement-Stats-Scraper\n",
      "name 'detect' is not defined Language is not detected: steam-achievementstatscom-rankings\n",
      "name 'detect' is not defined Language is not detected: patrickgendotti\n",
      "name 'detect' is not defined Language is not detected: Steam AchievementStats.com Rankings\n",
      "name 'detect' is not defined Language is not detected: Are you ranked on Steam?\n",
      "name 'detect' is not defined Language is not detected: This data was scraped from Achievement Stats (https://achievementstats.com/index.php) and provides info for the first 200k ranked players and the aggregated country data for all players with public profiles on Steam. \n",
      "\n",
      "My EDA notebook has some basic data visualizations and calculated columns that might be useful to you, as well as a couple of example linear regression models for the country data.\n",
      "\n",
      "If you want to learn more about the scraping methods or code behind the data presented here, check out my Github project here: https://github.com/Gendo90/Achievement-Stats-Scraper\n",
      "name 'detect' is not defined Language is not detected: lokeshparab/imdb-tv-shows-dataset-latest\n",
      "name 'detect' is not defined Language is not detected: imdb-tv-shows-dataset-latest\n",
      "name 'detect' is not defined Language is not detected: lokeshparab\n",
      "name 'detect' is not defined Language is not detected: IMDb TV Shows Latest 2023\n",
      "name 'detect' is not defined Language is not detected: All TV Shows Dataset\n",
      "name 'detect' is not defined Language is not detected:  This is a dataset of overall TV show ratings from all OTT platforms. It is extracted from [IMDb Website](https://www.imdb.com/) \n",
      "\n",
      "**This dataset contains seven columns that describe the television show, which are listed below:**\n",
      "# Features\n",
      "\n",
      "| Name         | Description |\n",
      "| ----------- | ----------- |\n",
      "|  id               |   Show id which is register by IMDb website                |\n",
      "|  name         |   Name of the Show along with season if any                |\n",
      "|  year           |   Year duration of the show which has start year and end year if any                |\n",
      "|  certificate |   The certificates section records the ratings certificates given to titles within each country  |\n",
      "|  runtime     |    Duration of Show              |\n",
      "|  genre        |    Genre / Category of the Shows Belongs                |\n",
      "|  ratings      |     IMDb rating given by all users               |\n",
      "\n",
      "# Interactive\n",
      "\n",
      "1. Tv Show Recommendation-based application\n",
      "2. Create Dashboards using PowerBi/ Tableau or Other Analytics Tool\n",
      "3. Create more datasets like Celebrities, Star or Director from this dataset\n",
      "name 'detect' is not defined Language is not detected: imdb-tv-shows-dataset-latest\n",
      "name 'detect' is not defined Language is not detected: lokeshparab\n",
      "name 'detect' is not defined Language is not detected: IMDb TV Shows Latest 2023\n",
      "name 'detect' is not defined Language is not detected: All TV Shows Dataset\n",
      "name 'detect' is not defined Language is not detected:  This is a dataset of overall TV show ratings from all OTT platforms. It is extracted from [IMDb Website](https://www.imdb.com/) \n",
      "\n",
      "**This dataset contains seven columns that describe the television show, which are listed below:**\n",
      "# Features\n",
      "\n",
      "| Name         | Description |\n",
      "| ----------- | ----------- |\n",
      "|  id               |   Show id which is register by IMDb website                |\n",
      "|  name         |   Name of the Show along with season if any                |\n",
      "|  year           |   Year duration of the show which has start year and end year if any                |\n",
      "|  certificate |   The certificates section records the ratings certificates given to titles within each country  |\n",
      "|  runtime     |    Duration of Show              |\n",
      "|  genre        |    Genre / Category of the Shows Belongs                |\n",
      "|  ratings      |     IMDb rating given by all users               |\n",
      "\n",
      "# Interactive\n",
      "\n",
      "1. Tv Show Recommendation-based application\n",
      "2. Create Dashboards using PowerBi/ Tableau or Other Analytics Tool\n",
      "3. Create more datasets like Celebrities, Star or Director from this dataset\n",
      "name 'detect' is not defined Language is not detected: lokeshparab/amazon-products-dataset\n",
      "name 'detect' is not defined Language is not detected: amazon-products-dataset\n",
      "name 'detect' is not defined Language is not detected: lokeshparab\n",
      "name 'detect' is not defined Language is not detected: Amazon Products Sales Dataset 2023\n",
      "name 'detect' is not defined Language is not detected: This dataset has 142 Categories and 300K+ Products Details\n",
      "name 'detect' is not defined Language is not detected: # About\n",
      "## This is a Product Sales Dataset scraped from the [Amazon website](https://www.amazon.com/) \n",
      "* Its product data are separated by 142 categories in csv format, along with the full dataset name **Amazon-Products.csv**. \n",
      "* Each csv files are consist of 10 columns and each row has products details accordingly\n",
      "\n",
      "# Features\n",
      "| name                 |  description                      |\n",
      "|-----------------|--------------------------|\n",
      "|  *name*                |  The name of the product |\n",
      "|  *main_category* |   The main category of the product belong |\n",
      "|  *sub_category*   |  The main category of the product belong  |\n",
      "|  *image*                |   The image of the product look like          |\n",
      "|  *link*                    |  The amazon website reference link of the product   |\n",
      "|  *ratings*              |  The ratings given by amazon customers of the product           |\n",
      "|  *no of ratings*    |  The number of ratings given to this product in amazon shopping   |\n",
      "|  *discount_price* |   The discount prices of the product  |\n",
      "|  *actual_price*     |  The actual MRP of the product  |\n",
      "\n",
      "# Inspiration\n",
      "Amazon is an American Tech Multi-National Company whose business interests include E-commerce, where they buy and store the inventory, and take care of everything from shipping and pricing to customer service and returns. I've created this dataset so that people can play with this dataset and do a lot of things as mentioned below\n",
      "\n",
      "* Dataset Walkthrough\n",
      "* Data Preprocessing\n",
      "* Understanding Dataset Hierarchy\n",
      "* Exploratory Data Analysis\n",
      "* Data Visualization *Using matlabplot and searborn*\n",
      "* Data Visualization *Using BI Tools such as **Tableau, PowerBI ,Kibana ,Grafana, Splunk***\n",
      "* **Making Recommendation System**\n",
      "* **Webscraping content of each products in detail**\n",
      "\n",
      "This is a list of some of that things that you can do on this dataset. It's not definitely limited to the one that is mentioned there but a lot more other things can also be done.\n",
      "name 'detect' is not defined Language is not detected: amazon-products-dataset\n",
      "name 'detect' is not defined Language is not detected: lokeshparab\n",
      "name 'detect' is not defined Language is not detected: Amazon Products Sales Dataset 2023\n",
      "name 'detect' is not defined Language is not detected: This dataset has 142 Categories and 300K+ Products Details\n",
      "name 'detect' is not defined Language is not detected: # About\n",
      "## This is a Product Sales Dataset scraped from the [Amazon website](https://www.amazon.com/) \n",
      "* Its product data are separated by 142 categories in csv format, along with the full dataset name **Amazon-Products.csv**. \n",
      "* Each csv files are consist of 10 columns and each row has products details accordingly\n",
      "\n",
      "# Features\n",
      "| name                 |  description                      |\n",
      "|-----------------|--------------------------|\n",
      "|  *name*                |  The name of the product |\n",
      "|  *main_category* |   The main category of the product belong |\n",
      "|  *sub_category*   |  The main category of the product belong  |\n",
      "|  *image*                |   The image of the product look like          |\n",
      "|  *link*                    |  The amazon website reference link of the product   |\n",
      "|  *ratings*              |  The ratings given by amazon customers of the product           |\n",
      "|  *no of ratings*    |  The number of ratings given to this product in amazon shopping   |\n",
      "|  *discount_price* |   The discount prices of the product  |\n",
      "|  *actual_price*     |  The actual MRP of the product  |\n",
      "\n",
      "# Inspiration\n",
      "Amazon is an American Tech Multi-National Company whose business interests include E-commerce, where they buy and store the inventory, and take care of everything from shipping and pricing to customer service and returns. I've created this dataset so that people can play with this dataset and do a lot of things as mentioned below\n",
      "\n",
      "* Dataset Walkthrough\n",
      "* Data Preprocessing\n",
      "* Understanding Dataset Hierarchy\n",
      "* Exploratory Data Analysis\n",
      "* Data Visualization *Using matlabplot and searborn*\n",
      "* Data Visualization *Using BI Tools such as **Tableau, PowerBI ,Kibana ,Grafana, Splunk***\n",
      "* **Making Recommendation System**\n",
      "* **Webscraping content of each products in detail**\n",
      "\n",
      "This is a list of some of that things that you can do on this dataset. It's not definitely limited to the one that is mentioned there but a lot more other things can also be done.\n",
      "name 'detect' is not defined Language is not detected: sujaykapadnis/whether-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: whether-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: sujaykapadnis\n",
      "name 'detect' is not defined Language is not detected: Whether Prediction dataset\n",
      "name 'detect' is not defined Language is not detected: EUROPEAN CLIMATE ASSESSMENT & DATASET (ECA&D) Datastet\n",
      "name 'detect' is not defined Language is not detected: File descriptions\n",
      "\n",
      "**weather_prediction_dataset.csv** - Main data file, tabular data, comma-separated CSV. Contains the data for different weather features (daily observations, see below for more details) for 18 European cities or places through the years 2000 to 2010.\n",
      "**weather_prediction_picnic_labels.csv **- Optional data to be used as potential labels for classification tasks. Contains booleans to characterize the daily weather conditions as suitable for a picnic (True) or not (False) for all 18 locations in the dataset.\n",
      "**metadata.txt **- Further information on the dataset, the data processing, and conversion, as well as the description and units of all weather features.\n",
      "\n",
      "Huber, F., van Kuppevelt, D., Steinbach, P., Sauze, C., Liu, Y., & Weel, B. (2022). Weather prediction dataset (Version v5) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7525955\n",
      "name 'detect' is not defined Language is not detected: whether-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: sujaykapadnis\n",
      "name 'detect' is not defined Language is not detected: Whether Prediction dataset\n",
      "name 'detect' is not defined Language is not detected: EUROPEAN CLIMATE ASSESSMENT & DATASET (ECA&D) Datastet\n",
      "name 'detect' is not defined Language is not detected: File descriptions\n",
      "\n",
      "**weather_prediction_dataset.csv** - Main data file, tabular data, comma-separated CSV. Contains the data for different weather features (daily observations, see below for more details) for 18 European cities or places through the years 2000 to 2010.\n",
      "**weather_prediction_picnic_labels.csv **- Optional data to be used as potential labels for classification tasks. Contains booleans to characterize the daily weather conditions as suitable for a picnic (True) or not (False) for all 18 locations in the dataset.\n",
      "**metadata.txt **- Further information on the dataset, the data processing, and conversion, as well as the description and units of all weather features.\n",
      "\n",
      "Huber, F., van Kuppevelt, D., Steinbach, P., Sauze, C., Liu, Y., & Weel, B. (2022). Weather prediction dataset (Version v5) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7525955\n",
      "name 'detect' is not defined Language is not detected: ceneksanzak/spotify-song-features\n",
      "name 'detect' is not defined Language is not detected: spotify-song-features\n",
      "name 'detect' is not defined Language is not detected: ceneksanzak\n",
      "name 'detect' is not defined Language is not detected: Spotify Song Features\n",
      "name 'detect' is not defined Language is not detected: Song-Album-Artist names and features data from Spotify API\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is from Spotify API. It was for my university project about a song recommendation program using prolog language. I processed the data and turned it to csv format\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are track name,  track id, album name and artist names as string. Other values are features of song in float type.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I have this data thanks to Spotify API and our course assistant.  \n",
      "name 'detect' is not defined Language is not detected: spotify-song-features\n",
      "name 'detect' is not defined Language is not detected: ceneksanzak\n",
      "name 'detect' is not defined Language is not detected: Spotify Song Features\n",
      "name 'detect' is not defined Language is not detected: Song-Album-Artist names and features data from Spotify API\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is from Spotify API. It was for my university project about a song recommendation program using prolog language. I processed the data and turned it to csv format\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are track name,  track id, album name and artist names as string. Other values are features of song in float type.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "I have this data thanks to Spotify API and our course assistant.  \n",
      "name 'detect' is not defined Language is not detected: aguado/nyc-taxi-jan-aug-2022\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-jan-aug-2022\n",
      "name 'detect' is not defined Language is not detected: aguado\n",
      "name 'detect' is not defined Language is not detected: NYC Taxi Jan-Aug 2022\n",
      "name 'detect' is not defined Language is not detected: New York Ciry Taxi Dataset Janaury & August 2022\n",
      "name 'detect' is not defined Language is not detected: ## Description\n",
      "\n",
      "TLC Trip Record Data\n",
      "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
      "\n",
      "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
      "\n",
      "## Proposed tasks\n",
      "\n",
      "* Identify the routes with the lowest price/km, time/km, and price/time ratios.\n",
      "* Visualize the evolution of the average travel time and distance throughout the day.\n",
      "* Calculate the probability of traveling from one zone to another in less than X minutes (where X is easily modifiable).\n",
      "* Analyze the zones where it is most likely to take a taxi based on the time of day.\n",
      "* Determine the best time of day to travel to the airport.\n",
      "* Design a model that predicts the travel duration and cost given the hour, origin, and destination zone. Show the relevance of the dataset attributes.\n",
      "\n",
      "## Features\n",
      "\n",
      "VendorID: An ID code indicating the taxi vendor, 1 for Creative Mobile Technologies, LLC and 2 for VeriFone Inc.\n",
      "lpep_pickup_datetime: The date and time when the taxi ride started.\n",
      "lpep_dropoff_datetime: The date and time when the taxi ride ended.\n",
      "store_and_fwd_flag: Indicates whether the trip record was held in vehicle memory before sending to the vendor, Y=store and forward; N=not a store and forward trip.\n",
      "RatecodeID: The rate code for the trip, 1=Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride\n",
      "PULocationID: The pickup location ID, corresponding to the taxi zone where the taximeter was engaged.\n",
      "DOLocationID: The dropoff location ID, corresponding to the taxi zone where the taximeter was disengaged.\n",
      "passenger_count: The number of passengers in the vehicle.\n",
      "trip_distance: The distance of the trip in miles.\n",
      "fare_amount: The metered fare for the trip.\n",
      "extra: Extra charges. Currently, this only includes the 0.5 dollars and 1 dollar rush hour and overnight charges.\n",
      "mta_tax: The 0.50 dollars MTA tax that is automatically triggered based on the metered rate in use.\n",
      "tip_amount: Tip amount – This field is automatically populated for credit card tips. Cash tips are not included.\n",
      "tolls_amount: Total amount of all tolls paid in trip.\n",
      "ehail_fee: This is a $1.00 surcharge that is automatically applied to every trip booked through the ehail platform.\n",
      "improvement_surcharge: 0.30 dollars improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.\n",
      "total_amount: The total amount charged to passengers. This field includes the metered fare, extra charges, mta_tax, tip_amount and tolls_amount plus any improvement_surcharge or ehail_fee.\n",
      "payment_type: A numeric code indicating the payment method: 1= Credit card, 2= Cash, 3= No charge, 4= Dispute, 5= Unknown, 6= Voided trip.\n",
      "trip_type: A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be overridden by the driver.\n",
      "congestion_surcharge: 2.75 dollars congestion surcharge assessed trips in yellow and green taxis in Manhattan south of 96th St. The surcharge began being levied in 2019.\n",
      "\n",
      "## License\n",
      "\n",
      "The data from the New York City Taxi and Limousine Commission (TLC) Trip Record Data website is available to the public under the Open Data Commons Open Database License (ODbL). This license allows for the use, sharing, and modification of the data as long as attribution is given to the original source and any derivative works are also licensed under the ODbL.\n",
      "\n",
      "## Citation\n",
      "\n",
      "New York City Taxi and Limousine Commission (2019). TLC Trip Record Data. Retrieved from https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-jan-aug-2022\n",
      "name 'detect' is not defined Language is not detected: aguado\n",
      "name 'detect' is not defined Language is not detected: NYC Taxi Jan-Aug 2022\n",
      "name 'detect' is not defined Language is not detected: New York Ciry Taxi Dataset Janaury & August 2022\n",
      "name 'detect' is not defined Language is not detected: ## Description\n",
      "\n",
      "TLC Trip Record Data\n",
      "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
      "\n",
      "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
      "\n",
      "## Proposed tasks\n",
      "\n",
      "* Identify the routes with the lowest price/km, time/km, and price/time ratios.\n",
      "* Visualize the evolution of the average travel time and distance throughout the day.\n",
      "* Calculate the probability of traveling from one zone to another in less than X minutes (where X is easily modifiable).\n",
      "* Analyze the zones where it is most likely to take a taxi based on the time of day.\n",
      "* Determine the best time of day to travel to the airport.\n",
      "* Design a model that predicts the travel duration and cost given the hour, origin, and destination zone. Show the relevance of the dataset attributes.\n",
      "\n",
      "## Features\n",
      "\n",
      "VendorID: An ID code indicating the taxi vendor, 1 for Creative Mobile Technologies, LLC and 2 for VeriFone Inc.\n",
      "lpep_pickup_datetime: The date and time when the taxi ride started.\n",
      "lpep_dropoff_datetime: The date and time when the taxi ride ended.\n",
      "store_and_fwd_flag: Indicates whether the trip record was held in vehicle memory before sending to the vendor, Y=store and forward; N=not a store and forward trip.\n",
      "RatecodeID: The rate code for the trip, 1=Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride\n",
      "PULocationID: The pickup location ID, corresponding to the taxi zone where the taximeter was engaged.\n",
      "DOLocationID: The dropoff location ID, corresponding to the taxi zone where the taximeter was disengaged.\n",
      "passenger_count: The number of passengers in the vehicle.\n",
      "trip_distance: The distance of the trip in miles.\n",
      "fare_amount: The metered fare for the trip.\n",
      "extra: Extra charges. Currently, this only includes the 0.5 dollars and 1 dollar rush hour and overnight charges.\n",
      "mta_tax: The 0.50 dollars MTA tax that is automatically triggered based on the metered rate in use.\n",
      "tip_amount: Tip amount – This field is automatically populated for credit card tips. Cash tips are not included.\n",
      "tolls_amount: Total amount of all tolls paid in trip.\n",
      "ehail_fee: This is a $1.00 surcharge that is automatically applied to every trip booked through the ehail platform.\n",
      "improvement_surcharge: 0.30 dollars improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.\n",
      "total_amount: The total amount charged to passengers. This field includes the metered fare, extra charges, mta_tax, tip_amount and tolls_amount plus any improvement_surcharge or ehail_fee.\n",
      "payment_type: A numeric code indicating the payment method: 1= Credit card, 2= Cash, 3= No charge, 4= Dispute, 5= Unknown, 6= Voided trip.\n",
      "trip_type: A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be overridden by the driver.\n",
      "congestion_surcharge: 2.75 dollars congestion surcharge assessed trips in yellow and green taxis in Manhattan south of 96th St. The surcharge began being levied in 2019.\n",
      "\n",
      "## License\n",
      "\n",
      "The data from the New York City Taxi and Limousine Commission (TLC) Trip Record Data website is available to the public under the Open Data Commons Open Database License (ODbL). This license allows for the use, sharing, and modification of the data as long as attribution is given to the original source and any derivative works are also licensed under the ODbL.\n",
      "\n",
      "## Citation\n",
      "\n",
      "New York City Taxi and Limousine Commission (2019). TLC Trip Record Data. Retrieved from https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
      "name 'detect' is not defined Language is not detected: nguyenphd/goodreads-best-books-ever-with-recommendations\n",
      "name 'detect' is not defined Language is not detected: goodreads-best-books-ever-with-recommendations\n",
      "name 'detect' is not defined Language is not detected: nguyenphd\n",
      "name 'detect' is not defined Language is not detected: Goodreads Best Books Ever with Recommendations\n",
      "name 'detect' is not defined Language is not detected: Manually scraped from Goodreads\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset was created for data exploration and to build a recommendation system based on authors, genres, and descriptions. \n",
      "\n",
      "### Content \n",
      "\n",
      "First 10000 books from the [Best Books Ever List](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) on Goodreads. This dataset is not much different from other Goodreads datasets here but I include a column where the **recommendations for each book** (last column) are recorded for evaluation of the recommendation system based on the data available.\n",
      "\n",
      "The dataset will be updated with more books (up to about 55000) when the need arises to have more data. \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: goodreads-best-books-ever-with-recommendations\n",
      "name 'detect' is not defined Language is not detected: nguyenphd\n",
      "name 'detect' is not defined Language is not detected: Goodreads Best Books Ever with Recommendations\n",
      "name 'detect' is not defined Language is not detected: Manually scraped from Goodreads\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset was created for data exploration and to build a recommendation system based on authors, genres, and descriptions. \n",
      "\n",
      "### Content \n",
      "\n",
      "First 10000 books from the [Best Books Ever List](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) on Goodreads. This dataset is not much different from other Goodreads datasets here but I include a column where the **recommendations for each book** (last column) are recorded for evaluation of the recommendation system based on the data available.\n",
      "\n",
      "The dataset will be updated with more books (up to about 55000) when the need arises to have more data. \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: davispeixoto/codenation-enem2\n",
      "name 'detect' is not defined Language is not detected: codenation-enem2\n",
      "name 'detect' is not defined Language is not detected: davispeixoto\n",
      "name 'detect' is not defined Language is not detected: Codenation - Enem-2\n",
      "name 'detect' is not defined Language is not detected: This is codenation challenge enem-2 datasets\n",
      "name 'detect' is not defined Language is not detected: ### Enem 2016\n",
      "\n",
      "In Brazil there is a high school national grading exam called ENEM (Exame Nacional do Ensino Médio) which takes place yearly, and sets grades for admission on most universities and colleges.\n",
      "\n",
      "There is also a contest oriented website in Brazil called [Codenation](https://www.codenation.com.br/), and this dataset is relative to its [enem-2 contest](https://www.codenation.com.br/journey/data-science/challenge/enem-2.html) where we should learn regression by predicting some grades\n",
      "\n",
      "### Content\n",
      "\n",
      "We have two datasets - train.csv e test.csv - for using to predict exam grades using regression. But the train file is a good amount of data from ENEM 2016 and contains most of the columns for those who would like to do some EDA.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Codenation. Ministério da Educação do Governo Federal do Brasil.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Can we predict the math grade based on other information?\n",
      "name 'detect' is not defined Language is not detected: codenation-enem2\n",
      "name 'detect' is not defined Language is not detected: davispeixoto\n",
      "name 'detect' is not defined Language is not detected: Codenation - Enem-2\n",
      "name 'detect' is not defined Language is not detected: This is codenation challenge enem-2 datasets\n",
      "name 'detect' is not defined Language is not detected: ### Enem 2016\n",
      "\n",
      "In Brazil there is a high school national grading exam called ENEM (Exame Nacional do Ensino Médio) which takes place yearly, and sets grades for admission on most universities and colleges.\n",
      "\n",
      "There is also a contest oriented website in Brazil called [Codenation](https://www.codenation.com.br/), and this dataset is relative to its [enem-2 contest](https://www.codenation.com.br/journey/data-science/challenge/enem-2.html) where we should learn regression by predicting some grades\n",
      "\n",
      "### Content\n",
      "\n",
      "We have two datasets - train.csv e test.csv - for using to predict exam grades using regression. But the train file is a good amount of data from ENEM 2016 and contains most of the columns for those who would like to do some EDA.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Codenation. Ministério da Educação do Governo Federal do Brasil.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Can we predict the math grade based on other information?\n",
      "name 'detect' is not defined Language is not detected: tanmoyx/covid19-patient-precondition-dataset\n",
      "name 'detect' is not defined Language is not detected: covid19-patient-precondition-dataset\n",
      "name 'detect' is not defined Language is not detected: tanmoyx\n",
      "name 'detect' is not defined Language is not detected: COVID-19 patient pre-condition dataset\n",
      "name 'detect' is not defined Language is not detected: Data obtained from Mexican government data set \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "A health crisis of massive proportion such as the current COVID-9 pandemic provides us with an opportunity to ponder and reflect over what we can better in the way we deal with healthcare to make us humans be more prepared and enabled to combat such an event in the future.\n",
      "During the entire course of the pandemic, one of the main problems that healthcare providers have faced is the shortage of medical resources and a proper plan to efficiently distribute them.\n",
      "They have been in the dark failing to understand how much resource they could even in the very next week as the COVID-19 curve has swayed very unpredictably. In these tough times, being able to predict what kind of resource an individual might require at the time of being tested positive or even before that will be of great help to the authorities as they would be able to procure and arrange for the resources necessary to save the life of that patient.\n",
      "\n",
      "### Content\n",
      "\n",
      "While the above are lofty thoughts, procuring patient data of COVID-19 patients containing patient-specific information regarding patient history and habits is a different ball game altogether. This is mainly due to the regulatory security laws such as HIPAA and GDPR which makes it almost impossible for anyone to get hands-on PHI data. I spend literally days and nights searching for a suitable data-set, called up people I knew for any directions towards a data-set which might be of use to me. Finally, I found this data-set https://www.gob.mx/salud/documentos/datos-abiertos-152127 which was released by the Mexican government. This data-set contains a huge number of anonymised patient-related information.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: covid19-patient-precondition-dataset\n",
      "name 'detect' is not defined Language is not detected: tanmoyx\n",
      "name 'detect' is not defined Language is not detected: COVID-19 patient pre-condition dataset\n",
      "name 'detect' is not defined Language is not detected: Data obtained from Mexican government data set \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "A health crisis of massive proportion such as the current COVID-9 pandemic provides us with an opportunity to ponder and reflect over what we can better in the way we deal with healthcare to make us humans be more prepared and enabled to combat such an event in the future.\n",
      "During the entire course of the pandemic, one of the main problems that healthcare providers have faced is the shortage of medical resources and a proper plan to efficiently distribute them.\n",
      "They have been in the dark failing to understand how much resource they could even in the very next week as the COVID-19 curve has swayed very unpredictably. In these tough times, being able to predict what kind of resource an individual might require at the time of being tested positive or even before that will be of great help to the authorities as they would be able to procure and arrange for the resources necessary to save the life of that patient.\n",
      "\n",
      "### Content\n",
      "\n",
      "While the above are lofty thoughts, procuring patient data of COVID-19 patients containing patient-specific information regarding patient history and habits is a different ball game altogether. This is mainly due to the regulatory security laws such as HIPAA and GDPR which makes it almost impossible for anyone to get hands-on PHI data. I spend literally days and nights searching for a suitable data-set, called up people I knew for any directions towards a data-set which might be of use to me. Finally, I found this data-set https://www.gob.mx/salud/documentos/datos-abiertos-152127 which was released by the Mexican government. This data-set contains a huge number of anonymised patient-related information.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: tugberkkaran/used-car-listings-features-and-prices-carscom\n",
      "name 'detect' is not defined Language is not detected: used-car-listings-features-and-prices-carscom\n",
      "name 'detect' is not defined Language is not detected: tugberkkaran\n",
      "name 'detect' is not defined Language is not detected: Used Car Listings: Features and Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Used Car Listings: Features & Prices Dataset (cars.com)\n",
      "name 'detect' is not defined Language is not detected: ## Dataset Description\n",
      "\n",
      "This dataset contains information about used car listings obtained through data scraping from cars.com. The dataset includes various features of the cars such as brand, model, year, mileage, engine details, transmission type, fuel type, drivetrain, and more. Additionally, it provides details about specific features like adaptive cruise control, navigation system, power liftgate, backup camera, and others.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13093068%2F0328263f1d41ad62833ce28a4a2b3d61%2Fprivate-car-deal.jpg?generation=1686487163548092&alt=media)\n",
      "\n",
      "The purpose of this dataset is to facilitate the development and evaluation of machine learning models for car price prediction. The dataset can be used for tasks such as regression analysis, feature engineering, and model training.\n",
      "\n",
      "Please note that the dataset is collected from a specific source and represents a snapshot of used car listings at a particular point in time. The dataset may contain missing values and some discrepancies. It is recommended to perform data preprocessing and exploratory analysis before using it for research or predictive modeling.\n",
      "\n",
      "We hope that this dataset serves as a valuable resource for the data science community and contributes to the advancement of automotive analytics and pricing models.\n",
      "\n",
      "## Feature Descriptions\n",
      "- **brand**: Brand of the car.\n",
      "- **model**: Model of the car.\n",
      "- **year**: Year of production of the car.\n",
      "- **mileage**: Mileage of the car.\n",
      "- **engine**: Information about the car's engine.\n",
      "- **engine_size**: Size of the car's engine.\n",
      "- **transmission**: Type of transmission of the car.\n",
      "- **automatic_transmission**: A binary value indicating the presence of automatic transmission (1: Yes, 0: No).\n",
      "- **fuel_type**: Fuel type of the car.\n",
      "- **drivetrain**: Drivetrain type of the car.\n",
      "- **min_mpg**: Minimum fuel efficiency of the car.\n",
      "- **max_mpg**: Maximum fuel efficiency of the car.\n",
      "- **damaged**: A binary value indicating the presence of damage in the car (1: Yes, 0: No).\n",
      "- **first_owner**: Is the car a 1-owner vehicle? (1: Yes, 0: No).\n",
      "- **personal_using**: Is the car for personal use only? (1: Yes, 0: No).\n",
      "- **turbo**: A binary value indicating the presence of a turbocharger in the car (1: Yes, 0: No).\n",
      "- **alloy_wheels**: Are there alloy wheels on the car? (1: Yes, 0: No).\n",
      "- **adaptive_cruise_control**: A binary value indicating the presence of adaptive cruise control (1: Yes, 0: No).\n",
      "- **navigation_system**: A binary value indicating the presence of a navigation system (1: Yes, 0: No).\n",
      "- **power_liftgate**: A binary value indicating the presence of a power liftgate (1: Yes, 0: No).\n",
      "- **backup_camera**: A binary value indicating the presence of a backup camera (1: Yes, 0: No).\n",
      "- **keyless_start**: A binary value indicating the presence of keyless start system (1: Yes, 0: No).\n",
      "- **remote_start**: A binary value indicating the presence of a remote start system (1: Yes, 0: No).\n",
      "- **sunroof/moonroof**: A binary value indicating the presence of a sunroof/moonroof (1: Yes, 0: No).\n",
      "- **automatic_emergency_braking**: A binary value indicating the presence of automatic emergency braking system (1: Yes, 0: No).\n",
      "- **stability_control**: A binary value indicating the presence of stability control system (1: Yes, 0: No).\n",
      "- **leather_seats**: Are there leather seats in the car? (1: Yes, 0: No).\n",
      "- **memory_seat**: Are there memory seats in the car? (1: Yes, 0: No).\n",
      "- **third_row_seating**: A binary value indicating the presence of third row seating (1: Yes, 0: No).\n",
      "- **apple_car_play/android_auto**: A binary value indicating the presence of Apple CarPlay / Android Auto integration (1: Yes, 0: No).\n",
      "- **bluetooth**: A binary value indicating the presence of Bluetooth connectivity (1: Yes, 0: No).\n",
      "- **usb_port**: A binary value indicating the presence of USB ports (1: Yes, 0: No).\n",
      "- **heated_seats**: Are there heated seats in the car? (1: Yes, 0: No).\n",
      "- **interior_color**: Interior color of the car.\n",
      "- **exterior_color**: Exterior color of the car.\n",
      "- **price**: Price of the car. This feature is the target feature of this dataset.\n",
      "name 'detect' is not defined Language is not detected: used-car-listings-features-and-prices-carscom\n",
      "name 'detect' is not defined Language is not detected: tugberkkaran\n",
      "name 'detect' is not defined Language is not detected: Used Car Listings: Features and Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Used Car Listings: Features & Prices Dataset (cars.com)\n",
      "name 'detect' is not defined Language is not detected: ## Dataset Description\n",
      "\n",
      "This dataset contains information about used car listings obtained through data scraping from cars.com. The dataset includes various features of the cars such as brand, model, year, mileage, engine details, transmission type, fuel type, drivetrain, and more. Additionally, it provides details about specific features like adaptive cruise control, navigation system, power liftgate, backup camera, and others.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13093068%2F0328263f1d41ad62833ce28a4a2b3d61%2Fprivate-car-deal.jpg?generation=1686487163548092&alt=media)\n",
      "\n",
      "The purpose of this dataset is to facilitate the development and evaluation of machine learning models for car price prediction. The dataset can be used for tasks such as regression analysis, feature engineering, and model training.\n",
      "\n",
      "Please note that the dataset is collected from a specific source and represents a snapshot of used car listings at a particular point in time. The dataset may contain missing values and some discrepancies. It is recommended to perform data preprocessing and exploratory analysis before using it for research or predictive modeling.\n",
      "\n",
      "We hope that this dataset serves as a valuable resource for the data science community and contributes to the advancement of automotive analytics and pricing models.\n",
      "\n",
      "## Feature Descriptions\n",
      "- **brand**: Brand of the car.\n",
      "- **model**: Model of the car.\n",
      "- **year**: Year of production of the car.\n",
      "- **mileage**: Mileage of the car.\n",
      "- **engine**: Information about the car's engine.\n",
      "- **engine_size**: Size of the car's engine.\n",
      "- **transmission**: Type of transmission of the car.\n",
      "- **automatic_transmission**: A binary value indicating the presence of automatic transmission (1: Yes, 0: No).\n",
      "- **fuel_type**: Fuel type of the car.\n",
      "- **drivetrain**: Drivetrain type of the car.\n",
      "- **min_mpg**: Minimum fuel efficiency of the car.\n",
      "- **max_mpg**: Maximum fuel efficiency of the car.\n",
      "- **damaged**: A binary value indicating the presence of damage in the car (1: Yes, 0: No).\n",
      "- **first_owner**: Is the car a 1-owner vehicle? (1: Yes, 0: No).\n",
      "- **personal_using**: Is the car for personal use only? (1: Yes, 0: No).\n",
      "- **turbo**: A binary value indicating the presence of a turbocharger in the car (1: Yes, 0: No).\n",
      "- **alloy_wheels**: Are there alloy wheels on the car? (1: Yes, 0: No).\n",
      "- **adaptive_cruise_control**: A binary value indicating the presence of adaptive cruise control (1: Yes, 0: No).\n",
      "- **navigation_system**: A binary value indicating the presence of a navigation system (1: Yes, 0: No).\n",
      "- **power_liftgate**: A binary value indicating the presence of a power liftgate (1: Yes, 0: No).\n",
      "- **backup_camera**: A binary value indicating the presence of a backup camera (1: Yes, 0: No).\n",
      "- **keyless_start**: A binary value indicating the presence of keyless start system (1: Yes, 0: No).\n",
      "- **remote_start**: A binary value indicating the presence of a remote start system (1: Yes, 0: No).\n",
      "- **sunroof/moonroof**: A binary value indicating the presence of a sunroof/moonroof (1: Yes, 0: No).\n",
      "- **automatic_emergency_braking**: A binary value indicating the presence of automatic emergency braking system (1: Yes, 0: No).\n",
      "- **stability_control**: A binary value indicating the presence of stability control system (1: Yes, 0: No).\n",
      "- **leather_seats**: Are there leather seats in the car? (1: Yes, 0: No).\n",
      "- **memory_seat**: Are there memory seats in the car? (1: Yes, 0: No).\n",
      "- **third_row_seating**: A binary value indicating the presence of third row seating (1: Yes, 0: No).\n",
      "- **apple_car_play/android_auto**: A binary value indicating the presence of Apple CarPlay / Android Auto integration (1: Yes, 0: No).\n",
      "- **bluetooth**: A binary value indicating the presence of Bluetooth connectivity (1: Yes, 0: No).\n",
      "- **usb_port**: A binary value indicating the presence of USB ports (1: Yes, 0: No).\n",
      "- **heated_seats**: Are there heated seats in the car? (1: Yes, 0: No).\n",
      "- **interior_color**: Interior color of the car.\n",
      "- **exterior_color**: Exterior color of the car.\n",
      "- **price**: Price of the car. This feature is the target feature of this dataset.\n",
      "name 'detect' is not defined Language is not detected: navpallav/av-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: av-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: navpallav\n",
      "name 'detect' is not defined Language is not detected: AV Recommendation System\n",
      "name 'detect' is not defined Language is not detected: av-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: navpallav\n",
      "name 'detect' is not defined Language is not detected: AV Recommendation System\n",
      "name 'detect' is not defined Language is not detected: navpallav/av-recommendation-system-new\n",
      "name 'detect' is not defined Language is not detected: av-recommendation-system-new\n",
      "name 'detect' is not defined Language is not detected: navpallav\n",
      "name 'detect' is not defined Language is not detected: AV_Recommendation_System_New\n",
      "name 'detect' is not defined Language is not detected: av-recommendation-system-new\n",
      "name 'detect' is not defined Language is not detected: navpallav\n",
      "name 'detect' is not defined Language is not detected: AV_Recommendation_System_New\n",
      "name 'detect' is not defined Language is not detected: vin1234/janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: vin1234\n",
      "name 'detect' is not defined Language is not detected: #Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "name 'detect' is not defined Language is not detected: ## Problem Statement\n",
      "Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "### Data Dictionary \n",
      "<ul>train.csv</ul>\n",
      "\n",
      "| Column      | Description                               |\n",
      "|-------------|:------------------------------:|\n",
      "|ID                |Unique ID for each article        |\n",
      "|TITLE          |Title of the research article    |\n",
      "|ABSTRACT       |\tAbstract of the research article|\n",
      "|Computer Science          |Whether article belongs to topic computer science (1/0)|\n",
      "|Physics\t     |Whether article belongs to topic physics (1/0)   |\n",
      "|Mathematics\t      |Whether article belongs to topic Mathematics (1/0)   |\n",
      "|Statistics\t |Whether article belongs to topic Statistics (1/0)      ||\n",
      "|Quantitative Biology\t |Whether article belongs to topic Quantitative Biology (1/0)|\n",
      "|Quantitative Finance         |Whether article belongs to topic Quantitative Finance (1/0)  |\n",
      "\n",
      "\n",
      "\n",
      "|ID                         |Unique ID for each article     |\n",
      "|----------------------|:-----------------------:|\n",
      "|TITLE                |Title of the research article   |\n",
      "|ABSTRACT\t  |Abstract of the research article  |\n",
      "\n",
      "\n",
      "\n",
      "|ID                         |Unique ID for each article     |\n",
      "|----------------------|:-----------------------:|\n",
      "|TITLE          |Title of the research article    |\n",
      "|ABSTRACT       |\tAbstract of the research article|\n",
      "|Computer Science          |Whether article belongs to topic computer science (1/0)|\n",
      "|Physics\t     |Whether article belongs to topic physics (1/0)   |\n",
      "|Mathematics\t      |Whether article belongs to topic Mathematics (1/0)   |\n",
      "|Statistics\t |Whether article belongs to topic Statistics (1/0)      ||\n",
      "|Quantitative Biology\t |Whether article belongs to topic Quantitative Biology (1/0)|\n",
      "|Quantitative Finance         |Whether article belongs to topic Quantitative Finance (1/0)  |\n",
      "\n",
      "\n",
      "## Evaluation Metric\n",
      "\n",
      "- Submissions are evaluated on __micro F1 Score__ between the predicted and observed topics for each article in the test set.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: vin1234\n",
      "name 'detect' is not defined Language is not detected: #Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "name 'detect' is not defined Language is not detected: ## Problem Statement\n",
      "Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "### Data Dictionary \n",
      "<ul>train.csv</ul>\n",
      "\n",
      "| Column      | Description                               |\n",
      "|-------------|:------------------------------:|\n",
      "|ID                |Unique ID for each article        |\n",
      "|TITLE          |Title of the research article    |\n",
      "|ABSTRACT       |\tAbstract of the research article|\n",
      "|Computer Science          |Whether article belongs to topic computer science (1/0)|\n",
      "|Physics\t     |Whether article belongs to topic physics (1/0)   |\n",
      "|Mathematics\t      |Whether article belongs to topic Mathematics (1/0)   |\n",
      "|Statistics\t |Whether article belongs to topic Statistics (1/0)      ||\n",
      "|Quantitative Biology\t |Whether article belongs to topic Quantitative Biology (1/0)|\n",
      "|Quantitative Finance         |Whether article belongs to topic Quantitative Finance (1/0)  |\n",
      "\n",
      "\n",
      "\n",
      "|ID                         |Unique ID for each article     |\n",
      "|----------------------|:-----------------------:|\n",
      "|TITLE                |Title of the research article   |\n",
      "|ABSTRACT\t  |Abstract of the research article  |\n",
      "\n",
      "\n",
      "\n",
      "|ID                         |Unique ID for each article     |\n",
      "|----------------------|:-----------------------:|\n",
      "|TITLE          |Title of the research article    |\n",
      "|ABSTRACT       |\tAbstract of the research article|\n",
      "|Computer Science          |Whether article belongs to topic computer science (1/0)|\n",
      "|Physics\t     |Whether article belongs to topic physics (1/0)   |\n",
      "|Mathematics\t      |Whether article belongs to topic Mathematics (1/0)   |\n",
      "|Statistics\t |Whether article belongs to topic Statistics (1/0)      ||\n",
      "|Quantitative Biology\t |Whether article belongs to topic Quantitative Biology (1/0)|\n",
      "|Quantitative Finance         |Whether article belongs to topic Quantitative Finance (1/0)  |\n",
      "\n",
      "\n",
      "## Evaluation Metric\n",
      "\n",
      "- Submissions are evaluated on __micro F1 Score__ between the predicted and observed topics for each article in the test set.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: ddatad/course-enrollments-dataset\n",
      "name 'detect' is not defined Language is not detected: course-enrollments-dataset\n",
      "name 'detect' is not defined Language is not detected: ddatad\n",
      "name 'detect' is not defined Language is not detected: Course Enrollments Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset for Course Recommendation System\n",
      "name 'detect' is not defined Language is not detected: In an online learning scenario, we have learners or students as users who enrolled in courses. In fact, to follow the standard recommender system naming convention, we call each learner a user, each course an item, and the enrollment mode or interaction as rating. So that's why we have columns named user, item, and rating instead of using learner, course, and enrollment.\n",
      "\n",
      "Source: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/ratings.csv\n",
      "\n",
      "## user column\n",
      "- user index\n",
      "\n",
      "## item column\n",
      "- course code\n",
      "\n",
      "## rating column\n",
      "- rating = 0 (Not Enrolled)\n",
      "- rating = 1 (Enrolled)\n",
      "name 'detect' is not defined Language is not detected: course-enrollments-dataset\n",
      "name 'detect' is not defined Language is not detected: ddatad\n",
      "name 'detect' is not defined Language is not detected: Course Enrollments Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset for Course Recommendation System\n",
      "name 'detect' is not defined Language is not detected: In an online learning scenario, we have learners or students as users who enrolled in courses. In fact, to follow the standard recommender system naming convention, we call each learner a user, each course an item, and the enrollment mode or interaction as rating. So that's why we have columns named user, item, and rating instead of using learner, course, and enrollment.\n",
      "\n",
      "Source: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/ratings.csv\n",
      "\n",
      "## user column\n",
      "- user index\n",
      "\n",
      "## item column\n",
      "- course code\n",
      "\n",
      "## rating column\n",
      "- rating = 0 (Not Enrolled)\n",
      "- rating = 1 (Enrolled)\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411/wikihow-features\n",
      "name 'detect' is not defined Language is not detected: wikihow-features\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411\n",
      "name 'detect' is not defined Language is not detected: wikiHow Features\n",
      "name 'detect' is not defined Language is not detected: Features extracted from wikiHow articles\n",
      "name 'detect' is not defined Language is not detected: wikiHow Features dataset consists of features extracted from ~19.9k wikiHow articles. The task is to predict the `percent_helpful` column, which represents the number of readers who found the article helpful.\n",
      "\n",
      "The articles are collected using the open-source tool [wikiHowUnofficialAPI](https://pypi.org/project/wikihowunofficialapi/).\n",
      "\n",
      "The dataset is introduced in the (under review) paper Automated Quality Estimation of Collaboratively Created Content. If you find it helpful, please cite the paper.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: wikihow-features\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411\n",
      "name 'detect' is not defined Language is not detected: wikiHow Features\n",
      "name 'detect' is not defined Language is not detected: Features extracted from wikiHow articles\n",
      "name 'detect' is not defined Language is not detected: wikiHow Features dataset consists of features extracted from ~19.9k wikiHow articles. The task is to predict the `percent_helpful` column, which represents the number of readers who found the article helpful.\n",
      "\n",
      "The articles are collected using the open-source tool [wikiHowUnofficialAPI](https://pypi.org/project/wikihowunofficialapi/).\n",
      "\n",
      "The dataset is introduced in the (under review) paper Automated Quality Estimation of Collaboratively Created Content. If you find it helpful, please cite the paper.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411/wikihow-raw-data\n",
      "name 'detect' is not defined Language is not detected: wikihow-raw-data\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411\n",
      "name 'detect' is not defined Language is not detected: wikiHow Raw Data\n",
      "name 'detect' is not defined Language is not detected: Raw extracted wikiHow articles\n",
      "name 'detect' is not defined Language is not detected: wikiHow Raw Data dataset consists of all relevant information about ~19.9k wikiHow articles.\n",
      "\n",
      "The features of the [wikiHow Features](https://www.kaggle.com/datasets/aniketsharma00411/wikihow-features) dataset are extracted from this raw data.\n",
      "\n",
      "The articles are collected using the open-source tool [wikiHowUnofficialAPI](https://pypi.org/project/wikihowunofficialapi/).\n",
      "\n",
      "How to load and use the data? Check [this](https://www.kaggle.com/code/aniketsharma00411/using-wikihow-raw-data) notebook.\n",
      "\n",
      "The dataset is introduced in the (under review) paper Automated Quality Estimation of Collaboratively Created Content. If you find it helpful, please cite the paper.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: wikihow-raw-data\n",
      "name 'detect' is not defined Language is not detected: aniketsharma00411\n",
      "name 'detect' is not defined Language is not detected: wikiHow Raw Data\n",
      "name 'detect' is not defined Language is not detected: Raw extracted wikiHow articles\n",
      "name 'detect' is not defined Language is not detected: wikiHow Raw Data dataset consists of all relevant information about ~19.9k wikiHow articles.\n",
      "\n",
      "The features of the [wikiHow Features](https://www.kaggle.com/datasets/aniketsharma00411/wikihow-features) dataset are extracted from this raw data.\n",
      "\n",
      "The articles are collected using the open-source tool [wikiHowUnofficialAPI](https://pypi.org/project/wikihowunofficialapi/).\n",
      "\n",
      "How to load and use the data? Check [this](https://www.kaggle.com/code/aniketsharma00411/using-wikihow-raw-data) notebook.\n",
      "\n",
      "The dataset is introduced in the (under review) paper Automated Quality Estimation of Collaboratively Created Content. If you find it helpful, please cite the paper.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: jsun13/toronto-subway-delay-data\n",
      "name 'detect' is not defined Language is not detected: toronto-subway-delay-data\n",
      "name 'detect' is not defined Language is not detected: jsun13\n",
      "name 'detect' is not defined Language is not detected: Toronto Subway Delay Data\n",
      "name 'detect' is not defined Language is not detected: Toronto Subway Delay and Time Length between Trains in Minutes (Jan 17-Jun 21)\n",
      "name 'detect' is not defined Language is not detected: \tSUB RMENU CODE\tCODE DESCRIPTION\n",
      "1\tEUAC\tAir Conditioning\n",
      "2\tEUAL\tAlternating Current\n",
      "3\tEUATC\tATC RC&S Equipment\n",
      "4\tEUBK\tBrakes\n",
      "5\tEUBO\tBody\n",
      "6\tEUCA\tCompressed Air\n",
      "7\tEUCD\tConsequential Delay (2nd Delay Same Fault)\n",
      "8\tEUCH\tChopper Control\n",
      "9\tEUCO\tCouplers\n",
      "10\tEUDO\tDoor Problems - Faulty Equipment\n",
      "11\tEUECD\tECD / Line Mechanic Related Prob.\n",
      "12\tEUHV\tHigh Voltage\n",
      "13\tEULT\tLighting System\n",
      "14\tEULV\tLow Voltage\n",
      "15\tEUME\tRC&S Maintenance Error - (Human)\n",
      "16\tEUNEA\tNo Equipment Available\n",
      "17\tEUNT\tEquipment - No Trouble Found\n",
      "18\tEUO\tRC&S Other\n",
      "19\tEUOE\tRail Cars & Shops Opr. Error\n",
      "20\tEUOPO\tOPTO RC&S Non-Train Door Monitoring\n",
      "21\tEUPI\tPropulsion System\n",
      "22\tEUSC\tSpeed Control Equipment  \n",
      "23\tEUTL\tTrainline System\n",
      "24\tEUTM\tTraction Motors\n",
      "25\tEUTR\tTrucks\n",
      "26\tEUTRD\tTR Cab Doors\n",
      "27\tEUVA\tWarning Alarm Systems\n",
      "28\tEUVE\tWork Vehicle\n",
      "29\tEUYRD\tYard/Carhouse Related Problems\n",
      "30\tMUATC\tATC Project\n",
      "31\tMUCL\tDivisional Clerk Related\n",
      "32\tMUD\tDoor Problems - Passenger Related\n",
      "33\tMUDD\tDoor Problems - Debris Related\n",
      "34\tMUEC\tMisc. Engineering & Construction Related Problems\n",
      "35\tMUESA\tNo Operator Immediately Available\n",
      "36\tMUFM\tForce Majeure\n",
      "37\tMUFS\tFire/Smoke Plan B - Source External to TTC\n",
      "38\tMUGD\tMiscellaneous General Delays\n",
      "39\tMUI\tInjured or ill Customer (On Train) - Transported\n",
      "40\tMUIE\tInjured Employee\n",
      "41\tMUIR\tInjured or ill Customer (On Train) - Medical Aid Refused\n",
      "42\tMUIRS\tInjured or ill Customer (In Station) - Medical Aid Refused\n",
      "43\tMUIS\tInjured or ill Customer (In Station) - Transported\n",
      "44\tMULD\tLabour Dispute - Subway\n",
      "45\tMUNOA\tNo Operator Immediately Available - Not E.S.A. Related \n",
      "46\tMUO\tMiscellaneous Other\n",
      "47\tMUODC\tOverhead Door Contact\n",
      "48\tMUPAA\tPassenger Assistance Alarm Activated - No Trouble Found\n",
      "49\tMUPLA\tFire/Smoke Plan A\n",
      "50\tMUPLB\tFire/Smoke Plan B - Source TTC\n",
      "51\tMUPLC\tFire/Smoke Plan C\n",
      "52\tMUPR1\tPriority One - Train in Contact With Person\n",
      "53\tMUSAN\tUnsanitary Vehicle\n",
      "54\tMUSC\tMiscellaneous Speed Control\n",
      "55\tMUTD\tTraining Department Related Delays\n",
      "56\tMUTO\tMisc. Transportation Other - Employee Non-Chargeable\n",
      "57\tMUWEA\tWeather Reports / Related Delays\n",
      "58\tMUWR\tWork Refusal\n",
      "59\tPUATC\tATC Signals Other\n",
      "60\tPUCBI\tCentral Logic Controller Failure\n",
      "61\tPUCSC\tSignal Control Problem - Signals\n",
      "62\tPUCSS\tCentral Office Signalling System\n",
      "63\tPUDCS\tData Communications System Failure\n",
      "64\tPUMEL\tEscalator/Elevator Incident\n",
      "65\tPUMO\tStation Other\n",
      "66\tPUMST\tStation Stairway Incident \n",
      "67\tPUOPO\tOPTO (COMMS) Train Door Monitoring\n",
      "68\tPUSAC\tSignals Axle Counter Block Failure\n",
      "69\tPUSBE\tBeacon Failure\n",
      "70\tPUSCA\tSCADA Related Problems \n",
      "71\tPUSCR\tSubway Car Radio Fault \n",
      "72\tPUSEA\tEAS Failure                               \n",
      "73\tPUSI\tSignals or Related Components Failure \n",
      "74\tPUSIO\tSmart IO Failure\n",
      "75\tPUSIS\tSignals Track Weather Related\n",
      "76\tPUSLC\tSignals Line Countroller Failure\n",
      "77\tPUSNT\tSignal Problem - No Trouble\n",
      "78\tPUSO\tS/E/C Department Other\n",
      "79\tPUSRA\tSubway Radio System Fault \n",
      "80\tPUSSW\tTrack Switch Failure - Signal Related Problem\n",
      "81\tPUSTC\tSignals - Track Circuit Problems\n",
      "82\tPUSTP\tTraction Power or Related Components Failure\n",
      "83\tPUSTS\tSignals - Train Stops\n",
      "84\tPUSWZ\tWork Zone Problems - Signals\n",
      "85\tPUSZC\tSignals Zone Countroller Failure\n",
      "86\tPUTCD\tT & S Contractor Problems\n",
      "87\tPUTD\tTrack Level Debris - Controllable\n",
      "88\tPUTDN \tDebris At Track Level - Uncontrollable\n",
      "89\tPUTIJ\tInsulated Joint Related Problem\n",
      "90\tPUTIS\tIce / Snow Related Problems\n",
      "91\tPUTNT\tT&S Related Problem - NTF\n",
      "92\tPUTO\tT&S Other\n",
      "93\tPUTOE\tT & S Operator Related Problems\n",
      "94\tPUTR\tRail Related Problem\n",
      "95\tPUTS\tStructure Related Problem\n",
      "96\tPUTSC\tSignal Control Problem - Track\n",
      "97\tPUTSM\tTrack Switch Failure - Track Related Problem\n",
      "98\tPUTTC\tTrack Circuit Problems - Re: Defective Bolts/Bonding\n",
      "99\tPUTTP\tTraction Power Rail Related\n",
      "100\tPUTWZ\tWork Zone Problems - Track\n",
      "101\tSUAE\tAssault / Employee Involved\n",
      "102\tSUAP\tAssault / Patron Involved\n",
      "103\tSUBT\tBomb Threat\n",
      "104\tSUCOL\tCollector Booth Alarm Activated\n",
      "105\tSUDP\tDisorderly Patron\n",
      "106\tSUEAS\tEmergency Alarm Station Activation\n",
      "107\tSUG\tGraffiti / Scratchiti\n",
      "108\tSUO\tPassenger Other\n",
      "109\tSUPOL\tHeld By Polce - Non-TTC Related\n",
      "110\tSUROB\tRobbery\n",
      "111\tSUSA\tSexual Assault\n",
      "112\tSUSP\tSuspicious Package\n",
      "113\tSUUT\tUnauthorized at Track Level\n",
      "114\tTUATC\tATC Operator Related\n",
      "115\tTUCC\tTransit Control Related Problems\n",
      "116\tTUDOE\tDoors Open in Error\n",
      "117\tTUKEY\tTwo Drum Switch Keys Activated\n",
      "118\tTUML\tMainline Storage\n",
      "119\tTUMVS\tOperator Violated Signal\n",
      "120\tTUNIP\tOperator Not In Position\n",
      "121\tTUNOA\tNo Operator Immediately Available \n",
      "122\tTUO\tTransportation Department - Other\n",
      "123\tTUOPO\tOPTO Operator Related\n",
      "124\tTUOS\tOperator Overshot Platform\n",
      "125\tTUS\tCrew Unable to Maintain Schedule\n",
      "126\tTUSC\tOperator Overspeeding\n",
      "127\tTUSET\tTrain Controls Improperly Shut Down\n",
      "128\tTUST\tStorm Trains\n",
      "129\tTUSUP\tSupervisory Error\n",
      "\n",
      "\n",
      "\tSRT RMENU CODE\tCODE DESCRIPTION\n",
      "1\tERAC\tAir Conditioning\n",
      "2\tERBO\tBody\n",
      "3\tERCD\tConsequential Delay (2nd Delay Same Fault)\n",
      "4\tERCO\tCouplers\n",
      "5\tERDB\tDisc Brakes\n",
      "6\tERDO\tDoor Problems - Faulty Equipment\n",
      "7\tERHV\tHigh Voltage\n",
      "8\tERLT\tLighting System\n",
      "9\tERLV\tLow Voltage\n",
      "10\tERME\tRC&S Maintenance Error - (Human)\n",
      "11\tERNEA\tNo Equipment Available\n",
      "12\tERNT\tEquipment - No Trouble Found\n",
      "13\tERO\tRC&S Other\n",
      "14\tERPR\tPropulsion System\n",
      "15\tERRA\tRadio\n",
      "16\tERTB\tTrack Brakes\n",
      "17\tERTC\tTrain Control - VOBC\n",
      "18\tERTL\tTrainline System\n",
      "19\tERTR\tTrucks\n",
      "20\tERVE\tVehicle\n",
      "21\tERWA\tWarning Alarm Systems\n",
      "22\tERWS\tWind Shield\n",
      "23\tMRCL\tDivisional Clerk Related\n",
      "24\tMRD\tDoor Problems - Passenger Related\n",
      "25\tMRDD\tDoor Problems - Debris Related\n",
      "26\tMREC\tMisc. Engineering & Construction Related Problems\n",
      "27\tMRESA\tNo Operator Immediately Available\n",
      "28\tMRFS\tFire/Smoke Plan B - Source External to TTC\n",
      "29\tMRIE\tInjured Employee\n",
      "30\tMRLD\tLabour Dispute - Subway\n",
      "31\tMRNOA\tNo Operator Immediately Available - Not E.S.A. Related \n",
      "32\tMRO\tMiscellaneous Other\n",
      "33\tMRPAA\tPassenger Assistance Alarm Activated - No Trouble Found\n",
      "34\tMRPLA\tFire/Smoke Plan A\n",
      "35\tMRPLB\tFire/Smoke Plan B\n",
      "36\tMRPLC\tFire/Smoke Plan C\n",
      "37\tMRPR1\tPriority One - Train in Contact With Person\n",
      "38\tMRSAN\tUnsanitary Vehicle\n",
      "39\tMRSTM\tScheduled Track Maintenance\n",
      "40\tMRTO\tTimeout\n",
      "41\tMRUI\tInjured or ill Customer (On Train) - Transported\n",
      "42\tMRUIR\tInjured or ill Customer (On Train) - Medical Aid Refused\n",
      "43\tMRWEA\tWeather Reports / Related Delays\n",
      "44\tPREL\tEscalator/Elevator Incident\n",
      "45\tPRO\tOther\n",
      "46\tPRS\tVCC/RCIU/CCR\n",
      "47\tPRSA\tAxle Counter Related\n",
      "48\tPRSL\tLoop Related Failures\n",
      "49\tPRSO\tSignals Other\n",
      "50\tPRSP\tSignals Power Supply Related\n",
      "51\tPRST\tStation Stairway Incident \n",
      "52\tPRSW\tSwitch Related\n",
      "53\tPRTST\tSignals - Train Stops\n",
      "54\tPRW\tRail Defect/Fastenings/Power Rail\n",
      "55\tSRAE\tAssault / Employee Involved\n",
      "56\tSRAP\tAssault / Patron Involved\n",
      "57\tSRBT\tBomb Threat\n",
      "58\tSRCOL\tCollector Booth Alarm Activated\n",
      "59\tSRDP\tDisorderly Patron\n",
      "60\tSREAS\tEmergency Alarm Station Activation\n",
      "61\tSRO\tPassenger Other\n",
      "62\tSRSA\tSexual Assault\n",
      "63\tSRSP\tSuspicious Package\n",
      "64\tSRUT\tUnauthorized at Track Level\n",
      "65\tTRDOE\tDoors Open in Error\n",
      "66\tTRNIP\tOperator Not In Position\n",
      "67\tTRNOA\tNo Operator Immediately Available \n",
      "68\tTRO\tTransportation Department - Other\n",
      "69\tTRSET\tTrain Controls Improperly Shut Down\n",
      "70\tTRST\tStorm Trains\n",
      "71\tTRTC\tTransit Control Related Problem\n",
      "name 'detect' is not defined Language is not detected: toronto-subway-delay-data\n",
      "name 'detect' is not defined Language is not detected: jsun13\n",
      "name 'detect' is not defined Language is not detected: Toronto Subway Delay Data\n",
      "name 'detect' is not defined Language is not detected: Toronto Subway Delay and Time Length between Trains in Minutes (Jan 17-Jun 21)\n",
      "name 'detect' is not defined Language is not detected: \tSUB RMENU CODE\tCODE DESCRIPTION\n",
      "1\tEUAC\tAir Conditioning\n",
      "2\tEUAL\tAlternating Current\n",
      "3\tEUATC\tATC RC&S Equipment\n",
      "4\tEUBK\tBrakes\n",
      "5\tEUBO\tBody\n",
      "6\tEUCA\tCompressed Air\n",
      "7\tEUCD\tConsequential Delay (2nd Delay Same Fault)\n",
      "8\tEUCH\tChopper Control\n",
      "9\tEUCO\tCouplers\n",
      "10\tEUDO\tDoor Problems - Faulty Equipment\n",
      "11\tEUECD\tECD / Line Mechanic Related Prob.\n",
      "12\tEUHV\tHigh Voltage\n",
      "13\tEULT\tLighting System\n",
      "14\tEULV\tLow Voltage\n",
      "15\tEUME\tRC&S Maintenance Error - (Human)\n",
      "16\tEUNEA\tNo Equipment Available\n",
      "17\tEUNT\tEquipment - No Trouble Found\n",
      "18\tEUO\tRC&S Other\n",
      "19\tEUOE\tRail Cars & Shops Opr. Error\n",
      "20\tEUOPO\tOPTO RC&S Non-Train Door Monitoring\n",
      "21\tEUPI\tPropulsion System\n",
      "22\tEUSC\tSpeed Control Equipment  \n",
      "23\tEUTL\tTrainline System\n",
      "24\tEUTM\tTraction Motors\n",
      "25\tEUTR\tTrucks\n",
      "26\tEUTRD\tTR Cab Doors\n",
      "27\tEUVA\tWarning Alarm Systems\n",
      "28\tEUVE\tWork Vehicle\n",
      "29\tEUYRD\tYard/Carhouse Related Problems\n",
      "30\tMUATC\tATC Project\n",
      "31\tMUCL\tDivisional Clerk Related\n",
      "32\tMUD\tDoor Problems - Passenger Related\n",
      "33\tMUDD\tDoor Problems - Debris Related\n",
      "34\tMUEC\tMisc. Engineering & Construction Related Problems\n",
      "35\tMUESA\tNo Operator Immediately Available\n",
      "36\tMUFM\tForce Majeure\n",
      "37\tMUFS\tFire/Smoke Plan B - Source External to TTC\n",
      "38\tMUGD\tMiscellaneous General Delays\n",
      "39\tMUI\tInjured or ill Customer (On Train) - Transported\n",
      "40\tMUIE\tInjured Employee\n",
      "41\tMUIR\tInjured or ill Customer (On Train) - Medical Aid Refused\n",
      "42\tMUIRS\tInjured or ill Customer (In Station) - Medical Aid Refused\n",
      "43\tMUIS\tInjured or ill Customer (In Station) - Transported\n",
      "44\tMULD\tLabour Dispute - Subway\n",
      "45\tMUNOA\tNo Operator Immediately Available - Not E.S.A. Related \n",
      "46\tMUO\tMiscellaneous Other\n",
      "47\tMUODC\tOverhead Door Contact\n",
      "48\tMUPAA\tPassenger Assistance Alarm Activated - No Trouble Found\n",
      "49\tMUPLA\tFire/Smoke Plan A\n",
      "50\tMUPLB\tFire/Smoke Plan B - Source TTC\n",
      "51\tMUPLC\tFire/Smoke Plan C\n",
      "52\tMUPR1\tPriority One - Train in Contact With Person\n",
      "53\tMUSAN\tUnsanitary Vehicle\n",
      "54\tMUSC\tMiscellaneous Speed Control\n",
      "55\tMUTD\tTraining Department Related Delays\n",
      "56\tMUTO\tMisc. Transportation Other - Employee Non-Chargeable\n",
      "57\tMUWEA\tWeather Reports / Related Delays\n",
      "58\tMUWR\tWork Refusal\n",
      "59\tPUATC\tATC Signals Other\n",
      "60\tPUCBI\tCentral Logic Controller Failure\n",
      "61\tPUCSC\tSignal Control Problem - Signals\n",
      "62\tPUCSS\tCentral Office Signalling System\n",
      "63\tPUDCS\tData Communications System Failure\n",
      "64\tPUMEL\tEscalator/Elevator Incident\n",
      "65\tPUMO\tStation Other\n",
      "66\tPUMST\tStation Stairway Incident \n",
      "67\tPUOPO\tOPTO (COMMS) Train Door Monitoring\n",
      "68\tPUSAC\tSignals Axle Counter Block Failure\n",
      "69\tPUSBE\tBeacon Failure\n",
      "70\tPUSCA\tSCADA Related Problems \n",
      "71\tPUSCR\tSubway Car Radio Fault \n",
      "72\tPUSEA\tEAS Failure                               \n",
      "73\tPUSI\tSignals or Related Components Failure \n",
      "74\tPUSIO\tSmart IO Failure\n",
      "75\tPUSIS\tSignals Track Weather Related\n",
      "76\tPUSLC\tSignals Line Countroller Failure\n",
      "77\tPUSNT\tSignal Problem - No Trouble\n",
      "78\tPUSO\tS/E/C Department Other\n",
      "79\tPUSRA\tSubway Radio System Fault \n",
      "80\tPUSSW\tTrack Switch Failure - Signal Related Problem\n",
      "81\tPUSTC\tSignals - Track Circuit Problems\n",
      "82\tPUSTP\tTraction Power or Related Components Failure\n",
      "83\tPUSTS\tSignals - Train Stops\n",
      "84\tPUSWZ\tWork Zone Problems - Signals\n",
      "85\tPUSZC\tSignals Zone Countroller Failure\n",
      "86\tPUTCD\tT & S Contractor Problems\n",
      "87\tPUTD\tTrack Level Debris - Controllable\n",
      "88\tPUTDN \tDebris At Track Level - Uncontrollable\n",
      "89\tPUTIJ\tInsulated Joint Related Problem\n",
      "90\tPUTIS\tIce / Snow Related Problems\n",
      "91\tPUTNT\tT&S Related Problem - NTF\n",
      "92\tPUTO\tT&S Other\n",
      "93\tPUTOE\tT & S Operator Related Problems\n",
      "94\tPUTR\tRail Related Problem\n",
      "95\tPUTS\tStructure Related Problem\n",
      "96\tPUTSC\tSignal Control Problem - Track\n",
      "97\tPUTSM\tTrack Switch Failure - Track Related Problem\n",
      "98\tPUTTC\tTrack Circuit Problems - Re: Defective Bolts/Bonding\n",
      "99\tPUTTP\tTraction Power Rail Related\n",
      "100\tPUTWZ\tWork Zone Problems - Track\n",
      "101\tSUAE\tAssault / Employee Involved\n",
      "102\tSUAP\tAssault / Patron Involved\n",
      "103\tSUBT\tBomb Threat\n",
      "104\tSUCOL\tCollector Booth Alarm Activated\n",
      "105\tSUDP\tDisorderly Patron\n",
      "106\tSUEAS\tEmergency Alarm Station Activation\n",
      "107\tSUG\tGraffiti / Scratchiti\n",
      "108\tSUO\tPassenger Other\n",
      "109\tSUPOL\tHeld By Polce - Non-TTC Related\n",
      "110\tSUROB\tRobbery\n",
      "111\tSUSA\tSexual Assault\n",
      "112\tSUSP\tSuspicious Package\n",
      "113\tSUUT\tUnauthorized at Track Level\n",
      "114\tTUATC\tATC Operator Related\n",
      "115\tTUCC\tTransit Control Related Problems\n",
      "116\tTUDOE\tDoors Open in Error\n",
      "117\tTUKEY\tTwo Drum Switch Keys Activated\n",
      "118\tTUML\tMainline Storage\n",
      "119\tTUMVS\tOperator Violated Signal\n",
      "120\tTUNIP\tOperator Not In Position\n",
      "121\tTUNOA\tNo Operator Immediately Available \n",
      "122\tTUO\tTransportation Department - Other\n",
      "123\tTUOPO\tOPTO Operator Related\n",
      "124\tTUOS\tOperator Overshot Platform\n",
      "125\tTUS\tCrew Unable to Maintain Schedule\n",
      "126\tTUSC\tOperator Overspeeding\n",
      "127\tTUSET\tTrain Controls Improperly Shut Down\n",
      "128\tTUST\tStorm Trains\n",
      "129\tTUSUP\tSupervisory Error\n",
      "\n",
      "\n",
      "\tSRT RMENU CODE\tCODE DESCRIPTION\n",
      "1\tERAC\tAir Conditioning\n",
      "2\tERBO\tBody\n",
      "3\tERCD\tConsequential Delay (2nd Delay Same Fault)\n",
      "4\tERCO\tCouplers\n",
      "5\tERDB\tDisc Brakes\n",
      "6\tERDO\tDoor Problems - Faulty Equipment\n",
      "7\tERHV\tHigh Voltage\n",
      "8\tERLT\tLighting System\n",
      "9\tERLV\tLow Voltage\n",
      "10\tERME\tRC&S Maintenance Error - (Human)\n",
      "11\tERNEA\tNo Equipment Available\n",
      "12\tERNT\tEquipment - No Trouble Found\n",
      "13\tERO\tRC&S Other\n",
      "14\tERPR\tPropulsion System\n",
      "15\tERRA\tRadio\n",
      "16\tERTB\tTrack Brakes\n",
      "17\tERTC\tTrain Control - VOBC\n",
      "18\tERTL\tTrainline System\n",
      "19\tERTR\tTrucks\n",
      "20\tERVE\tVehicle\n",
      "21\tERWA\tWarning Alarm Systems\n",
      "22\tERWS\tWind Shield\n",
      "23\tMRCL\tDivisional Clerk Related\n",
      "24\tMRD\tDoor Problems - Passenger Related\n",
      "25\tMRDD\tDoor Problems - Debris Related\n",
      "26\tMREC\tMisc. Engineering & Construction Related Problems\n",
      "27\tMRESA\tNo Operator Immediately Available\n",
      "28\tMRFS\tFire/Smoke Plan B - Source External to TTC\n",
      "29\tMRIE\tInjured Employee\n",
      "30\tMRLD\tLabour Dispute - Subway\n",
      "31\tMRNOA\tNo Operator Immediately Available - Not E.S.A. Related \n",
      "32\tMRO\tMiscellaneous Other\n",
      "33\tMRPAA\tPassenger Assistance Alarm Activated - No Trouble Found\n",
      "34\tMRPLA\tFire/Smoke Plan A\n",
      "35\tMRPLB\tFire/Smoke Plan B\n",
      "36\tMRPLC\tFire/Smoke Plan C\n",
      "37\tMRPR1\tPriority One - Train in Contact With Person\n",
      "38\tMRSAN\tUnsanitary Vehicle\n",
      "39\tMRSTM\tScheduled Track Maintenance\n",
      "40\tMRTO\tTimeout\n",
      "41\tMRUI\tInjured or ill Customer (On Train) - Transported\n",
      "42\tMRUIR\tInjured or ill Customer (On Train) - Medical Aid Refused\n",
      "43\tMRWEA\tWeather Reports / Related Delays\n",
      "44\tPREL\tEscalator/Elevator Incident\n",
      "45\tPRO\tOther\n",
      "46\tPRS\tVCC/RCIU/CCR\n",
      "47\tPRSA\tAxle Counter Related\n",
      "48\tPRSL\tLoop Related Failures\n",
      "49\tPRSO\tSignals Other\n",
      "50\tPRSP\tSignals Power Supply Related\n",
      "51\tPRST\tStation Stairway Incident \n",
      "52\tPRSW\tSwitch Related\n",
      "53\tPRTST\tSignals - Train Stops\n",
      "54\tPRW\tRail Defect/Fastenings/Power Rail\n",
      "55\tSRAE\tAssault / Employee Involved\n",
      "56\tSRAP\tAssault / Patron Involved\n",
      "57\tSRBT\tBomb Threat\n",
      "58\tSRCOL\tCollector Booth Alarm Activated\n",
      "59\tSRDP\tDisorderly Patron\n",
      "60\tSREAS\tEmergency Alarm Station Activation\n",
      "61\tSRO\tPassenger Other\n",
      "62\tSRSA\tSexual Assault\n",
      "63\tSRSP\tSuspicious Package\n",
      "64\tSRUT\tUnauthorized at Track Level\n",
      "65\tTRDOE\tDoors Open in Error\n",
      "66\tTRNIP\tOperator Not In Position\n",
      "67\tTRNOA\tNo Operator Immediately Available \n",
      "68\tTRO\tTransportation Department - Other\n",
      "69\tTRSET\tTrain Controls Improperly Shut Down\n",
      "70\tTRST\tStorm Trains\n",
      "71\tTRTC\tTransit Control Related Problem\n",
      "name 'detect' is not defined Language is not detected: manishkc06/electronics-product-pricing-dataset\n",
      "name 'detect' is not defined Language is not detected: electronics-product-pricing-dataset\n",
      "name 'detect' is not defined Language is not detected: manishkc06\n",
      "name 'detect' is not defined Language is not detected: Electronics Product Pricing Dataset\n",
      "name 'detect' is not defined Language is not detected: Build a machine learning model to predict the price of electronic products\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "There are numerous electronic products that we use on a daily basis like computers, TV, heaters, mobile devices, etc. \n",
      "\n",
      "We buy various electronic products on a day-to-day basis online or offline on reliable prices. There could be various parameters which decide the price of an electronic product like brand, product name, the product condition (i.e. new or old), the merchant, category of the product, date and time of buying, etc.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4789522%2F34e685fa7eb8d7d0c21ab93a9d6a1a86%2Felectronic_product.jpg?generation=1608390049274879&alt=media)\n",
      "\n",
      "\n",
      "### Problem Statement\n",
      "Imagine that you have recently started a new electronic shop. You want to sell your products online and you need to come up with an appropriate selling price for each product. Being a data scientist you think of utilizing the power of data science on the available online data.\n",
      "\n",
      "### Data\n",
      "This dataset contains pricing information of electronic products. There are 25 columns including the target variable. Some of the variables are listed below:\n",
      "\n",
      "prices.availability: if the product is available at the given price\n",
      "prices.condition: condition of the product\n",
      "prices.currency: price currency\n",
      "prices.isSale: Is the product on sale at given price\n",
      "prices.merchant: The merchant\n",
      "imageURLs: product image url\n",
      "manufacturer: Manufacturer of the product\n",
      "manufacturerNumber: Manufacturer number\n",
      "name: Name of the product\n",
      "primaryCategories: Primary category of the product\n",
      "weight: weight of the product\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "The dataset is taken from data world.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: electronics-product-pricing-dataset\n",
      "name 'detect' is not defined Language is not detected: manishkc06\n",
      "name 'detect' is not defined Language is not detected: Electronics Product Pricing Dataset\n",
      "name 'detect' is not defined Language is not detected: Build a machine learning model to predict the price of electronic products\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "There are numerous electronic products that we use on a daily basis like computers, TV, heaters, mobile devices, etc. \n",
      "\n",
      "We buy various electronic products on a day-to-day basis online or offline on reliable prices. There could be various parameters which decide the price of an electronic product like brand, product name, the product condition (i.e. new or old), the merchant, category of the product, date and time of buying, etc.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4789522%2F34e685fa7eb8d7d0c21ab93a9d6a1a86%2Felectronic_product.jpg?generation=1608390049274879&alt=media)\n",
      "\n",
      "\n",
      "### Problem Statement\n",
      "Imagine that you have recently started a new electronic shop. You want to sell your products online and you need to come up with an appropriate selling price for each product. Being a data scientist you think of utilizing the power of data science on the available online data.\n",
      "\n",
      "### Data\n",
      "This dataset contains pricing information of electronic products. There are 25 columns including the target variable. Some of the variables are listed below:\n",
      "\n",
      "prices.availability: if the product is available at the given price\n",
      "prices.condition: condition of the product\n",
      "prices.currency: price currency\n",
      "prices.isSale: Is the product on sale at given price\n",
      "prices.merchant: The merchant\n",
      "imageURLs: product image url\n",
      "manufacturer: Manufacturer of the product\n",
      "manufacturerNumber: Manufacturer number\n",
      "name: Name of the product\n",
      "primaryCategories: Primary category of the product\n",
      "weight: weight of the product\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "The dataset is taken from data world.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: muhammadusmanfarooq/pakistan-solar-radiation-dataset\n",
      "name 'detect' is not defined Language is not detected: pakistan-solar-radiation-dataset\n",
      "name 'detect' is not defined Language is not detected: muhammadusmanfarooq\n",
      "name 'detect' is not defined Language is not detected: Pakistan Solar Radiation Dataset\n",
      "name 'detect' is not defined Language is not detected: Multiple cities solar radiation measurements dataset in different stations.\n",
      "name 'detect' is not defined Language is not detected: Multiple cities dataset in Pakistan including:\n",
      "- Karachi\n",
      "- Lahore\n",
      "- Islamabad\n",
      "- Peshawar\n",
      "\n",
      "There is also a combined CSV which includes combined data of these cities, consisting of:\n",
      " - 479618 rows\n",
      " - 16 columns\n",
      "\n",
      "\n",
      "Features metadata description:\n",
      "\n",
      "- 'time' = Date time\n",
      "- 'ghi_pyr' = GHI Pyranometer\n",
      "- 'dni' = Direct normal irradiance (DNI)\n",
      "- 'dhi' = Diffuse horizontal irradiance (DHI)\n",
      "- 'air_temperature' = Temperature\n",
      "- 'relative_humidity', = Humidity\n",
      "- 'wind_speed' = Wind speed\n",
      "- 'wind_speed_of_gust' = Speed of gust\n",
      "- 'wind_from_direction_st_dev', = Wind direction standard deviation\n",
      "- 'wind_from_direction' = Direction\n",
      "- 'barometric_pressure' = Pressure\n",
      "- 'sensor_cleaning' = Sensor cleaning\n",
      "- 'comments' = Station comments\n",
      "- 'actual_date' = Datetime\n",
      "- 'day_lenght' = Day lenght (sunset - sunrise)\n",
      "- 'ghi_rsi' = GHI Rotating Shadowband Irradiometer (RSI)\n",
      "\n",
      "## Please do UPVOTE to show your support  🙌 \n",
      "name 'detect' is not defined Language is not detected: pakistan-solar-radiation-dataset\n",
      "name 'detect' is not defined Language is not detected: muhammadusmanfarooq\n",
      "name 'detect' is not defined Language is not detected: Pakistan Solar Radiation Dataset\n",
      "name 'detect' is not defined Language is not detected: Multiple cities solar radiation measurements dataset in different stations.\n",
      "name 'detect' is not defined Language is not detected: Multiple cities dataset in Pakistan including:\n",
      "- Karachi\n",
      "- Lahore\n",
      "- Islamabad\n",
      "- Peshawar\n",
      "\n",
      "There is also a combined CSV which includes combined data of these cities, consisting of:\n",
      " - 479618 rows\n",
      " - 16 columns\n",
      "\n",
      "\n",
      "Features metadata description:\n",
      "\n",
      "- 'time' = Date time\n",
      "- 'ghi_pyr' = GHI Pyranometer\n",
      "- 'dni' = Direct normal irradiance (DNI)\n",
      "- 'dhi' = Diffuse horizontal irradiance (DHI)\n",
      "- 'air_temperature' = Temperature\n",
      "- 'relative_humidity', = Humidity\n",
      "- 'wind_speed' = Wind speed\n",
      "- 'wind_speed_of_gust' = Speed of gust\n",
      "- 'wind_from_direction_st_dev', = Wind direction standard deviation\n",
      "- 'wind_from_direction' = Direction\n",
      "- 'barometric_pressure' = Pressure\n",
      "- 'sensor_cleaning' = Sensor cleaning\n",
      "- 'comments' = Station comments\n",
      "- 'actual_date' = Datetime\n",
      "- 'day_lenght' = Day lenght (sunset - sunrise)\n",
      "- 'ghi_rsi' = GHI Rotating Shadowband Irradiometer (RSI)\n",
      "\n",
      "## Please do UPVOTE to show your support  🙌 \n",
      "name 'detect' is not defined Language is not detected: kshitijmohan/data-mining-2\n",
      "name 'detect' is not defined Language is not detected: data-mining-2\n",
      "name 'detect' is not defined Language is not detected: kshitijmohan\n",
      "name 'detect' is not defined Language is not detected: data mining 2\n",
      "name 'detect' is not defined Language is not detected: data-mining-2\n",
      "name 'detect' is not defined Language is not detected: kshitijmohan\n",
      "name 'detect' is not defined Language is not detected: data mining 2\n",
      "name 'detect' is not defined Language is not detected: ramoliyafenil/dataset\n",
      "name 'detect' is not defined Language is not detected: dataset\n",
      "name 'detect' is not defined Language is not detected: ramoliyafenil\n",
      "name 'detect' is not defined Language is not detected: The Flavors of India: A Recipe Collection\n",
      "name 'detect' is not defined Language is not detected: Discover the Rich Culinary Heritage of India\n",
      "name 'detect' is not defined Language is not detected: This dataset is a meticulously curated collection of authentic Indian recipes, capturing the diverse flavors and culinary traditions from various regions of the subcontinent. Each recipe provides detailed information, including the recipe name, a list of ingredients, total cooking time, cuisine type, a full description, a shortened and cleaned version of the description, the ingredient count, and a URL for reference. This comprehensive dataset offers a gateway to explore the vibrant and flavorful world of Indian cuisine, catering to food enthusiasts, chefs, and culinary researchers alike.\n",
      "Potential Real-World Applications and Advanced Usefulness:\n",
      "**1) Personalized Meal Planning and Recommendation Systems:** This dataset can be utilized to develop intelligent recommendation systems that suggest personalized meal plans or recipe recommendations based on an individual's dietary preferences, allergies, and taste profiles.\n",
      "**2) Nutritional Analysis and Dietary Guidance:** By combining this dataset with nutritional information, researchers and healthcare professionals can conduct in-depth analyses to provide dietary guidance, identify healthier alternatives, and support individuals with specific dietary requirements or health conditions.\n",
      "**3) Cultural and Culinary Education:** Educational institutions, culinary schools, and cultural organizations can leverage this dataset to develop interactive learning resources, teaching materials, or online courses focused on Indian cuisine, its history, and its cultural significance.\n",
      "**4) Food Industry Research and Development:** Food manufacturers, restaurants, and catering services can utilize this dataset to conduct market research, identify emerging trends, and develop new products or menu offerings tailored to specific regional preferences or dietary needs.\n",
      "**5) Computational Gastronomy and Flavor Analysis:** Researchers in the field of computational gastronomy can use this dataset to explore flavor combinations, ingredient interactions, and develop predictive models for novel recipe creation or flavor enhancement.\n",
      "**6) Culinary Tourism and Travel Planning:** Travel agencies, tour operators, and online platforms can integrate this dataset into their services, enabling travelers to plan culinary experiences, explore local cuisines, and discover authentic restaurants or cooking classes in various regions of India.\n",
      "**7) Food Photography and Content Creation:** Food bloggers, photographers, and content creators can leverage the descriptions and visuals from this dataset to enhance their storytelling, create engaging food-related content, and inspire their audiences with the rich flavors and traditions of Indian cuisine.\n",
      "name 'detect' is not defined Language is not detected: dataset\n",
      "name 'detect' is not defined Language is not detected: ramoliyafenil\n",
      "name 'detect' is not defined Language is not detected: The Flavors of India: A Recipe Collection\n",
      "name 'detect' is not defined Language is not detected: Discover the Rich Culinary Heritage of India\n",
      "name 'detect' is not defined Language is not detected: This dataset is a meticulously curated collection of authentic Indian recipes, capturing the diverse flavors and culinary traditions from various regions of the subcontinent. Each recipe provides detailed information, including the recipe name, a list of ingredients, total cooking time, cuisine type, a full description, a shortened and cleaned version of the description, the ingredient count, and a URL for reference. This comprehensive dataset offers a gateway to explore the vibrant and flavorful world of Indian cuisine, catering to food enthusiasts, chefs, and culinary researchers alike.\n",
      "Potential Real-World Applications and Advanced Usefulness:\n",
      "**1) Personalized Meal Planning and Recommendation Systems:** This dataset can be utilized to develop intelligent recommendation systems that suggest personalized meal plans or recipe recommendations based on an individual's dietary preferences, allergies, and taste profiles.\n",
      "**2) Nutritional Analysis and Dietary Guidance:** By combining this dataset with nutritional information, researchers and healthcare professionals can conduct in-depth analyses to provide dietary guidance, identify healthier alternatives, and support individuals with specific dietary requirements or health conditions.\n",
      "**3) Cultural and Culinary Education:** Educational institutions, culinary schools, and cultural organizations can leverage this dataset to develop interactive learning resources, teaching materials, or online courses focused on Indian cuisine, its history, and its cultural significance.\n",
      "**4) Food Industry Research and Development:** Food manufacturers, restaurants, and catering services can utilize this dataset to conduct market research, identify emerging trends, and develop new products or menu offerings tailored to specific regional preferences or dietary needs.\n",
      "**5) Computational Gastronomy and Flavor Analysis:** Researchers in the field of computational gastronomy can use this dataset to explore flavor combinations, ingredient interactions, and develop predictive models for novel recipe creation or flavor enhancement.\n",
      "**6) Culinary Tourism and Travel Planning:** Travel agencies, tour operators, and online platforms can integrate this dataset into their services, enabling travelers to plan culinary experiences, explore local cuisines, and discover authentic restaurants or cooking classes in various regions of India.\n",
      "**7) Food Photography and Content Creation:** Food bloggers, photographers, and content creators can leverage the descriptions and visuals from this dataset to enhance their storytelling, create engaging food-related content, and inspire their audiences with the rich flavors and traditions of Indian cuisine.\n",
      "name 'detect' is not defined Language is not detected: ocakhsn/istanbul-airbnb-dataset\n",
      "name 'detect' is not defined Language is not detected: istanbul-airbnb-dataset\n",
      "name 'detect' is not defined Language is not detected: ocakhsn\n",
      "name 'detect' is not defined Language is not detected: Istanbul Airbnb Dataset\n",
      "name 'detect' is not defined Language is not detected: Istanbul Airbnb Dataset with Features \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset collected from [airbnb](http://insideairbnb.com/get-the-data.html). It is collected to see how airbnb is used in Turkey Istanbul.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are 16 columns which shows the latitude, longitude etc. It also shows the price. So, a regression problem such as finding the price of an house can be applied to this dataset. To see an example you can check  [my notebook](https://www.kaggle.com/ocakhsn/airbnb-price-prediction-multiple-datasets) from airbnb newyork dataset\n",
      "name 'detect' is not defined Language is not detected: istanbul-airbnb-dataset\n",
      "name 'detect' is not defined Language is not detected: ocakhsn\n",
      "name 'detect' is not defined Language is not detected: Istanbul Airbnb Dataset\n",
      "name 'detect' is not defined Language is not detected: Istanbul Airbnb Dataset with Features \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset collected from [airbnb](http://insideairbnb.com/get-the-data.html). It is collected to see how airbnb is used in Turkey Istanbul.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "There are 16 columns which shows the latitude, longitude etc. It also shows the price. So, a regression problem such as finding the price of an house can be applied to this dataset. To see an example you can check  [my notebook](https://www.kaggle.com/ocakhsn/airbnb-price-prediction-multiple-datasets) from airbnb newyork dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic/urban-sound\n",
      "name 'detect' is not defined Language is not detected: urban-sound\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Research Articles Dataset\n",
      "name 'detect' is not defined Language is not detected: Predict the topics for each article from the abstract and titles\n",
      "name 'detect' is not defined Language is not detected: ### Context and Content\n",
      "\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "\n",
      "Note that a research article can possibly have more than 1 topics. The research article abstracts are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- Given the abstract and titles for a set of research articles, predict the topics for each article included in the test set. \n",
      "name 'detect' is not defined Language is not detected: urban-sound\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Research Articles Dataset\n",
      "name 'detect' is not defined Language is not detected: Predict the topics for each article from the abstract and titles\n",
      "name 'detect' is not defined Language is not detected: ### Context and Content\n",
      "\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "\n",
      "Note that a research article can possibly have more than 1 topics. The research article abstracts are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- Given the abstract and titles for a set of research articles, predict the topics for each article included in the test set. \n",
      "name 'detect' is not defined Language is not detected: arashnic/book-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: book-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Book Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Build state-of-the-art models for book recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.\n",
      "In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).\n",
      "\n",
      "Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.\n",
      "\n",
      "\n",
      "\n",
      "#   \n",
      "\n",
      "![child](https://galeriemagazine.com/wp-content/uploads/2018/05/StuttgartSelect.jpg)\n",
      "*Image: Stuttgart City Library | Stuttgart, Germany, PHOTO: DIETER WEINELT, FLICKR*\n",
      "#     \n",
      "### Content\n",
      "\n",
      "The Book-Crossing dataset comprises 3 files.\n",
      "- Users: \n",
      "Contains the users. Note that user IDs (`User-ID`) have been anonymized and map to integers. Demographic data is provided (`Location`, `Age`) if available. Otherwise, these fields contain NULL-values.\n",
      "#  \n",
      "- Books: \n",
      "Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (`Image-URL-S`, `Image-URL-M`, `Image-URL-L`), i.e., small, medium, large. These URLs point to the Amazon web site.\n",
      "#  \n",
      "- Ratings: \n",
      "Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
      "#  \n",
      "\n",
      "### Starter Kernel(s)\n",
      "- [Recom I: Data Understanding and Simple Recommendation](https://www.kaggle.com/arashnic/recom-i-data-understanding-and-simple-recomm)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the [Book-Crossing community](https://www.bookcrossing.com/?) with kind permission from Ron Hornbaker, CTO of Humankind Systems. Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
      "\n",
      "## More Readings\n",
      "####  My Recommendation Article Series  in Medium: \n",
      "- [Evolution of Recommendation Algorithms, Part I: Fundamentals , History Overview, Core and Classical Algorithms](https://medium.com/@anicomanesh/evolution-of-recommendation-algorithms-part-i-fundamentals-and-classical-recommendation-bb1c0bce78a9)\n",
      "\n",
      "![Taxonomy](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxHsxS8-e15ZgipX9ZCBJQ.png)\n",
      "\n",
      "\n",
      "##   \n",
      "\n",
      "#     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: book-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Book Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Build state-of-the-art models for book recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.\n",
      "In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).\n",
      "\n",
      "Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.\n",
      "\n",
      "\n",
      "\n",
      "#   \n",
      "\n",
      "![child](https://galeriemagazine.com/wp-content/uploads/2018/05/StuttgartSelect.jpg)\n",
      "*Image: Stuttgart City Library | Stuttgart, Germany, PHOTO: DIETER WEINELT, FLICKR*\n",
      "#     \n",
      "### Content\n",
      "\n",
      "The Book-Crossing dataset comprises 3 files.\n",
      "- Users: \n",
      "Contains the users. Note that user IDs (`User-ID`) have been anonymized and map to integers. Demographic data is provided (`Location`, `Age`) if available. Otherwise, these fields contain NULL-values.\n",
      "#  \n",
      "- Books: \n",
      "Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (`Image-URL-S`, `Image-URL-M`, `Image-URL-L`), i.e., small, medium, large. These URLs point to the Amazon web site.\n",
      "#  \n",
      "- Ratings: \n",
      "Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
      "#  \n",
      "\n",
      "### Starter Kernel(s)\n",
      "- [Recom I: Data Understanding and Simple Recommendation](https://www.kaggle.com/arashnic/recom-i-data-understanding-and-simple-recomm)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the [Book-Crossing community](https://www.bookcrossing.com/?) with kind permission from Ron Hornbaker, CTO of Humankind Systems. Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
      "\n",
      "## More Readings\n",
      "####  My Recommendation Article Series  in Medium: \n",
      "- [Evolution of Recommendation Algorithms, Part I: Fundamentals , History Overview, Core and Classical Algorithms](https://medium.com/@anicomanesh/evolution-of-recommendation-algorithms-part-i-fundamentals-and-classical-recommendation-bb1c0bce78a9)\n",
      "\n",
      "![Taxonomy](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxHsxS8-e15ZgipX9ZCBJQ.png)\n",
      "\n",
      "\n",
      "##   \n",
      "\n",
      "#     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: arashnic/mind-news-dataset\n",
      "name 'detect' is not defined Language is not detected: mind-news-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: MIND: Microsoft News Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Implement news recommendation methods \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset.\n",
      "\n",
      "What's New (June 21, 2021)\n",
      "We have a new release Recommenders 0.6.0!\n",
      "\n",
      "Recommenders is now on PyPI and can be installed using pip! In addition there are lots of bug fixes and utilities improvements.\n",
      "\n",
      "Here you can find the PyPi page: https://pypi.org/project/recommenders/\n",
      "\n",
      "Here you can find the package documentation: https://microsoft-recommenders.readthedocs.io/en/latest/\n",
      "\n",
      "### Content\n",
      "\n",
      "The MIND dataset for news recommendation was collected from anonymized behavior logs of Microsoft News website. The data randomly sampled 1 million users who had at least 5 news clicks during 6 weeks from October 12 to November 22, 2019. To protect user privacy, each user is de-linked from the production system when securely hashed into an anonymized ID. Also collected the news click behaviors of these users in this period, which are formatted into impression logs. The impression logs have been used in the last week for test, and the logs in the fifth week for training. For samples in training set,  used the click behaviors in the first four weeks to construct the news click history for user modeling. Among the training data,  the samples in the last day of the fifth week used as validation set. This dataset is a small version of MIND (MIND-small), by randomly sampling 50,000 users and their behavior logs. Only training and validation sets are contained in the MIND-small dataset.\n",
      "\n",
      "&gt;Both the training and validation data are a zip-compressed folder, which contains four different files:\n",
      "- behaviors.tsv:\tThe click histories and impression logs of users\n",
      "- news.tsv:\tThe information of news articles\n",
      "- entity_embedding.vec:\tThe embeddings of entities in news extracted from knowledge graph\n",
      "- relation_embedding.vec:\tThe embeddings of relations between entities extracted from knowledge graph\n",
      "\n",
      "\n",
      "#### **behaviors.tsv**\n",
      "The behaviors.tsv file contains the impression logs and users' news click histories. It has 5 columns divided by the tab symbol:\n",
      "\n",
      "- Impression ID. The ID of an impression.\n",
      "- User ID. The anonymous ID of a user.\n",
      "- Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".\n",
      "- History. The news click history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.\n",
      "- Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.\n",
      "\n",
      "#### **news.tsv**\n",
      "The docs.tsv  contains the detailed information of news articles involved in the behaviors.tsv file. It has 7 columns, which are divided by the tab symbol:\n",
      "\n",
      "- News ID\n",
      "- Category\n",
      "- SubCategory\n",
      "- Title\n",
      "- Abstract\n",
      "- URL\n",
      "- Title Entities (entities contained in the title of this news)\n",
      "- Abstract Entities (entites contained in the abstract of this news)\n",
      "\n",
      "\n",
      "#### **entity_embedding.vec & relation_embedding.vec**\n",
      "The entity_embedding.vec and relation_embedding.vec files contain the 100-dimensional embeddings of the entities and relations learned from the subgraph (from WikiData knowledge graph) by TransE method. In both files, the first column is the ID of entity/relation, and the other columns are the embedding vector values. We hope this data can facilitate the research of knowledge-aware news recommendation. An example is shown as follows:\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "\n",
      "- [Paper: MIND: A Large-scale Dataset for News Recommendation](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf)\n",
      "Fangzhao Wu, Ying Qiao,  Jiun-Hung Chen, Chuhan Wu, Tao Qi§,Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou\n",
      "- https://www.microsoft.com/en-us/research/publication/mind-a-large-scale-dataset-for-news-recommendation/\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "[Implementation and practice of several existing news recommendation methods](https://github.com/microsoft/recommenders), or your own methods. \n",
      "\n",
      "### How to start?\n",
      "- [Get familiar with the news recommendation scenario](https://github.com/microsoft/recommenders/tree/master/scenarios/news)\n",
      "- Then dive into some baselines such as DKN, LSTUR, NAML, NPA and NRMS and start:\n",
      "   - [DKN : Deep Knowledge-Aware Network for News Recommendation](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/dkn_MIND.ipynb)\n",
      "   - [LSTUR: Neural News Recommendation with Long- and Short-term User Representations¶](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/lstur_MIND.ipynb)\n",
      "   - [NAML: Neural News Recommendation with Attentive Multi-View Learning](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/naml_MIND.ipynb) \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: mind-news-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: MIND: Microsoft News Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: Implement news recommendation methods \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset.\n",
      "\n",
      "What's New (June 21, 2021)\n",
      "We have a new release Recommenders 0.6.0!\n",
      "\n",
      "Recommenders is now on PyPI and can be installed using pip! In addition there are lots of bug fixes and utilities improvements.\n",
      "\n",
      "Here you can find the PyPi page: https://pypi.org/project/recommenders/\n",
      "\n",
      "Here you can find the package documentation: https://microsoft-recommenders.readthedocs.io/en/latest/\n",
      "\n",
      "### Content\n",
      "\n",
      "The MIND dataset for news recommendation was collected from anonymized behavior logs of Microsoft News website. The data randomly sampled 1 million users who had at least 5 news clicks during 6 weeks from October 12 to November 22, 2019. To protect user privacy, each user is de-linked from the production system when securely hashed into an anonymized ID. Also collected the news click behaviors of these users in this period, which are formatted into impression logs. The impression logs have been used in the last week for test, and the logs in the fifth week for training. For samples in training set,  used the click behaviors in the first four weeks to construct the news click history for user modeling. Among the training data,  the samples in the last day of the fifth week used as validation set. This dataset is a small version of MIND (MIND-small), by randomly sampling 50,000 users and their behavior logs. Only training and validation sets are contained in the MIND-small dataset.\n",
      "\n",
      "&gt;Both the training and validation data are a zip-compressed folder, which contains four different files:\n",
      "- behaviors.tsv:\tThe click histories and impression logs of users\n",
      "- news.tsv:\tThe information of news articles\n",
      "- entity_embedding.vec:\tThe embeddings of entities in news extracted from knowledge graph\n",
      "- relation_embedding.vec:\tThe embeddings of relations between entities extracted from knowledge graph\n",
      "\n",
      "\n",
      "#### **behaviors.tsv**\n",
      "The behaviors.tsv file contains the impression logs and users' news click histories. It has 5 columns divided by the tab symbol:\n",
      "\n",
      "- Impression ID. The ID of an impression.\n",
      "- User ID. The anonymous ID of a user.\n",
      "- Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".\n",
      "- History. The news click history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.\n",
      "- Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.\n",
      "\n",
      "#### **news.tsv**\n",
      "The docs.tsv  contains the detailed information of news articles involved in the behaviors.tsv file. It has 7 columns, which are divided by the tab symbol:\n",
      "\n",
      "- News ID\n",
      "- Category\n",
      "- SubCategory\n",
      "- Title\n",
      "- Abstract\n",
      "- URL\n",
      "- Title Entities (entities contained in the title of this news)\n",
      "- Abstract Entities (entites contained in the abstract of this news)\n",
      "\n",
      "\n",
      "#### **entity_embedding.vec & relation_embedding.vec**\n",
      "The entity_embedding.vec and relation_embedding.vec files contain the 100-dimensional embeddings of the entities and relations learned from the subgraph (from WikiData knowledge graph) by TransE method. In both files, the first column is the ID of entity/relation, and the other columns are the embedding vector values. We hope this data can facilitate the research of knowledge-aware news recommendation. An example is shown as follows:\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "\n",
      "- [Paper: MIND: A Large-scale Dataset for News Recommendation](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf)\n",
      "Fangzhao Wu, Ying Qiao,  Jiun-Hung Chen, Chuhan Wu, Tao Qi§,Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou\n",
      "- https://www.microsoft.com/en-us/research/publication/mind-a-large-scale-dataset-for-news-recommendation/\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "[Implementation and practice of several existing news recommendation methods](https://github.com/microsoft/recommenders), or your own methods. \n",
      "\n",
      "### How to start?\n",
      "- [Get familiar with the news recommendation scenario](https://github.com/microsoft/recommenders/tree/master/scenarios/news)\n",
      "- Then dive into some baselines such as DKN, LSTUR, NAML, NPA and NRMS and start:\n",
      "   - [DKN : Deep Knowledge-Aware Network for News Recommendation](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/dkn_MIND.ipynb)\n",
      "   - [LSTUR: Neural News Recommendation with Long- and Short-term User Representations¶](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/lstur_MIND.ipynb)\n",
      "   - [NAML: Neural News Recommendation with Attentive Multi-View Learning](https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/naml_MIND.ipynb) \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: arashnic/food-demand\n",
      "name 'detect' is not defined Language is not detected: food-demand\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Food Demand Dataset\n",
      "name 'detect' is not defined Language is not detected: Predict the weekly demand for the center-meal combinations \n",
      "name 'detect' is not defined Language is not detected: ### Context and Content\n",
      "\n",
      "I found this dataset useful and well structured in AnalyticsVidhya for practice so you can test your model in [Food Demand Forecasting](https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/#ProblemStatement). The goal is to practice regression and  time-series forecasting by related tasks i defined for this dataset. \n",
      "\n",
      "The story is that suppose your client is a meal delivery company which operates in multiple cities. They have various fulfillment centers in these cities for dispatching meal orders to their customers. The client wants you to help these centers with demand forecasting for upcoming weeks so that these centers will plan the stock of raw materials accordingly.\n",
      "\n",
      "The replenishment of majority of raw materials is done on weekly basis and since the raw material is perishable, the procurement planning is of utmost importance. Secondly, staffing of the centers is also one area wherein accurate demand forecasts are really helpful. Given the following information, the task is to predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations in the test set:  \n",
      "\n",
      "- Historical data of demand for a product-center combination (Weeks: 1 to 145)\n",
      "- Product(Meal) features such as category, sub-category, current price and discount\n",
      "- Information for fulfillment center like center area, city information etc.\n",
      "#   \n",
      "Data Files:\n",
      "- Weekly Demand data (train.csv)\n",
      "\n",
      "&gt;id:\tUnique ID\n",
      "\n",
      "&gt;week:\tWeek No\n",
      "\n",
      "&gt;center_id:\tUnique ID for fulfillment center \n",
      "\n",
      "&gt;meal_id:\tUnique ID for Meal\n",
      "\n",
      "&gt;checkout_price:\tFinal price including discount, taxes & delivery charges\n",
      "\n",
      "&gt;base_price:\tBase price of the meal\n",
      "\n",
      "&gt;emailer_for_promotion:\tEmailer sent for promotion of meal\n",
      "\n",
      "&gt;homepage_featured:\tMeal featured at homepage\n",
      "\n",
      "&gt;num_orders:\t(Target) Orders Count\n",
      "\n",
      "- fulfilment_center_info.csv\n",
      "\n",
      "&gt;center_id:\tUnique ID for fulfillment center\n",
      "\n",
      "&gt;city_code:\tUnique code for city\n",
      "\n",
      "&gt;region_code:\tUnique code for region\n",
      "\n",
      "&gt;center_type:\tAnonymized center type\n",
      "\n",
      "&gt;op_area:\tArea of operation (in km^2)\n",
      "\n",
      "- meal_info.csv\n",
      "\n",
      "&gt;meal_id:\tUnique ID for the meal\n",
      "\n",
      "&gt;category:\tType of meal (beverages/snacks/soups….)\n",
      "\n",
      "&gt;cuisine:\tMeal cuisine (Indian/Italian/…)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "https://datahack.analyticsvidhya.com/contest/all/\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- Given the dataset information,  predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations in the test set\n",
      "\n",
      "#  <hr style=\"border: 2px solid gray\"> \n",
      "#### *If you find the data useful your **upvote** is an explicit feedback for future works, Have fun exploring data!*\n",
      "\n",
      "name 'detect' is not defined Language is not detected: food-demand\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Food Demand Dataset\n",
      "name 'detect' is not defined Language is not detected: Predict the weekly demand for the center-meal combinations \n",
      "name 'detect' is not defined Language is not detected: ### Context and Content\n",
      "\n",
      "I found this dataset useful and well structured in AnalyticsVidhya for practice so you can test your model in [Food Demand Forecasting](https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/#ProblemStatement). The goal is to practice regression and  time-series forecasting by related tasks i defined for this dataset. \n",
      "\n",
      "The story is that suppose your client is a meal delivery company which operates in multiple cities. They have various fulfillment centers in these cities for dispatching meal orders to their customers. The client wants you to help these centers with demand forecasting for upcoming weeks so that these centers will plan the stock of raw materials accordingly.\n",
      "\n",
      "The replenishment of majority of raw materials is done on weekly basis and since the raw material is perishable, the procurement planning is of utmost importance. Secondly, staffing of the centers is also one area wherein accurate demand forecasts are really helpful. Given the following information, the task is to predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations in the test set:  \n",
      "\n",
      "- Historical data of demand for a product-center combination (Weeks: 1 to 145)\n",
      "- Product(Meal) features such as category, sub-category, current price and discount\n",
      "- Information for fulfillment center like center area, city information etc.\n",
      "#   \n",
      "Data Files:\n",
      "- Weekly Demand data (train.csv)\n",
      "\n",
      "&gt;id:\tUnique ID\n",
      "\n",
      "&gt;week:\tWeek No\n",
      "\n",
      "&gt;center_id:\tUnique ID for fulfillment center \n",
      "\n",
      "&gt;meal_id:\tUnique ID for Meal\n",
      "\n",
      "&gt;checkout_price:\tFinal price including discount, taxes & delivery charges\n",
      "\n",
      "&gt;base_price:\tBase price of the meal\n",
      "\n",
      "&gt;emailer_for_promotion:\tEmailer sent for promotion of meal\n",
      "\n",
      "&gt;homepage_featured:\tMeal featured at homepage\n",
      "\n",
      "&gt;num_orders:\t(Target) Orders Count\n",
      "\n",
      "- fulfilment_center_info.csv\n",
      "\n",
      "&gt;center_id:\tUnique ID for fulfillment center\n",
      "\n",
      "&gt;city_code:\tUnique code for city\n",
      "\n",
      "&gt;region_code:\tUnique code for region\n",
      "\n",
      "&gt;center_type:\tAnonymized center type\n",
      "\n",
      "&gt;op_area:\tArea of operation (in km^2)\n",
      "\n",
      "- meal_info.csv\n",
      "\n",
      "&gt;meal_id:\tUnique ID for the meal\n",
      "\n",
      "&gt;category:\tType of meal (beverages/snacks/soups….)\n",
      "\n",
      "&gt;cuisine:\tMeal cuisine (Indian/Italian/…)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "https://datahack.analyticsvidhya.com/contest/all/\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "- Given the dataset information,  predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations in the test set\n",
      "\n",
      "#  <hr style=\"border: 2px solid gray\"> \n",
      "#### *If you find the data useful your **upvote** is an explicit feedback for future works, Have fun exploring data!*\n",
      "\n",
      "name 'detect' is not defined Language is not detected: arashnic/car-racing-dataset\n",
      "name 'detect' is not defined Language is not detected: car-racing-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Indy500 Car Racing Dataset\n",
      "name 'detect' is not defined Language is not detected: Rank Position Forecasting in Car Racing\n",
      "name 'detect' is not defined Language is not detected: Rank position forecasting in car racing is a challenging problem, which is featured with highly complex global\n",
      "dependency among the cars, with uncertainty resulted from existing exogenous factors, and as a sparse data problem.\n",
      "Existing methods, including statistical models, machine learning regression models, and several state-of-the-art deep forecasting\n",
      "models all perform not well on this problem. By elaborative analysis of pit stops events, we find it is critical to decompose\n",
      "the cause effects and model them, the rank position and pitstop events, separately. In the choice of sub-model from different\n",
      "deep models, we find the model with weak assumptions on the global dependency structure performs the best. Based on these\n",
      "observations, we propose RankNet, a combination of encoderdecoder network and separate MLP network that capable of\n",
      "delivering probabilistic forecasting to model the pit stop events and rank position in car racing. Further with the help of feature\n",
      "optimizations, RankNet demonstrates a significant performance improvement over the baselines, e.g., MAE improves more than\n",
      "10% consistently, and is also more stable when adapting to unseen new data. Details of model optimization, performance profiling\n",
      "are presented. It is promising to provide useful forecasting tools for the car racing analysis and shine a light on solutions to similar\n",
      "challenging issues in general forecasting problems. \n",
      "\n",
      "\n",
      "\n",
      "### Data and Problem\n",
      "Indy500 is the premier event of the IndyCar series. Each year, 33 cars compete on a 2.5-mile oval track for 200 laps.\n",
      "The track is split into several sections or timeline. E.g., SF/SFP indicate the start and finish line on the track or on the pit lane,\n",
      "respectively. A local communication network broadcasts race information to all the teams, following a general data exchange\n",
      "protocol .\n",
      "More:\n",
      "IndyCar Understanding-The-Sport. https://ftw.usatoday.com/lists/indy-500-2022-time-lineup-stats-traditions-history-beginners-guide\n",
      "INDYCAR-101/Understanding-The-Sport/Timing-and-Scoring. visited\n",
      "on 04/15/2020.\n",
      "\n",
      "Rank position is the order of the cars crossing SF/SFP. In motorsports, a pit stop is a pause for refueling, new tires,\n",
      "repairs, mechanical adjustments, a driver change, a penalty, or any combination of them . Unexpected events happen in\n",
      "a race, including mechanical failures or a crash. Depending on the severity level of the event, sometimes it leads to a\n",
      "dangerous situation for other cars to continue the racing with high speed on the track. In these cases, a full course yellow\n",
      "flag rises to indicate the race entering a caution laps mode, in which all the cars slow down and follow a safety car and\n",
      "can not overtake until another green flag raised.\n",
      "\n",
      "Full data generation and source code:\n",
      "https://github.com/DSC-SPIDAL/rankpredictor \n",
      "\n",
      "### Citiation\n",
      "\n",
      "@article{peng_rank_2020,\n",
      "title = {Rank {Position} {Forecasting} in {Car} {Racing}},\n",
      "url = {http://arxiv.org/abs/2010.01707},\n",
      "urldate = {2020-12-14},\n",
      "journal = {arXiv:2010.01707 [cs, stat]},\n",
      "author = {Peng, Bo and Li, Jiayu and Akkas, Selahattin and Wang, Fugang and Araki, Takuya and Yoshiyuki, Ohno and Qiu, Judy},\n",
      "month = oct,\n",
      "year = {2020},\n",
      "note = {arXiv: 2010.01707},\n",
      "}\n",
      "\n",
      "### Inspiration\n",
      "- Pitstop analysis (resource constraints, anomaly events, and Race strategies)\n",
      "- Predict the future rank position of a car given the race’s observed history\n",
      "    \n",
      "name 'detect' is not defined Language is not detected: car-racing-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Indy500 Car Racing Dataset\n",
      "name 'detect' is not defined Language is not detected: Rank Position Forecasting in Car Racing\n",
      "name 'detect' is not defined Language is not detected: Rank position forecasting in car racing is a challenging problem, which is featured with highly complex global\n",
      "dependency among the cars, with uncertainty resulted from existing exogenous factors, and as a sparse data problem.\n",
      "Existing methods, including statistical models, machine learning regression models, and several state-of-the-art deep forecasting\n",
      "models all perform not well on this problem. By elaborative analysis of pit stops events, we find it is critical to decompose\n",
      "the cause effects and model them, the rank position and pitstop events, separately. In the choice of sub-model from different\n",
      "deep models, we find the model with weak assumptions on the global dependency structure performs the best. Based on these\n",
      "observations, we propose RankNet, a combination of encoderdecoder network and separate MLP network that capable of\n",
      "delivering probabilistic forecasting to model the pit stop events and rank position in car racing. Further with the help of feature\n",
      "optimizations, RankNet demonstrates a significant performance improvement over the baselines, e.g., MAE improves more than\n",
      "10% consistently, and is also more stable when adapting to unseen new data. Details of model optimization, performance profiling\n",
      "are presented. It is promising to provide useful forecasting tools for the car racing analysis and shine a light on solutions to similar\n",
      "challenging issues in general forecasting problems. \n",
      "\n",
      "\n",
      "\n",
      "### Data and Problem\n",
      "Indy500 is the premier event of the IndyCar series. Each year, 33 cars compete on a 2.5-mile oval track for 200 laps.\n",
      "The track is split into several sections or timeline. E.g., SF/SFP indicate the start and finish line on the track or on the pit lane,\n",
      "respectively. A local communication network broadcasts race information to all the teams, following a general data exchange\n",
      "protocol .\n",
      "More:\n",
      "IndyCar Understanding-The-Sport. https://ftw.usatoday.com/lists/indy-500-2022-time-lineup-stats-traditions-history-beginners-guide\n",
      "INDYCAR-101/Understanding-The-Sport/Timing-and-Scoring. visited\n",
      "on 04/15/2020.\n",
      "\n",
      "Rank position is the order of the cars crossing SF/SFP. In motorsports, a pit stop is a pause for refueling, new tires,\n",
      "repairs, mechanical adjustments, a driver change, a penalty, or any combination of them . Unexpected events happen in\n",
      "a race, including mechanical failures or a crash. Depending on the severity level of the event, sometimes it leads to a\n",
      "dangerous situation for other cars to continue the racing with high speed on the track. In these cases, a full course yellow\n",
      "flag rises to indicate the race entering a caution laps mode, in which all the cars slow down and follow a safety car and\n",
      "can not overtake until another green flag raised.\n",
      "\n",
      "Full data generation and source code:\n",
      "https://github.com/DSC-SPIDAL/rankpredictor \n",
      "\n",
      "### Citiation\n",
      "\n",
      "@article{peng_rank_2020,\n",
      "title = {Rank {Position} {Forecasting} in {Car} {Racing}},\n",
      "url = {http://arxiv.org/abs/2010.01707},\n",
      "urldate = {2020-12-14},\n",
      "journal = {arXiv:2010.01707 [cs, stat]},\n",
      "author = {Peng, Bo and Li, Jiayu and Akkas, Selahattin and Wang, Fugang and Araki, Takuya and Yoshiyuki, Ohno and Qiu, Judy},\n",
      "month = oct,\n",
      "year = {2020},\n",
      "note = {arXiv: 2010.01707},\n",
      "}\n",
      "\n",
      "### Inspiration\n",
      "- Pitstop analysis (resource constraints, anomaly events, and Race strategies)\n",
      "- Predict the future rank position of a car given the race’s observed history\n",
      "    \n",
      "name 'detect' is not defined Language is not detected: arashnic/an-unbiased-sequential-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: an-unbiased-sequential-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: An Unbiased Sequential Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: KuaiRand: video-sharing unbiased sequential with Randomly Exposed Videos \n",
      "name 'detect' is not defined Language is not detected: ## **An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos**\n",
      "## \n",
      "## \n",
      "The original dataset  has been released in three versions of KuaiRand for different uses:\n",
      "\n",
      "**1.** **KuaiRand-27K (23GB logs +23GB features)**: the complete KuaiRand dataset that has over 27K users and 32 million videos. Can be downloaded by :\n",
      "wget https://zenodo.org/records/10439422/files/KuaiRand-27K.tar.gz command\n",
      "\n",
      "**2.** **KuaiRand-1K (829MB logs + 3.5GB features)**: randomly sample 1,000 users from KuaiRand-27K, then remove all irrelevant videos. There are 4 million videos rest.Can be downloaded by :\n",
      "wget https://zenodo.org/records/10439422/files/KuaiRand-1K.tar.gz   command\n",
      "\n",
      "**3.**  **KuaiRand-Pure (184MB logs + 10MB features)**: only keeps the logs for the 7583 videos in the candidate pool.\n",
      "(Uploaded in this page data) \n",
      "\n",
      "There are three log files in each version e.g in KuaiRand-Pure:\n",
      "\n",
      "**-** log_random_4_22_to_5_08.csv contains all interactions resulting from random intervention.\n",
      "\n",
      "**-** log_standard_4_22_to_5_08.csv contains all interactions of standard recommendation.\n",
      "\n",
      "**-** log_standard_4_08_to_4_21.csv contains all interactions of standard recommendation for the same users in the previous two weeks (2022.04.08 ~ 2022.04.21).\n",
      "\n",
      "Complete files and features description in: https://kuairand.com/ \n",
      "\n",
      "### **How to Use:**\n",
      "\n",
      "**1.** **Reasons to use KuaiRand-27K or KuaiRand-1K:**\n",
      "**-** Your research needs rigorous sequential logs, such as off-policy evaluation (OPE), Reinforcement learning (RL), or long sequential recommendation.\n",
      "\n",
      "**2.** **Reasons to use KuaiRand-Pure:**\n",
      "**-** The sequential information is not necessary for your research OR If you are OK with the incomplete sequential logs. For example, if you are studying debiasing in collaborative filtering models or multi-task modeling in recommendation.\n",
      "**-** If your model can only run with small-size data.\n",
      "\n",
      "### **Acknowledgment**\n",
      "Chongming Gao et al, 2022. [KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos](https://arxiv.org/abs/2208.08696)\n",
      "\n",
      "### **Advantages**\n",
      "\n",
      "Compared with other datasets with random exposure, KuaiRand has the following advantages:\n",
      "\n",
      "✅ It is the first sequential recommendation dataset with millions of intervened interactions of randomly exposed items inserted in the standard recommendation feeds.\n",
      "\n",
      "✅ It has the most comprehensive side information including explicit user IDs, interaction timestamps, and rich features for users and items.\n",
      "\n",
      "✅ It has 15 policies with each catered for a special recommendation scenario in the Kuaishou App.\n",
      "\n",
      "✅ introduced by 12 feedback signals (e.g., click, like, and view time) for each interaction to describe the user’s comprehensive feedback.\n",
      "\n",
      "✅ Each user has thousands of historical interactions on average.\n",
      "\n",
      "✅ It has three versions to support various research directions in recommendation.\n",
      "\n",
      "### **Inspiration**\n",
      "\n",
      "Recommender systems suffer from various biases in the data collection stage . Most existing datasets are very sparse and affected by user-selection bias or exposure bias . It is of critical importance to develop models that can alleviate biases. To evaluate the models, we need reliable unbiased data. KuaiRand is the first dataset that inserts the random items into the normal recommendation feeds with rich side information and all item/user IDs provided. With this authentic unbiased data, we can evaluate and thus improve the recommender policy. \n",
      "\n",
      "KuaiRand can further support the following promising research directions in recommendation. \n",
      "\n",
      "**-** *Off-policy Evaluation (OPE)*\n",
      "\n",
      "**-** *Interactive Recommendation*\n",
      "\n",
      "**-** *Long Sequential Behavior Modeling*\n",
      "\n",
      "**-** *Multi-Task Learning*\n",
      "\n",
      "### **More References**\n",
      "**-** *[Bias and Debias in Recommender System: A Survey and Future Directions](https://arxiv.org/pdf/2010.03240.pdf)*\n",
      "\n",
      "**-** *[A Survey on Popularity Bias in Recommender\n",
      "Systems](https://arxiv.org/pdf/2308.01118.pdf)*\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: an-unbiased-sequential-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: An Unbiased Sequential Recommendation Dataset\n",
      "name 'detect' is not defined Language is not detected: KuaiRand: video-sharing unbiased sequential with Randomly Exposed Videos \n",
      "name 'detect' is not defined Language is not detected: ## **An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos**\n",
      "## \n",
      "## \n",
      "The original dataset  has been released in three versions of KuaiRand for different uses:\n",
      "\n",
      "**1.** **KuaiRand-27K (23GB logs +23GB features)**: the complete KuaiRand dataset that has over 27K users and 32 million videos. Can be downloaded by :\n",
      "wget https://zenodo.org/records/10439422/files/KuaiRand-27K.tar.gz command\n",
      "\n",
      "**2.** **KuaiRand-1K (829MB logs + 3.5GB features)**: randomly sample 1,000 users from KuaiRand-27K, then remove all irrelevant videos. There are 4 million videos rest.Can be downloaded by :\n",
      "wget https://zenodo.org/records/10439422/files/KuaiRand-1K.tar.gz   command\n",
      "\n",
      "**3.**  **KuaiRand-Pure (184MB logs + 10MB features)**: only keeps the logs for the 7583 videos in the candidate pool.\n",
      "(Uploaded in this page data) \n",
      "\n",
      "There are three log files in each version e.g in KuaiRand-Pure:\n",
      "\n",
      "**-** log_random_4_22_to_5_08.csv contains all interactions resulting from random intervention.\n",
      "\n",
      "**-** log_standard_4_22_to_5_08.csv contains all interactions of standard recommendation.\n",
      "\n",
      "**-** log_standard_4_08_to_4_21.csv contains all interactions of standard recommendation for the same users in the previous two weeks (2022.04.08 ~ 2022.04.21).\n",
      "\n",
      "Complete files and features description in: https://kuairand.com/ \n",
      "\n",
      "### **How to Use:**\n",
      "\n",
      "**1.** **Reasons to use KuaiRand-27K or KuaiRand-1K:**\n",
      "**-** Your research needs rigorous sequential logs, such as off-policy evaluation (OPE), Reinforcement learning (RL), or long sequential recommendation.\n",
      "\n",
      "**2.** **Reasons to use KuaiRand-Pure:**\n",
      "**-** The sequential information is not necessary for your research OR If you are OK with the incomplete sequential logs. For example, if you are studying debiasing in collaborative filtering models or multi-task modeling in recommendation.\n",
      "**-** If your model can only run with small-size data.\n",
      "\n",
      "### **Acknowledgment**\n",
      "Chongming Gao et al, 2022. [KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos](https://arxiv.org/abs/2208.08696)\n",
      "\n",
      "### **Advantages**\n",
      "\n",
      "Compared with other datasets with random exposure, KuaiRand has the following advantages:\n",
      "\n",
      "✅ It is the first sequential recommendation dataset with millions of intervened interactions of randomly exposed items inserted in the standard recommendation feeds.\n",
      "\n",
      "✅ It has the most comprehensive side information including explicit user IDs, interaction timestamps, and rich features for users and items.\n",
      "\n",
      "✅ It has 15 policies with each catered for a special recommendation scenario in the Kuaishou App.\n",
      "\n",
      "✅ introduced by 12 feedback signals (e.g., click, like, and view time) for each interaction to describe the user’s comprehensive feedback.\n",
      "\n",
      "✅ Each user has thousands of historical interactions on average.\n",
      "\n",
      "✅ It has three versions to support various research directions in recommendation.\n",
      "\n",
      "### **Inspiration**\n",
      "\n",
      "Recommender systems suffer from various biases in the data collection stage . Most existing datasets are very sparse and affected by user-selection bias or exposure bias . It is of critical importance to develop models that can alleviate biases. To evaluate the models, we need reliable unbiased data. KuaiRand is the first dataset that inserts the random items into the normal recommendation feeds with rich side information and all item/user IDs provided. With this authentic unbiased data, we can evaluate and thus improve the recommender policy. \n",
      "\n",
      "KuaiRand can further support the following promising research directions in recommendation. \n",
      "\n",
      "**-** *Off-policy Evaluation (OPE)*\n",
      "\n",
      "**-** *Interactive Recommendation*\n",
      "\n",
      "**-** *Long Sequential Behavior Modeling*\n",
      "\n",
      "**-** *Multi-Task Learning*\n",
      "\n",
      "### **More References**\n",
      "**-** *[Bias and Debias in Recommender System: A Survey and Future Directions](https://arxiv.org/pdf/2010.03240.pdf)*\n",
      "\n",
      "**-** *[A Survey on Popularity Bias in Recommender\n",
      "Systems](https://arxiv.org/pdf/2308.01118.pdf)*\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: arashnic/cinema-ticket\n",
      "name 'detect' is not defined Language is not detected: cinema-ticket\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Cinema Tickets\n",
      "name 'detect' is not defined Language is not detected: Time series analysis and forecasting\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "Cinema industry is not excluded of getting advantage of predictive modeling. Like other industry e.g. retail , banking and restaurants , sale forecast \n",
      "can help cinemas for cost reduction and better ROI. By forecasting sale,  screening in different location could be optimized as well as effective market targeting and pricing. \n",
      "\n",
      "Also historical data of sale and movies details e.g. cost, cast and crews, and other project details like schedule,  could help producers to select high performance cast and crews and planning for better projects  ROI . Also it helps to assign  screening location  on hot spots and areas.  \n",
      "###     \n",
      "###   \n",
      "![image](https://cdn.cnn.com/cnnnext/dam/assets/200820103238-movie-theater-covid-19-gfx-exlarge-169.jpg)\n",
      "#   \n",
      "## Content\n",
      "\n",
      "About eight months sales  history of different cinemas with detailed data of screening , during 2018  with encoded annonymized locations .  \n",
      "\n",
      "### Starter Kernels \n",
      "\n",
      "-  [EDA , Temporal Feat Eng and XGBoost](https://www.kaggle.com/arashnic/agile-eda-base-models) \n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Time series analysis\n",
      "- Cinema Clustering\n",
      "- Forecast sales for each cinema\n",
      "- Recommendation:\n",
      "     - Movie genre recommendation for cinemas\n",
      "     - Cinema location recommendation\n",
      "     - Cast and crew ratings \n",
      "[Recommendations required more detailed data about movies which will be added to dataset during next versions] \n",
      "\n",
      " \n",
      "name 'detect' is not defined Language is not detected: cinema-ticket\n",
      "name 'detect' is not defined Language is not detected: arashnic\n",
      "name 'detect' is not defined Language is not detected: Cinema Tickets\n",
      "name 'detect' is not defined Language is not detected: Time series analysis and forecasting\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "Cinema industry is not excluded of getting advantage of predictive modeling. Like other industry e.g. retail , banking and restaurants , sale forecast \n",
      "can help cinemas for cost reduction and better ROI. By forecasting sale,  screening in different location could be optimized as well as effective market targeting and pricing. \n",
      "\n",
      "Also historical data of sale and movies details e.g. cost, cast and crews, and other project details like schedule,  could help producers to select high performance cast and crews and planning for better projects  ROI . Also it helps to assign  screening location  on hot spots and areas.  \n",
      "###     \n",
      "###   \n",
      "![image](https://cdn.cnn.com/cnnnext/dam/assets/200820103238-movie-theater-covid-19-gfx-exlarge-169.jpg)\n",
      "#   \n",
      "## Content\n",
      "\n",
      "About eight months sales  history of different cinemas with detailed data of screening , during 2018  with encoded annonymized locations .  \n",
      "\n",
      "### Starter Kernels \n",
      "\n",
      "-  [EDA , Temporal Feat Eng and XGBoost](https://www.kaggle.com/arashnic/agile-eda-base-models) \n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Time series analysis\n",
      "- Cinema Clustering\n",
      "- Forecast sales for each cinema\n",
      "- Recommendation:\n",
      "     - Movie genre recommendation for cinemas\n",
      "     - Cinema location recommendation\n",
      "     - Cast and crew ratings \n",
      "[Recommendations required more detailed data about movies which will be added to dataset during next versions] \n",
      "\n",
      " \n",
      "name 'detect' is not defined Language is not detected: greegtitan/financial-statement-idx-stocks\n",
      "name 'detect' is not defined Language is not detected: financial-statement-idx-stocks\n",
      "name 'detect' is not defined Language is not detected: greegtitan\n",
      "name 'detect' is not defined Language is not detected: Financial Statement IDX Stocks\n",
      "name 'detect' is not defined Language is not detected: Balance Sheet, Cash Flow, and Income Statement of Indonesia Public Company\n",
      "name 'detect' is not defined Language is not detected: &gt; Analyst often asked, which method bring higher return? Technical or Fundamental? When should we use techinal analysis or when fundamental analysis is more relevant?. These question sparks a lot of debate and each side assume that they are better than the other. Warren Buffet himself is a value investor, who bought stock depends on how the company perform, and mention that buying a stock based on recent price alone is kind of foolish. Market decide price, and our turn to buy it if the price favor our assumption on how valueable a company is. The intelligent investor buy the business, not the stock.\\nWith these financial reports we could perform a small fundamental analysis based on the value on each item in statement. Of course fundamental analysis covers a lot more than just financial statement alone, but this could be a jumping stone to see how the actual price as stock supposed to be. Combine it with the [stock price](https://www.kaggle.com/datasets/greegtitan/stock-price-and-volume-idn), and we'll proseper.\n",
      "\n",
      "&gt; This dataset contains **almost** all public company financial statement annually and quarterly. **Company that's not included in this dataset either do not report their financial statement or contains some irrelevant publishing date** \n",
      "\n",
      "## How to use this dataset\n",
      "&gt; EDA\n",
      "&gt; Classifier\n",
      "&gt; Stock Recommendation\n",
      "&gt; Fundamental Analysis\n",
      "\n",
      "## Features\n",
      "|Feature|Description|\n",
      "|---|---|\n",
      "|  **symbol**| unique stock identifier|\n",
      "|  **account**| Financial statement on financial reports. `BS` = Balance Sheet, `CF` = Cash Flow, `IS` = Income Statement |\n",
      "|  **type** | item in financial statement. Some value are poorly written |\n",
      "|  **timestamp** | annual and quarter reports differ. Annual covers from 2018-2021 and quarter covers Q32021-Q32022|\n",
      "\n",
      "## Other Information\n",
      "\n",
      "&gt; You also could use dataset outside this one. [This dataset](https://www.kaggle.com/datasets/greegtitan/public-company-idn) present all public company data in Indonesia and [Stock Price](https://www.kaggle.com/datasets/greegtitan/stock-price-and-volume-idn) cover the stock price movement of those companies. Might be helpful to do certain task, e.g. classification for the industry, etc.\n",
      "\n",
      "## Acknowledgement\n",
      "\n",
      "&gt; Yahoo Finance\n",
      "\n",
      "name 'detect' is not defined Language is not detected: financial-statement-idx-stocks\n",
      "name 'detect' is not defined Language is not detected: greegtitan\n",
      "name 'detect' is not defined Language is not detected: Financial Statement IDX Stocks\n",
      "name 'detect' is not defined Language is not detected: Balance Sheet, Cash Flow, and Income Statement of Indonesia Public Company\n",
      "name 'detect' is not defined Language is not detected: &gt; Analyst often asked, which method bring higher return? Technical or Fundamental? When should we use techinal analysis or when fundamental analysis is more relevant?. These question sparks a lot of debate and each side assume that they are better than the other. Warren Buffet himself is a value investor, who bought stock depends on how the company perform, and mention that buying a stock based on recent price alone is kind of foolish. Market decide price, and our turn to buy it if the price favor our assumption on how valueable a company is. The intelligent investor buy the business, not the stock.\\nWith these financial reports we could perform a small fundamental analysis based on the value on each item in statement. Of course fundamental analysis covers a lot more than just financial statement alone, but this could be a jumping stone to see how the actual price as stock supposed to be. Combine it with the [stock price](https://www.kaggle.com/datasets/greegtitan/stock-price-and-volume-idn), and we'll proseper.\n",
      "\n",
      "&gt; This dataset contains **almost** all public company financial statement annually and quarterly. **Company that's not included in this dataset either do not report their financial statement or contains some irrelevant publishing date** \n",
      "\n",
      "## How to use this dataset\n",
      "&gt; EDA\n",
      "&gt; Classifier\n",
      "&gt; Stock Recommendation\n",
      "&gt; Fundamental Analysis\n",
      "\n",
      "## Features\n",
      "|Feature|Description|\n",
      "|---|---|\n",
      "|  **symbol**| unique stock identifier|\n",
      "|  **account**| Financial statement on financial reports. `BS` = Balance Sheet, `CF` = Cash Flow, `IS` = Income Statement |\n",
      "|  **type** | item in financial statement. Some value are poorly written |\n",
      "|  **timestamp** | annual and quarter reports differ. Annual covers from 2018-2021 and quarter covers Q32021-Q32022|\n",
      "\n",
      "## Other Information\n",
      "\n",
      "&gt; You also could use dataset outside this one. [This dataset](https://www.kaggle.com/datasets/greegtitan/public-company-idn) present all public company data in Indonesia and [Stock Price](https://www.kaggle.com/datasets/greegtitan/stock-price-and-volume-idn) cover the stock price movement of those companies. Might be helpful to do certain task, e.g. classification for the industry, etc.\n",
      "\n",
      "## Acknowledgement\n",
      "\n",
      "&gt; Yahoo Finance\n",
      "\n",
      "name 'detect' is not defined Language is not detected: greegtitan/stock-price-and-volume-idn\n",
      "name 'detect' is not defined Language is not detected: stock-price-and-volume-idn\n",
      "name 'detect' is not defined Language is not detected: greegtitan\n",
      "name 'detect' is not defined Language is not detected: Stock Price and Volume IDN\n",
      "name 'detect' is not defined Language is not detected: Movement of Indonesia Public Company Stock Price. Cover: Maxim Hopman\n",
      "name 'detect' is not defined Language is not detected: ## About this dataset\n",
      "\n",
      "&gt; Stock market has become of the wonderful place to make money. Many loss and many gains. Many have tried to predict the price of a stock but fails miserably. Those who say they're able to do so, are the one who hide their biggest losses. If stock price cannot be determined by price alone, then there might be other way to predict it, or say to invest it in the \"better\" way. Otherwise Warren Buffet wouldn't as rich as he is now by luck alone. But who says we cannot play around with it and create our standard of investing in stock? \n",
      "\n",
      "## How to use this dataset\n",
      "&gt; EDA\n",
      "&gt; RNN to predict future price\n",
      "&gt; Trend identifier\n",
      "&gt; Classifier\n",
      "&gt; Stock Recommendation\n",
      "\n",
      "## Features\n",
      "|Feature|Description|\n",
      "|---|---|\n",
      "|  **Date** | date of the price movement |\n",
      "|  **Open** | the first price of security traded  in a day | \n",
      "|  **High** |  highest price in a day |\n",
      "|  **Low** | lowest price in a day | \n",
      "|  **Close** | the last price of security traded  in a day | \n",
      "|**Adj Close**| stands for `adjusting price` or [stock's closing price to reflect that stock's value after accounting for any corporate action](https://www.investopedia.com/terms/a/adjusted_closing_price.asp#:~:text=What%20Is%20the%20Adjusted%20Closing,detailed%20analysis%20of%20past%20performance.) |\n",
      "|**Volume**| total stock traded in a day |\n",
      "\n",
      "## Other Information\n",
      "\n",
      "&gt; You also could use dataset outside this one. [This dataset](https://www.kaggle.com/datasets/greegtitan/public-company-idn) present all public company data in Indonesia. Might be helpful to do certain task, e.g. classification for the industry, etc.\n",
      "\n",
      "## Acknowledgement\n",
      "\n",
      "&gt; Yahoo Finance\n",
      "name 'detect' is not defined Language is not detected: stock-price-and-volume-idn\n",
      "name 'detect' is not defined Language is not detected: greegtitan\n",
      "name 'detect' is not defined Language is not detected: Stock Price and Volume IDN\n",
      "name 'detect' is not defined Language is not detected: Movement of Indonesia Public Company Stock Price. Cover: Maxim Hopman\n",
      "name 'detect' is not defined Language is not detected: ## About this dataset\n",
      "\n",
      "&gt; Stock market has become of the wonderful place to make money. Many loss and many gains. Many have tried to predict the price of a stock but fails miserably. Those who say they're able to do so, are the one who hide their biggest losses. If stock price cannot be determined by price alone, then there might be other way to predict it, or say to invest it in the \"better\" way. Otherwise Warren Buffet wouldn't as rich as he is now by luck alone. But who says we cannot play around with it and create our standard of investing in stock? \n",
      "\n",
      "## How to use this dataset\n",
      "&gt; EDA\n",
      "&gt; RNN to predict future price\n",
      "&gt; Trend identifier\n",
      "&gt; Classifier\n",
      "&gt; Stock Recommendation\n",
      "\n",
      "## Features\n",
      "|Feature|Description|\n",
      "|---|---|\n",
      "|  **Date** | date of the price movement |\n",
      "|  **Open** | the first price of security traded  in a day | \n",
      "|  **High** |  highest price in a day |\n",
      "|  **Low** | lowest price in a day | \n",
      "|  **Close** | the last price of security traded  in a day | \n",
      "|**Adj Close**| stands for `adjusting price` or [stock's closing price to reflect that stock's value after accounting for any corporate action](https://www.investopedia.com/terms/a/adjusted_closing_price.asp#:~:text=What%20Is%20the%20Adjusted%20Closing,detailed%20analysis%20of%20past%20performance.) |\n",
      "|**Volume**| total stock traded in a day |\n",
      "\n",
      "## Other Information\n",
      "\n",
      "&gt; You also could use dataset outside this one. [This dataset](https://www.kaggle.com/datasets/greegtitan/public-company-idn) present all public company data in Indonesia. Might be helpful to do certain task, e.g. classification for the industry, etc.\n",
      "\n",
      "## Acknowledgement\n",
      "\n",
      "&gt; Yahoo Finance\n",
      "name 'detect' is not defined Language is not detected: mrmorj/insurance-recommendation\n",
      "name 'detect' is not defined Language is not detected: insurance-recommendation\n",
      "name 'detect' is not defined Language is not detected: mrmorj\n",
      "name 'detect' is not defined Language is not detected: Insurance Recommendation\n",
      "name 'detect' is not defined Language is not detected: Improve market outcomes\n",
      "name 'detect' is not defined Language is not detected: ### Description\n",
      "\n",
      "For insurance markets to work well, insurance companies need to be able to pool and spread risk across a broad customer base. This works best where the population to be insured is diverse and large. In Africa, formal insurance against risk has been hampered by lack of private sector companies offering insurance, with no way to diversify and pool risk across populations.\n",
      "\n",
      "Understanding the varied insurance needs of a population, and matching them to appropriate products offered by insurance companies, makes insurance more effective and makes insurance companies more successful.\n",
      "\n",
      "### Evaluation\n",
      "\n",
      "The error metric for this competition is the **log loss**. For every customer ID in the test set, for each product code, you must submit a prediction between 0 and 1 for likelihood that that customer has that product.\n",
      "\n",
      "### Data\n",
      "\n",
      "In Train, there is a 1 in the relevant column for each product that a customer has. Test is similar, except that for each customer ONE product has been removed (the 1 replaced with a 0). Your goal is to build a model to predict the missing product.\n",
      "name 'detect' is not defined Language is not detected: insurance-recommendation\n",
      "name 'detect' is not defined Language is not detected: mrmorj\n",
      "name 'detect' is not defined Language is not detected: Insurance Recommendation\n",
      "name 'detect' is not defined Language is not detected: Improve market outcomes\n",
      "name 'detect' is not defined Language is not detected: ### Description\n",
      "\n",
      "For insurance markets to work well, insurance companies need to be able to pool and spread risk across a broad customer base. This works best where the population to be insured is diverse and large. In Africa, formal insurance against risk has been hampered by lack of private sector companies offering insurance, with no way to diversify and pool risk across populations.\n",
      "\n",
      "Understanding the varied insurance needs of a population, and matching them to appropriate products offered by insurance companies, makes insurance more effective and makes insurance companies more successful.\n",
      "\n",
      "### Evaluation\n",
      "\n",
      "The error metric for this competition is the **log loss**. For every customer ID in the test set, for each product code, you must submit a prediction between 0 and 1 for likelihood that that customer has that product.\n",
      "\n",
      "### Data\n",
      "\n",
      "In Train, there is a 1 in the relevant column for each product that a customer has. Test is similar, except that for each customer ONE product has been removed (the 1 replaced with a 0). Your goal is to build a model to predict the missing product.\n",
      "name 'detect' is not defined Language is not detected: kevinmorgado/spanish-news-classification\n",
      "name 'detect' is not defined Language is not detected: spanish-news-classification\n",
      "name 'detect' is not defined Language is not detected: kevinmorgado\n",
      "name 'detect' is not defined Language is not detected: Spanish News Classification\n",
      "name 'detect' is not defined Language is not detected: Natural Language Processing in Spanish\n",
      "name 'detect' is not defined Language is not detected: This dataset was built with a web scraping tool for the Dataton 2022 of Bancolombia for training supervised models to use in a News recommendation of the following categories:\n",
      "\n",
      "1. Macroeconomics\n",
      "2. Sustainability\n",
      "3. Innovation\n",
      "4. Regulations\n",
      "5. Alliances\n",
      "6. Reputation\n",
      "7. Other\n",
      "\n",
      "# Columns\n",
      "This CSV document consists of the following columns:\n",
      "\n",
      "1. **Url:** source of the information, but it could be unavailable in some months\n",
      "2. **News:** text of the news - used for classification\n",
      "3. **Type:** Label of type of news.\n",
      "name 'detect' is not defined Language is not detected: spanish-news-classification\n",
      "name 'detect' is not defined Language is not detected: kevinmorgado\n",
      "name 'detect' is not defined Language is not detected: Spanish News Classification\n",
      "name 'detect' is not defined Language is not detected: Natural Language Processing in Spanish\n",
      "name 'detect' is not defined Language is not detected: This dataset was built with a web scraping tool for the Dataton 2022 of Bancolombia for training supervised models to use in a News recommendation of the following categories:\n",
      "\n",
      "1. Macroeconomics\n",
      "2. Sustainability\n",
      "3. Innovation\n",
      "4. Regulations\n",
      "5. Alliances\n",
      "6. Reputation\n",
      "7. Other\n",
      "\n",
      "# Columns\n",
      "This CSV document consists of the following columns:\n",
      "\n",
      "1. **Url:** source of the information, but it could be unavailable in some months\n",
      "2. **News:** text of the news - used for classification\n",
      "3. **Type:** Label of type of news.\n",
      "name 'detect' is not defined Language is not detected: kiranshahi/life-expectancy-dataset\n",
      "name 'detect' is not defined Language is not detected: life-expectancy-dataset\n",
      "name 'detect' is not defined Language is not detected: kiranshahi\n",
      "name 'detect' is not defined Language is not detected: life expectancy dataset\n",
      "name 'detect' is not defined Language is not detected: Life expectancy prediction dataset based on World Development Indicator (WDI).\n",
      "name 'detect' is not defined Language is not detected: &gt; These datasets were collected to fulfil the requirement of University coursework.\n",
      " \n",
      "[The complete source code and paper are available on GitHub.  **Click here**.](https://github.com/kiranshahi/Life-expectancy-prediction)\n",
      "\n",
      "## About Dataset\n",
      "These datasets contain the information of the World Development Indicator (WDI) provided by the world bank, the non-communicable mortality rate, the suicide rate and the number of health workforce data by the World Health Organization (WHO).\n",
      "\n",
      "\n",
      "| Dataset | Description |\n",
      "| --- | --- |\n",
      "| [World Development Indicators](https://datatopics.worldbank.org/world-development-indicators/) | This dataset contains the data of 1444 development indicators for 2666 countries and country groups between the years 1960 to 2020. This dataset was downloaded from the world bank’s data hub. |\n",
      "| [Health workforce](https://apps.who.int/gho/data/node.main.HWFGRP_0020?lang=en) | This dataset contains the health workforce information such as medical doctors (per 10000 population), number of medical doctors, number of Generalist medical practitioners, etc. |\n",
      "| [Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%)](https://data.worldbank.org/indicator/SH.DYN.NCOM.ZS) | This dataset contains information on mortality caused by various non-communicable diseases such as cardiovascular disease (CVD), cancer, diabetes etc. We have used two files for this dataset. Separately for both males and females. This dataset was downloaded from the world bank’s databank. |\n",
      "| [Suicide mortality rate (per 100,000 population)](https://data.worldbank.org/indicator/SH.STA.SUIC.P5) | This data set contains information on the suicide mortality rate per 100,000 population. We have used two files for this dataset. Separately for both males and females. This dataset was downloaded from the world bank’s databank. |\n",
      "\n",
      "## Implementation \n",
      "- [Data Cleaning and Preparation](https://www.kaggle.com/code/kiranshahi/data-cleaning-and-preparation)\n",
      "- [Exploratory Data Analysis](https://www.kaggle.com/code/kiranshahi/exploratory-data-analysis)\n",
      "- [Machine Learning Prediction - Random Forest](https://www.kaggle.com/code/kiranshahi/machine-learning-prediction-random-forest)\n",
      "- [Deep Learning Prediction - Neural Network](https://www.kaggle.com/code/kiranshahi/deep-learning-prediction-neural-network)\n",
      "name 'detect' is not defined Language is not detected: life-expectancy-dataset\n",
      "name 'detect' is not defined Language is not detected: kiranshahi\n",
      "name 'detect' is not defined Language is not detected: life expectancy dataset\n",
      "name 'detect' is not defined Language is not detected: Life expectancy prediction dataset based on World Development Indicator (WDI).\n",
      "name 'detect' is not defined Language is not detected: &gt; These datasets were collected to fulfil the requirement of University coursework.\n",
      " \n",
      "[The complete source code and paper are available on GitHub.  **Click here**.](https://github.com/kiranshahi/Life-expectancy-prediction)\n",
      "\n",
      "## About Dataset\n",
      "These datasets contain the information of the World Development Indicator (WDI) provided by the world bank, the non-communicable mortality rate, the suicide rate and the number of health workforce data by the World Health Organization (WHO).\n",
      "\n",
      "\n",
      "| Dataset | Description |\n",
      "| --- | --- |\n",
      "| [World Development Indicators](https://datatopics.worldbank.org/world-development-indicators/) | This dataset contains the data of 1444 development indicators for 2666 countries and country groups between the years 1960 to 2020. This dataset was downloaded from the world bank’s data hub. |\n",
      "| [Health workforce](https://apps.who.int/gho/data/node.main.HWFGRP_0020?lang=en) | This dataset contains the health workforce information such as medical doctors (per 10000 population), number of medical doctors, number of Generalist medical practitioners, etc. |\n",
      "| [Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%)](https://data.worldbank.org/indicator/SH.DYN.NCOM.ZS) | This dataset contains information on mortality caused by various non-communicable diseases such as cardiovascular disease (CVD), cancer, diabetes etc. We have used two files for this dataset. Separately for both males and females. This dataset was downloaded from the world bank’s databank. |\n",
      "| [Suicide mortality rate (per 100,000 population)](https://data.worldbank.org/indicator/SH.STA.SUIC.P5) | This data set contains information on the suicide mortality rate per 100,000 population. We have used two files for this dataset. Separately for both males and females. This dataset was downloaded from the world bank’s databank. |\n",
      "\n",
      "## Implementation \n",
      "- [Data Cleaning and Preparation](https://www.kaggle.com/code/kiranshahi/data-cleaning-and-preparation)\n",
      "- [Exploratory Data Analysis](https://www.kaggle.com/code/kiranshahi/exploratory-data-analysis)\n",
      "- [Machine Learning Prediction - Random Forest](https://www.kaggle.com/code/kiranshahi/machine-learning-prediction-random-forest)\n",
      "- [Deep Learning Prediction - Neural Network](https://www.kaggle.com/code/kiranshahi/deep-learning-prediction-neural-network)\n",
      "name 'detect' is not defined Language is not detected: anthonytherrien/20000-coding-questions-solved-with-code-llama-70b\n",
      "name 'detect' is not defined Language is not detected: 20000-coding-questions-solved-with-code-llama-70b\n",
      "name 'detect' is not defined Language is not detected: anthonytherrien\n",
      "name 'detect' is not defined Language is not detected: Coding Questions Solved With Code Llama 70B\n",
      "name 'detect' is not defined Language is not detected: 19,983 AI-Generated Responses by CodeLlama-70b-Instruct-hf\n",
      "name 'detect' is not defined Language is not detected: ## Dataset Overview\n",
      "This dataset is a rich collection of programming-related prompts and responses, meticulously generated by the advanced AI model `codellama/CodeLlama-70b-Instruct-hf`. It serves as a valuable resource for researchers, data scientists, and enthusiasts in the field of natural language processing, specifically focusing on code generation and understanding.\n",
      "\n",
      "### Dataset Content\n",
      "Each entry in the dataset comprises a pair of:\n",
      "- `prompt`: A problem statement or a task description, often related to programming, algorithms, or computational thinking.\n",
      "- `response`: The AI-generated solution or response to the prompt, showcasing the model's capability to understand and generate code-related content.\n",
      "\n",
      "### Size and Structure\n",
      "- Number of Entries: 19,983\n",
      "- Format: JSON Lines (`.jsonl`)\n",
      "\n",
      "The JSON Lines format makes it convenient for iterative processing, allowing users to read one entry at a time without loading the entire dataset into memory.\n",
      "\n",
      "## Model Description\n",
      "The responses in this dataset are generated by the model `codellama/CodeLlama-70b-Instruct-hf`. This model is known for its robust performance in code generation and comprehension tasks, making it an ideal choice for creating datasets that require a deep understanding of programming concepts and natural language.\n",
      "\n",
      "## Potential Use-Cases\n",
      "This dataset can be utilized in various research and practical applications, including but not limited to:\n",
      "- Training and evaluating models for code generation.\n",
      "- Enhancing AI-based code recommendation systems.\n",
      "- Analyzing the patterns and quality of AI-generated code.\n",
      "- Benchmarking the performance of code-related language models.\n",
      "\n",
      "## Acknowledgements\n",
      "The dataset is generated using the `codellama/CodeLlama-70b-Instruct-hf` model. Users of this dataset are encouraged to cite the model appropriately in their research or applications.\n",
      "\n",
      "## Disclaimer\n",
      "The responses in the dataset are AI-generated and might not always provide the most accurate or secure code solutions. Users are advised to review and test the generated code thoroughly before any practical implementation.\n",
      "name 'detect' is not defined Language is not detected: 20000-coding-questions-solved-with-code-llama-70b\n",
      "name 'detect' is not defined Language is not detected: anthonytherrien\n",
      "name 'detect' is not defined Language is not detected: Coding Questions Solved With Code Llama 70B\n",
      "name 'detect' is not defined Language is not detected: 19,983 AI-Generated Responses by CodeLlama-70b-Instruct-hf\n",
      "name 'detect' is not defined Language is not detected: ## Dataset Overview\n",
      "This dataset is a rich collection of programming-related prompts and responses, meticulously generated by the advanced AI model `codellama/CodeLlama-70b-Instruct-hf`. It serves as a valuable resource for researchers, data scientists, and enthusiasts in the field of natural language processing, specifically focusing on code generation and understanding.\n",
      "\n",
      "### Dataset Content\n",
      "Each entry in the dataset comprises a pair of:\n",
      "- `prompt`: A problem statement or a task description, often related to programming, algorithms, or computational thinking.\n",
      "- `response`: The AI-generated solution or response to the prompt, showcasing the model's capability to understand and generate code-related content.\n",
      "\n",
      "### Size and Structure\n",
      "- Number of Entries: 19,983\n",
      "- Format: JSON Lines (`.jsonl`)\n",
      "\n",
      "The JSON Lines format makes it convenient for iterative processing, allowing users to read one entry at a time without loading the entire dataset into memory.\n",
      "\n",
      "## Model Description\n",
      "The responses in this dataset are generated by the model `codellama/CodeLlama-70b-Instruct-hf`. This model is known for its robust performance in code generation and comprehension tasks, making it an ideal choice for creating datasets that require a deep understanding of programming concepts and natural language.\n",
      "\n",
      "## Potential Use-Cases\n",
      "This dataset can be utilized in various research and practical applications, including but not limited to:\n",
      "- Training and evaluating models for code generation.\n",
      "- Enhancing AI-based code recommendation systems.\n",
      "- Analyzing the patterns and quality of AI-generated code.\n",
      "- Benchmarking the performance of code-related language models.\n",
      "\n",
      "## Acknowledgements\n",
      "The dataset is generated using the `codellama/CodeLlama-70b-Instruct-hf` model. Users of this dataset are encouraged to cite the model appropriately in their research or applications.\n",
      "\n",
      "## Disclaimer\n",
      "The responses in the dataset are AI-generated and might not always provide the most accurate or secure code solutions. Users are advised to review and test the generated code thoroughly before any practical implementation.\n",
      "name 'detect' is not defined Language is not detected: santhraul/telecom-churn-data\n",
      "name 'detect' is not defined Language is not detected: telecom-churn-data\n",
      "name 'detect' is not defined Language is not detected: santhraul\n",
      "name 'detect' is not defined Language is not detected: telecom_churn_data\n",
      "name 'detect' is not defined Language is not detected: Telecom Customer Churn data\n",
      "name 'detect' is not defined Language is not detected: telecom-churn-data\n",
      "name 'detect' is not defined Language is not detected: santhraul\n",
      "name 'detect' is not defined Language is not detected: telecom_churn_data\n",
      "name 'detect' is not defined Language is not detected: Telecom Customer Churn data\n",
      "name 'detect' is not defined Language is not detected: praveenmaripeti/avrecommendation-system\n",
      "name 'detect' is not defined Language is not detected: avrecommendation-system\n",
      "name 'detect' is not defined Language is not detected: praveenmaripeti\n",
      "name 'detect' is not defined Language is not detected: AV-Recommendation System\n",
      "name 'detect' is not defined Language is not detected: avrecommendation-system\n",
      "name 'detect' is not defined Language is not detected: praveenmaripeti\n",
      "name 'detect' is not defined Language is not detected: AV-Recommendation System\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01/porter-delivery-time-estimation-dataset\n",
      "name 'detect' is not defined Language is not detected: porter-delivery-time-estimation-dataset\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: Porter Delivery Time Estimation Dataset\n",
      "name 'detect' is not defined Language is not detected: Regression with Neural networks\n",
      "name 'detect' is not defined Language is not detected: Predicting delivery time for the order Problem Statement\n",
      "\n",
      "Porter is India's Largest Marketplace for Intra-City Logistics. Leader in the country's $40 billion intra-city logistics market, Porter strives to improve the lives of 1,50,000+ driver-partners by providing them with consistent earning & independence. Currently, the company has serviced 5+ million customers\n",
      "\n",
      "Porter works with a wide range of restaurants for delivering their items directly to the people.\n",
      "\n",
      "Porter has a number of delivery partners available for delivering the food, from various restaurants and wants to get an estimated delivery time that it can provide the customers on the basis of what they are ordering, from where and also the delivery partners.\n",
      "\n",
      "This dataset has the required data to train a regression model that will do the delivery time estimation, based on all those features\n",
      "name 'detect' is not defined Language is not detected: porter-delivery-time-estimation-dataset\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: Porter Delivery Time Estimation Dataset\n",
      "name 'detect' is not defined Language is not detected: Regression with Neural networks\n",
      "name 'detect' is not defined Language is not detected: Predicting delivery time for the order Problem Statement\n",
      "\n",
      "Porter is India's Largest Marketplace for Intra-City Logistics. Leader in the country's $40 billion intra-city logistics market, Porter strives to improve the lives of 1,50,000+ driver-partners by providing them with consistent earning & independence. Currently, the company has serviced 5+ million customers\n",
      "\n",
      "Porter works with a wide range of restaurants for delivering their items directly to the people.\n",
      "\n",
      "Porter has a number of delivery partners available for delivering the food, from various restaurants and wants to get an estimated delivery time that it can provide the customers on the basis of what they are ordering, from where and also the delivery partners.\n",
      "\n",
      "This dataset has the required data to train a regression model that will do the delivery time estimation, based on all those features\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01/porter-delivery-time-estimation\n",
      "name 'detect' is not defined Language is not detected: porter-delivery-time-estimation\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: Porter Delivery Time Estimation\n",
      "name 'detect' is not defined Language is not detected: Regression with Neural networks\n",
      "name 'detect' is not defined Language is not detected: **Predicting delivery time for the order**\n",
      "**Problem Statement**\n",
      "\n",
      "Porter is India's Largest Marketplace for Intra-City Logistics. Leader in the country's $40 billion intra-city logistics market, Porter strives to improve the lives of 1,50,000+ driver-partners by providing them with consistent earning & independence. Currently, the company has serviced 5+ million customers\n",
      "\n",
      "Porter works with a wide range of restaurants for delivering their items directly to the people.\n",
      "\n",
      "Porter has a number of delivery partners available for delivering the food, from various restaurants and wants to get an estimated delivery time that it can provide the customers on the basis of what they are ordering, from where and also the delivery partners.\n",
      "\n",
      "This dataset has the required data to train a regression model that will do the delivery time estimation, based on all those features\n",
      "name 'detect' is not defined Language is not detected: porter-delivery-time-estimation\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: Porter Delivery Time Estimation\n",
      "name 'detect' is not defined Language is not detected: Regression with Neural networks\n",
      "name 'detect' is not defined Language is not detected: **Predicting delivery time for the order**\n",
      "**Problem Statement**\n",
      "\n",
      "Porter is India's Largest Marketplace for Intra-City Logistics. Leader in the country's $40 billion intra-city logistics market, Porter strives to improve the lives of 1,50,000+ driver-partners by providing them with consistent earning & independence. Currently, the company has serviced 5+ million customers\n",
      "\n",
      "Porter works with a wide range of restaurants for delivering their items directly to the people.\n",
      "\n",
      "Porter has a number of delivery partners available for delivering the food, from various restaurants and wants to get an estimated delivery time that it can provide the customers on the basis of what they are ordering, from where and also the delivery partners.\n",
      "\n",
      "This dataset has the required data to train a regression model that will do the delivery time estimation, based on all those features\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01/loantap-logisticregression\n",
      "name 'detect' is not defined Language is not detected: loantap-logisticregression\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: LoanTap-LogisticRegression\n",
      "name 'detect' is not defined Language is not detected: LoanTap - Business Case Study\n",
      "name 'detect' is not defined Language is not detected: Context:\n",
      "Loan Tap is an online platform committed to delivering customized loan products to millennials. They innovate in an otherwise dull loan segment, to deliver instant, flexible loans on consumer friendly terms to salaried professionals and businessmen.\n",
      "The data science team at Loan Tap is building an underwriting layer to determine the creditworthiness of MSMEs as well as individuals.\n",
      "Loan Tap deploys formal credit to salaried individuals and businesses 4 main financial instruments:\n",
      "1. Personal Loan\n",
      "2. EMI Free Loan\n",
      "3. Personal Overdraft\n",
      "4. Advance Salary Loan\n",
      "This case study will focus on the underwriting process behind Personal Loan\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Given a set of attributes for an Individual, determine if a credit line should be extended to them. If so, what should the repayment terms be in business recommendations?\n",
      "\n",
      "name 'detect' is not defined Language is not detected: loantap-logisticregression\n",
      "name 'detect' is not defined Language is not detected: ranitsarkar01\n",
      "name 'detect' is not defined Language is not detected: LoanTap-LogisticRegression\n",
      "name 'detect' is not defined Language is not detected: LoanTap - Business Case Study\n",
      "name 'detect' is not defined Language is not detected: Context:\n",
      "Loan Tap is an online platform committed to delivering customized loan products to millennials. They innovate in an otherwise dull loan segment, to deliver instant, flexible loans on consumer friendly terms to salaried professionals and businessmen.\n",
      "The data science team at Loan Tap is building an underwriting layer to determine the creditworthiness of MSMEs as well as individuals.\n",
      "Loan Tap deploys formal credit to salaried individuals and businesses 4 main financial instruments:\n",
      "1. Personal Loan\n",
      "2. EMI Free Loan\n",
      "3. Personal Overdraft\n",
      "4. Advance Salary Loan\n",
      "This case study will focus on the underwriting process behind Personal Loan\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Given a set of attributes for an Individual, determine if a credit line should be extended to them. If so, what should the repayment terms be in business recommendations?\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sudarshanvaidya/medium-data-science-articles-topic-modelling\n",
      "name 'detect' is not defined Language is not detected: medium-data-science-articles-topic-modelling\n",
      "name 'detect' is not defined Language is not detected: sudarshanvaidya\n",
      "name 'detect' is not defined Language is not detected: Medium Data Science Articles Topic Modelling \n",
      "name 'detect' is not defined Language is not detected: Medium Data Science Articles Topic Modelling using TFIDF, LDA, NMF for 10 topics \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset can be used in content based recommendation system for data science articles. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains NMF based 10 topic modelling sparse vectorization of Data Science articles \n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Inspired from: https://www.kaggle.com/aiswaryaramachandran/medium-articles-with-content  \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Sharing the dataset to help others learn and try out recommendation system implementations.  \n",
      "name 'detect' is not defined Language is not detected: medium-data-science-articles-topic-modelling\n",
      "name 'detect' is not defined Language is not detected: sudarshanvaidya\n",
      "name 'detect' is not defined Language is not detected: Medium Data Science Articles Topic Modelling \n",
      "name 'detect' is not defined Language is not detected: Medium Data Science Articles Topic Modelling using TFIDF, LDA, NMF for 10 topics \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The dataset can be used in content based recommendation system for data science articles. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset contains NMF based 10 topic modelling sparse vectorization of Data Science articles \n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Inspired from: https://www.kaggle.com/aiswaryaramachandran/medium-articles-with-content  \n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Sharing the dataset to help others learn and try out recommendation system implementations.  \n",
      "name 'detect' is not defined Language is not detected: rshah1990/bibliothecamodelingdata\n",
      "name 'detect' is not defined Language is not detected: bibliothecamodelingdata\n",
      "name 'detect' is not defined Language is not detected: rshah1990\n",
      "name 'detect' is not defined Language is not detected: BibliothecaModelingData\n",
      "name 'detect' is not defined Language is not detected: ### Recommendation System\n",
      "\n",
      "Book recommendation for Bibliotheca\n",
      "name 'detect' is not defined Language is not detected: bibliothecamodelingdata\n",
      "name 'detect' is not defined Language is not detected: rshah1990\n",
      "name 'detect' is not defined Language is not detected: BibliothecaModelingData\n",
      "name 'detect' is not defined Language is not detected: ### Recommendation System\n",
      "\n",
      "Book recommendation for Bibliotheca\n",
      "name 'detect' is not defined Language is not detected: suraj520/music-dataset-song-information-and-lyrics\n",
      "name 'detect' is not defined Language is not detected: music-dataset-song-information-and-lyrics\n",
      "name 'detect' is not defined Language is not detected: suraj520\n",
      "name 'detect' is not defined Language is not detected: Music Dataset: Song Information and Lyrics\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Collection of Songs with Metadata and Lyrics for Research & Dev\n",
      "name 'detect' is not defined Language is not detected: **The Dataset's Purpose:**\n",
      "This dataset's goal is to give a complete collection of music facts and lyrics for study and development. It aspires to be a useful resource for a variety of applications such as music analysis, natural language processing, sentiment analysis, recommendation systems, and others. This dataset, which combines song information and lyrics, can help academics, developers, and music fans examine and analyse the link between listeners' preferences and lyrical content.\n",
      "\n",
      "**Dataset Description:**\n",
      "\n",
      "The music dataset contains around 660 songs, each with its own set of characteristics. The following characteristics are included in the dataset:\n",
      "\n",
      "Name: The title of the song.\n",
      "Lyrics: The lyrics of the song.\n",
      "Singer: The name of the singer or artist who performed the song.\n",
      "Movie: The movie or album associated with the song (if applicable).\n",
      "Genre: The genre or genres to which the song belongs.\n",
      "Rating: The rating or popularity score of the song from Spotify.\n",
      "\n",
      "The dataset is intended to give a wide variety of songs from various genres, performers, and films. It includes popular songs from numerous ages and places, as well as a wide spectrum of musical styles. The lyrics were obtained from publically accessible services such as Spotify and Soundcloud, and were converted from audio to text using speech recognition algorithms. While every attempt has been taken to assure correctness, please keep in mind that owing to the limits of the data sources and voice recognition algorithms, there may be  inaccuracies or missing lyrics encountered upon transcribing.\n",
      "\n",
      "**Use Cases in Research and Development:**\n",
      "\n",
      "This music dataset has several research and development applications. Among the possible applications are:\n",
      "\n",
      "1. Music Analysis: By analysing the links between song elements such as genre, vocalist, and rating, researchers can acquire insights into the features and patterns of various music genres.\n",
      "2. Natural Language Processing (NLP): NLP researchers may use the lyrics to create language models, sentiment analysis algorithms, topic modelling approaches, and other text-based music studies.\n",
      "3. Recommendation Systems: Using the information, developers may create recommendation systems that offer music based on user preferences, lyrics sentiment, or genre similarities.\n",
      "4. Music Generating Machine Learning Models: The dataset may be used to train machine learning models for generating new lyrics or making music compositions.\n",
      "5. Music Sentiment Analysis: To get insights into the emotional components of music and its influence on listeners, researchers might analyse the feelings conveyed in song lyrics.\n",
      "6. Movie Soundtracks Analysis: Researchers can explore the association between song attributes and their use in movie soundtracks by investigating the movie attribute.\n",
      "\n",
      "Overall, the goal of this music dataset is to provide a rich resource for academics, developers, and music fans to investigate the complicated relationships between song features, lyrics, and numerous research and development applications in the music domain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: music-dataset-song-information-and-lyrics\n",
      "name 'detect' is not defined Language is not detected: suraj520\n",
      "name 'detect' is not defined Language is not detected: Music Dataset: Song Information and Lyrics\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Collection of Songs with Metadata and Lyrics for Research & Dev\n",
      "name 'detect' is not defined Language is not detected: **The Dataset's Purpose:**\n",
      "This dataset's goal is to give a complete collection of music facts and lyrics for study and development. It aspires to be a useful resource for a variety of applications such as music analysis, natural language processing, sentiment analysis, recommendation systems, and others. This dataset, which combines song information and lyrics, can help academics, developers, and music fans examine and analyse the link between listeners' preferences and lyrical content.\n",
      "\n",
      "**Dataset Description:**\n",
      "\n",
      "The music dataset contains around 660 songs, each with its own set of characteristics. The following characteristics are included in the dataset:\n",
      "\n",
      "Name: The title of the song.\n",
      "Lyrics: The lyrics of the song.\n",
      "Singer: The name of the singer or artist who performed the song.\n",
      "Movie: The movie or album associated with the song (if applicable).\n",
      "Genre: The genre or genres to which the song belongs.\n",
      "Rating: The rating or popularity score of the song from Spotify.\n",
      "\n",
      "The dataset is intended to give a wide variety of songs from various genres, performers, and films. It includes popular songs from numerous ages and places, as well as a wide spectrum of musical styles. The lyrics were obtained from publically accessible services such as Spotify and Soundcloud, and were converted from audio to text using speech recognition algorithms. While every attempt has been taken to assure correctness, please keep in mind that owing to the limits of the data sources and voice recognition algorithms, there may be  inaccuracies or missing lyrics encountered upon transcribing.\n",
      "\n",
      "**Use Cases in Research and Development:**\n",
      "\n",
      "This music dataset has several research and development applications. Among the possible applications are:\n",
      "\n",
      "1. Music Analysis: By analysing the links between song elements such as genre, vocalist, and rating, researchers can acquire insights into the features and patterns of various music genres.\n",
      "2. Natural Language Processing (NLP): NLP researchers may use the lyrics to create language models, sentiment analysis algorithms, topic modelling approaches, and other text-based music studies.\n",
      "3. Recommendation Systems: Using the information, developers may create recommendation systems that offer music based on user preferences, lyrics sentiment, or genre similarities.\n",
      "4. Music Generating Machine Learning Models: The dataset may be used to train machine learning models for generating new lyrics or making music compositions.\n",
      "5. Music Sentiment Analysis: To get insights into the emotional components of music and its influence on listeners, researchers might analyse the feelings conveyed in song lyrics.\n",
      "6. Movie Soundtracks Analysis: Researchers can explore the association between song attributes and their use in movie soundtracks by investigating the movie attribute.\n",
      "\n",
      "Overall, the goal of this music dataset is to provide a rich resource for academics, developers, and music fans to investigate the complicated relationships between song features, lyrics, and numerous research and development applications in the music domain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: suraj520/imdb-tv-series-data\n",
      "name 'detect' is not defined Language is not detected: imdb-tv-series-data\n",
      "name 'detect' is not defined Language is not detected: suraj520\n",
      "name 'detect' is not defined Language is not detected: IMDb TV Series Data\n",
      "name 'detect' is not defined Language is not detected: TV Series Data from IMDb\n",
      "name 'detect' is not defined Language is not detected: **Description:**\n",
      "This dataset contains information about TV series from IMDb, including details such as title, IMDb ID, release year, genre, cast, synopsis, rating, runtime, certificate, number of votes, and gross revenue. The data is scraped from the IMDb website using web scraping techniques and is organized into separate CSV files for each genre.\n",
      "\n",
      "**Features:**\n",
      "1. Title: The title of the TV series.\n",
      "2. IMDb ID: The unique identifier for the series on IMDb.\n",
      "3. Release Year: The year in which the series was released.\n",
      "4. Genre: The genre(s) of the series.\n",
      "5. Cast: The main cast members of the series.\n",
      "6. Synopsis: A brief summary or description of the series.\n",
      "7. Rating: The average rating of the series on IMDb (scaled from 1 to 10).\n",
      "8. Runtime: The duration of each episode or the total runtime of the series.\n",
      "9. Certificate: The content rating or certificate assigned to the series (e.g., PG-13, TV-MA).\n",
      "10. Number of Votes: The total number of votes or ratings received by the series.\n",
      "11. Gross Revenue: The total gross revenue generated by the series (if available).\n",
      "\n",
      "**Usage:**\n",
      "This dataset can be used for various purposes, including but not limited to:\n",
      "\n",
      "1. TV Series Analysis: Researchers, analysts, and enthusiasts can explore and analyze the characteristics, trends, and patterns of TV series across different genres.\n",
      "2. Recommendation Systems: The dataset can be used to build recommendation systems that suggest TV series based on user preferences, genre preferences, or similar series.\n",
      "3. Genre-based Analysis: By considering different genres, users can perform comparative analysis, identify popular genres, and examine the relationships between genres and other variables such as ratings, runtime, certificate, number of votes, or gross revenue.\n",
      "4. Content Curation: Media companies or streaming platforms can utilize this dataset to curate and recommend TV series to their users based on genre preferences, ratings, certificate, number of votes, or other factors.\n",
      "5. Machine Learning and Natural Language Processing (NLP): The dataset can be used for training machine learning models or NLP tasks, such as sentiment analysis, text classification, or text generation using the synopsis and other textual features.\n",
      "\n",
      "**CSV Files:**\n",
      "The dataset is organized into the following CSV files, each corresponding to a specific genre of TV series:\n",
      "- action_series.csv\n",
      "- adventure_series.csv\n",
      "- animation_series.csv\n",
      "- biography_series.csv\n",
      "- comedy_series.csv\n",
      "- crime_series.csv\n",
      "- documentary_series.csv\n",
      "- drama_series.csv\n",
      "- family_series.csv\n",
      "- fantasy_series.csv\n",
      "- history_series.csv\n",
      "- horror_series.csv\n",
      "- music_series.csv\n",
      "- musical_series.csv\n",
      "- mystery_series.csv\n",
      "- romance_series.csv\n",
      "- sci-fi_series.csv\n",
      "- sport_series.csv\n",
      "- superhero_series.csv\n",
      "- thriller_series.csv\n",
      "- war_series.csv\n",
      "- western_series.csv\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: imdb-tv-series-data\n",
      "name 'detect' is not defined Language is not detected: suraj520\n",
      "name 'detect' is not defined Language is not detected: IMDb TV Series Data\n",
      "name 'detect' is not defined Language is not detected: TV Series Data from IMDb\n",
      "name 'detect' is not defined Language is not detected: **Description:**\n",
      "This dataset contains information about TV series from IMDb, including details such as title, IMDb ID, release year, genre, cast, synopsis, rating, runtime, certificate, number of votes, and gross revenue. The data is scraped from the IMDb website using web scraping techniques and is organized into separate CSV files for each genre.\n",
      "\n",
      "**Features:**\n",
      "1. Title: The title of the TV series.\n",
      "2. IMDb ID: The unique identifier for the series on IMDb.\n",
      "3. Release Year: The year in which the series was released.\n",
      "4. Genre: The genre(s) of the series.\n",
      "5. Cast: The main cast members of the series.\n",
      "6. Synopsis: A brief summary or description of the series.\n",
      "7. Rating: The average rating of the series on IMDb (scaled from 1 to 10).\n",
      "8. Runtime: The duration of each episode or the total runtime of the series.\n",
      "9. Certificate: The content rating or certificate assigned to the series (e.g., PG-13, TV-MA).\n",
      "10. Number of Votes: The total number of votes or ratings received by the series.\n",
      "11. Gross Revenue: The total gross revenue generated by the series (if available).\n",
      "\n",
      "**Usage:**\n",
      "This dataset can be used for various purposes, including but not limited to:\n",
      "\n",
      "1. TV Series Analysis: Researchers, analysts, and enthusiasts can explore and analyze the characteristics, trends, and patterns of TV series across different genres.\n",
      "2. Recommendation Systems: The dataset can be used to build recommendation systems that suggest TV series based on user preferences, genre preferences, or similar series.\n",
      "3. Genre-based Analysis: By considering different genres, users can perform comparative analysis, identify popular genres, and examine the relationships between genres and other variables such as ratings, runtime, certificate, number of votes, or gross revenue.\n",
      "4. Content Curation: Media companies or streaming platforms can utilize this dataset to curate and recommend TV series to their users based on genre preferences, ratings, certificate, number of votes, or other factors.\n",
      "5. Machine Learning and Natural Language Processing (NLP): The dataset can be used for training machine learning models or NLP tasks, such as sentiment analysis, text classification, or text generation using the synopsis and other textual features.\n",
      "\n",
      "**CSV Files:**\n",
      "The dataset is organized into the following CSV files, each corresponding to a specific genre of TV series:\n",
      "- action_series.csv\n",
      "- adventure_series.csv\n",
      "- animation_series.csv\n",
      "- biography_series.csv\n",
      "- comedy_series.csv\n",
      "- crime_series.csv\n",
      "- documentary_series.csv\n",
      "- drama_series.csv\n",
      "- family_series.csv\n",
      "- fantasy_series.csv\n",
      "- history_series.csv\n",
      "- horror_series.csv\n",
      "- music_series.csv\n",
      "- musical_series.csv\n",
      "- mystery_series.csv\n",
      "- romance_series.csv\n",
      "- sci-fi_series.csv\n",
      "- sport_series.csv\n",
      "- superhero_series.csv\n",
      "- thriller_series.csv\n",
      "- war_series.csv\n",
      "- western_series.csv\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: blessondensil294/topic-modeling-for-research-articles\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles\n",
      "name 'detect' is not defined Language is not detected: blessondensil294\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "name 'detect' is not defined Language is not detected: NLP Topic Modelling based on Research Articles.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Since the lockdown was announced in the country back in March, we started with a 1 day hackathon called Janatahack inspired from Janata cerfew to start our war against the pandemic.  Looking at the amazing response and demand for more, we continued the hackathons over the weekends every week. Janatahack today is a phenomena where loads of esteemed members of our community regularly participate to showcase their machine learning skills by sharing their approaches and more important to learn how to apply machine learning and predictive analytics to new domains such as agriculture, Banking, IOT, forecasting and so on. \n",
      "\n",
      "This time we bring to you hackathon, this time a 10 day extravaganza launching on the independence day for India, 15th August 2020. Open to all data practitioners, beginners in data science and data scientists. Register today to test your skills and earn AV Points. The theme for this hackathon will be launched on the independence day along with the problem statement and the dataset. So stay tuned and register today to receive all the updates regarding this exciting event.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to Analytics Vidhya for the Dataset\n",
      "\n",
      "name 'detect' is not defined Language is not detected: topic-modeling-for-research-articles\n",
      "name 'detect' is not defined Language is not detected: blessondensil294\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "name 'detect' is not defined Language is not detected: NLP Topic Modelling based on Research Articles.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Since the lockdown was announced in the country back in March, we started with a 1 day hackathon called Janatahack inspired from Janata cerfew to start our war against the pandemic.  Looking at the amazing response and demand for more, we continued the hackathons over the weekends every week. Janatahack today is a phenomena where loads of esteemed members of our community regularly participate to showcase their machine learning skills by sharing their approaches and more important to learn how to apply machine learning and predictive analytics to new domains such as agriculture, Banking, IOT, forecasting and so on. \n",
      "\n",
      "This time we bring to you hackathon, this time a 10 day extravaganza launching on the independence day for India, 15th August 2020. Open to all data practitioners, beginners in data science and data scientists. Register today to test your skills and earn AV Points. The theme for this hackathon will be launched on the independence day along with the problem statement and the dataset. So stay tuned and register today to receive all the updates regarding this exciting event.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Thanks to Analytics Vidhya for the Dataset\n",
      "\n",
      "name 'detect' is not defined Language is not detected: blessondensil294/zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: blessondensil294\n",
      "name 'detect' is not defined Language is not detected: Zimnat Insurance Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: Competition to predict which insurance products existing clients will want next?\n",
      "name 'detect' is not defined Language is not detected: zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: blessondensil294\n",
      "name 'detect' is not defined Language is not detected: Zimnat Insurance Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: Competition to predict which insurance products existing clients will want next?\n",
      "name 'detect' is not defined Language is not detected: raqhea/medium-app-reviews-from-google-play-store\n",
      "name 'detect' is not defined Language is not detected: medium-app-reviews-from-google-play-store\n",
      "name 'detect' is not defined Language is not detected: raqhea\n",
      "name 'detect' is not defined Language is not detected: Medium App Reviews from Google Play Store\n",
      "name 'detect' is not defined Language is not detected: Medium application reviews from google play store for text classification\n",
      "name 'detect' is not defined Language is not detected: Please upvote the dataset if you find it helpful :) \n",
      "\n",
      "The data was collected by using the **google\\_play\\_scraper** library to pull the application reviews. I have filtered the downloaded data to English and collected every review. A small portion of the dataset is used to label the data manually. The labeling is done to categorize the reviews into four categories:\n",
      "\n",
      "**Subscription:** Since medium has a subscription option, anything related to users' opinions about subscription features should belong here.\n",
      "\n",
      "**Content:** Medium is a sharing platform, there are lots of writings from poetry to advanced artificial intelligence research. Users’ opinions about a variety of topics, and the quality of the content should belong here.\n",
      "\n",
      "**Interface:** Thoughts about UI, searching articles, recommendation engine, and anything related to the interface should belong here. This also includes payment-related issues.\n",
      "\n",
      "**User Experience:** The user’s general thoughts and opinions about the application. Which should be generally abstract without indicating another category.\n",
      "\n",
      "~350 samples were used to fine-tune a roBERTa model to classify the reviews, then I used the same model for sentiment analysis (without fine-tuning). \n",
      "\n",
      "More detailed information about the data collection can be found in this notebook:\n",
      "https://www.kaggle.com/code/raqhea/automatically-collect-process-and-label-text-data\n",
      "\n",
      "The model used for fine-tuning: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
      "name 'detect' is not defined Language is not detected: medium-app-reviews-from-google-play-store\n",
      "name 'detect' is not defined Language is not detected: raqhea\n",
      "name 'detect' is not defined Language is not detected: Medium App Reviews from Google Play Store\n",
      "name 'detect' is not defined Language is not detected: Medium application reviews from google play store for text classification\n",
      "name 'detect' is not defined Language is not detected: Please upvote the dataset if you find it helpful :) \n",
      "\n",
      "The data was collected by using the **google\\_play\\_scraper** library to pull the application reviews. I have filtered the downloaded data to English and collected every review. A small portion of the dataset is used to label the data manually. The labeling is done to categorize the reviews into four categories:\n",
      "\n",
      "**Subscription:** Since medium has a subscription option, anything related to users' opinions about subscription features should belong here.\n",
      "\n",
      "**Content:** Medium is a sharing platform, there are lots of writings from poetry to advanced artificial intelligence research. Users’ opinions about a variety of topics, and the quality of the content should belong here.\n",
      "\n",
      "**Interface:** Thoughts about UI, searching articles, recommendation engine, and anything related to the interface should belong here. This also includes payment-related issues.\n",
      "\n",
      "**User Experience:** The user’s general thoughts and opinions about the application. Which should be generally abstract without indicating another category.\n",
      "\n",
      "~350 samples were used to fine-tune a roBERTa model to classify the reviews, then I used the same model for sentiment analysis (without fine-tuning). \n",
      "\n",
      "More detailed information about the data collection can be found in this notebook:\n",
      "https://www.kaggle.com/code/raqhea/automatically-collect-process-and-label-text-data\n",
      "\n",
      "The model used for fine-tuning: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
      "name 'detect' is not defined Language is not detected: kushagra1211/usa-sales-product-datasetcleaned\n",
      "name 'detect' is not defined Language is not detected: usa-sales-product-datasetcleaned\n",
      "name 'detect' is not defined Language is not detected: kushagra1211\n",
      "name 'detect' is not defined Language is not detected: USA Sales Product Dataset(Cleaned)\n",
      "name 'detect' is not defined Language is not detected: This data contains orders of electronic appliances price, quantity and sales.\n",
      "name 'detect' is not defined Language is not detected: # **Content**\n",
      "\n",
      "### The data has 185951 Rows and following colums:\n",
      "**Order ID\n",
      "Product Name\n",
      "Product Type\n",
      "Order Date\n",
      "Quantity\n",
      "Price\n",
      "City**\n",
      "\n",
      "# **Acknowledgements**\n",
      "\n",
      "**The data set was originally posted by @knightbearr, I cleaned the data set and re uploading it for the ML model builders to use it directly.**\n",
      "\n",
      "name 'detect' is not defined Language is not detected: usa-sales-product-datasetcleaned\n",
      "name 'detect' is not defined Language is not detected: kushagra1211\n",
      "name 'detect' is not defined Language is not detected: USA Sales Product Dataset(Cleaned)\n",
      "name 'detect' is not defined Language is not detected: This data contains orders of electronic appliances price, quantity and sales.\n",
      "name 'detect' is not defined Language is not detected: # **Content**\n",
      "\n",
      "### The data has 185951 Rows and following colums:\n",
      "**Order ID\n",
      "Product Name\n",
      "Product Type\n",
      "Order Date\n",
      "Quantity\n",
      "Price\n",
      "City**\n",
      "\n",
      "# **Acknowledgements**\n",
      "\n",
      "**The data set was originally posted by @knightbearr, I cleaned the data set and re uploading it for the ML model builders to use it directly.**\n",
      "\n",
      "name 'detect' is not defined Language is not detected: zygmunt/goodbooks-10k\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k\n",
      "name 'detect' is not defined Language is not detected: zygmunt\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k\n",
      "name 'detect' is not defined Language is not detected: Ten thousand books, one million ratings. Also books marked to read, and tags.\n",
      "name 'detect' is not defined Language is not detected: **This version of the dataset is obsolete. It contains duplicate ratings (same user_id,book_id), as reported by Philipp Spachtholz in his illustrious notebook.**\n",
      "\n",
      "**The current version has duplicates removed, and more ratings (six million), sorted by time. Book and user IDs are the same.** \n",
      "\n",
      "**It is available at https://github.com/zygmuntz/goodbooks-10k. **\n",
      "\n",
      "---\n",
      "\n",
      "There have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now. \n",
      "\n",
      "This dataset contains ratings for ten thousand popular books. As to the source, let's say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.\n",
      "\n",
      "Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.\n",
      "\n",
      "There are also books marked to read by the users, book metadata (author, year, etc.) and tags.\n",
      "\n",
      "## Contents\n",
      "\n",
      "**ratings.csv** contains ratings and looks like that:\n",
      "\n",
      "\tbook_id,user_id,rating\n",
      "\t1,314,5\n",
      "\t1,439,3\n",
      "\t1,588,5\n",
      "\t1,1169,4\n",
      "\t1,1185,4\n",
      "\n",
      "**to_read.csv** provides IDs of the books marked \"to read\" by each user, as **user_id,book_id** pairs.\n",
      "\n",
      "**books.csv** has metadata for each book (goodreads IDs, authors, title, average rating, etc.).\n",
      "\n",
      "The metadata have been extracted from goodreads XML files, available in the third version of this dataset as **books_xml.tar.gz**. The archive contains 10000 XML files. One of them is available as **sample_book.xml**. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.\n",
      "\n",
      "**book_tags.csv** contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.\n",
      "\n",
      "**tags.csv** translates tag IDs to names.\n",
      "\n",
      "See the [notebook][1] for some basic stats of the dataset.\n",
      "\n",
      "### goodreads IDs\n",
      "\n",
      "Each book may have many editions.  **goodreads_book_id** and **best_book_id** generally point to the most popular edition of a given book, while goodreads  **work_id** refers to the book in the abstract sense.\n",
      "\n",
      "You can use the goodreads book and work IDs to create URLs as follows:\n",
      "\n",
      "https://www.goodreads.com/book/show/2767052   \n",
      "https://www.goodreads.com/work/editions/2792775\n",
      "\n",
      "  [1]: https://github.com/zygmuntz/misc/blob/master/goodbooks-10k/basic_stats.ipynb\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k\n",
      "name 'detect' is not defined Language is not detected: zygmunt\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k\n",
      "name 'detect' is not defined Language is not detected: Ten thousand books, one million ratings. Also books marked to read, and tags.\n",
      "name 'detect' is not defined Language is not detected: **This version of the dataset is obsolete. It contains duplicate ratings (same user_id,book_id), as reported by Philipp Spachtholz in his illustrious notebook.**\n",
      "\n",
      "**The current version has duplicates removed, and more ratings (six million), sorted by time. Book and user IDs are the same.** \n",
      "\n",
      "**It is available at https://github.com/zygmuntz/goodbooks-10k. **\n",
      "\n",
      "---\n",
      "\n",
      "There have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now. \n",
      "\n",
      "This dataset contains ratings for ten thousand popular books. As to the source, let's say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.\n",
      "\n",
      "Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.\n",
      "\n",
      "There are also books marked to read by the users, book metadata (author, year, etc.) and tags.\n",
      "\n",
      "## Contents\n",
      "\n",
      "**ratings.csv** contains ratings and looks like that:\n",
      "\n",
      "\tbook_id,user_id,rating\n",
      "\t1,314,5\n",
      "\t1,439,3\n",
      "\t1,588,5\n",
      "\t1,1169,4\n",
      "\t1,1185,4\n",
      "\n",
      "**to_read.csv** provides IDs of the books marked \"to read\" by each user, as **user_id,book_id** pairs.\n",
      "\n",
      "**books.csv** has metadata for each book (goodreads IDs, authors, title, average rating, etc.).\n",
      "\n",
      "The metadata have been extracted from goodreads XML files, available in the third version of this dataset as **books_xml.tar.gz**. The archive contains 10000 XML files. One of them is available as **sample_book.xml**. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.\n",
      "\n",
      "**book_tags.csv** contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.\n",
      "\n",
      "**tags.csv** translates tag IDs to names.\n",
      "\n",
      "See the [notebook][1] for some basic stats of the dataset.\n",
      "\n",
      "### goodreads IDs\n",
      "\n",
      "Each book may have many editions.  **goodreads_book_id** and **best_book_id** generally point to the most popular edition of a given book, while goodreads  **work_id** refers to the book in the abstract sense.\n",
      "\n",
      "You can use the goodreads book and work IDs to create URLs as follows:\n",
      "\n",
      "https://www.goodreads.com/book/show/2767052   \n",
      "https://www.goodreads.com/work/editions/2792775\n",
      "\n",
      "  [1]: https://github.com/zygmuntz/misc/blob/master/goodbooks-10k/basic_stats.ipynb\n",
      "name 'detect' is not defined Language is not detected: adhamelkomy/news-classification-and-analysis-using-nlp\n",
      "name 'detect' is not defined Language is not detected: news-classification-and-analysis-using-nlp\n",
      "name 'detect' is not defined Language is not detected: adhamelkomy\n",
      "name 'detect' is not defined Language is not detected: News Classification and Analysis using NLP\n",
      "name 'detect' is not defined Language is not detected: **Data Description:**\n",
      "- Headline: The headline or title of the news article.\n",
      "- Content: The textual content of the news article.\n",
      "- Category: The category or topic of the news article, indicating the subject matter it covers.\n",
      "- Date (of scraping): The date when the news article was scraped and added to the dataset.\n",
      "**Background:**\n",
      "The dataset comprises scraped news articles from various topics, sourced from inshorts.com, a news\n",
      "aggregation platform. With the rapid growth of digital media and news consumption, analyzing and\n",
      "classifying news articles have become crucial for understanding public discourse, tracking emerging\n",
      "trends, and monitoring events across different domains. Leveraging natural language processing (NLP)\n",
      "techniques, this project aims to classify news articles into predefined categories and extract valuable\n",
      "insights from the textual content.\n",
      "**Objective:**\n",
      "The objective of this internship project is to perform news classification and analysis using natural\n",
      "language processing (NLP) techniques to categorize news articles into predefined topics and extract\n",
      "actionable insights. By automating the classification process and analyzing news content, the project aims\n",
      "to facilitate efficient information retrieval and trend identification in the rapidly evolving news landscape.\n",
      "**Key Components:**\n",
      "1. Data Collection and Preprocessing: Understand the dataset of news articles from inshorts.com,\n",
      "preprocess the textual content by removing noise, such as special characters and stopwords, and tokenize\n",
      "the text for further analysis.\n",
      "2. Topic Classification: Develop NLP classification models to categorize news articles into predefined topics\n",
      "or categories based on their headlines and content. Explore techniques such as text classification\n",
      "algorithms, including logistic regression, random forests, or deep learning architectures like convolutional\n",
      "neural networks (CNNs) or transformers (e.g., BERT).\n",
      "3. Model Training and Evaluation: Train the classification models on the labeled dataset of news articles\n",
      "and evaluate their performance using appropriate evaluation metrics, such as accuracy, precision, recall,\n",
      "and F1-score. Fine-tune the models to improve classification accuracy and robustness.\n",
      "4. Topic Analysis and Insights: Analyze the classified news articles to gain insights into the distribution of\n",
      "topics, emerging trends, and patterns in news coverage. Identify popular topics, recurring themes, and\n",
      "changes in public discourse over time.\n",
      "5. Visualization and Reporting: Create visually informative representations, such as topic distribution plots,\n",
      "word clouds, and trend graphs, to present the findings. Generate comprehensive reports summarizing the\n",
      "analysis, insights, and recommendations for further research or decision-making.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "- Trained NLP classification models capable of accurately categorizing news articles into predefined topics.\n",
      "- Insights into the distribution of topics and trends in news coverage across different categories.\n",
      "- Recommendations for improving news classification accuracy and enhancing understanding of public\n",
      "discourse.\n",
      "**Deliverables:**\n",
      "- Preprocessed dataset of news articles with labeled categories.\n",
      "- Trained classification models with evaluation results.\n",
      "- Report summarizing findings, insights, and recommendations for further analysis and application.\n",
      "**Conclusion:**\n",
      "This project aims to leverage natural language processing techniques to perform news classification and\n",
      "analysis, providing valuable insights into the distribution of topics and trends in news coverage. By\n",
      "automating the classification process and extracting actionable insights from news content, this project\n",
      "contributes to enhancing information retrieval and understanding in the dynamic news landscape.\n",
      "name 'detect' is not defined Language is not detected: news-classification-and-analysis-using-nlp\n",
      "name 'detect' is not defined Language is not detected: adhamelkomy\n",
      "name 'detect' is not defined Language is not detected: News Classification and Analysis using NLP\n",
      "name 'detect' is not defined Language is not detected: **Data Description:**\n",
      "- Headline: The headline or title of the news article.\n",
      "- Content: The textual content of the news article.\n",
      "- Category: The category or topic of the news article, indicating the subject matter it covers.\n",
      "- Date (of scraping): The date when the news article was scraped and added to the dataset.\n",
      "**Background:**\n",
      "The dataset comprises scraped news articles from various topics, sourced from inshorts.com, a news\n",
      "aggregation platform. With the rapid growth of digital media and news consumption, analyzing and\n",
      "classifying news articles have become crucial for understanding public discourse, tracking emerging\n",
      "trends, and monitoring events across different domains. Leveraging natural language processing (NLP)\n",
      "techniques, this project aims to classify news articles into predefined categories and extract valuable\n",
      "insights from the textual content.\n",
      "**Objective:**\n",
      "The objective of this internship project is to perform news classification and analysis using natural\n",
      "language processing (NLP) techniques to categorize news articles into predefined topics and extract\n",
      "actionable insights. By automating the classification process and analyzing news content, the project aims\n",
      "to facilitate efficient information retrieval and trend identification in the rapidly evolving news landscape.\n",
      "**Key Components:**\n",
      "1. Data Collection and Preprocessing: Understand the dataset of news articles from inshorts.com,\n",
      "preprocess the textual content by removing noise, such as special characters and stopwords, and tokenize\n",
      "the text for further analysis.\n",
      "2. Topic Classification: Develop NLP classification models to categorize news articles into predefined topics\n",
      "or categories based on their headlines and content. Explore techniques such as text classification\n",
      "algorithms, including logistic regression, random forests, or deep learning architectures like convolutional\n",
      "neural networks (CNNs) or transformers (e.g., BERT).\n",
      "3. Model Training and Evaluation: Train the classification models on the labeled dataset of news articles\n",
      "and evaluate their performance using appropriate evaluation metrics, such as accuracy, precision, recall,\n",
      "and F1-score. Fine-tune the models to improve classification accuracy and robustness.\n",
      "4. Topic Analysis and Insights: Analyze the classified news articles to gain insights into the distribution of\n",
      "topics, emerging trends, and patterns in news coverage. Identify popular topics, recurring themes, and\n",
      "changes in public discourse over time.\n",
      "5. Visualization and Reporting: Create visually informative representations, such as topic distribution plots,\n",
      "word clouds, and trend graphs, to present the findings. Generate comprehensive reports summarizing the\n",
      "analysis, insights, and recommendations for further research or decision-making.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "- Trained NLP classification models capable of accurately categorizing news articles into predefined topics.\n",
      "- Insights into the distribution of topics and trends in news coverage across different categories.\n",
      "- Recommendations for improving news classification accuracy and enhancing understanding of public\n",
      "discourse.\n",
      "**Deliverables:**\n",
      "- Preprocessed dataset of news articles with labeled categories.\n",
      "- Trained classification models with evaluation results.\n",
      "- Report summarizing findings, insights, and recommendations for further analysis and application.\n",
      "**Conclusion:**\n",
      "This project aims to leverage natural language processing techniques to perform news classification and\n",
      "analysis, providing valuable insights into the distribution of topics and trends in news coverage. By\n",
      "automating the classification process and extracting actionable insights from news content, this project\n",
      "contributes to enhancing information retrieval and understanding in the dynamic news landscape.\n",
      "name 'detect' is not defined Language is not detected: rumitpathare/indian-recipes\n",
      "name 'detect' is not defined Language is not detected: indian-recipes\n",
      "name 'detect' is not defined Language is not detected: rumitpathare\n",
      "name 'detect' is not defined Language is not detected: 7000+ International Cuisine\n",
      "name 'detect' is not defined Language is not detected: A Culinary Compilation: The Archana's Kitchen Recipe Dataset Crafted through Web\n",
      "name 'detect' is not defined Language is not detected: Dataset Description:\n",
      "\n",
      "File Information:\n",
      "- File Name: `indian_food_recipes.csv`\n",
      "- Format: CSV\n",
      "- Size: Approximately 18 MB\n",
      "- Columns: 11\n",
      "- Date Created: 24/08/2023\n",
      "\n",
      "Column Descriptions:\n",
      "\n",
      "| Column Name         | Description                                                                              |\n",
      "|---------------------|------------------------------------------------------------------------------------------|\n",
      "| Name                | The name of the recipe.                                                                 |\n",
      "| Description         | A brief description of the recipe, highlighting its unique flavors and ingredients.     |\n",
      "| Cuisine             | The regional or cultural cuisine to which the recipe belongs.                            |\n",
      "| Course              | The meal course for which the recipe is intended (e.g., Lunch, Dinner, Snack).           |\n",
      "| Diet                | Dietary preference or suitability of the recipe (e.g., Vegetarian, Non-Vegetarian).      |\n",
      "| Ingredients Name    | Names of ingredients used in the recipe, separated by commas.                             |\n",
      "| Ingredients Quantity| Quantity or measurements of ingredients, including specific details.                      |\n",
      "| Prep Time (in mins) | The time required for preparation in minutes.                                              |\n",
      "| Cook Time (in mins) | The time required for cooking in minutes.                                                   |\n",
      "| Instructions        | Detailed instructions for preparing the recipe.                                             |\n",
      "| Image URL           | URL of an image representing the dish.                                                       |\n",
      "\n",
      "\n",
      "          This dataset was meticulously collected and structured with the aim of creating a recommendation system for Indian food recipes. The motivation behind creating this dataset was to assist data scientists and culinary enthusiasts in developing innovative recommendation algorithms and exploring the diverse world of Indian cuisine. The dataset's clean and organized structure makes it an ideal resource for conducting research, analysis, and the development of culinary recommendation systems.\n",
      "\n",
      "Whether you are a data scientist looking to build recommendation models, a food blogger seeking culinary inspiration, or a researcher exploring Indian cuisine trends, this dataset provides a versatile and comprehensive collection of Indian recipes to cater to your needs. Enjoy exploring and analyzing the rich flavors of Indian cooking!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: indian-recipes\n",
      "name 'detect' is not defined Language is not detected: rumitpathare\n",
      "name 'detect' is not defined Language is not detected: 7000+ International Cuisine\n",
      "name 'detect' is not defined Language is not detected: A Culinary Compilation: The Archana's Kitchen Recipe Dataset Crafted through Web\n",
      "name 'detect' is not defined Language is not detected: Dataset Description:\n",
      "\n",
      "File Information:\n",
      "- File Name: `indian_food_recipes.csv`\n",
      "- Format: CSV\n",
      "- Size: Approximately 18 MB\n",
      "- Columns: 11\n",
      "- Date Created: 24/08/2023\n",
      "\n",
      "Column Descriptions:\n",
      "\n",
      "| Column Name         | Description                                                                              |\n",
      "|---------------------|------------------------------------------------------------------------------------------|\n",
      "| Name                | The name of the recipe.                                                                 |\n",
      "| Description         | A brief description of the recipe, highlighting its unique flavors and ingredients.     |\n",
      "| Cuisine             | The regional or cultural cuisine to which the recipe belongs.                            |\n",
      "| Course              | The meal course for which the recipe is intended (e.g., Lunch, Dinner, Snack).           |\n",
      "| Diet                | Dietary preference or suitability of the recipe (e.g., Vegetarian, Non-Vegetarian).      |\n",
      "| Ingredients Name    | Names of ingredients used in the recipe, separated by commas.                             |\n",
      "| Ingredients Quantity| Quantity or measurements of ingredients, including specific details.                      |\n",
      "| Prep Time (in mins) | The time required for preparation in minutes.                                              |\n",
      "| Cook Time (in mins) | The time required for cooking in minutes.                                                   |\n",
      "| Instructions        | Detailed instructions for preparing the recipe.                                             |\n",
      "| Image URL           | URL of an image representing the dish.                                                       |\n",
      "\n",
      "\n",
      "          This dataset was meticulously collected and structured with the aim of creating a recommendation system for Indian food recipes. The motivation behind creating this dataset was to assist data scientists and culinary enthusiasts in developing innovative recommendation algorithms and exploring the diverse world of Indian cuisine. The dataset's clean and organized structure makes it an ideal resource for conducting research, analysis, and the development of culinary recommendation systems.\n",
      "\n",
      "Whether you are a data scientist looking to build recommendation models, a food blogger seeking culinary inspiration, or a researcher exploring Indian cuisine trends, this dataset provides a versatile and comprehensive collection of Indian recipes to cater to your needs. Enjoy exploring and analyzing the rich flavors of Indian cooking!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nikhilmahajan29/crop-production-statistics-india\n",
      "name 'detect' is not defined Language is not detected: crop-production-statistics-india\n",
      "name 'detect' is not defined Language is not detected: nikhilmahajan29\n",
      "name 'detect' is not defined Language is not detected: Crop Production Statistics - India\n",
      "name 'detect' is not defined Language is not detected: Analyzing India's Crop Production: A Comprehensive State and District-wise Data\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "The dataset contains comprehensive data on crop production statistics for India, categorized by state and district. The dataset covers four major crop seasons, namely kharif, rabbi, summer, and autumn, from the year 1997 to 2023. The data provides information on the annual production and yield of crops grown in different parts of the country.\n",
      "\n",
      "The dataset will be useful for researchers, policymakers, and farmers who are interested in understanding crop production patterns in different regions of India. By analyzing the data, researchers can identify the factors that influence crop yields and production and can make informed decisions on how to improve agricultural productivity in the country. Policymakers can use the data to design and implement agricultural policies that promote sustainable farming practices and improve food security.\n",
      "\n",
      "Farmers can also benefit from the dataset by gaining insights into the best crops to grow in their region and making informed decisions on crop management practices. Additionally, the dataset can be used to train machine learning models to predict crop yields and production in different parts of the country, which can be valuable for agricultural businesses and organizations. Overall, the dataset provides a comprehensive overview of crop production statistics in India, which is essential for understanding the country's agricultural landscape and developing effective strategies for sustainable agriculture.\n",
      "\n",
      "# Sources\n",
      "This dataset contains comprehensive information on agricultural production statistics in India, sourced from the Indian government's Area Production Statistics (APS) database. The APS is maintained by the Ministry of Agriculture and Farmers Welfare and provides detailed data on crop production, yield, and area under cultivation across different states and districts in India.\n",
      "Overall, this dataset is an important resource for anyone interested in agriculture and its impact on the Indian economy and society.\n",
      "## Link :- ***https://aps.dac.gov.in/APY/Public_Report1.aspx***\n",
      "\n",
      "# Inspiration \n",
      "Agriculture is the backbone of the Indian economy, providing employment to millions of people and contributing significantly to the country's GDP. However, the sector is faced with several challenges, including climate change, low productivity, and food security issues. To address these challenges, there is a need for data-driven solutions that can inform policy and decision-making.\n",
      "\n",
      "Creating a dataset that captures agricultural production statistics in India can help in this regard. The dataset can provide valuable insights into crop yields, area under cultivation, and other metrics that can inform agricultural policies and practices. It can also be used to identify trends and patterns in agricultural production, helping farmers and policymakers make informed decisions about crop selection, irrigation, and other important factors that affect agricultural productivity.\n",
      "\n",
      "Additionally, the dataset can be used for machine learning and predictive modeling to generate insights and make accurate predictions about crop production in different parts of the country. Overall, the dataset has the potential to contribute significantly to the development of the agriculture sector in India and help address some of the challenges faced by the sector.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: crop-production-statistics-india\n",
      "name 'detect' is not defined Language is not detected: nikhilmahajan29\n",
      "name 'detect' is not defined Language is not detected: Crop Production Statistics - India\n",
      "name 'detect' is not defined Language is not detected: Analyzing India's Crop Production: A Comprehensive State and District-wise Data\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "The dataset contains comprehensive data on crop production statistics for India, categorized by state and district. The dataset covers four major crop seasons, namely kharif, rabbi, summer, and autumn, from the year 1997 to 2023. The data provides information on the annual production and yield of crops grown in different parts of the country.\n",
      "\n",
      "The dataset will be useful for researchers, policymakers, and farmers who are interested in understanding crop production patterns in different regions of India. By analyzing the data, researchers can identify the factors that influence crop yields and production and can make informed decisions on how to improve agricultural productivity in the country. Policymakers can use the data to design and implement agricultural policies that promote sustainable farming practices and improve food security.\n",
      "\n",
      "Farmers can also benefit from the dataset by gaining insights into the best crops to grow in their region and making informed decisions on crop management practices. Additionally, the dataset can be used to train machine learning models to predict crop yields and production in different parts of the country, which can be valuable for agricultural businesses and organizations. Overall, the dataset provides a comprehensive overview of crop production statistics in India, which is essential for understanding the country's agricultural landscape and developing effective strategies for sustainable agriculture.\n",
      "\n",
      "# Sources\n",
      "This dataset contains comprehensive information on agricultural production statistics in India, sourced from the Indian government's Area Production Statistics (APS) database. The APS is maintained by the Ministry of Agriculture and Farmers Welfare and provides detailed data on crop production, yield, and area under cultivation across different states and districts in India.\n",
      "Overall, this dataset is an important resource for anyone interested in agriculture and its impact on the Indian economy and society.\n",
      "## Link :- ***https://aps.dac.gov.in/APY/Public_Report1.aspx***\n",
      "\n",
      "# Inspiration \n",
      "Agriculture is the backbone of the Indian economy, providing employment to millions of people and contributing significantly to the country's GDP. However, the sector is faced with several challenges, including climate change, low productivity, and food security issues. To address these challenges, there is a need for data-driven solutions that can inform policy and decision-making.\n",
      "\n",
      "Creating a dataset that captures agricultural production statistics in India can help in this regard. The dataset can provide valuable insights into crop yields, area under cultivation, and other metrics that can inform agricultural policies and practices. It can also be used to identify trends and patterns in agricultural production, helping farmers and policymakers make informed decisions about crop selection, irrigation, and other important factors that affect agricultural productivity.\n",
      "\n",
      "Additionally, the dataset can be used for machine learning and predictive modeling to generate insights and make accurate predictions about crop production in different parts of the country. Overall, the dataset has the potential to contribute significantly to the development of the agriculture sector in India and help address some of the challenges faced by the sector.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: emreokcular/turkish-movies\n",
      "name 'detect' is not defined Language is not detected: turkish-movies\n",
      "name 'detect' is not defined Language is not detected: emreokcular\n",
      "name 'detect' is not defined Language is not detected: Turkish Movies\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 8000+ Turkish movies including rating, cast, genres, etc.\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 8000+ Turkish movies including reviews, rating, cast, genres, plot, etc.\n",
      "\n",
      "A great resource for many ML tasks, including rating prediction, recommendation systems, and NLP with reviews.\n",
      "\n",
      "Check [this dataset](https://www.kaggle.com/datasets/emreokcular/turkish-movies-with-posters) for the same dataset with full-size movie posters.\n",
      "\n",
      "---\n",
      "\n",
      "Türk filmleri veri seti. IMDb web sitesinde toplanmıştır. Yorumlar, reyting, oyuncular, tür, özet, vb. gibi bilgiler içerir.\n",
      "name 'detect' is not defined Language is not detected: turkish-movies\n",
      "name 'detect' is not defined Language is not detected: emreokcular\n",
      "name 'detect' is not defined Language is not detected: Turkish Movies\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 8000+ Turkish movies including rating, cast, genres, etc.\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 8000+ Turkish movies including reviews, rating, cast, genres, plot, etc.\n",
      "\n",
      "A great resource for many ML tasks, including rating prediction, recommendation systems, and NLP with reviews.\n",
      "\n",
      "Check [this dataset](https://www.kaggle.com/datasets/emreokcular/turkish-movies-with-posters) for the same dataset with full-size movie posters.\n",
      "\n",
      "---\n",
      "\n",
      "Türk filmleri veri seti. IMDb web sitesinde toplanmıştır. Yorumlar, reyting, oyuncular, tür, özet, vb. gibi bilgiler içerir.\n",
      "name 'detect' is not defined Language is not detected: emreokcular/turkish-tv-series\n",
      "name 'detect' is not defined Language is not detected: turkish-tv-series\n",
      "name 'detect' is not defined Language is not detected: emreokcular\n",
      "name 'detect' is not defined Language is not detected: Turkish TV Series\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 2000+ Turkish TV Series including rating, cast, genres, etc.\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 2000+ Turkish TV Series including reviews, rating, cast, genres, plot, etc. \n",
      "\n",
      "A great resource for many ML tasks, including rating prediction, recommendation systems, and NLP with reviews.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: turkish-tv-series\n",
      "name 'detect' is not defined Language is not detected: emreokcular\n",
      "name 'detect' is not defined Language is not detected: Turkish TV Series\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 2000+ Turkish TV Series including rating, cast, genres, etc.\n",
      "name 'detect' is not defined Language is not detected: Detailed dataset of 2000+ Turkish TV Series including reviews, rating, cast, genres, plot, etc. \n",
      "\n",
      "A great resource for many ML tasks, including rating prediction, recommendation systems, and NLP with reviews.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: chenyanglim/imdb-v2\n",
      "name 'detect' is not defined Language is not detected: imdb-v2\n",
      "name 'detect' is not defined Language is not detected: chenyanglim\n",
      "name 'detect' is not defined Language is not detected: IMDb Movies dataset from 2000 - 2020\n",
      "name 'detect' is not defined Language is not detected: Cleaned 5,487 Movies with casts, year, plot, votes etc.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "IMDb is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews.\n",
      "\n",
      "IMDb stores information related to more than 6 million titles (of which almost 500,000 are featured films) and it is owned by Amazon since 1998.\n",
      "\n",
      "### Content\n",
      "\n",
      "This is a cleaned and feature transformed dataset used for a content-based recommendation engine I did for a data science exercise.\n",
      "\n",
      "The dataset contains most movies listed in IMDB from 2000 - 2020 (until 31/07/2020) with a total of 5,487 with attributes such as year published, genre, duration, language, actors, directors, ratings and votes etc\n",
      "\n",
      "There are also additional features created such as \"actors_f2\" and \"desc35\" which are the first two actors and first 35 characters from the description respectively; Through multiple rounds of tests, I have discovered hundreds of rows of missing data (mainly in descriptions) and listing errors (missing key casts, wrong genres) have been fixed/added manually from IMDb (May 2021)\n",
      "\n",
      "For the purpose of relevance for my recommendation engine, all movies with less than 10,000 votes have been considered noise (aka less popular) and thus excluded from the dataset.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset is originally derived from Stefano Leone's IMDb.com scrape, which is also available on Kaggle:\n",
      "https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset\n",
      "\n",
      "I highly recommend having a look at Stefano's dataset as well.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This has been a very enjoyable dataset to work with in creating a content-based recommendation engine. I hope you will find this educational and that it will be an inspiration to how you can create your recommendation engine too.\n",
      "name 'detect' is not defined Language is not detected: imdb-v2\n",
      "name 'detect' is not defined Language is not detected: chenyanglim\n",
      "name 'detect' is not defined Language is not detected: IMDb Movies dataset from 2000 - 2020\n",
      "name 'detect' is not defined Language is not detected: Cleaned 5,487 Movies with casts, year, plot, votes etc.\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "IMDb is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews.\n",
      "\n",
      "IMDb stores information related to more than 6 million titles (of which almost 500,000 are featured films) and it is owned by Amazon since 1998.\n",
      "\n",
      "### Content\n",
      "\n",
      "This is a cleaned and feature transformed dataset used for a content-based recommendation engine I did for a data science exercise.\n",
      "\n",
      "The dataset contains most movies listed in IMDB from 2000 - 2020 (until 31/07/2020) with a total of 5,487 with attributes such as year published, genre, duration, language, actors, directors, ratings and votes etc\n",
      "\n",
      "There are also additional features created such as \"actors_f2\" and \"desc35\" which are the first two actors and first 35 characters from the description respectively; Through multiple rounds of tests, I have discovered hundreds of rows of missing data (mainly in descriptions) and listing errors (missing key casts, wrong genres) have been fixed/added manually from IMDb (May 2021)\n",
      "\n",
      "For the purpose of relevance for my recommendation engine, all movies with less than 10,000 votes have been considered noise (aka less popular) and thus excluded from the dataset.\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The dataset is originally derived from Stefano Leone's IMDb.com scrape, which is also available on Kaggle:\n",
      "https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset\n",
      "\n",
      "I highly recommend having a look at Stefano's dataset as well.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This has been a very enjoyable dataset to work with in creating a content-based recommendation engine. I hope you will find this educational and that it will be an inspiration to how you can create your recommendation engine too.\n",
      "name 'detect' is not defined Language is not detected: sirapatsam/airbnb-new-york-4dec2021\n",
      "name 'detect' is not defined Language is not detected: airbnb-new-york-4dec2021\n",
      "name 'detect' is not defined Language is not detected: sirapatsam\n",
      "name 'detect' is not defined Language is not detected: Airbnb New York @ 4.DEC.2021\n",
      "name 'detect' is not defined Language is not detected: From Inside Airbnb (Practice your price prediction model here)\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.\n",
      "\n",
      "Content\n",
      "This data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions.\n",
      "\n",
      "Acknowledgments\n",
      "This public dataset is part of Airbnb, and the original source can be found on this website.\n",
      "\n",
      "Inspiration\n",
      "What can we learn about different hosts and areas?\n",
      "What can we learn from predictions? (ex: locations, prices, reviews, etc)\n",
      "Which hosts are the busiest and why?\n",
      "Is there any noticeable difference of traffic among different areas and what could be the reason for it?\n",
      "name 'detect' is not defined Language is not detected: airbnb-new-york-4dec2021\n",
      "name 'detect' is not defined Language is not detected: sirapatsam\n",
      "name 'detect' is not defined Language is not detected: Airbnb New York @ 4.DEC.2021\n",
      "name 'detect' is not defined Language is not detected: From Inside Airbnb (Practice your price prediction model here)\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.\n",
      "\n",
      "Content\n",
      "This data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions.\n",
      "\n",
      "Acknowledgments\n",
      "This public dataset is part of Airbnb, and the original source can be found on this website.\n",
      "\n",
      "Inspiration\n",
      "What can we learn about different hosts and areas?\n",
      "What can we learn from predictions? (ex: locations, prices, reviews, etc)\n",
      "Which hosts are the busiest and why?\n",
      "Is there any noticeable difference of traffic among different areas and what could be the reason for it?\n",
      "name 'detect' is not defined Language is not detected: olegoleshchuk/productreviews\n",
      "name 'detect' is not defined Language is not detected: productreviews\n",
      "name 'detect' is not defined Language is not detected: olegoleshchuk\n",
      "name 'detect' is not defined Language is not detected: Amazon Product Reviews\n",
      "name 'detect' is not defined Language is not detected: 21000 product reviews, 4200 records per score (1-5)\n",
      "name 'detect' is not defined Language is not detected: This dataset is a collection of 21000 Amazon product reviews, meticulously divided into 5 different rating scales (1-5). The data is collected from various product categories and provides a rich source for text analysis, natural language processing, sentiment analysis, and machine learning algorithm testing.\n",
      "\n",
      "The dataset is structured with five separate folders, each representing a different rating scale. Each folder consists of 4200 text files containing an individual review. Hence, you will find 4200 reviews each for scores 1 through 5.\n",
      "\n",
      "**Contents**\n",
      "Each text file contains a single Amazon product review, ranging from a rating of 1 to 5. The files are distributed across five folders named as follows: \"1\", \"2\", \"3\", \"4\", and \"5\". Each folder contains 4200 reviews, which are anonymous, in English, and vary in length.\n",
      "\n",
      "**Format**\n",
      "Folder \"1\": 4200 text files with reviews scoring 1\n",
      "Folder \"2\": 4200 text files with reviews scoring 2\n",
      "Folder \"3\": 4200 text files with reviews scoring 3\n",
      "Folder \"4\": 4200 text files with reviews scoring 4\n",
      "Folder \"5\": 4200 text files with reviews scoring 5\n",
      "The filename of each text file is a number (1-4200).txt.\n",
      "\n",
      "**Usage**\n",
      "This dataset can be used to train sentiment analysis models, for product review classification, to train recommendation systems, or for natural language processing tasks. It also offers a great resource for conducting exploratory data analysis to understand the key features of a good or bad review, as well as to investigate possible bias in reviews and ratings.\n",
      "\n",
      "Please note that this data should be used for educational and research purposes only. The reviews contained within the dataset remain the intellectual property of the original authors.\n",
      "\n",
      "Acknowledgments\n",
      "Amazon is the original source of these reviews. The reviews are provided as-is and are representative of the authors' opinions and not those of the dataset creator or Kaggle.\n",
      "name 'detect' is not defined Language is not detected: productreviews\n",
      "name 'detect' is not defined Language is not detected: olegoleshchuk\n",
      "name 'detect' is not defined Language is not detected: Amazon Product Reviews\n",
      "name 'detect' is not defined Language is not detected: 21000 product reviews, 4200 records per score (1-5)\n",
      "name 'detect' is not defined Language is not detected: This dataset is a collection of 21000 Amazon product reviews, meticulously divided into 5 different rating scales (1-5). The data is collected from various product categories and provides a rich source for text analysis, natural language processing, sentiment analysis, and machine learning algorithm testing.\n",
      "\n",
      "The dataset is structured with five separate folders, each representing a different rating scale. Each folder consists of 4200 text files containing an individual review. Hence, you will find 4200 reviews each for scores 1 through 5.\n",
      "\n",
      "**Contents**\n",
      "Each text file contains a single Amazon product review, ranging from a rating of 1 to 5. The files are distributed across five folders named as follows: \"1\", \"2\", \"3\", \"4\", and \"5\". Each folder contains 4200 reviews, which are anonymous, in English, and vary in length.\n",
      "\n",
      "**Format**\n",
      "Folder \"1\": 4200 text files with reviews scoring 1\n",
      "Folder \"2\": 4200 text files with reviews scoring 2\n",
      "Folder \"3\": 4200 text files with reviews scoring 3\n",
      "Folder \"4\": 4200 text files with reviews scoring 4\n",
      "Folder \"5\": 4200 text files with reviews scoring 5\n",
      "The filename of each text file is a number (1-4200).txt.\n",
      "\n",
      "**Usage**\n",
      "This dataset can be used to train sentiment analysis models, for product review classification, to train recommendation systems, or for natural language processing tasks. It also offers a great resource for conducting exploratory data analysis to understand the key features of a good or bad review, as well as to investigate possible bias in reviews and ratings.\n",
      "\n",
      "Please note that this data should be used for educational and research purposes only. The reviews contained within the dataset remain the intellectual property of the original authors.\n",
      "\n",
      "Acknowledgments\n",
      "Amazon is the original source of these reviews. The reviews are provided as-is and are representative of the authors' opinions and not those of the dataset creator or Kaggle.\n",
      "name 'detect' is not defined Language is not detected: sijovm/used-cars-data-from-ebay-kleinanzeigen\n",
      "name 'detect' is not defined Language is not detected: used-cars-data-from-ebay-kleinanzeigen\n",
      "name 'detect' is not defined Language is not detected: sijovm\n",
      "name 'detect' is not defined Language is not detected: Used Cars Data from eBay kleinanzeigen\n",
      "name 'detect' is not defined Language is not detected: Can you predict how long a car will be online before it is sold? \n",
      "name 'detect' is not defined Language is not detected: The content of the data is in german, so one has to translate it first if one can not speak german. Those fields are included:\n",
      "\n",
      "• dateCrawled : when this ad was first crawled, all field-values are taken from this date\n",
      "•name : \"name\" of the car\n",
      "•seller : private or dealer\n",
      "•offerType\n",
      "•price : the price on the ad to sell the car\n",
      "•abtest\n",
      "•vehicleType\n",
      "•yearOfRegistration : at which year the car was first registered\n",
      "•gearbox\n",
      "•powerPS : power of the car in PS\n",
      "•model\n",
      "•kilometer : how many kilometers the car has driven\n",
      "•monthOfRegistration : at which month the car was first registered\n",
      "•fuelType\n",
      "•brand\n",
      "•notRepairedDamage : if the car has a damage which is not repaired yet\n",
      "•dateCreated : the date for which the ad at ebay was created\n",
      "•nrOfPictures : number of pictures in the ad\n",
      "•postalCode\n",
      "•lastSeenOnline : when the crawler saw this ad last online\n",
      "\n",
      "The fields lastSeen and dateCrawled could be used to estimate how long a car will be at least online before it is sold.\n",
      "name 'detect' is not defined Language is not detected: used-cars-data-from-ebay-kleinanzeigen\n",
      "name 'detect' is not defined Language is not detected: sijovm\n",
      "name 'detect' is not defined Language is not detected: Used Cars Data from eBay kleinanzeigen\n",
      "name 'detect' is not defined Language is not detected: Can you predict how long a car will be online before it is sold? \n",
      "name 'detect' is not defined Language is not detected: The content of the data is in german, so one has to translate it first if one can not speak german. Those fields are included:\n",
      "\n",
      "• dateCrawled : when this ad was first crawled, all field-values are taken from this date\n",
      "•name : \"name\" of the car\n",
      "•seller : private or dealer\n",
      "•offerType\n",
      "•price : the price on the ad to sell the car\n",
      "•abtest\n",
      "•vehicleType\n",
      "•yearOfRegistration : at which year the car was first registered\n",
      "•gearbox\n",
      "•powerPS : power of the car in PS\n",
      "•model\n",
      "•kilometer : how many kilometers the car has driven\n",
      "•monthOfRegistration : at which month the car was first registered\n",
      "•fuelType\n",
      "•brand\n",
      "•notRepairedDamage : if the car has a damage which is not repaired yet\n",
      "•dateCreated : the date for which the ad at ebay was created\n",
      "•nrOfPictures : number of pictures in the ad\n",
      "•postalCode\n",
      "•lastSeenOnline : when the crawler saw this ad last online\n",
      "\n",
      "The fields lastSeen and dateCrawled could be used to estimate how long a car will be at least online before it is sold.\n",
      "name 'detect' is not defined Language is not detected: vipulgohel/diabetic-data\n",
      "name 'detect' is not defined Language is not detected: diabetic-data\n",
      "name 'detect' is not defined Language is not detected: vipulgohel\n",
      "name 'detect' is not defined Language is not detected: Diabetic data\n",
      "name 'detect' is not defined Language is not detected: Identify High-Risk Diabetic Patients\n",
      "name 'detect' is not defined Language is not detected: Identify High-Risk Diabetic Patients\n",
      "name 'detect' is not defined Language is not detected: diabetic-data\n",
      "name 'detect' is not defined Language is not detected: vipulgohel\n",
      "name 'detect' is not defined Language is not detected: Diabetic data\n",
      "name 'detect' is not defined Language is not detected: Identify High-Risk Diabetic Patients\n",
      "name 'detect' is not defined Language is not detected: Identify High-Risk Diabetic Patients\n",
      "name 'detect' is not defined Language is not detected: gourab8889/farm-futro\n",
      "name 'detect' is not defined Language is not detected: farm-futro\n",
      "name 'detect' is not defined Language is not detected: gourab8889\n",
      "name 'detect' is not defined Language is not detected: Farm Futro\n",
      "name 'detect' is not defined Language is not detected: A Machine Learning-Based Crop Recommendation System for Precision  Agriculture .\n",
      "name 'detect' is not defined Language is not detected: FARM FUTURO is a cutting-edge Machine Learning (ML) project designed to address the \n",
      "challenges faced by farmers in making informed crop decisions. This innovative system utilizes \n",
      "advanced ML algorithms to analyze user-provided inputs, such as geographical location (states), \n",
      "crop preferences (cereals, pulses, fruits, cash crops), and environmental factors (rainfall, \n",
      "temperature,humidity, soil pH). FARM FUTURO aims to empower farmers with precise and \n",
      "timely recommendations, guiding them towards optimal crop selection based on their specific \n",
      "conditions and seasonal variations. This project represents a pivotal advancement in precision \n",
      "agriculture, fostering sustainable farming practices and maximizing agricultural productivity.\n",
      "The heart of FARM FUTURO lies in its sophisticated ML algorithms that analyze historical \n",
      "agricultural data, climatic patterns, and soil conditions. By employing advanced data analytics \n",
      "techniques, the system identifies patterns and correlations that traditional farming methods may \n",
      "overlook. This enables FARM FUTURO to offer precise and timely crop recommendations that \n",
      "are not only suited to the farmer's preferences but also optimized for the specific environmental \n",
      "conditions of the region.\n",
      "Key features of FARM FUTURO include its ability to predict the most suitable crops for \n",
      "cultivation based on real-time and historical data, taking into account the variations in \n",
      "temperature, rainfall, and soil pH throughout the year. The system also considers seasonal \n",
      "factors, ensuring that farmers receive recommendations tailored to the specific planting and \n",
      "harvesting windows for each crop.\n",
      "FARM FUTURO represents a significant step forward in the realm of precision agriculture, \n",
      "harnessing the power of machine learning to empower farmers with actionable insights for \n",
      "informed decision-making. As agriculture faces increasing challenges posed by climate change \n",
      "and resource constraints, FARM FUTURO stands as a beacon of innovation, offering a scalable \n",
      "solution to enhance agricultural productivity, profitability, and sustainability.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: farm-futro\n",
      "name 'detect' is not defined Language is not detected: gourab8889\n",
      "name 'detect' is not defined Language is not detected: Farm Futro\n",
      "name 'detect' is not defined Language is not detected: A Machine Learning-Based Crop Recommendation System for Precision  Agriculture .\n",
      "name 'detect' is not defined Language is not detected: FARM FUTURO is a cutting-edge Machine Learning (ML) project designed to address the \n",
      "challenges faced by farmers in making informed crop decisions. This innovative system utilizes \n",
      "advanced ML algorithms to analyze user-provided inputs, such as geographical location (states), \n",
      "crop preferences (cereals, pulses, fruits, cash crops), and environmental factors (rainfall, \n",
      "temperature,humidity, soil pH). FARM FUTURO aims to empower farmers with precise and \n",
      "timely recommendations, guiding them towards optimal crop selection based on their specific \n",
      "conditions and seasonal variations. This project represents a pivotal advancement in precision \n",
      "agriculture, fostering sustainable farming practices and maximizing agricultural productivity.\n",
      "The heart of FARM FUTURO lies in its sophisticated ML algorithms that analyze historical \n",
      "agricultural data, climatic patterns, and soil conditions. By employing advanced data analytics \n",
      "techniques, the system identifies patterns and correlations that traditional farming methods may \n",
      "overlook. This enables FARM FUTURO to offer precise and timely crop recommendations that \n",
      "are not only suited to the farmer's preferences but also optimized for the specific environmental \n",
      "conditions of the region.\n",
      "Key features of FARM FUTURO include its ability to predict the most suitable crops for \n",
      "cultivation based on real-time and historical data, taking into account the variations in \n",
      "temperature, rainfall, and soil pH throughout the year. The system also considers seasonal \n",
      "factors, ensuring that farmers receive recommendations tailored to the specific planting and \n",
      "harvesting windows for each crop.\n",
      "FARM FUTURO represents a significant step forward in the realm of precision agriculture, \n",
      "harnessing the power of machine learning to empower farmers with actionable insights for \n",
      "informed decision-making. As agriculture faces increasing challenges posed by climate change \n",
      "and resource constraints, FARM FUTURO stands as a beacon of innovation, offering a scalable \n",
      "solution to enhance agricultural productivity, profitability, and sustainability.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: asaniczka/52000-animation-movie-details-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: 52000-animation-movie-details-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: 52,000 Animation Movie Details (2024)\n",
      "name 'detect' is not defined Language is not detected: Scraped dataset from TMDB API. Detailed information about animation movies.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains detailed information about 52,000 animation movies scraped from the TMDB API. This dataset is a subset of [Full TMDB Movies Dataset 2024](https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies)\n",
      "\n",
      "&gt; If you find this dataset helpful, feel free to give it an upvote! 😊💖\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the popularity of animation movies across different genres.\n",
      "2. Train a machine learning model to predict movie ratings based on various features.\n",
      "3. Extract key insights on budget and revenue trends in the animation film industry.\n",
      "4. Explore the relationship between runtime and movie popularity.\n",
      "5. Identify the most common languages and production companies in animation movies.\n",
      "6. Create a recommendation system for animation movie enthusiasts.\n",
      "7. Analyze the release date patterns and their impact on revenue.\n",
      "\n",
      "---\n",
      "\n",
      "Photo by <a href=\"https://unsplash.com/@javaistan?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Afif Ramdhasuma</a> on <a href=\"https://unsplash.com/photos/red-haired-girl-in-green-dress-figurine-on-green-grass-during-daytime-eTMtdh85TOk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
      "  \n",
      "name 'detect' is not defined Language is not detected: 52000-animation-movie-details-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: 52,000 Animation Movie Details (2024)\n",
      "name 'detect' is not defined Language is not detected: Scraped dataset from TMDB API. Detailed information about animation movies.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains detailed information about 52,000 animation movies scraped from the TMDB API. This dataset is a subset of [Full TMDB Movies Dataset 2024](https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies)\n",
      "\n",
      "&gt; If you find this dataset helpful, feel free to give it an upvote! 😊💖\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the popularity of animation movies across different genres.\n",
      "2. Train a machine learning model to predict movie ratings based on various features.\n",
      "3. Extract key insights on budget and revenue trends in the animation film industry.\n",
      "4. Explore the relationship between runtime and movie popularity.\n",
      "5. Identify the most common languages and production companies in animation movies.\n",
      "6. Create a recommendation system for animation movie enthusiasts.\n",
      "7. Analyze the release date patterns and their impact on revenue.\n",
      "\n",
      "---\n",
      "\n",
      "Photo by <a href=\"https://unsplash.com/@javaistan?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Afif Ramdhasuma</a> on <a href=\"https://unsplash.com/photos/red-haired-girl-in-green-dress-figurine-on-green-grass-during-daytime-eTMtdh85TOk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
      "  \n",
      "name 'detect' is not defined Language is not detected: asaniczka/upwork-job-postings-dataset-2024-50k-records\n",
      "name 'detect' is not defined Language is not detected: upwork-job-postings-dataset-2024-50k-records\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: Upwork Job Postings Dataset 2024 (50K Records)\n",
      "name 'detect' is not defined Language is not detected: Real-time data collected over 2 weeks\n",
      "name 'detect' is not defined Language is not detected: Upwork is a popular online job platform where freelancers and businesses connect. This dataset contains 50,000 job postings from Upwork, spanning various categories and countries. \n",
      "\n",
      "With this dataset, you can analyze job trends, pricing strategies, and geographical preferences of Upwork users.\n",
      "\n",
      "&gt;If you find this dataset helpful, please leave an upvote 😊🚀\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the most in-demand skills across different job categories.\n",
      "2. Predict the budget range based on job titles and descriptions.\n",
      "3. Identify countries with the highest number of job postings.\n",
      "4. Develop a recommendation system for freelancers based on their skillset.\n",
      "5. Compare hourly rates for different types of jobs.\n",
      "6. Predict the likelihood of a job being hourly or fixed-price\n",
      "7. Explore how the number of job postings fluctuates over time.\n",
      "8. Use Natural Language Processing (NLP) techniques to categorize job descriptions.\n",
      "\n",
      "## Note:\n",
      "\n",
      "I collected this data to see what technologies are in demand for my profession. But since this has pretty much all job postings for the past 2 weeks, I decided to share the dataset on Kaggle.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F16993589%2F7af9709c45d05b000225694aaa327101%2FScreenshot%20from%202024-02-24%2015-50-26.png?generation=1708770103635774&alt=media)\n",
      "\n",
      "---\n",
      "\n",
      "Photo by <a href=\"https://unsplash.com/@perloov?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Per Lööv</a> on <a href=\"https://unsplash.com/photos/man-sitting-near-table-using-computer-4wOkqiXNP7M?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
      "  \n",
      "name 'detect' is not defined Language is not detected: upwork-job-postings-dataset-2024-50k-records\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: Upwork Job Postings Dataset 2024 (50K Records)\n",
      "name 'detect' is not defined Language is not detected: Real-time data collected over 2 weeks\n",
      "name 'detect' is not defined Language is not detected: Upwork is a popular online job platform where freelancers and businesses connect. This dataset contains 50,000 job postings from Upwork, spanning various categories and countries. \n",
      "\n",
      "With this dataset, you can analyze job trends, pricing strategies, and geographical preferences of Upwork users.\n",
      "\n",
      "&gt;If you find this dataset helpful, please leave an upvote 😊🚀\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the most in-demand skills across different job categories.\n",
      "2. Predict the budget range based on job titles and descriptions.\n",
      "3. Identify countries with the highest number of job postings.\n",
      "4. Develop a recommendation system for freelancers based on their skillset.\n",
      "5. Compare hourly rates for different types of jobs.\n",
      "6. Predict the likelihood of a job being hourly or fixed-price\n",
      "7. Explore how the number of job postings fluctuates over time.\n",
      "8. Use Natural Language Processing (NLP) techniques to categorize job descriptions.\n",
      "\n",
      "## Note:\n",
      "\n",
      "I collected this data to see what technologies are in demand for my profession. But since this has pretty much all job postings for the past 2 weeks, I decided to share the dataset on Kaggle.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F16993589%2F7af9709c45d05b000225694aaa327101%2FScreenshot%20from%202024-02-24%2015-50-26.png?generation=1708770103635774&alt=media)\n",
      "\n",
      "---\n",
      "\n",
      "Photo by <a href=\"https://unsplash.com/@perloov?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Per Lööv</a> on <a href=\"https://unsplash.com/photos/man-sitting-near-table-using-computer-4wOkqiXNP7M?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
      "  \n",
      "name 'detect' is not defined Language is not detected: asaniczka/helpful-life-tips-from-reddit-dataset-13k-tips\n",
      "name 'detect' is not defined Language is not detected: helpful-life-tips-from-reddit-dataset-13k-tips\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: Helpful Life Tips from Reddit Dataset (13K Tips)\n",
      "name 'detect' is not defined Language is not detected: Posts over 1000 upvotes from r/lifeProTips & r/YouShouldKnow. 2005 - 2022\n",
      "name 'detect' is not defined Language is not detected: &gt; Discover truly valuable life tips shared by real humans.\n",
      "\n",
      "## About the Dataset:\n",
      "\n",
      "Reddit is a treasure trove of genuine life experiences from millions of people. Subreddits like r/lifeProTips and r/YouShouldKnow are well-known for containing some of the best and most practical tips that anyone can apply to their life.\n",
      "\n",
      "This dataset is a cleaned version of the [split reddit dump by u/Watchful1](https://www.reddit.com/r/pushshift/comments/11ef9if/separate_dump_files_for_the_top_20k_subreddits/).\n",
      "\n",
      "&gt; Each row in the dataset contains a helpful life tip.\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Develop a web app that presents users with an interesting tip each day.\n",
      "2. Explore the data to determine the most popular types of tips.\n",
      "3. Build a recommendation system that suggests relevant tips based on specific life situations or topics.\n",
      "4. Develop AI-powered models that generate useful life tips using the examples in the dataset.\n",
      "5. Analyze popular life topics and their corresponding tips to uncover patterns and common themes.\n",
      "\n",
      "---\n",
      "\n",
      "If you find this dataset valuable, don't forget to hit the upvote button! 😊💝\n",
      "\n",
      "---\n",
      "\n",
      "### Checkout my other datasets\n",
      "\n",
      "[Gender Wage Gap in the USA](https://www.kaggle.com/datasets/asaniczka/gender-wage-gap-in-the-usa-1973-2022)\n",
      "\n",
      "[USA Hispanic-White Wage Gap Dataset ](https://www.kaggle.com/datasets/asaniczka/usa-hispanic-white-wage-gap-dataset-1973-2022)\n",
      "\n",
      "[USA Unemployment Rates by Demographics & Race](https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023)\n",
      "\n",
      "[USA Wage Comparison for College vs. High School](https://www.kaggle.com/datasets/asaniczka/usa-wage-comparison-for-college-vs-high-school)\n",
      "\n",
      "[Employment-to-Population Ratio for USA ](https://www.kaggle.com/datasets/asaniczka/employment-to-population-ratio-for-usa-1979-2023)\n",
      "name 'detect' is not defined Language is not detected: helpful-life-tips-from-reddit-dataset-13k-tips\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: Helpful Life Tips from Reddit Dataset (13K Tips)\n",
      "name 'detect' is not defined Language is not detected: Posts over 1000 upvotes from r/lifeProTips & r/YouShouldKnow. 2005 - 2022\n",
      "name 'detect' is not defined Language is not detected: &gt; Discover truly valuable life tips shared by real humans.\n",
      "\n",
      "## About the Dataset:\n",
      "\n",
      "Reddit is a treasure trove of genuine life experiences from millions of people. Subreddits like r/lifeProTips and r/YouShouldKnow are well-known for containing some of the best and most practical tips that anyone can apply to their life.\n",
      "\n",
      "This dataset is a cleaned version of the [split reddit dump by u/Watchful1](https://www.reddit.com/r/pushshift/comments/11ef9if/separate_dump_files_for_the_top_20k_subreddits/).\n",
      "\n",
      "&gt; Each row in the dataset contains a helpful life tip.\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Develop a web app that presents users with an interesting tip each day.\n",
      "2. Explore the data to determine the most popular types of tips.\n",
      "3. Build a recommendation system that suggests relevant tips based on specific life situations or topics.\n",
      "4. Develop AI-powered models that generate useful life tips using the examples in the dataset.\n",
      "5. Analyze popular life topics and their corresponding tips to uncover patterns and common themes.\n",
      "\n",
      "---\n",
      "\n",
      "If you find this dataset valuable, don't forget to hit the upvote button! 😊💝\n",
      "\n",
      "---\n",
      "\n",
      "### Checkout my other datasets\n",
      "\n",
      "[Gender Wage Gap in the USA](https://www.kaggle.com/datasets/asaniczka/gender-wage-gap-in-the-usa-1973-2022)\n",
      "\n",
      "[USA Hispanic-White Wage Gap Dataset ](https://www.kaggle.com/datasets/asaniczka/usa-hispanic-white-wage-gap-dataset-1973-2022)\n",
      "\n",
      "[USA Unemployment Rates by Demographics & Race](https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023)\n",
      "\n",
      "[USA Wage Comparison for College vs. High School](https://www.kaggle.com/datasets/asaniczka/usa-wage-comparison-for-college-vs-high-school)\n",
      "\n",
      "[Employment-to-Population Ratio for USA ](https://www.kaggle.com/datasets/asaniczka/employment-to-population-ratio-for-usa-1979-2023)\n",
      "name 'detect' is not defined Language is not detected: asaniczka/all-jobs-on-upwork-200k-plus\n",
      "name 'detect' is not defined Language is not detected: all-jobs-on-upwork-200k-plus\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: All Upwork Job Postings - Monthly Tracker\n",
      "name 'detect' is not defined Language is not detected: All job postings on Upwork collected minute by minute (200k plus)\n",
      "name 'detect' is not defined Language is not detected: This dataset tracks job postings on Upwork, a popular freelancing platform, providing valuable insights into current job trends, salary ranges, and demand for various skills.\n",
      "\n",
      "It contains **all job posting** on Upwork\n",
      "\n",
      "&gt; If you find this dataset helpful, don't forget to leave a upvote ❤️\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the correlation between job title keywords and offered salaries.\n",
      "2. Identify emerging job categories based on posting frequency.\n",
      "3. Predict high-demand job roles by analyzing job posting patterns over time.\n",
      "4. Compare average hourly rates across different countries.\n",
      "5. Create a job recommendation engine based on current job postings.\n",
      "6. Track changes in job market dynamics over months.\n",
      "7. Investigate trends in the remote work landscape.\n",
      "8. Use as a basis for predicting future job market trends.\n",
      "\n",
      "## Note:\n",
      "\n",
      "I collect data from upwork pretty much every minute. I only plan on updating this dataset once a month.\n",
      "\n",
      "If there's demand, I can increase the update frequency to daily.\n",
      "name 'detect' is not defined Language is not detected: all-jobs-on-upwork-200k-plus\n",
      "name 'detect' is not defined Language is not detected: asaniczka\n",
      "name 'detect' is not defined Language is not detected: All Upwork Job Postings - Monthly Tracker\n",
      "name 'detect' is not defined Language is not detected: All job postings on Upwork collected minute by minute (200k plus)\n",
      "name 'detect' is not defined Language is not detected: This dataset tracks job postings on Upwork, a popular freelancing platform, providing valuable insights into current job trends, salary ranges, and demand for various skills.\n",
      "\n",
      "It contains **all job posting** on Upwork\n",
      "\n",
      "&gt; If you find this dataset helpful, don't forget to leave a upvote ❤️\n",
      "\n",
      "## Interesting Task Ideas:\n",
      "\n",
      "1. Analyze the correlation between job title keywords and offered salaries.\n",
      "2. Identify emerging job categories based on posting frequency.\n",
      "3. Predict high-demand job roles by analyzing job posting patterns over time.\n",
      "4. Compare average hourly rates across different countries.\n",
      "5. Create a job recommendation engine based on current job postings.\n",
      "6. Track changes in job market dynamics over months.\n",
      "7. Investigate trends in the remote work landscape.\n",
      "8. Use as a basis for predicting future job market trends.\n",
      "\n",
      "## Note:\n",
      "\n",
      "I collect data from upwork pretty much every minute. I only plan on updating this dataset once a month.\n",
      "\n",
      "If there's demand, I can increase the update frequency to daily.\n",
      "name 'detect' is not defined Language is not detected: turconiandrea/water-footprint-recommender-system-data\n",
      "name 'detect' is not defined Language is not detected: water-footprint-recommender-system-data\n",
      "name 'detect' is not defined Language is not detected: turconiandrea\n",
      "name 'detect' is not defined Language is not detected: Water Footprint Recommender System Data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset contains all the data and embeddings used for the production of this project: A Food Recommendation System for reducing the Water Footprint (https://github.com/TurconiAndrea/water-footprint-reducer-rs). \n",
      "It contains data from two different realities: Food.com, a well-known American recipe site, and Planeat, an Italian site that allows you to plan recipes to save food waste. The dataset is divided into two parts: embeddings, which can be used directly to execute the work and receive suggestions, and raw data, which must first be processed into embeddings.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains: \n",
      "#### Embedding\n",
      "* food.com\n",
      "    * ingredients.pkl\n",
      "    * model.pkl\n",
      "    * recipes.pkl\n",
      "    * reviews.pkl\n",
      "    * users_scores.pkl\n",
      "* planeat\n",
      "    * ingredients.pkl\n",
      "    * model.pkl\n",
      "    * recipes.pkl\n",
      "    * reviews.pkl\n",
      "    * users_scores.pkl\n",
      "\n",
      "#### Input data\n",
      "* food.com\n",
      "    * orders.csv\n",
      "    * recipes.csv\n",
      "* planeat\n",
      "    * orders.csv\n",
      "    * recipes.csv\n",
      "name 'detect' is not defined Language is not detected: water-footprint-recommender-system-data\n",
      "name 'detect' is not defined Language is not detected: turconiandrea\n",
      "name 'detect' is not defined Language is not detected: Water Footprint Recommender System Data\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This dataset contains all the data and embeddings used for the production of this project: A Food Recommendation System for reducing the Water Footprint (https://github.com/TurconiAndrea/water-footprint-reducer-rs). \n",
      "It contains data from two different realities: Food.com, a well-known American recipe site, and Planeat, an Italian site that allows you to plan recipes to save food waste. The dataset is divided into two parts: embeddings, which can be used directly to execute the work and receive suggestions, and raw data, which must first be processed into embeddings.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset contains: \n",
      "#### Embedding\n",
      "* food.com\n",
      "    * ingredients.pkl\n",
      "    * model.pkl\n",
      "    * recipes.pkl\n",
      "    * reviews.pkl\n",
      "    * users_scores.pkl\n",
      "* planeat\n",
      "    * ingredients.pkl\n",
      "    * model.pkl\n",
      "    * recipes.pkl\n",
      "    * reviews.pkl\n",
      "    * users_scores.pkl\n",
      "\n",
      "#### Input data\n",
      "* food.com\n",
      "    * orders.csv\n",
      "    * recipes.csv\n",
      "* planeat\n",
      "    * orders.csv\n",
      "    * recipes.csv\n",
      "name 'detect' is not defined Language is not detected: rohitdass/audible-dataset\n",
      "name 'detect' is not defined Language is not detected: audible-dataset\n",
      "name 'detect' is not defined Language is not detected: rohitdass\n",
      "name 'detect' is not defined Language is not detected: Audible Dataset\n",
      "name 'detect' is not defined Language is not detected: Audible Updated Dataset including Reviews\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Full Dataset for audible dataset recommendation engine. Total books 2200+(Including Duplicates)\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Audible.com\n",
      "Selenium\n",
      "Python\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "For the whole automation process goto https://github.com/rohit7044/Audible-Dataset-Generator-v1.0\n",
      "name 'detect' is not defined Language is not detected: audible-dataset\n",
      "name 'detect' is not defined Language is not detected: rohitdass\n",
      "name 'detect' is not defined Language is not detected: Audible Dataset\n",
      "name 'detect' is not defined Language is not detected: Audible Updated Dataset including Reviews\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Full Dataset for audible dataset recommendation engine. Total books 2200+(Including Duplicates)\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Audible.com\n",
      "Selenium\n",
      "Python\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "For the whole automation process goto https://github.com/rohit7044/Audible-Dataset-Generator-v1.0\n",
      "name 'detect' is not defined Language is not detected: francescodaghero/linaige\n",
      "name 'detect' is not defined Language is not detected: linaige\n",
      "name 'detect' is not defined Language is not detected: francescodaghero\n",
      "name 'detect' is not defined Language is not detected: LINAIGE\n",
      "name 'detect' is not defined Language is not detected: Low-resolution INfrared-array data for AI on the edGE\n",
      "name 'detect' is not defined Language is not detected: ### Data Description\n",
      "\n",
      "The dataset consists of low-resolution multi-pixel infrared sensor samples, to be used for AI regression and classification applications (person counting, presence detection, etc.) and of the corresponding ground truth values. The main purpose of the dataset is to support the development and validation of the infrared sensor demonstrator in the project.\n",
      "\n",
      "### Data Format\n",
      "\n",
      "The dataset consists of a set of Comma-Separated Value (CSV) files, where each row stores the information about one collected infrared frame, in particular:\n",
      "•\tFrame timestamp\n",
      "•\tFrame infrared pixels (8x8 array, stored in row-major, left to right, order).\n",
      "•\tRoom temperature, measured by a thermistor\n",
      "•\tGround truth label (e.g., person count in the frame).\n",
      "•\tLabel confidence (a letter, \"e\" for \"easy-to-label\", \"h\" for \"hard-to-label\"), used for indicating human labeller's confidence to assign the label to specific frame, because of the difficulty of exactly matching the alignment and viewing angles of the infrared sensor and an optical camera used as reference.\n",
      "\n",
      "Samples in each CSV file are collected at 10Hz and ordered by timestamp. Different CSV files refer to different data collection sessions (e.g., different environments, different days or times of the day, etc.).\n",
      "\n",
      "### Data Size\n",
      "The dataset will grow throughout the project. Currently, it includes a total of 25000 samples collected over 6 different sessions. The total file size is 13.2MB. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "If you use LINAIGE in your experiments, please make sure to cite our paper:\n",
      "\n",
      "@inproceedings{xie2022privacy,\n",
      "\tauthor = {Xie, Chen and Daghero, Francesco and Chen, Yukai and Castellano, Marco and Gandolfi, Luca and Calimera, Andrea and Macii, Enrico and Poncino, Massimo and Jahier Pagliari, Daniele},\n",
      "\ttitle = {Privacy-preserving Social Distance Monitoring on Microcontrollers with Low-Resolution Infrared Sensors and CNNs},\n",
      "\tyear = {2022},\n",
      "\tpublisher = {IEEE},\n",
      "\tbooktitle = {Proceedings of the 2022 IEEE International Symposium on Circuits and Systems (ISCAS)},\n",
      "\tseries = {ISCAS 2022}\n",
      "}\n",
      "\n",
      "This dataset is part of the StorAIge project that has received funding from the ECSEL Joint Undertaking (JU) under grant agreement No 101007321. The JU receives support from the European Union’s Horizon 2020 research and innovation programme and France, Belgium, Czech Republic, Germany, Italy, Sweden, Switzerland, Turkey.\n",
      "name 'detect' is not defined Language is not detected: linaige\n",
      "name 'detect' is not defined Language is not detected: francescodaghero\n",
      "name 'detect' is not defined Language is not detected: LINAIGE\n",
      "name 'detect' is not defined Language is not detected: Low-resolution INfrared-array data for AI on the edGE\n",
      "name 'detect' is not defined Language is not detected: ### Data Description\n",
      "\n",
      "The dataset consists of low-resolution multi-pixel infrared sensor samples, to be used for AI regression and classification applications (person counting, presence detection, etc.) and of the corresponding ground truth values. The main purpose of the dataset is to support the development and validation of the infrared sensor demonstrator in the project.\n",
      "\n",
      "### Data Format\n",
      "\n",
      "The dataset consists of a set of Comma-Separated Value (CSV) files, where each row stores the information about one collected infrared frame, in particular:\n",
      "•\tFrame timestamp\n",
      "•\tFrame infrared pixels (8x8 array, stored in row-major, left to right, order).\n",
      "•\tRoom temperature, measured by a thermistor\n",
      "•\tGround truth label (e.g., person count in the frame).\n",
      "•\tLabel confidence (a letter, \"e\" for \"easy-to-label\", \"h\" for \"hard-to-label\"), used for indicating human labeller's confidence to assign the label to specific frame, because of the difficulty of exactly matching the alignment and viewing angles of the infrared sensor and an optical camera used as reference.\n",
      "\n",
      "Samples in each CSV file are collected at 10Hz and ordered by timestamp. Different CSV files refer to different data collection sessions (e.g., different environments, different days or times of the day, etc.).\n",
      "\n",
      "### Data Size\n",
      "The dataset will grow throughout the project. Currently, it includes a total of 25000 samples collected over 6 different sessions. The total file size is 13.2MB. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "If you use LINAIGE in your experiments, please make sure to cite our paper:\n",
      "\n",
      "@inproceedings{xie2022privacy,\n",
      "\tauthor = {Xie, Chen and Daghero, Francesco and Chen, Yukai and Castellano, Marco and Gandolfi, Luca and Calimera, Andrea and Macii, Enrico and Poncino, Massimo and Jahier Pagliari, Daniele},\n",
      "\ttitle = {Privacy-preserving Social Distance Monitoring on Microcontrollers with Low-Resolution Infrared Sensors and CNNs},\n",
      "\tyear = {2022},\n",
      "\tpublisher = {IEEE},\n",
      "\tbooktitle = {Proceedings of the 2022 IEEE International Symposium on Circuits and Systems (ISCAS)},\n",
      "\tseries = {ISCAS 2022}\n",
      "}\n",
      "\n",
      "This dataset is part of the StorAIge project that has received funding from the ECSEL Joint Undertaking (JU) under grant agreement No 101007321. The JU receives support from the European Union’s Horizon 2020 research and innovation programme and France, Belgium, Czech Republic, Germany, Italy, Sweden, Switzerland, Turkey.\n",
      "name 'detect' is not defined Language is not detected: jaiganeshnagidi/infectious-disease-cases-data\n",
      "name 'detect' is not defined Language is not detected: infectious-disease-cases-data\n",
      "name 'detect' is not defined Language is not detected: jaiganeshnagidi\n",
      "name 'detect' is not defined Language is not detected: Infectious Disease Cases Data\n",
      "name 'detect' is not defined Language is not detected: Worldwide Infectious Disease cases from 2001-2014\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "During this pandemic , let us explore about different infectious diseases which we came across in entire world from 2001 to 2014\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "**About Dataset**\n",
      "\n",
      "- Disease    :  Name of the Disease (disease type)\n",
      "- country    :  Name of the Country\n",
      "- year          : happened in which year\n",
      "- Sex           : type of gender\n",
      "- Count       : no of people effected\n",
      "- Population : population of a country at particular year\n",
      "- Rate            : most happening rate\n",
      "- CI.lower      : lower Class Interval  (The confidence interval [CI] is a range of values that's likely to include a population \n",
      "                          value with a certain degree of confidence.)\n",
      "- CI.upper      : upper Class Interval  (The confidence interval [CI] is a range of values that's likely to include a \n",
      "                          population value with a certain degree of confidence.)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This Data was officially given by Data.world \n",
      "name 'detect' is not defined Language is not detected: infectious-disease-cases-data\n",
      "name 'detect' is not defined Language is not detected: jaiganeshnagidi\n",
      "name 'detect' is not defined Language is not detected: Infectious Disease Cases Data\n",
      "name 'detect' is not defined Language is not detected: Worldwide Infectious Disease cases from 2001-2014\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "During this pandemic , let us explore about different infectious diseases which we came across in entire world from 2001 to 2014\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "**About Dataset**\n",
      "\n",
      "- Disease    :  Name of the Disease (disease type)\n",
      "- country    :  Name of the Country\n",
      "- year          : happened in which year\n",
      "- Sex           : type of gender\n",
      "- Count       : no of people effected\n",
      "- Population : population of a country at particular year\n",
      "- Rate            : most happening rate\n",
      "- CI.lower      : lower Class Interval  (The confidence interval [CI] is a range of values that's likely to include a population \n",
      "                          value with a certain degree of confidence.)\n",
      "- CI.upper      : upper Class Interval  (The confidence interval [CI] is a range of values that's likely to include a \n",
      "                          population value with a certain degree of confidence.)\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This Data was officially given by Data.world \n",
      "name 'detect' is not defined Language is not detected: pypiahmad/clothing-fit-data\n",
      "name 'detect' is not defined Language is not detected: clothing-fit-data\n",
      "name 'detect' is not defined Language is not detected: pypiahmad\n",
      "name 'detect' is not defined Language is not detected: Clothing Fit Data\n",
      "name 'detect' is not defined Language is not detected: Clothing Fit Dataset\n",
      "name 'detect' is not defined Language is not detected: Title: Clothing Fit Dataset\n",
      "\n",
      "Description:\n",
      "The Clothing Fit Dataset is derived from ModCloth and RentTheRunway, containing measurements related to clothing fit. This dataset provides an opportunity to develop and evaluate models targeting product size recommendations, understanding user preferences, and analyzing customer feedback regarding the fit of clothing items.\n",
      "\n",
      "Basic Statistics:\n",
      "- ModCloth:\n",
      "  - Users: 47,958\n",
      "  - Items: 1,378\n",
      "  - Transactions: 82,790\n",
      "- RentTheRunway:\n",
      "  - Users: 105,508\n",
      "  - Items: 5,850\n",
      "  - Transactions: 192,544\n",
      "\n",
      "Metadata:\n",
      "- Ratings and Reviews: Customer ratings and textual reviews for the clothing items.\n",
      "- Fit Feedback: Feedback regarding the fit of the clothing items (e.g., small, fit, large, etc.).\n",
      "- User/Item Measurements: Physical measurements of users and items.\n",
      "- Category Information: Information regarding the category of clothing items.\n",
      "\n",
      "Example (RentTheRunway):\n",
      "Each entry in the dataset contains detailed information about a user's experience with a clothing item, including the fit, user and item measurements, rating, review text, and category.\n",
      "```json\n",
      "{\n",
      "  \"fit\": \"fit\",\n",
      "  \"user_id\": \"420272\",\n",
      "  \"bust size\": \"34d\",\n",
      "  \"item_id\": \"2260466\",\n",
      "  \"weight\": \"137lbs\",\n",
      "  \"rating\": \"10\",\n",
      "  \"rented for\": \"vacation\",\n",
      "  \"review_text\": \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
      "  \"body type\": \"hourglass\",\n",
      "  \"review_summary\": \"So many compliments!\",\n",
      "  \"category\": \"romper\",\n",
      "  \"height\": \"5' 8\\\"\",\n",
      "  \"size\": 14,\n",
      "  \"age\": \"28\",\n",
      "  \"review_date\": \"April 20, 2016\"\n",
      "}\n",
      "```\n",
      "\n",
      "Download Links:\n",
      "- Modcloth: [Download Link (8.5mb)](https://datarepo.eng.ucsd.edu/mcauley_group/data/modcloth/modcloth_final_data.json.gz)\n",
      "- Renttherunway: [Download Link (31mb)](https://datarepo.eng.ucsd.edu/mcauley_group/data/renttherunway/renttherunway_final_data.json.gz)\n",
      "\n",
      "Citation:\n",
      "If you utilize this dataset, please cite the following paper:\n",
      "Title: Decomposing fit semantics for product size recommendation in metric spaces\n",
      "Authors: Rishabh Misra, Mengting Wan, Julian McAuley\n",
      "Published in: RecSys, 2018\n",
      "[Link to paper](http://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18e.pdf)\n",
      "\n",
      "This dataset is valuable for researchers and practitioners in the domain of e-commerce, fashion retail, and recommender systems.\n",
      "name 'detect' is not defined Language is not detected: clothing-fit-data\n",
      "name 'detect' is not defined Language is not detected: pypiahmad\n",
      "name 'detect' is not defined Language is not detected: Clothing Fit Data\n",
      "name 'detect' is not defined Language is not detected: Clothing Fit Dataset\n",
      "name 'detect' is not defined Language is not detected: Title: Clothing Fit Dataset\n",
      "\n",
      "Description:\n",
      "The Clothing Fit Dataset is derived from ModCloth and RentTheRunway, containing measurements related to clothing fit. This dataset provides an opportunity to develop and evaluate models targeting product size recommendations, understanding user preferences, and analyzing customer feedback regarding the fit of clothing items.\n",
      "\n",
      "Basic Statistics:\n",
      "- ModCloth:\n",
      "  - Users: 47,958\n",
      "  - Items: 1,378\n",
      "  - Transactions: 82,790\n",
      "- RentTheRunway:\n",
      "  - Users: 105,508\n",
      "  - Items: 5,850\n",
      "  - Transactions: 192,544\n",
      "\n",
      "Metadata:\n",
      "- Ratings and Reviews: Customer ratings and textual reviews for the clothing items.\n",
      "- Fit Feedback: Feedback regarding the fit of the clothing items (e.g., small, fit, large, etc.).\n",
      "- User/Item Measurements: Physical measurements of users and items.\n",
      "- Category Information: Information regarding the category of clothing items.\n",
      "\n",
      "Example (RentTheRunway):\n",
      "Each entry in the dataset contains detailed information about a user's experience with a clothing item, including the fit, user and item measurements, rating, review text, and category.\n",
      "```json\n",
      "{\n",
      "  \"fit\": \"fit\",\n",
      "  \"user_id\": \"420272\",\n",
      "  \"bust size\": \"34d\",\n",
      "  \"item_id\": \"2260466\",\n",
      "  \"weight\": \"137lbs\",\n",
      "  \"rating\": \"10\",\n",
      "  \"rented for\": \"vacation\",\n",
      "  \"review_text\": \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
      "  \"body type\": \"hourglass\",\n",
      "  \"review_summary\": \"So many compliments!\",\n",
      "  \"category\": \"romper\",\n",
      "  \"height\": \"5' 8\\\"\",\n",
      "  \"size\": 14,\n",
      "  \"age\": \"28\",\n",
      "  \"review_date\": \"April 20, 2016\"\n",
      "}\n",
      "```\n",
      "\n",
      "Download Links:\n",
      "- Modcloth: [Download Link (8.5mb)](https://datarepo.eng.ucsd.edu/mcauley_group/data/modcloth/modcloth_final_data.json.gz)\n",
      "- Renttherunway: [Download Link (31mb)](https://datarepo.eng.ucsd.edu/mcauley_group/data/renttherunway/renttherunway_final_data.json.gz)\n",
      "\n",
      "Citation:\n",
      "If you utilize this dataset, please cite the following paper:\n",
      "Title: Decomposing fit semantics for product size recommendation in metric spaces\n",
      "Authors: Rishabh Misra, Mengting Wan, Julian McAuley\n",
      "Published in: RecSys, 2018\n",
      "[Link to paper](http://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18e.pdf)\n",
      "\n",
      "This dataset is valuable for researchers and practitioners in the domain of e-commerce, fashion retail, and recommender systems.\n",
      "name 'detect' is not defined Language is not detected: pypiahmad/shop-the-look-dataset\n",
      "name 'detect' is not defined Language is not detected: shop-the-look-dataset\n",
      "name 'detect' is not defined Language is not detected: pypiahmad\n",
      "name 'detect' is not defined Language is not detected: Pinterest Fashion Compatibility Dataset\n",
      "name 'detect' is not defined Language is not detected: Shop the Look Dataset\n",
      "name 'detect' is not defined Language is not detected: The Pinterest Fashion Compatibility dataset comprises images showcasing fashion products, each annotated with bounding boxes and associated with links directing to the corresponding products. This dataset facilitates the exploration of scene-based complementary product recommendation, aiming to complete the look presented in each scene by recommending compatible fashion items.\n",
      "\n",
      "Basic Statistics:\n",
      "- Scenes: 47,739\n",
      "- Products: 38,111\n",
      "- Scene-Product Pairs: 93,274\n",
      "\n",
      "Metadata:\n",
      "- Product IDs: Identifiers for the products featured in the images.\n",
      "- Bounding Boxes: Coordinates specifying the location of each product within the image.\n",
      "\n",
      "Example (fashion.json):\n",
      "The dataset contains JSON entries where each entry associates a product with a scene, along with the bounding box coordinates for the product within the scene.\n",
      "```json\n",
      "{\n",
      "    \"product\": \"0027e30879ce3d87f82f699f148bff7e\", \n",
      "    \"scene\": \"cdab9160072dd1800038227960ff6467\", \n",
      "    \"bbox\": [\n",
      "        0.434097, \n",
      "        0.859363, \n",
      "        0.560254, \n",
      "        1.0\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "Citation:\n",
      "If you utilize this dataset, please cite the following paper:\n",
      "Title: Complete the Look: Scene-based complementary product recommendation\n",
      "Authors: Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian McAuley\n",
      "Published in: CVPR, 2019\n",
      "[Link to paper](http://openaccess.thecvf.com/content_CVPR_2019/html/Kang_Complete_the_Look_Scene-Based_Complementary_Product_Recommendation_CVPR_2019_paper.html)\n",
      "\n",
      "Code and Additional Resources:\n",
      "For additional resources, sample code, and instructions on how to collect the product images from Pinterest, you can visit the [GitHub repository](https://github.com/kang205/STL-Dataset).\n",
      "\n",
      "This dataset provides a rich ground for research and development in the domain of fashion-based image recognition, product recommendation, and the exploration of fashion styles and trends through machine learning and computer vision techniques.\n",
      "name 'detect' is not defined Language is not detected: shop-the-look-dataset\n",
      "name 'detect' is not defined Language is not detected: pypiahmad\n",
      "name 'detect' is not defined Language is not detected: Pinterest Fashion Compatibility Dataset\n",
      "name 'detect' is not defined Language is not detected: Shop the Look Dataset\n",
      "name 'detect' is not defined Language is not detected: The Pinterest Fashion Compatibility dataset comprises images showcasing fashion products, each annotated with bounding boxes and associated with links directing to the corresponding products. This dataset facilitates the exploration of scene-based complementary product recommendation, aiming to complete the look presented in each scene by recommending compatible fashion items.\n",
      "\n",
      "Basic Statistics:\n",
      "- Scenes: 47,739\n",
      "- Products: 38,111\n",
      "- Scene-Product Pairs: 93,274\n",
      "\n",
      "Metadata:\n",
      "- Product IDs: Identifiers for the products featured in the images.\n",
      "- Bounding Boxes: Coordinates specifying the location of each product within the image.\n",
      "\n",
      "Example (fashion.json):\n",
      "The dataset contains JSON entries where each entry associates a product with a scene, along with the bounding box coordinates for the product within the scene.\n",
      "```json\n",
      "{\n",
      "    \"product\": \"0027e30879ce3d87f82f699f148bff7e\", \n",
      "    \"scene\": \"cdab9160072dd1800038227960ff6467\", \n",
      "    \"bbox\": [\n",
      "        0.434097, \n",
      "        0.859363, \n",
      "        0.560254, \n",
      "        1.0\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "Citation:\n",
      "If you utilize this dataset, please cite the following paper:\n",
      "Title: Complete the Look: Scene-based complementary product recommendation\n",
      "Authors: Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian McAuley\n",
      "Published in: CVPR, 2019\n",
      "[Link to paper](http://openaccess.thecvf.com/content_CVPR_2019/html/Kang_Complete_the_Look_Scene-Based_Complementary_Product_Recommendation_CVPR_2019_paper.html)\n",
      "\n",
      "Code and Additional Resources:\n",
      "For additional resources, sample code, and instructions on how to collect the product images from Pinterest, you can visit the [GitHub repository](https://github.com/kang205/STL-Dataset).\n",
      "\n",
      "This dataset provides a rich ground for research and development in the domain of fashion-based image recognition, product recommendation, and the exploration of fashion styles and trends through machine learning and computer vision techniques.\n",
      "name 'detect' is not defined Language is not detected: kaushikmetha/imdb-movies\n",
      "name 'detect' is not defined Language is not detected: imdb-movies\n",
      "name 'detect' is not defined Language is not detected: kaushikmetha\n",
      "name 'detect' is not defined Language is not detected: IMDB MOVIES\n",
      "name 'detect' is not defined Language is not detected: A dataset consisting of 12500 movies for building recommendation systems\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Use the dataset to recommend movies by applying content based filtering. Learn about content based filtering <a href=\"https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/\">here</a>\n",
      "\n",
      "### Content\n",
      "\n",
      "Scraped around 12500 movies from IMDB using Scrapy. The description of the columns is as follows:\n",
      "<ul>\n",
      "     <li>cast_crew : The director(s) and main cast of the movie</li>\n",
      "     <li>description: A short summary about the movie</li>\n",
      "     <li>genre: The genre of the movie</li>\n",
      "     <li>gross: Money earned by the movie</li>\n",
      "     <li>imdb_score: The imdb rating of the movie</li>\n",
      "     <li>movie_name: The name of the movie</li>\n",
      "     <li>rating: Film Rating. for e.g. PG-13</li>\n",
      "     <li>runtime: The duration of the movie</li>\n",
      "     <li>votes: Vote count</li>\n",
      "\n",
      "</ul>\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "<a href=\"https://www.imdb.com/\">IMDB</a>\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Try to recommend relevant movies using techniques like content based filtering\n",
      "name 'detect' is not defined Language is not detected: imdb-movies\n",
      "name 'detect' is not defined Language is not detected: kaushikmetha\n",
      "name 'detect' is not defined Language is not detected: IMDB MOVIES\n",
      "name 'detect' is not defined Language is not detected: A dataset consisting of 12500 movies for building recommendation systems\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Use the dataset to recommend movies by applying content based filtering. Learn about content based filtering <a href=\"https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/\">here</a>\n",
      "\n",
      "### Content\n",
      "\n",
      "Scraped around 12500 movies from IMDB using Scrapy. The description of the columns is as follows:\n",
      "<ul>\n",
      "     <li>cast_crew : The director(s) and main cast of the movie</li>\n",
      "     <li>description: A short summary about the movie</li>\n",
      "     <li>genre: The genre of the movie</li>\n",
      "     <li>gross: Money earned by the movie</li>\n",
      "     <li>imdb_score: The imdb rating of the movie</li>\n",
      "     <li>movie_name: The name of the movie</li>\n",
      "     <li>rating: Film Rating. for e.g. PG-13</li>\n",
      "     <li>runtime: The duration of the movie</li>\n",
      "     <li>votes: Vote count</li>\n",
      "\n",
      "</ul>\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "<a href=\"https://www.imdb.com/\">IMDB</a>\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Try to recommend relevant movies using techniques like content based filtering\n",
      "name 'detect' is not defined Language is not detected: matwario/wikipedia-music-links\n",
      "name 'detect' is not defined Language is not detected: wikipedia-music-links\n",
      "name 'detect' is not defined Language is not detected: matwario\n",
      "name 'detect' is not defined Language is not detected: Wikipedia Music Links\n",
      "name 'detect' is not defined Language is not detected: Dataset of Wikipedia links between musical artists\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Besides containing information in a textual format, Wikipedia is known for having links between its articles, and these can be used to produce interesting insights and visualizations on how articles relate to one another. In the case of musical artists, links or mentions to articles of other peers can indicate relationships of influence, stylistic proximity, or even collaboration.\n",
      "\n",
      "Take the following excerpt extracted from the Wikipedia article on [The Rolling Stones](https://en.wikipedia.org/wiki/The_Rolling_Stones). Other artists that are referred to have been highlighted in bold.\n",
      "\n",
      "*\"Having a charting single gave the band entree to play outside London, starting with a booking at the Outlook Club in Middlesbrough on 13 July, sharing the billing with **the Hollies**. Later in 1963 Oldham and Easton arranged the band's first big UK concert tour as a supporting act for American stars including **Bo Diddley**, **Little Richard** and the **Everly Brothers**. The tour gave the band the opportunity to hone their stagecraft. During the tour the band recorded their second single, a **Lennon–McCartney**-penned number entitled 'I Wanna Be Your Man'. The song was written and given to the Stones when **John Lennon** and **Paul McCartney** visited them in the studio as the two Beatles liked giving the copyrights to songs away to their friends.\"*\n",
      "\n",
      "In just a few sentences we see how The Rolling Stones have historical links to many other artists. And this happens throughout the band's article as well as those from other bands and musicians around Wikipedia. The question, then, is: what insights can be produced via these links and mentions? In order to find out, the first step is building a dataset that maps these links throughout Wikipedia and stores them neatly in a file. This is the main purpose of this dataset.\n",
      "\n",
      "### Content\n",
      "\n",
      "For the dataset to be generated exclusively with articles about musical artists, they had to be acquired manually from Wikipedia. The site employs categories to organize articles, and these can in turn be used in [this page](https://en.wikipedia.org/wiki/Special:Export) so that a XML can be downloaded. This XML will have the full content of all articles from the informed category.\n",
      "\n",
      "The dataset, as made available here, was created with articles/artists that fell into the following cateogories.\n",
      "\n",
      "* \"Musical groups established in 1950\" until \"Musical groups established in 2020\".\n",
      "* \"20th\\_century\\_american\\_rappers\" and \"21st\\_century\\_american\\_rappers\".\n",
      "* \"20th\\_century\\_american\\_singers\" and \"21st\\_century\\_american\\_singers\".\n",
      "* \"20th\\_century\\_australian\\_singers\" and \"21st\\_century\\_australian\\_singers\".\n",
      "* \"20th\\_century\\_brazilian\\_singers\" and \"21st\\_century\\_brazilian\\_singers\".\n",
      "* \"20th\\_century\\_canadian\\_singers\" and \"21st\\_century\\_canadian\\_singers\".\n",
      "* \"20th\\_century\\_english\\_singers\" and \"21st\\_century\\_english\\_singers\".\n",
      "* \"20th\\_century\\_french\\_singers\" and \"21st\\_century\\_french\\_singers\".\n",
      "\n",
      "The final dataset is pretty simple, being made up of only three columns. One that lists the artist (ARTIST\\_NAME), another that shows the artists that are mentioned in that Wikipedia article as well as how many times those mentions occur (MENTIONED\\_ARTISTS), and one (ARTIST\\_CATEGORY) that has the Wikipedia category from which the article came.\n",
      "\n",
      "### What If I Wanted More or Less Artists?\n",
      "\n",
      "I have made the code that generated this dataset available on my [GitHub page](https://github.com/MatCorr/Wikipedia_Music_Tree). You can use it to create your own version of it or perhaps even enrich it in some way. Instructions on how to do so are there. You will have, however, to download the desired XMLs from Wikipedia. \n",
      "\n",
      "The code, in its current version, has been parallelized but may take a little time to run depending on how many XMLs are used because there are a lot of text searches that need to be done.\n",
      "\n",
      "Note that, in the code, the category column (ARTIST\\_CATEGORY) is extracted from the name of the XML file that contains the raw article.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset can be used to:\n",
      "* Enrich music recommendation algorithms;\n",
      "* Calculate the similarity between artists according to the links of their articles;\n",
      "* Check if artists that come from the same categories have similar links;\n",
      "* Predict the year in which bands formed based on the links of their articles;\n",
      "* Use graph metrics to try to estimate how influential an artist is;\n",
      "* Use machine learning to do some link prediction;\n",
      "* Create graph visualizations of part of the data, such as this one, which was done using a few scripts you can also find on my [GitHub page](https://github.com/MatCorr/Wikipedia_Music_Tree) plus Gephi.\n",
      "\n",
      "![Mentions From Bob Dylan Article](https://github.com/MatCorr/Wikipedia_Music_Tree/blob/main/images/bob_dylan_mentions_from.png?raw=true)\n",
      "name 'detect' is not defined Language is not detected: wikipedia-music-links\n",
      "name 'detect' is not defined Language is not detected: matwario\n",
      "name 'detect' is not defined Language is not detected: Wikipedia Music Links\n",
      "name 'detect' is not defined Language is not detected: Dataset of Wikipedia links between musical artists\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Besides containing information in a textual format, Wikipedia is known for having links between its articles, and these can be used to produce interesting insights and visualizations on how articles relate to one another. In the case of musical artists, links or mentions to articles of other peers can indicate relationships of influence, stylistic proximity, or even collaboration.\n",
      "\n",
      "Take the following excerpt extracted from the Wikipedia article on [The Rolling Stones](https://en.wikipedia.org/wiki/The_Rolling_Stones). Other artists that are referred to have been highlighted in bold.\n",
      "\n",
      "*\"Having a charting single gave the band entree to play outside London, starting with a booking at the Outlook Club in Middlesbrough on 13 July, sharing the billing with **the Hollies**. Later in 1963 Oldham and Easton arranged the band's first big UK concert tour as a supporting act for American stars including **Bo Diddley**, **Little Richard** and the **Everly Brothers**. The tour gave the band the opportunity to hone their stagecraft. During the tour the band recorded their second single, a **Lennon–McCartney**-penned number entitled 'I Wanna Be Your Man'. The song was written and given to the Stones when **John Lennon** and **Paul McCartney** visited them in the studio as the two Beatles liked giving the copyrights to songs away to their friends.\"*\n",
      "\n",
      "In just a few sentences we see how The Rolling Stones have historical links to many other artists. And this happens throughout the band's article as well as those from other bands and musicians around Wikipedia. The question, then, is: what insights can be produced via these links and mentions? In order to find out, the first step is building a dataset that maps these links throughout Wikipedia and stores them neatly in a file. This is the main purpose of this dataset.\n",
      "\n",
      "### Content\n",
      "\n",
      "For the dataset to be generated exclusively with articles about musical artists, they had to be acquired manually from Wikipedia. The site employs categories to organize articles, and these can in turn be used in [this page](https://en.wikipedia.org/wiki/Special:Export) so that a XML can be downloaded. This XML will have the full content of all articles from the informed category.\n",
      "\n",
      "The dataset, as made available here, was created with articles/artists that fell into the following cateogories.\n",
      "\n",
      "* \"Musical groups established in 1950\" until \"Musical groups established in 2020\".\n",
      "* \"20th\\_century\\_american\\_rappers\" and \"21st\\_century\\_american\\_rappers\".\n",
      "* \"20th\\_century\\_american\\_singers\" and \"21st\\_century\\_american\\_singers\".\n",
      "* \"20th\\_century\\_australian\\_singers\" and \"21st\\_century\\_australian\\_singers\".\n",
      "* \"20th\\_century\\_brazilian\\_singers\" and \"21st\\_century\\_brazilian\\_singers\".\n",
      "* \"20th\\_century\\_canadian\\_singers\" and \"21st\\_century\\_canadian\\_singers\".\n",
      "* \"20th\\_century\\_english\\_singers\" and \"21st\\_century\\_english\\_singers\".\n",
      "* \"20th\\_century\\_french\\_singers\" and \"21st\\_century\\_french\\_singers\".\n",
      "\n",
      "The final dataset is pretty simple, being made up of only three columns. One that lists the artist (ARTIST\\_NAME), another that shows the artists that are mentioned in that Wikipedia article as well as how many times those mentions occur (MENTIONED\\_ARTISTS), and one (ARTIST\\_CATEGORY) that has the Wikipedia category from which the article came.\n",
      "\n",
      "### What If I Wanted More or Less Artists?\n",
      "\n",
      "I have made the code that generated this dataset available on my [GitHub page](https://github.com/MatCorr/Wikipedia_Music_Tree). You can use it to create your own version of it or perhaps even enrich it in some way. Instructions on how to do so are there. You will have, however, to download the desired XMLs from Wikipedia. \n",
      "\n",
      "The code, in its current version, has been parallelized but may take a little time to run depending on how many XMLs are used because there are a lot of text searches that need to be done.\n",
      "\n",
      "Note that, in the code, the category column (ARTIST\\_CATEGORY) is extracted from the name of the XML file that contains the raw article.\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset can be used to:\n",
      "* Enrich music recommendation algorithms;\n",
      "* Calculate the similarity between artists according to the links of their articles;\n",
      "* Check if artists that come from the same categories have similar links;\n",
      "* Predict the year in which bands formed based on the links of their articles;\n",
      "* Use graph metrics to try to estimate how influential an artist is;\n",
      "* Use machine learning to do some link prediction;\n",
      "* Create graph visualizations of part of the data, such as this one, which was done using a few scripts you can also find on my [GitHub page](https://github.com/MatCorr/Wikipedia_Music_Tree) plus Gephi.\n",
      "\n",
      "![Mentions From Bob Dylan Article](https://github.com/MatCorr/Wikipedia_Music_Tree/blob/main/images/bob_dylan_mentions_from.png?raw=true)\n",
      "name 'detect' is not defined Language is not detected: junaidk0012/anime-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: anime-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: junaidk0012\n",
      "name 'detect' is not defined Language is not detected: Anime Dataset 2024\n",
      "name 'detect' is not defined Language is not detected: Latest dataset for Animes\n",
      "name 'detect' is not defined Language is not detected: This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n",
      "\n",
      "The dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n",
      "\n",
      "Overall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.\n",
      "name 'detect' is not defined Language is not detected: anime-dataset-2024\n",
      "name 'detect' is not defined Language is not detected: junaidk0012\n",
      "name 'detect' is not defined Language is not detected: Anime Dataset 2024\n",
      "name 'detect' is not defined Language is not detected: Latest dataset for Animes\n",
      "name 'detect' is not defined Language is not detected: This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n",
      "\n",
      "The dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n",
      "\n",
      "Overall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.\n",
      "name 'detect' is not defined Language is not detected: vandittyagi0909/ml-techniques\n",
      "name 'detect' is not defined Language is not detected: ml-techniques\n",
      "name 'detect' is not defined Language is not detected: vandittyagi0909\n",
      "name 'detect' is not defined Language is not detected: ml_techniques\n",
      "name 'detect' is not defined Language is not detected: ml-techniques\n",
      "name 'detect' is not defined Language is not detected: vandittyagi0909\n",
      "name 'detect' is not defined Language is not detected: ml_techniques\n",
      "name 'detect' is not defined Language is not detected: sailorstrike/earthquakes-and-general-index-in-stock-markets\n",
      "name 'detect' is not defined Language is not detected: earthquakes-and-general-index-in-stock-markets\n",
      "name 'detect' is not defined Language is not detected: sailorstrike\n",
      "name 'detect' is not defined Language is not detected: Earthquakes and General Index in Stock Markets\n",
      "name 'detect' is not defined Language is not detected: Major earthquakes and there impact to Stock Markets\n",
      "name 'detect' is not defined Language is not detected: These excel datasheets have data of major earthquakes  (above 6.0M) and the general index of the stock market.\n",
      "Excel datasheet columme explanation :\n",
      "\n",
      "Sheet 1 : index prices\n",
      " A) Date B) General Index Price of Stock Exchange C) pseudo variable that 0 = no earthquake and 1 = earthquake E) is the logarithmic deference between the day and the previous day\n",
      "\n",
      "Sheet 2 : earthquakes\n",
      "A) Date B) Magnitude C) Place of Earthquake D) Depth of earthquake\n",
      "\n",
      "Sheet 3 : output results\n",
      "The results of the simple regression\n",
      "\n",
      "name 'detect' is not defined Language is not detected: earthquakes-and-general-index-in-stock-markets\n",
      "name 'detect' is not defined Language is not detected: sailorstrike\n",
      "name 'detect' is not defined Language is not detected: Earthquakes and General Index in Stock Markets\n",
      "name 'detect' is not defined Language is not detected: Major earthquakes and there impact to Stock Markets\n",
      "name 'detect' is not defined Language is not detected: These excel datasheets have data of major earthquakes  (above 6.0M) and the general index of the stock market.\n",
      "Excel datasheet columme explanation :\n",
      "\n",
      "Sheet 1 : index prices\n",
      " A) Date B) General Index Price of Stock Exchange C) pseudo variable that 0 = no earthquake and 1 = earthquake E) is the logarithmic deference between the day and the previous day\n",
      "\n",
      "Sheet 2 : earthquakes\n",
      "A) Date B) Magnitude C) Place of Earthquake D) Depth of earthquake\n",
      "\n",
      "Sheet 3 : output results\n",
      "The results of the simple regression\n",
      "\n",
      "name 'detect' is not defined Language is not detected: prepinstaprime/tomato-reviews\n",
      "name 'detect' is not defined Language is not detected: tomato-reviews\n",
      "name 'detect' is not defined Language is not detected: prepinstaprime\n",
      "name 'detect' is not defined Language is not detected: Zomato Reviews \n",
      "name 'detect' is not defined Language is not detected: Zomato reviews data for data analytics project\n",
      "name 'detect' is not defined Language is not detected: ## You have to perform the following tasks:\n",
      "\n",
      "- **Data Cleaning:**\n",
      "\n",
      "    - Deleting redundant columns.\n",
      "    - Renaming the columns.\n",
      "    - Dropping duplicates.\n",
      "    - Cleaning individual columns.\n",
      "    - Remove the NaN values from the dataset\n",
      "    - Check for some more Transformations\n",
      "\n",
      "- **Data Visualization:**\n",
      "\n",
      "    - Restaurants delivering Online or not\n",
      "    - Restaurants allowing table booking or not\n",
      "    - Table booking Rate vs Rate\n",
      "    - Best Location\n",
      "    - Relation between Location and Rating\n",
      "    - Restaurant Type\n",
      "    - Gaussian Rest type and Rating\n",
      "    - Types of Services\n",
      "    - Relation between Type and Rating\n",
      "    - Cost of Restaurant\n",
      "    - No. of restaurants in a Location\n",
      "    - Restaurant type\n",
      "    - Most famous restaurant chains in Bengaluru\n",
      "\n",
      "- **Regression Analysis:**\n",
      "\n",
      "    - Linear Regression\n",
      "    - Decision Tree Regression\n",
      "    - Random Forest Regression\n",
      "name 'detect' is not defined Language is not detected: tomato-reviews\n",
      "name 'detect' is not defined Language is not detected: prepinstaprime\n",
      "name 'detect' is not defined Language is not detected: Zomato Reviews \n",
      "name 'detect' is not defined Language is not detected: Zomato reviews data for data analytics project\n",
      "name 'detect' is not defined Language is not detected: ## You have to perform the following tasks:\n",
      "\n",
      "- **Data Cleaning:**\n",
      "\n",
      "    - Deleting redundant columns.\n",
      "    - Renaming the columns.\n",
      "    - Dropping duplicates.\n",
      "    - Cleaning individual columns.\n",
      "    - Remove the NaN values from the dataset\n",
      "    - Check for some more Transformations\n",
      "\n",
      "- **Data Visualization:**\n",
      "\n",
      "    - Restaurants delivering Online or not\n",
      "    - Restaurants allowing table booking or not\n",
      "    - Table booking Rate vs Rate\n",
      "    - Best Location\n",
      "    - Relation between Location and Rating\n",
      "    - Restaurant Type\n",
      "    - Gaussian Rest type and Rating\n",
      "    - Types of Services\n",
      "    - Relation between Type and Rating\n",
      "    - Cost of Restaurant\n",
      "    - No. of restaurants in a Location\n",
      "    - Restaurant type\n",
      "    - Most famous restaurant chains in Bengaluru\n",
      "\n",
      "- **Regression Analysis:**\n",
      "\n",
      "    - Linear Regression\n",
      "    - Decision Tree Regression\n",
      "    - Random Forest Regression\n",
      "name 'detect' is not defined Language is not detected: die9origephit/temperature-data-albany-new-york\n",
      "name 'detect' is not defined Language is not detected: temperature-data-albany-new-york\n",
      "name 'detect' is not defined Language is not detected: die9origephit\n",
      "name 'detect' is not defined Language is not detected: Climate Data - New York State\n",
      "name 'detect' is not defined Language is not detected: Hourly, daily and monthly temperature data of Albany (NY) from 2015 to 2021\n",
      "name 'detect' is not defined Language is not detected: \n",
      "The climate data are related to Albany and they cover a period that goes from 01/01/2015 to 05/31/2022. \n",
      "They include wind, temperature, pressure, humidity and precipitation data. \n",
      "Four datasets are included: \n",
      "\n",
      "- daily_data.csv contains all the daily data.\n",
      "- hourly_data.csv with the hourly data.\n",
      "- monthly_data.csv includes data for each month.\n",
      "- three_hour_data.csv where data were collected every three hours. \n",
      "\n",
      "## ** **\n",
      "\n",
      "\n",
      "\n",
      "For more information, check out here: https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf.\n",
      "\n",
      "\n",
      "The following values can be encountered:\n",
      "s = suspect value (appears together with value).\n",
      "T = trace precipitation amount or snow depth (an amount too small to measure, usually &lt; 0.005 inches water\n",
      "equivalent) (appears instead of numeric value).\n",
      "M = missing value (appears instead of value).\n",
      "VRB = variable wind direction.\n",
      "Remember to upvote if you found the dataset useful :).\n",
      "### **Inspiration**\n",
      "\n",
      "The dataset can be used to perform supervised learning to predict one of the numerical features in the dataset, given a set of selected input features.\n",
      "You can perform an exploratory data analysis of the data, working with `Pandas` or `Numpy`(if you use Python).\n",
      "\n",
      "\n",
      "Interesting visualizations can be performed using, for instance, Python libraries like `Matplotlib`.\n",
      "A time series analysis and forecasting can be performed too.\n",
      "Moreover, this dataset is very good to practice queries using `SQL` or `Pandas`.\n",
      "\n",
      "### **Collection methodology**\n",
      "The data were fetched from [NCOI](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00684) website. The data were split in 4 columns according to the `REPORT_TYPE`. Rows containing null values were dropped and empty or partially empty columns were not considered.\n",
      "\n",
      "### **Acknowledgment**\n",
      "\n",
      "DOC/NOAA/NESDIS/NCEI &gt; National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce\n",
      "DOC/NOAA/NWS &gt; National Weather Service, NOAA, U.S. Department of Commerce\n",
      "DOD/USAF &gt; U.S. Air Force, U.S. Department of Defense\n",
      "DOT/FAA &gt; Federal Aviation Agency, U.S. Department of Transportation\n",
      "name 'detect' is not defined Language is not detected: temperature-data-albany-new-york\n",
      "name 'detect' is not defined Language is not detected: die9origephit\n",
      "name 'detect' is not defined Language is not detected: Climate Data - New York State\n",
      "name 'detect' is not defined Language is not detected: Hourly, daily and monthly temperature data of Albany (NY) from 2015 to 2021\n",
      "name 'detect' is not defined Language is not detected: \n",
      "The climate data are related to Albany and they cover a period that goes from 01/01/2015 to 05/31/2022. \n",
      "They include wind, temperature, pressure, humidity and precipitation data. \n",
      "Four datasets are included: \n",
      "\n",
      "- daily_data.csv contains all the daily data.\n",
      "- hourly_data.csv with the hourly data.\n",
      "- monthly_data.csv includes data for each month.\n",
      "- three_hour_data.csv where data were collected every three hours. \n",
      "\n",
      "## ** **\n",
      "\n",
      "\n",
      "\n",
      "For more information, check out here: https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf.\n",
      "\n",
      "\n",
      "The following values can be encountered:\n",
      "s = suspect value (appears together with value).\n",
      "T = trace precipitation amount or snow depth (an amount too small to measure, usually &lt; 0.005 inches water\n",
      "equivalent) (appears instead of numeric value).\n",
      "M = missing value (appears instead of value).\n",
      "VRB = variable wind direction.\n",
      "Remember to upvote if you found the dataset useful :).\n",
      "### **Inspiration**\n",
      "\n",
      "The dataset can be used to perform supervised learning to predict one of the numerical features in the dataset, given a set of selected input features.\n",
      "You can perform an exploratory data analysis of the data, working with `Pandas` or `Numpy`(if you use Python).\n",
      "\n",
      "\n",
      "Interesting visualizations can be performed using, for instance, Python libraries like `Matplotlib`.\n",
      "A time series analysis and forecasting can be performed too.\n",
      "Moreover, this dataset is very good to practice queries using `SQL` or `Pandas`.\n",
      "\n",
      "### **Collection methodology**\n",
      "The data were fetched from [NCOI](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00684) website. The data were split in 4 columns according to the `REPORT_TYPE`. Rows containing null values were dropped and empty or partially empty columns were not considered.\n",
      "\n",
      "### **Acknowledgment**\n",
      "\n",
      "DOC/NOAA/NESDIS/NCEI &gt; National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce\n",
      "DOC/NOAA/NWS &gt; National Weather Service, NOAA, U.S. Department of Commerce\n",
      "DOD/USAF &gt; U.S. Air Force, U.S. Department of Defense\n",
      "DOT/FAA &gt; Federal Aviation Agency, U.S. Department of Transportation\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2/akeed-restaurant-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: akeed-restaurant-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2\n",
      "name 'detect' is not defined Language is not detected: Akeed Restaurant Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: akeed-restaurant-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2\n",
      "name 'detect' is not defined Language is not detected: Akeed Restaurant Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2/zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2\n",
      "name 'detect' is not defined Language is not detected:  Zimnat Insurance Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: zimnat-insurance-recommendation-challenge\n",
      "name 'detect' is not defined Language is not detected: darisdzakwanhoesien2\n",
      "name 'detect' is not defined Language is not detected:  Zimnat Insurance Recommendation Challenge\n",
      "name 'detect' is not defined Language is not detected: tanmoypias/vehicle-recognition-dataset\n",
      "name 'detect' is not defined Language is not detected: vehicle-recognition-dataset\n",
      "name 'detect' is not defined Language is not detected: tanmoypias\n",
      "name 'detect' is not defined Language is not detected: Vehicle Recognition Dataset\n",
      "name 'detect' is not defined Language is not detected: Accuracy Improvement Challenge!\n",
      "name 'detect' is not defined Language is not detected: Vehicle Recognition Dataset-2022\n",
      "\n",
      "Accelerometer and gyroscope data were collected by the Vieyra Software app on a Google Pixel 4XL smartphone for the bicycle, bus, and Toyota Camry 2005. Accelerometer and gyroscope data were collected by the Vieyra Software app on a Samsung Galaxy phone for a bicycle, train, and Honda Insight 2013.\n",
      "\n",
      "File description: \n",
      "bike.csv [City, No Traffic light, Clear Weather, Google Pixel]\n",
      "bus.csv [City, No Traffic light, Clear Weather,  Google Pixel]\n",
      "car-city-newark-light-rain.csv [City, Traffic light, Rain, Samsung Galaxy]\n",
      "car-clear.csv (Honda) [City, No Traffic light, Clear Weather, Samsung Galaxy]\n",
      "car-clear2.csv (Toyota) [City, No Traffic light, Clear Weather, Google Pixel]\n",
      "car-rain.csv (Honda) [City, No Traffic light, Rain, Samsung Galaxy]\n",
      "rail.csv [City, No Traffic light, Clear Weather, Samsung Galaxy]\n",
      "\n",
      "Column Description:\n",
      "ax, ay, az: accelerometer sensor data\n",
      "wx, wy, wz: gyroscope sensor data\n",
      "gFx, gFy, gFz: gravity sensor data\n",
      "\n",
      "Result:\n",
      "ML-Accuracy\n",
      "Logistic Regression-45.36%\n",
      "Naive Bayes-58.13%\n",
      "SVM-76.12%\n",
      "XGBoost-84.73%\n",
      "CNN-98%\n",
      "\n",
      "Corresponding paper:\n",
      "Pias, Tanmoy Sarkar, David Eisenberg, and Jorge Fresneda Fernandez. \"Accuracy Improvement of Vehicle Recognition by Using Smart Device Sensors.\" Sensors 22, no. 12 (2022): 4397.\n",
      "https://www.mdpi.com/1671762 \n",
      "\n",
      "Code:\n",
      "https://github.com/PiasTanmoy/Vehicle-Recognition-Using-Smart-Sensors \n",
      "name 'detect' is not defined Language is not detected: vehicle-recognition-dataset\n",
      "name 'detect' is not defined Language is not detected: tanmoypias\n",
      "name 'detect' is not defined Language is not detected: Vehicle Recognition Dataset\n",
      "name 'detect' is not defined Language is not detected: Accuracy Improvement Challenge!\n",
      "name 'detect' is not defined Language is not detected: Vehicle Recognition Dataset-2022\n",
      "\n",
      "Accelerometer and gyroscope data were collected by the Vieyra Software app on a Google Pixel 4XL smartphone for the bicycle, bus, and Toyota Camry 2005. Accelerometer and gyroscope data were collected by the Vieyra Software app on a Samsung Galaxy phone for a bicycle, train, and Honda Insight 2013.\n",
      "\n",
      "File description: \n",
      "bike.csv [City, No Traffic light, Clear Weather, Google Pixel]\n",
      "bus.csv [City, No Traffic light, Clear Weather,  Google Pixel]\n",
      "car-city-newark-light-rain.csv [City, Traffic light, Rain, Samsung Galaxy]\n",
      "car-clear.csv (Honda) [City, No Traffic light, Clear Weather, Samsung Galaxy]\n",
      "car-clear2.csv (Toyota) [City, No Traffic light, Clear Weather, Google Pixel]\n",
      "car-rain.csv (Honda) [City, No Traffic light, Rain, Samsung Galaxy]\n",
      "rail.csv [City, No Traffic light, Clear Weather, Samsung Galaxy]\n",
      "\n",
      "Column Description:\n",
      "ax, ay, az: accelerometer sensor data\n",
      "wx, wy, wz: gyroscope sensor data\n",
      "gFx, gFy, gFz: gravity sensor data\n",
      "\n",
      "Result:\n",
      "ML-Accuracy\n",
      "Logistic Regression-45.36%\n",
      "Naive Bayes-58.13%\n",
      "SVM-76.12%\n",
      "XGBoost-84.73%\n",
      "CNN-98%\n",
      "\n",
      "Corresponding paper:\n",
      "Pias, Tanmoy Sarkar, David Eisenberg, and Jorge Fresneda Fernandez. \"Accuracy Improvement of Vehicle Recognition by Using Smart Device Sensors.\" Sensors 22, no. 12 (2022): 4397.\n",
      "https://www.mdpi.com/1671762 \n",
      "\n",
      "Code:\n",
      "https://github.com/PiasTanmoy/Vehicle-Recognition-Using-Smart-Sensors \n",
      "name 'detect' is not defined Language is not detected: jasineri/stock-market-dataset\n",
      "name 'detect' is not defined Language is not detected: stock-market-dataset\n",
      "name 'detect' is not defined Language is not detected: jasineri\n",
      "name 'detect' is not defined Language is not detected: Stock Market Dataset\n",
      "name 'detect' is not defined Language is not detected: Disclaimer: Educational Purposes Only\n",
      "\n",
      "The financial and International Securities Identification Number (ISIN) data listed on this platform is provided solely for educational purposes. The information is intended to serve as general guidance and does not constitute financial advice, an endorsement, or a recommendation for the purchase or sale of any securities.\n",
      "\n",
      "While we strive to ensure the accuracy and timeliness of the information presented, we make no representations or warranties, express or implied, regarding the completeness, accuracy, reliability, suitability, or availability of the provided data. Users are encouraged to independently verify any information obtained from this platform before making any investment decisions.\n",
      "\n",
      "This platform and its operators are not responsible for any errors, omissions, or inaccuracies in the provided data, nor for any actions taken in reliance on such information. Users are strongly advised to conduct thorough research and seek the advice of qualified financial professionals before making any investment decisions.\n",
      "\n",
      "The use of International Securities Identification Numbers (ISINs) and other financial data is subject to various regulations and licensing agreements. Users are responsible for complying with all applicable laws and respecting any terms and conditions associated with the use of such data.\n",
      "\n",
      "By accessing and using this platform, users acknowledge and agree that they are doing so at their own risk and discretion. This educational content is not a substitute for professional financial advice, and users should consult with qualified professionals for specific guidance tailored to their individual circumstances.\n",
      "name 'detect' is not defined Language is not detected: stock-market-dataset\n",
      "name 'detect' is not defined Language is not detected: jasineri\n",
      "name 'detect' is not defined Language is not detected: Stock Market Dataset\n",
      "name 'detect' is not defined Language is not detected: Disclaimer: Educational Purposes Only\n",
      "\n",
      "The financial and International Securities Identification Number (ISIN) data listed on this platform is provided solely for educational purposes. The information is intended to serve as general guidance and does not constitute financial advice, an endorsement, or a recommendation for the purchase or sale of any securities.\n",
      "\n",
      "While we strive to ensure the accuracy and timeliness of the information presented, we make no representations or warranties, express or implied, regarding the completeness, accuracy, reliability, suitability, or availability of the provided data. Users are encouraged to independently verify any information obtained from this platform before making any investment decisions.\n",
      "\n",
      "This platform and its operators are not responsible for any errors, omissions, or inaccuracies in the provided data, nor for any actions taken in reliance on such information. Users are strongly advised to conduct thorough research and seek the advice of qualified financial professionals before making any investment decisions.\n",
      "\n",
      "The use of International Securities Identification Numbers (ISINs) and other financial data is subject to various regulations and licensing agreements. Users are responsible for complying with all applicable laws and respecting any terms and conditions associated with the use of such data.\n",
      "\n",
      "By accessing and using this platform, users acknowledge and agree that they are doing so at their own risk and discretion. This educational content is not a substitute for professional financial advice, and users should consult with qualified professionals for specific guidance tailored to their individual circumstances.\n",
      "name 'detect' is not defined Language is not detected: yufengsui/portuguese-bank-marketing-data-set\n",
      "name 'detect' is not defined Language is not detected: portuguese-bank-marketing-data-set\n",
      "name 'detect' is not defined Language is not detected: yufengsui\n",
      "name 'detect' is not defined Language is not detected: Portuguese Bank Marketing Data Set \n",
      "name 'detect' is not defined Language is not detected: Telemarketing campaign about term deposits\n",
      "name 'detect' is not defined Language is not detected: This dataset is about the direct phone call marketing campaigns, which aim to promote term deposits among existing customers, by a Portuguese banking institution from May 2008 to November 2010. It is publicly available in the UCI Machine Learning Repository, which can be retrieved from http://archive.ics.uci.edu/ml/datasets/Bank+Marketing#.\n",
      "name 'detect' is not defined Language is not detected: portuguese-bank-marketing-data-set\n",
      "name 'detect' is not defined Language is not detected: yufengsui\n",
      "name 'detect' is not defined Language is not detected: Portuguese Bank Marketing Data Set \n",
      "name 'detect' is not defined Language is not detected: Telemarketing campaign about term deposits\n",
      "name 'detect' is not defined Language is not detected: This dataset is about the direct phone call marketing campaigns, which aim to promote term deposits among existing customers, by a Portuguese banking institution from May 2008 to November 2010. It is publicly available in the UCI Machine Learning Repository, which can be retrieved from http://archive.ics.uci.edu/ml/datasets/Bank+Marketing#.\n",
      "name 'detect' is not defined Language is not detected: datazng/wireless-company-retail-dataset-t-mobileultraetc\n",
      "name 'detect' is not defined Language is not detected: wireless-company-retail-dataset-t-mobileultraetc\n",
      "name 'detect' is not defined Language is not detected: datazng\n",
      "name 'detect' is not defined Language is not detected: Wireless Corp Retail Business Data Authentic\n",
      "name 'detect' is not defined Language is not detected: T-Mobile,Ultra-Mobile,Business Analytics, Real Data, Original, Data Science\n",
      "name 'detect' is not defined Language is not detected: Retail business data is considered hard to acquire and mysterious. To demystify the wireless service industry and provide business insights for general public, I created this dataset which was complied based on real world business transaction data, supplier data and other data in wireless service industry. \n",
      "\n",
      "The dataset describes a top-performing local wireless company's business transaction details during a 4 month period. You can expect to learn about potential costs, net earnings, product coverage and think about the business model in the wireless service industry.\n",
      "\n",
      "Dataset has been preprocessed to include useful features like item, service category, supplier, profit contribution ratio, KPI benchmark, agent performance, etc.\n",
      "\n",
      "Recommended data science tasks: Sales Dashboard Visualization, Classification, Regression,etc\n",
      "\n",
      "The dataset was disclosed based on my consulting client's permission. All information guaranteed to be original.\n",
      "name 'detect' is not defined Language is not detected: wireless-company-retail-dataset-t-mobileultraetc\n",
      "name 'detect' is not defined Language is not detected: datazng\n",
      "name 'detect' is not defined Language is not detected: Wireless Corp Retail Business Data Authentic\n",
      "name 'detect' is not defined Language is not detected: T-Mobile,Ultra-Mobile,Business Analytics, Real Data, Original, Data Science\n",
      "name 'detect' is not defined Language is not detected: Retail business data is considered hard to acquire and mysterious. To demystify the wireless service industry and provide business insights for general public, I created this dataset which was complied based on real world business transaction data, supplier data and other data in wireless service industry. \n",
      "\n",
      "The dataset describes a top-performing local wireless company's business transaction details during a 4 month period. You can expect to learn about potential costs, net earnings, product coverage and think about the business model in the wireless service industry.\n",
      "\n",
      "Dataset has been preprocessed to include useful features like item, service category, supplier, profit contribution ratio, KPI benchmark, agent performance, etc.\n",
      "\n",
      "Recommended data science tasks: Sales Dashboard Visualization, Classification, Regression,etc\n",
      "\n",
      "The dataset was disclosed based on my consulting client's permission. All information guaranteed to be original.\n",
      "name 'detect' is not defined Language is not detected: datazng/delta-airline-review-dataset-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: delta-airline-review-dataset-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: datazng\n",
      "name 'detect' is not defined Language is not detected: Delta Airline Review data for Sentiment Analysis\n",
      "name 'detect' is not defined Language is not detected: Suitable for Text Mining, Sentiment Analysis, Service Improvement Recommendation\n",
      "name 'detect' is not defined Language is not detected: The dataset was scrapped and complied from website https://www.airlinequality.com/airline-reviews/delta-air-lines by utilizing Python BeautifulSoup package and data manipulation packages including pandas and numpy.\n",
      "\n",
      "The expected data science tasks including:\n",
      "1. Sentiment analysis utilizing text mining\n",
      "2. Data Visualization: popular flight, high rating seat type, customers segmentation, etc \n",
      "3. Booking recommendation prediction by using predictive modeling\n",
      "\n",
      "Key data features are:\n",
      "1. Flight route ( geographic location )\n",
      "2. Flight date\n",
      "3. Rating score\n",
      "4. Seat Type ( Economy, Business, First Class)\n",
      "5. Reviewer country\n",
      "6. Reviewer travel type ( for segment analysis )\n",
      "....... \n",
      "\n",
      "The dateset has been slightly pre-processed. Please enjoy!\n",
      "name 'detect' is not defined Language is not detected: delta-airline-review-dataset-sentiment-analysis\n",
      "name 'detect' is not defined Language is not detected: datazng\n",
      "name 'detect' is not defined Language is not detected: Delta Airline Review data for Sentiment Analysis\n",
      "name 'detect' is not defined Language is not detected: Suitable for Text Mining, Sentiment Analysis, Service Improvement Recommendation\n",
      "name 'detect' is not defined Language is not detected: The dataset was scrapped and complied from website https://www.airlinequality.com/airline-reviews/delta-air-lines by utilizing Python BeautifulSoup package and data manipulation packages including pandas and numpy.\n",
      "\n",
      "The expected data science tasks including:\n",
      "1. Sentiment analysis utilizing text mining\n",
      "2. Data Visualization: popular flight, high rating seat type, customers segmentation, etc \n",
      "3. Booking recommendation prediction by using predictive modeling\n",
      "\n",
      "Key data features are:\n",
      "1. Flight route ( geographic location )\n",
      "2. Flight date\n",
      "3. Rating score\n",
      "4. Seat Type ( Economy, Business, First Class)\n",
      "5. Reviewer country\n",
      "6. Reviewer travel type ( for segment analysis )\n",
      "....... \n",
      "\n",
      "The dateset has been slightly pre-processed. Please enjoy!\n",
      "name 'detect' is not defined Language is not detected: arindamsahoo/social-media-users\n",
      "name 'detect' is not defined Language is not detected: social-media-users\n",
      "name 'detect' is not defined Language is not detected: arindamsahoo\n",
      "name 'detect' is not defined Language is not detected: Social-Media-Users-Dataset\n",
      "name 'detect' is not defined Language is not detected: The Ultimate Dummy Users Dataset for Social Media Recommendation Models\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information about users for a social media friend recommendation project. It includes fields such as UserID, Name, Gender, Date of Birth (DOB), Interests, City, and Country. The dataset aims to capture diverse user profiles and their characteristics in terms of personal information, interests, and geographical locations.\n",
      "name 'detect' is not defined Language is not detected: social-media-users\n",
      "name 'detect' is not defined Language is not detected: arindamsahoo\n",
      "name 'detect' is not defined Language is not detected: Social-Media-Users-Dataset\n",
      "name 'detect' is not defined Language is not detected: The Ultimate Dummy Users Dataset for Social Media Recommendation Models\n",
      "name 'detect' is not defined Language is not detected: This dataset contains information about users for a social media friend recommendation project. It includes fields such as UserID, Name, Gender, Date of Birth (DOB), Interests, City, and Country. The dataset aims to capture diverse user profiles and their characteristics in terms of personal information, interests, and geographical locations.\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro/human-segmentation-dataset\n",
      "name 'detect' is not defined Language is not detected: human-segmentation-dataset\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro\n",
      "name 'detect' is not defined Language is not detected: Body Segmentation - 6,700 Photos\n",
      "name 'detect' is not defined Language is not detected: 7 different types of human body segmentation: 4 for women and 3 for men\n",
      "name 'detect' is not defined Language is not detected: # 7 Types of Human Segmentation\n",
      "\n",
      "The dataset includes 7 different types of image segmentation of people in underwear. \n",
      "For women, 4 types of labeling are provided, and for men, 3 types of labeling are provided. \n",
      "The dataset solves tasks in the field of recommendation systems and e-commerce.\n",
      "\n",
      "# 💴 For Commercial Usage: Full version of the dataset includes 6,700 images & Annotations, leave a request on **[TrainingData](https://trainingdata.pro/datasets/people-in-underwear?utm_source=kaggle&utm_medium=cpc&utm_campaign=human-segmentation-dataset)** to buy the dataset\n",
      "\n",
      "## OTHER BIOMETRIC DATASETS:\n",
      "- **[Selfies, ID Images dataset (5591 sets of 15 files)](https://www.kaggle.com/datasets/tapakah68/selfies-id-images-dataset)**\n",
      "- **[Selfies and video dataset (4 052 sets)](https://www.kaggle.com/datasets/tapakah68/selfies-and-video-dataset-4-000-people)**\n",
      "- **[Dataset of bald people, 5000 images](https://www.kaggle.com/datasets/tapakah68/dataset-of-bald-people)**\n",
      "- **[Anti Spoofing Real Dataset](https://www.kaggle.com/datasets/trainingdatapro/anti-spoofing-live)**\n",
      "- **[Antispoofing Replay Dataset](https://www.kaggle.com/datasets/trainingdatapro/antispoofing-replay-dataset)**\n",
      "\n",
      "# Types of labeling\n",
      "\n",
      "**Women I** - distinctively detailed labeling of women. Special emphasis is placed on distinguishing the internal, external side, and lower breast depending on the type of underwear. The labeling also includes the face and hair, hands, forearms, shoulders, armpits, thighs, shins, underwear, accessories, and smartphones.\n",
      "\n",
      "![![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2Fe157d0b7db89497f85c9b2d79d301086%2Fgirls_1_227.png?generation=1681741881080579&alt=media)\n",
      "\n",
      "**Women II** - labeling of images of women with attention to the side abs area (highlighted in gray on the labeling). The labeling also includes the face and hair, hands, forearms, thighs, underwear, accessories, and smartphones.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F901d120c0273ea9a5a328fff15e26583%2Fgirls_2_-1087839647-1867563540.png?generation=1681741958025976&alt=media)\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F75fb0412edf631adce5f42ab6b9e8052%2Fwomen_fat_image_56570.png?generation=1681742864993159&alt=media)\n",
      "\n",
      "**Women III** - primarily labeling of underwear. In addition to the underwear itself, the labeling includes the face and hair, abdomen, and arms and legs.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F6f32a06f0754a5a116fc994feae8c6f1%2Fgirls_5_111.png?generation=1681742011331681&alt=media)\n",
      "\n",
      "**Women IV** - labeling of both underwear and body parts. It includes labeling of underwear, face and hair, hands, forearms, body, legs, as well as smartphones and tattoos.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F0dc22fcfd8b6e4fad3aa1806d14223ef%2Fgirls_6_image_4534.png?generation=1681742073295272&alt=media)\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F4a398547e13c555fdad142f521e62a5f%2Fsports_girls_IadpBSd3mI%20(1).png?generation=1681742947264828&alt=media)\n",
      "\n",
      "**Men I** - labeling of the upper part of men's bodies. It includes labeling of hands and wrists, shoulders, body, neck, face and hair, as well as phones and accessories.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F3dae9889adb2b1415353769ccdd9c01b%2Fman_regular_1532667_38709335.png?generation=1681742128995529&alt=media)\n",
      "\n",
      "**Men II** - more detailed labeling of men's bodies. The labeling includes hands and wrists, shoulders, body and neck, head and hair, underwear, tattoos and accessories, nipple and navel area.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2Fa57123e41066aa277bfeac140f4457da%2Fmen_1_3046.png?generation=1681742173310957&alt=media)\n",
      "\n",
      "**Men Neuro** - labeling produced by a neural network for subsequent correction by annotators.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F5281cd644bc3f5949aaa9c40fb1cafd4%2F4595%20(1).png?generation=1681742187215164&alt=media)\n",
      "\n",
      "# 💴 Buy the Dataset: This is just an example of the data. Leave a request on **[https://trainingdata.pro/datasets](https://trainingdata.pro/datasets/people-in-underwear?utm_source=kaggle&utm_medium=cpc&utm_campaign=human-segmentation-dataset)** to discuss your requirements, learn about the price and buy the dataset\n",
      "\n",
      "*keywords: body segmentation dataset, human part segmentation dataset, human semantic part segmentation, human body segmentation data, human body segmentation deep learning, computer vision dataset, people images dataset, biometric data dataset, biometric dataset, images database, image-to-image, people segmentation, machine learning*\n",
      "name 'detect' is not defined Language is not detected: human-segmentation-dataset\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro\n",
      "name 'detect' is not defined Language is not detected: Body Segmentation - 6,700 Photos\n",
      "name 'detect' is not defined Language is not detected: 7 different types of human body segmentation: 4 for women and 3 for men\n",
      "name 'detect' is not defined Language is not detected: # 7 Types of Human Segmentation\n",
      "\n",
      "The dataset includes 7 different types of image segmentation of people in underwear. \n",
      "For women, 4 types of labeling are provided, and for men, 3 types of labeling are provided. \n",
      "The dataset solves tasks in the field of recommendation systems and e-commerce.\n",
      "\n",
      "# 💴 For Commercial Usage: Full version of the dataset includes 6,700 images & Annotations, leave a request on **[TrainingData](https://trainingdata.pro/datasets/people-in-underwear?utm_source=kaggle&utm_medium=cpc&utm_campaign=human-segmentation-dataset)** to buy the dataset\n",
      "\n",
      "## OTHER BIOMETRIC DATASETS:\n",
      "- **[Selfies, ID Images dataset (5591 sets of 15 files)](https://www.kaggle.com/datasets/tapakah68/selfies-id-images-dataset)**\n",
      "- **[Selfies and video dataset (4 052 sets)](https://www.kaggle.com/datasets/tapakah68/selfies-and-video-dataset-4-000-people)**\n",
      "- **[Dataset of bald people, 5000 images](https://www.kaggle.com/datasets/tapakah68/dataset-of-bald-people)**\n",
      "- **[Anti Spoofing Real Dataset](https://www.kaggle.com/datasets/trainingdatapro/anti-spoofing-live)**\n",
      "- **[Antispoofing Replay Dataset](https://www.kaggle.com/datasets/trainingdatapro/antispoofing-replay-dataset)**\n",
      "\n",
      "# Types of labeling\n",
      "\n",
      "**Women I** - distinctively detailed labeling of women. Special emphasis is placed on distinguishing the internal, external side, and lower breast depending on the type of underwear. The labeling also includes the face and hair, hands, forearms, shoulders, armpits, thighs, shins, underwear, accessories, and smartphones.\n",
      "\n",
      "![![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2Fe157d0b7db89497f85c9b2d79d301086%2Fgirls_1_227.png?generation=1681741881080579&alt=media)\n",
      "\n",
      "**Women II** - labeling of images of women with attention to the side abs area (highlighted in gray on the labeling). The labeling also includes the face and hair, hands, forearms, thighs, underwear, accessories, and smartphones.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F901d120c0273ea9a5a328fff15e26583%2Fgirls_2_-1087839647-1867563540.png?generation=1681741958025976&alt=media)\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F75fb0412edf631adce5f42ab6b9e8052%2Fwomen_fat_image_56570.png?generation=1681742864993159&alt=media)\n",
      "\n",
      "**Women III** - primarily labeling of underwear. In addition to the underwear itself, the labeling includes the face and hair, abdomen, and arms and legs.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F6f32a06f0754a5a116fc994feae8c6f1%2Fgirls_5_111.png?generation=1681742011331681&alt=media)\n",
      "\n",
      "**Women IV** - labeling of both underwear and body parts. It includes labeling of underwear, face and hair, hands, forearms, body, legs, as well as smartphones and tattoos.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F0dc22fcfd8b6e4fad3aa1806d14223ef%2Fgirls_6_image_4534.png?generation=1681742073295272&alt=media)\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F4a398547e13c555fdad142f521e62a5f%2Fsports_girls_IadpBSd3mI%20(1).png?generation=1681742947264828&alt=media)\n",
      "\n",
      "**Men I** - labeling of the upper part of men's bodies. It includes labeling of hands and wrists, shoulders, body, neck, face and hair, as well as phones and accessories.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F3dae9889adb2b1415353769ccdd9c01b%2Fman_regular_1532667_38709335.png?generation=1681742128995529&alt=media)\n",
      "\n",
      "**Men II** - more detailed labeling of men's bodies. The labeling includes hands and wrists, shoulders, body and neck, head and hair, underwear, tattoos and accessories, nipple and navel area.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2Fa57123e41066aa277bfeac140f4457da%2Fmen_1_3046.png?generation=1681742173310957&alt=media)\n",
      "\n",
      "**Men Neuro** - labeling produced by a neural network for subsequent correction by annotators.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F5281cd644bc3f5949aaa9c40fb1cafd4%2F4595%20(1).png?generation=1681742187215164&alt=media)\n",
      "\n",
      "# 💴 Buy the Dataset: This is just an example of the data. Leave a request on **[https://trainingdata.pro/datasets](https://trainingdata.pro/datasets/people-in-underwear?utm_source=kaggle&utm_medium=cpc&utm_campaign=human-segmentation-dataset)** to discuss your requirements, learn about the price and buy the dataset\n",
      "\n",
      "*keywords: body segmentation dataset, human part segmentation dataset, human semantic part segmentation, human body segmentation data, human body segmentation deep learning, computer vision dataset, people images dataset, biometric data dataset, biometric dataset, images database, image-to-image, people segmentation, machine learning*\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro/asos-e-commerce-dataset-30845-products\n",
      "name 'detect' is not defined Language is not detected: asos-e-commerce-dataset-30845-products\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro\n",
      "name 'detect' is not defined Language is not detected: Asos E-Commerce Dataset - 30,845 products\n",
      "name 'detect' is not defined Language is not detected: The comprehensive collection of parsed data from the Asos.com\n",
      "name 'detect' is not defined Language is not detected: # [Asos](https://asos.com) E-Commerce Dataset - 30,845 products, text classification dataset\n",
      "\n",
      "Using web scraping, we collected information on over **30,845** clothing items from the Asos website. \n",
      "The dataset can be applied in E-commerce analytics in the fashion industry. The dataset is similar to [SheIn E-Commerce Dataset](https://www.kaggle.com/datasets/trainingdatapro/shein-e-commerce-dataset).\n",
      "\n",
      "# 💴 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** to buy the dataset\n",
      "\n",
      "# Dataset Info\n",
      "For each item, we extracted:\n",
      "\n",
      "- **url** - link to the item on the website\n",
      "- **name** - item's name\n",
      "- **size** - sizes available on the website\n",
      "- **category** - product's category\n",
      "- **price** - item's price\n",
      "- **color** - item's color\n",
      "- **SKU** - unique identifier of the item\n",
      "- **date** - date of web scraping; for all items - March 11, 2023\n",
      "- **description** - additional description, including product's brand, composition, and care instructions, in JSON format\n",
      "- **images** - photographs from the item description\n",
      "\n",
      "# 💴 Buy the Dataset: Leave a request on **[https://trainingdata.pro/datasets](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** to discuss your requirements, learn about the price and buy the dataset\n",
      "\n",
      "**[TrainingData](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** provides high-quality data annotation tailored to your needs.\n",
      "\n",
      "*keywords: web scraping dataset, dataset marketplace, web scraping data, e-commerce dataset, e-commerce marketplace, e-commerce marketplace scraping dataset, e-commerce sales dataset, ecommerce clothing site, e-commerce user behavior dataset, e-commerce text dataset, e-commerce product dataset, text dataset, ratings, product recommendation, text classification, text mining dataset, text data*\n",
      "name 'detect' is not defined Language is not detected: asos-e-commerce-dataset-30845-products\n",
      "name 'detect' is not defined Language is not detected: trainingdatapro\n",
      "name 'detect' is not defined Language is not detected: Asos E-Commerce Dataset - 30,845 products\n",
      "name 'detect' is not defined Language is not detected: The comprehensive collection of parsed data from the Asos.com\n",
      "name 'detect' is not defined Language is not detected: # [Asos](https://asos.com) E-Commerce Dataset - 30,845 products, text classification dataset\n",
      "\n",
      "Using web scraping, we collected information on over **30,845** clothing items from the Asos website. \n",
      "The dataset can be applied in E-commerce analytics in the fashion industry. The dataset is similar to [SheIn E-Commerce Dataset](https://www.kaggle.com/datasets/trainingdatapro/shein-e-commerce-dataset).\n",
      "\n",
      "# 💴 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** to buy the dataset\n",
      "\n",
      "# Dataset Info\n",
      "For each item, we extracted:\n",
      "\n",
      "- **url** - link to the item on the website\n",
      "- **name** - item's name\n",
      "- **size** - sizes available on the website\n",
      "- **category** - product's category\n",
      "- **price** - item's price\n",
      "- **color** - item's color\n",
      "- **SKU** - unique identifier of the item\n",
      "- **date** - date of web scraping; for all items - March 11, 2023\n",
      "- **description** - additional description, including product's brand, composition, and care instructions, in JSON format\n",
      "- **images** - photographs from the item description\n",
      "\n",
      "# 💴 Buy the Dataset: Leave a request on **[https://trainingdata.pro/datasets](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** to discuss your requirements, learn about the price and buy the dataset\n",
      "\n",
      "**[TrainingData](https://trainingdata.pro/datasets/marketplace-scraping-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=asos-e-commerce-dataset-30845-products)** provides high-quality data annotation tailored to your needs.\n",
      "\n",
      "*keywords: web scraping dataset, dataset marketplace, web scraping data, e-commerce dataset, e-commerce marketplace, e-commerce marketplace scraping dataset, e-commerce sales dataset, ecommerce clothing site, e-commerce user behavior dataset, e-commerce text dataset, e-commerce product dataset, text dataset, ratings, product recommendation, text classification, text mining dataset, text data*\n",
      "name 'detect' is not defined Language is not detected: bhavinmoriya/loan-csv\n",
      "name 'detect' is not defined Language is not detected: loan-csv\n",
      "name 'detect' is not defined Language is not detected: bhavinmoriya\n",
      "name 'detect' is not defined Language is not detected: Practical Statistics for Data Scientists\n",
      "name 'detect' is not defined Language is not detected: Autors: Andrew Bruce, Peter C. Bruce, Peter Gedeck\n",
      "name 'detect' is not defined Language is not detected: loan-csv\n",
      "name 'detect' is not defined Language is not detected: bhavinmoriya\n",
      "name 'detect' is not defined Language is not detected: Practical Statistics for Data Scientists\n",
      "name 'detect' is not defined Language is not detected: Autors: Andrew Bruce, Peter C. Bruce, Peter Gedeck\n",
      "name 'detect' is not defined Language is not detected: yekahaaagayeham/job-interview-assignments-test\n",
      "name 'detect' is not defined Language is not detected: job-interview-assignments-test\n",
      "name 'detect' is not defined Language is not detected: yekahaaagayeham\n",
      "name 'detect' is not defined Language is not detected: Job Interview Assignments test\n",
      "name 'detect' is not defined Language is not detected: This dataset contains collection of dataset asked in different job interviews\n",
      "name 'detect' is not defined Language is not detected: ## Task 1\n",
      "\n",
      "Business roles at AgroStar require a baseline of analytical skills, and it is also critical that we are able to explain complex concepts in a simple way to a variety of audiences. This test is structured so that someone with the baseline skills needed to succeed in the role should be able to complete this in under 4 hours without assistance.\n",
      "\n",
      "Use the data in the included sheet to address the following scenario...\n",
      "\n",
      "Since its inception, AgroStar has been leveraging an assisted marketplace model. Given that the market potential is huge and that the target customer appreciates a physical store nearby, we have taken a call to explore the offline retail model to drive growth. The primary objective is to get a larger wallet share for AgroStar among existing customers.\n",
      "\n",
      "**Assume you are back in time, in August 2018 and you have been asked to determine the location (taluka) of the first AgroStar offline retail store.**\n",
      "1. What are the key factors you would use to determine the location? Why?\n",
      "2. What taluka (across three states) would you look open in? Why?\n",
      "\n",
      "### Guidelines:\n",
      "-- (1) Please mention any assumptions you have made and the underlying thought process\n",
      "-- (2) Please treat the assignment as standalone (it should be self-explanatory to someone who reads it), but we will have a follow-up discussion with you in which we will walk through your approach to this assignment.\n",
      "-- (3) Mention any data that may be missing that would make this study more meaningful\n",
      "-- (4) `Kindly conduct your analysis within the spreadsheet, we would like to see the working sheet`. If you face any issues due to the file size, kindly download this file and share an excel sheet with us\n",
      "-- (5) If you would like to append a word document/presentation to summarize, please go ahead.\n",
      "-- (6) In case you use any external data source/article, kindly share the source.\n",
      "\n",
      "## Task 4 Cohort \n",
      "The file CDNOW_master.txt contains the entire purchase history up to the end of June 1998 of the cohort of 23,570 individuals who made their first-ever purchase at CDNOW in the first quarter of 1997. This CDNOW dataset was first used by Fader and Hardie (2001).\n",
      "\n",
      "Each record in this file, 69,659 in total, comprises four fields: the customer's ID, the date of the transaction, the number of CDs purchased, and the dollar value of the transaction.\n",
      "\n",
      "CustID = CDNOW_master(:,1); % customer id\n",
      "Date = CDNOW_master(:,2); % transaction date\n",
      "Quant = CDNOW_master(:,3); % number of CDs purchased\n",
      "Spend = CDNOW_master(:,4); % dollar value (excl. S&H)\n",
      "\n",
      "\n",
      "See \"Notes on the CDNOW Master Data Set\" (http://brucehardie.com/notes/026/) for details of how the 1/10th systematic sample (http://brucehardie.com/datasets/CDNOW_sample.zip) used in many papers was created. \n",
      "\n",
      "Reference:\n",
      "\n",
      "Fader, Peter S. and Bruce G.,S. Hardie, (2001), \"Forecasting Repeat Sales at CDNOW: A Case Study,\" Interfaces, 31 (May-June), Part 2 of 2, S94-S107.\n",
      "\n",
      "\n",
      "## Task 6 Zupee.csv\n",
      "\n",
      "I have merged all three datasets into one file and also did some feature engineering.    \n",
      "Available Data: You will be given anonymized user gameplay data in the form of 3 csv files.\n",
      "Fields in the data are as described below:\n",
      "Gameplay_Data.csv contains the following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Eventtime: DateTime on which user played the tournament\n",
      "*  Entry_Fee: Entry Fee of tournament\n",
      "*  Win_Loss: ‘W’ if the user won that particular tournament, ‘L’ otherwise\n",
      "* Winnings: How much money the user won in the tournament (0 for ‘L’)\n",
      "*  Tournament_Type: Type of tournament user played (A / B / C / D)\n",
      "*  Num_Players: Number of players that played in this tournament\n",
      "\n",
      "Wallet_Balance.csv contains following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Timestamp: DateTime at which user’s wallet balance is given\n",
      "*  Wallet_Balance: User’s wallet balance at given time stamp\n",
      "\n",
      "Demographic.csv contains following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Installed_At: Timestamp at which user installed the app\n",
      "*  Connection_Type: User’s internet connection type (Ex: Cellular / Dial Up)\n",
      "*  Cpu_Type: Cpu type of device that the user is playing with\n",
      "*  Network_Type: Network type in encoded form\n",
      "*  Device_Manufacturer: Ex: Realme\n",
      "*  ISP: Internet Service Provider. Ex: Airtel\n",
      "*  Country\n",
      "*  Country_Subdivision\n",
      "*  City\n",
      "*  Postal_Code\n",
      "* Language: Language that user has selected for gameplay\n",
      "*  Device_Name\n",
      "*  Device_Type\n",
      "\n",
      "**Build a basic recommendation system** which is able to rank/recommend relevant\n",
      "tournaments and entry prices to the user. The main objectives are:\n",
      "1. A user should not have to scroll too much before selecting a tournament of their\n",
      "preference\n",
      "2. We would like the user to play as high an entry fee tournament as possible\n",
      "name 'detect' is not defined Language is not detected: job-interview-assignments-test\n",
      "name 'detect' is not defined Language is not detected: yekahaaagayeham\n",
      "name 'detect' is not defined Language is not detected: Job Interview Assignments test\n",
      "name 'detect' is not defined Language is not detected: This dataset contains collection of dataset asked in different job interviews\n",
      "name 'detect' is not defined Language is not detected: ## Task 1\n",
      "\n",
      "Business roles at AgroStar require a baseline of analytical skills, and it is also critical that we are able to explain complex concepts in a simple way to a variety of audiences. This test is structured so that someone with the baseline skills needed to succeed in the role should be able to complete this in under 4 hours without assistance.\n",
      "\n",
      "Use the data in the included sheet to address the following scenario...\n",
      "\n",
      "Since its inception, AgroStar has been leveraging an assisted marketplace model. Given that the market potential is huge and that the target customer appreciates a physical store nearby, we have taken a call to explore the offline retail model to drive growth. The primary objective is to get a larger wallet share for AgroStar among existing customers.\n",
      "\n",
      "**Assume you are back in time, in August 2018 and you have been asked to determine the location (taluka) of the first AgroStar offline retail store.**\n",
      "1. What are the key factors you would use to determine the location? Why?\n",
      "2. What taluka (across three states) would you look open in? Why?\n",
      "\n",
      "### Guidelines:\n",
      "-- (1) Please mention any assumptions you have made and the underlying thought process\n",
      "-- (2) Please treat the assignment as standalone (it should be self-explanatory to someone who reads it), but we will have a follow-up discussion with you in which we will walk through your approach to this assignment.\n",
      "-- (3) Mention any data that may be missing that would make this study more meaningful\n",
      "-- (4) `Kindly conduct your analysis within the spreadsheet, we would like to see the working sheet`. If you face any issues due to the file size, kindly download this file and share an excel sheet with us\n",
      "-- (5) If you would like to append a word document/presentation to summarize, please go ahead.\n",
      "-- (6) In case you use any external data source/article, kindly share the source.\n",
      "\n",
      "## Task 4 Cohort \n",
      "The file CDNOW_master.txt contains the entire purchase history up to the end of June 1998 of the cohort of 23,570 individuals who made their first-ever purchase at CDNOW in the first quarter of 1997. This CDNOW dataset was first used by Fader and Hardie (2001).\n",
      "\n",
      "Each record in this file, 69,659 in total, comprises four fields: the customer's ID, the date of the transaction, the number of CDs purchased, and the dollar value of the transaction.\n",
      "\n",
      "CustID = CDNOW_master(:,1); % customer id\n",
      "Date = CDNOW_master(:,2); % transaction date\n",
      "Quant = CDNOW_master(:,3); % number of CDs purchased\n",
      "Spend = CDNOW_master(:,4); % dollar value (excl. S&H)\n",
      "\n",
      "\n",
      "See \"Notes on the CDNOW Master Data Set\" (http://brucehardie.com/notes/026/) for details of how the 1/10th systematic sample (http://brucehardie.com/datasets/CDNOW_sample.zip) used in many papers was created. \n",
      "\n",
      "Reference:\n",
      "\n",
      "Fader, Peter S. and Bruce G.,S. Hardie, (2001), \"Forecasting Repeat Sales at CDNOW: A Case Study,\" Interfaces, 31 (May-June), Part 2 of 2, S94-S107.\n",
      "\n",
      "\n",
      "## Task 6 Zupee.csv\n",
      "\n",
      "I have merged all three datasets into one file and also did some feature engineering.    \n",
      "Available Data: You will be given anonymized user gameplay data in the form of 3 csv files.\n",
      "Fields in the data are as described below:\n",
      "Gameplay_Data.csv contains the following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Eventtime: DateTime on which user played the tournament\n",
      "*  Entry_Fee: Entry Fee of tournament\n",
      "*  Win_Loss: ‘W’ if the user won that particular tournament, ‘L’ otherwise\n",
      "* Winnings: How much money the user won in the tournament (0 for ‘L’)\n",
      "*  Tournament_Type: Type of tournament user played (A / B / C / D)\n",
      "*  Num_Players: Number of players that played in this tournament\n",
      "\n",
      "Wallet_Balance.csv contains following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Timestamp: DateTime at which user’s wallet balance is given\n",
      "*  Wallet_Balance: User’s wallet balance at given time stamp\n",
      "\n",
      "Demographic.csv contains following fields:\n",
      "*  Uid: Alphanumeric unique Id assigned to user\n",
      "*  Installed_At: Timestamp at which user installed the app\n",
      "*  Connection_Type: User’s internet connection type (Ex: Cellular / Dial Up)\n",
      "*  Cpu_Type: Cpu type of device that the user is playing with\n",
      "*  Network_Type: Network type in encoded form\n",
      "*  Device_Manufacturer: Ex: Realme\n",
      "*  ISP: Internet Service Provider. Ex: Airtel\n",
      "*  Country\n",
      "*  Country_Subdivision\n",
      "*  City\n",
      "*  Postal_Code\n",
      "* Language: Language that user has selected for gameplay\n",
      "*  Device_Name\n",
      "*  Device_Type\n",
      "\n",
      "**Build a basic recommendation system** which is able to rank/recommend relevant\n",
      "tournaments and entry prices to the user. The main objectives are:\n",
      "1. A user should not have to scroll too much before selecting a tournament of their\n",
      "preference\n",
      "2. We would like the user to play as high an entry fee tournament as possible\n",
      "name 'detect' is not defined Language is not detected: olumayowaoyeyinka/amazon-electronics-products\n",
      "name 'detect' is not defined Language is not detected: amazon-electronics-products\n",
      "name 'detect' is not defined Language is not detected: olumayowaoyeyinka\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics Products\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "**Context**\n",
      "This dataset was used in a data science contest by Capgemini. The main concept was to build a product recommendation system.\n",
      "\n",
      "**Content**\n",
      "The dataset gives us electronics sales data at Amazon. It contains user ratings for various electronics items sold, along with category of each item and time of sell.\n",
      "\n",
      "**Acknowledgements**\n",
      "The dataset is shared at\n",
      "*[https://www.techgig.com/files/contest_upload_files/electronics.csv](url)*\n",
      "\n",
      "**Inspiration**\n",
      "The dataset is a great start for building a product recommendation system based on user ratings, brands, and timestamps.\n",
      "name 'detect' is not defined Language is not detected: amazon-electronics-products\n",
      "name 'detect' is not defined Language is not detected: olumayowaoyeyinka\n",
      "name 'detect' is not defined Language is not detected: Amazon Electronics Products\n",
      "name 'detect' is not defined Language is not detected: **About Dataset**\n",
      "**Context**\n",
      "This dataset was used in a data science contest by Capgemini. The main concept was to build a product recommendation system.\n",
      "\n",
      "**Content**\n",
      "The dataset gives us electronics sales data at Amazon. It contains user ratings for various electronics items sold, along with category of each item and time of sell.\n",
      "\n",
      "**Acknowledgements**\n",
      "The dataset is shared at\n",
      "*[https://www.techgig.com/files/contest_upload_files/electronics.csv](url)*\n",
      "\n",
      "**Inspiration**\n",
      "The dataset is a great start for building a product recommendation system based on user ratings, brands, and timestamps.\n",
      "name 'detect' is not defined Language is not detected: mobasshir/restaurantrecommendationdata\n",
      "name 'detect' is not defined Language is not detected: restaurantrecommendationdata\n",
      "name 'detect' is not defined Language is not detected: mobasshir\n",
      "name 'detect' is not defined Language is not detected: restaurant-recommendation-data\n",
      "name 'detect' is not defined Language is not detected: Restaurnt order data with customer and restaurant locations and other behavior\n",
      "name 'detect' is not defined Language is not detected: restaurantrecommendationdata\n",
      "name 'detect' is not defined Language is not detected: mobasshir\n",
      "name 'detect' is not defined Language is not detected: restaurant-recommendation-data\n",
      "name 'detect' is not defined Language is not detected: Restaurnt order data with customer and restaurant locations and other behavior\n",
      "name 'detect' is not defined Language is not detected: dylanjcastillo/7k-books-with-metadata\n",
      "name 'detect' is not defined Language is not detected: 7k-books-with-metadata\n",
      "name 'detect' is not defined Language is not detected: dylanjcastillo\n",
      "name 'detect' is not defined Language is not detected: 7k Books\n",
      "name 'detect' is not defined Language is not detected: Dataset of books with title, author, description, rating, thumbnail, and more\n",
      "name 'detect' is not defined Language is not detected: ### Do we really need another dataset of books?\n",
      "\n",
      "My initial plan was to build a toy example for a recommender system article I was writing. After a bit of googling, I found a few datasets. Sadly, most of them had some issues that made them unusable for me (e.g, missing description of the book, a mix of different languages but no column to specify the language per row or weird delimiters). \n",
      "\n",
      "So I decided to make a dataset that would match my purposes.\n",
      "\n",
      "First, I got ISBNs from [Soumik's Goodreads-books dataset](https://www.kaggle.com/jealousleopard/goodreadsbooks). Using those identifiers, I crawled the Google Books API to extract the books' information.\n",
      "\n",
      "Then, I merged those results with some of the original columns from the dataset and after some cleaning I got the dataset you see here.\n",
      "\n",
      "### What can I do with this?\n",
      "Different Exploratory Data Analysis, clustering of books by topics/category, content-based recommendation engine using different fields from the book's description. \n",
      "\n",
      "### Why is this dataset smaller than Soumik's Goodreads-books?\n",
      "Many of the ISBNs of that dataset did not return valid results from the Google Books API. I plan to update this in the future, using more fields (e.g., title, author) in the API requests, as to have a bigger dataset.\n",
      "\n",
      "### What did you use to build this dataset?\n",
      "Check out the repoistory here [Google Books Crawler](https://github.com/dylanjcastillo/google_books_crawler)\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset relied heavily on [Soumik's Goodreads-books dataset](https://www.kaggle.com/jealousleopard/goodreadsbooks).\n",
      "name 'detect' is not defined Language is not detected: 7k-books-with-metadata\n",
      "name 'detect' is not defined Language is not detected: dylanjcastillo\n",
      "name 'detect' is not defined Language is not detected: 7k Books\n",
      "name 'detect' is not defined Language is not detected: Dataset of books with title, author, description, rating, thumbnail, and more\n",
      "name 'detect' is not defined Language is not detected: ### Do we really need another dataset of books?\n",
      "\n",
      "My initial plan was to build a toy example for a recommender system article I was writing. After a bit of googling, I found a few datasets. Sadly, most of them had some issues that made them unusable for me (e.g, missing description of the book, a mix of different languages but no column to specify the language per row or weird delimiters). \n",
      "\n",
      "So I decided to make a dataset that would match my purposes.\n",
      "\n",
      "First, I got ISBNs from [Soumik's Goodreads-books dataset](https://www.kaggle.com/jealousleopard/goodreadsbooks). Using those identifiers, I crawled the Google Books API to extract the books' information.\n",
      "\n",
      "Then, I merged those results with some of the original columns from the dataset and after some cleaning I got the dataset you see here.\n",
      "\n",
      "### What can I do with this?\n",
      "Different Exploratory Data Analysis, clustering of books by topics/category, content-based recommendation engine using different fields from the book's description. \n",
      "\n",
      "### Why is this dataset smaller than Soumik's Goodreads-books?\n",
      "Many of the ISBNs of that dataset did not return valid results from the Google Books API. I plan to update this in the future, using more fields (e.g., title, author) in the API requests, as to have a bigger dataset.\n",
      "\n",
      "### What did you use to build this dataset?\n",
      "Check out the repoistory here [Google Books Crawler](https://github.com/dylanjcastillo/google_books_crawler)\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "This dataset relied heavily on [Soumik's Goodreads-books dataset](https://www.kaggle.com/jealousleopard/goodreadsbooks).\n",
      "name 'detect' is not defined Language is not detected: suyogya/recommendationmovieviacorrelation\n",
      "name 'detect' is not defined Language is not detected: recommendationmovieviacorrelation\n",
      "name 'detect' is not defined Language is not detected: suyogya\n",
      "name 'detect' is not defined Language is not detected: RecommendationMovieViaCorrelation\n",
      "name 'detect' is not defined Language is not detected: recommendationmovieviacorrelation\n",
      "name 'detect' is not defined Language is not detected: suyogya\n",
      "name 'detect' is not defined Language is not detected: RecommendationMovieViaCorrelation\n",
      "name 'detect' is not defined Language is not detected: debashis74017/nifty-50-minute-data\n",
      "name 'detect' is not defined Language is not detected: nifty-50-minute-data\n",
      "name 'detect' is not defined Language is not detected: debashis74017\n",
      "name 'detect' is not defined Language is not detected: NSE - Nifty 50 Index Minute data (2015 to 2024)\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 index (OHLC) data for 1min data\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "- The NIFTY 50 is a well-diversified 50 stock index and it represents 13 important sectors of the economy.  \n",
      "- It is used for a variety of purposes such as benchmarking fund portfolios, index-based derivatives, and index funds. \n",
      "- NIFTY 50 is owned and managed by NSE Indices Limited.\n",
      "- The NIFTY 50 index has shaped up to be the largest single financial product in India. \n",
      "\n",
      "## File Information and Column Descriptons\n",
      "\n",
      "Nifty 50 index data with **1 minute, 3 minutes, 5 minutes,  15 minutes, 1 hour, and daily data**. The dataset contains OHLC (Open, High, Low, and Close) prices from Jan 2015 to Jan 2022. \n",
      "- This dataset can be used for time series analysis, regression problems, and time series forecasting both for one step and multi-step ahead in the future. \n",
      "- Options data can be integrated with this minute data, to get more insight about this data. \n",
      "- Different backtesting strategies can be built on this data. \n",
      "\n",
      "## **File Information**\n",
      "- This dataset contains 6 files, each file contains nifty 50 data with different intervals. \n",
      "- Different intervals are - 1 min, 3 min, 5 min, 15 min, and 1 hour, Daily data from intervals of 2015 Jan to 2022 Feb. \n",
      "\n",
      "## **Column Descriptors**\n",
      "- Each file contains OHLC (Open, High, Low, and Close) prices and Data time information. Since these are Nifty 50 index data, so volume is not present. \n",
      "\n",
      "## **Inspiration**\n",
      "### Time series forecasting - Predict stock price\n",
      "- Predict future stock price one step ahead and multi-step ahead in time. \n",
      "- Use different time series forecasting techniques for forecasting the future stock price. \n",
      "### Machine learning and Deep learning techniques\n",
      "- Possible ML and DL models include Neural networks, RNNs, LSTMs, Transformers, Attention networks, etc. \n",
      "- Different error functions can be considered like RMSE, MAE, RMSEP etc. \n",
      "### Feature engineering\n",
      "-  Different augmented features can be created and that can be used for forecasting. \n",
      "- Correlation analysis, Feature importance to justify the important features.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: nifty-50-minute-data\n",
      "name 'detect' is not defined Language is not detected: debashis74017\n",
      "name 'detect' is not defined Language is not detected: NSE - Nifty 50 Index Minute data (2015 to 2024)\n",
      "name 'detect' is not defined Language is not detected: Nifty 50 index (OHLC) data for 1min data\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "- The NIFTY 50 is a well-diversified 50 stock index and it represents 13 important sectors of the economy.  \n",
      "- It is used for a variety of purposes such as benchmarking fund portfolios, index-based derivatives, and index funds. \n",
      "- NIFTY 50 is owned and managed by NSE Indices Limited.\n",
      "- The NIFTY 50 index has shaped up to be the largest single financial product in India. \n",
      "\n",
      "## File Information and Column Descriptons\n",
      "\n",
      "Nifty 50 index data with **1 minute, 3 minutes, 5 minutes,  15 minutes, 1 hour, and daily data**. The dataset contains OHLC (Open, High, Low, and Close) prices from Jan 2015 to Jan 2022. \n",
      "- This dataset can be used for time series analysis, regression problems, and time series forecasting both for one step and multi-step ahead in the future. \n",
      "- Options data can be integrated with this minute data, to get more insight about this data. \n",
      "- Different backtesting strategies can be built on this data. \n",
      "\n",
      "## **File Information**\n",
      "- This dataset contains 6 files, each file contains nifty 50 data with different intervals. \n",
      "- Different intervals are - 1 min, 3 min, 5 min, 15 min, and 1 hour, Daily data from intervals of 2015 Jan to 2022 Feb. \n",
      "\n",
      "## **Column Descriptors**\n",
      "- Each file contains OHLC (Open, High, Low, and Close) prices and Data time information. Since these are Nifty 50 index data, so volume is not present. \n",
      "\n",
      "## **Inspiration**\n",
      "### Time series forecasting - Predict stock price\n",
      "- Predict future stock price one step ahead and multi-step ahead in time. \n",
      "- Use different time series forecasting techniques for forecasting the future stock price. \n",
      "### Machine learning and Deep learning techniques\n",
      "- Possible ML and DL models include Neural networks, RNNs, LSTMs, Transformers, Attention networks, etc. \n",
      "- Different error functions can be considered like RMSE, MAE, RMSEP etc. \n",
      "### Feature engineering\n",
      "-  Different augmented features can be created and that can be used for forecasting. \n",
      "- Correlation analysis, Feature importance to justify the important features.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: debashis74017/stock-market-index-data-india-1990-2022\n",
      "name 'detect' is not defined Language is not detected: stock-market-index-data-india-1990-2022\n",
      "name 'detect' is not defined Language is not detected: debashis74017\n",
      "name 'detect' is not defined Language is not detected: Stock Market Index Data India (1990 - 2022)\n",
      "name 'detect' is not defined Language is not detected: Historical daily prices of Indian stock indices and VIX data\n",
      "name 'detect' is not defined Language is not detected: **Disclaimer!!!**\n",
      "Data uploaded here are collected from the internet. The sole purposes of uploading these data are to provide this Kaggle community with a good source of data for analysis and research. I don't own these datasets and am also not responsible for them legally by any means. I am not charging anything (either monetary or any favor) for this dataset. \n",
      "\n",
      "## **Overview**\n",
      "\n",
      "This dataset contains historical daily prices for indices currently trading on the Indian Stock Market. The historical data are retrieved from the NSE India website. Daily gold price from 1979 to 2022 in INR is uploaded here. \n",
      "\n",
      "- **Premier**\n",
      "    - PE, P/B, Div Yield Data. \n",
      "    - Gold price to INR Data. \n",
      "\n",
      "## **Content**\n",
      "This data contains daily OHLC data for all indices in NSE from 1990 to 2022. Along with Indices OHLC data, there are PE(Price to Earning ratio), P/B (Price to book value), and Dividend Yield data also available for all indices. \n",
      "Lastly, Volatility Index (VIX) data is also available from 1990 to 2022. \n",
      "\n",
      "For Example - \n",
      "- \"Nifty 50 data\" contains the below columns: \n",
      "    1. Date - Date of observation\n",
      "    2. Open - Open price of the index on a particular day\n",
      "    3. High - High price of the index on a particular day\n",
      "    4. Low - Low price of the index on a particular day\n",
      "    5. Close - Close price of the index on a particular day\n",
      "- \"NIFTY 50 - HistoricalPE_PBDIV_Data\" contains the below columns: \n",
      "    1. Date - Date of observation\n",
      "    2. P/E - Price to Earnings Ratio\n",
      "    3. P/B - Price-to-book value\n",
      "    4. Div Yield % - Dividend Yield = Cash Dividend per share / Market Price per share * 100\n",
      "\n",
      "- \"Gold price INR.csv\" contains Date and Gold price (INR per troy ounce). \n",
      "Where 1 Troy ounce = 31.1035 gram\n",
      "\n",
      "The list of indices is: \n",
      "1.   NIFTY 50                               \n",
      "2.  NIFTY 100             \n",
      "3.  NIFTY BANK                         \n",
      "4.  NIFTY COMMODITIES \n",
      "5.  NIFTY ENERGY                      \n",
      "6.  NIFTY FMCG \n",
      "7.  NIFTY HOUSING                    \n",
      "8.  NIFTY INDIA MANUFACTURING \n",
      "9.  NIFTY INFRASTRUCTURE     \n",
      "10. NIFTY IT\n",
      "11.  NIFTY MEDIA                          \n",
      "12.  NIFTY METAL \n",
      "13.  NIFTY MIDCAP 100                \n",
      "14.  NIFTY NEXT 50\n",
      "15.  NIFTY OIL & GAS                    \n",
      "16.  NIFTY PHARMA \n",
      "17.  NIFTY PRIVATE BANK\n",
      "18.  NIFTY PSU BANK\n",
      "19. NIFTY AUTO\n",
      "20.  VIX History                               \n",
      "\n",
      "##**Inspiration**\n",
      "- Data is uploaded for Research and Educational purposes. \n",
      "- The data scientists and researchers can download any index OHLC data, along with P/B, PE, and Dividend Yield values and VIX data. \n",
      "- Even Gold prices can help researchers to get more insight into their investment decisions. \n",
      "- A time series forecasting for future index price, based on multiple features along with OHLC data and P/B, P/E, and Div Yield percentage, VIX. \n",
      "\n",
      "\n",
      "##**Data Source**\n",
      "For the gold price - https://gold.org\n",
      "For stock indices - https://www.niftyindices.com/reports/historical-data\n",
      "name 'detect' is not defined Language is not detected: stock-market-index-data-india-1990-2022\n",
      "name 'detect' is not defined Language is not detected: debashis74017\n",
      "name 'detect' is not defined Language is not detected: Stock Market Index Data India (1990 - 2022)\n",
      "name 'detect' is not defined Language is not detected: Historical daily prices of Indian stock indices and VIX data\n",
      "name 'detect' is not defined Language is not detected: **Disclaimer!!!**\n",
      "Data uploaded here are collected from the internet. The sole purposes of uploading these data are to provide this Kaggle community with a good source of data for analysis and research. I don't own these datasets and am also not responsible for them legally by any means. I am not charging anything (either monetary or any favor) for this dataset. \n",
      "\n",
      "## **Overview**\n",
      "\n",
      "This dataset contains historical daily prices for indices currently trading on the Indian Stock Market. The historical data are retrieved from the NSE India website. Daily gold price from 1979 to 2022 in INR is uploaded here. \n",
      "\n",
      "- **Premier**\n",
      "    - PE, P/B, Div Yield Data. \n",
      "    - Gold price to INR Data. \n",
      "\n",
      "## **Content**\n",
      "This data contains daily OHLC data for all indices in NSE from 1990 to 2022. Along with Indices OHLC data, there are PE(Price to Earning ratio), P/B (Price to book value), and Dividend Yield data also available for all indices. \n",
      "Lastly, Volatility Index (VIX) data is also available from 1990 to 2022. \n",
      "\n",
      "For Example - \n",
      "- \"Nifty 50 data\" contains the below columns: \n",
      "    1. Date - Date of observation\n",
      "    2. Open - Open price of the index on a particular day\n",
      "    3. High - High price of the index on a particular day\n",
      "    4. Low - Low price of the index on a particular day\n",
      "    5. Close - Close price of the index on a particular day\n",
      "- \"NIFTY 50 - HistoricalPE_PBDIV_Data\" contains the below columns: \n",
      "    1. Date - Date of observation\n",
      "    2. P/E - Price to Earnings Ratio\n",
      "    3. P/B - Price-to-book value\n",
      "    4. Div Yield % - Dividend Yield = Cash Dividend per share / Market Price per share * 100\n",
      "\n",
      "- \"Gold price INR.csv\" contains Date and Gold price (INR per troy ounce). \n",
      "Where 1 Troy ounce = 31.1035 gram\n",
      "\n",
      "The list of indices is: \n",
      "1.   NIFTY 50                               \n",
      "2.  NIFTY 100             \n",
      "3.  NIFTY BANK                         \n",
      "4.  NIFTY COMMODITIES \n",
      "5.  NIFTY ENERGY                      \n",
      "6.  NIFTY FMCG \n",
      "7.  NIFTY HOUSING                    \n",
      "8.  NIFTY INDIA MANUFACTURING \n",
      "9.  NIFTY INFRASTRUCTURE     \n",
      "10. NIFTY IT\n",
      "11.  NIFTY MEDIA                          \n",
      "12.  NIFTY METAL \n",
      "13.  NIFTY MIDCAP 100                \n",
      "14.  NIFTY NEXT 50\n",
      "15.  NIFTY OIL & GAS                    \n",
      "16.  NIFTY PHARMA \n",
      "17.  NIFTY PRIVATE BANK\n",
      "18.  NIFTY PSU BANK\n",
      "19. NIFTY AUTO\n",
      "20.  VIX History                               \n",
      "\n",
      "##**Inspiration**\n",
      "- Data is uploaded for Research and Educational purposes. \n",
      "- The data scientists and researchers can download any index OHLC data, along with P/B, PE, and Dividend Yield values and VIX data. \n",
      "- Even Gold prices can help researchers to get more insight into their investment decisions. \n",
      "- A time series forecasting for future index price, based on multiple features along with OHLC data and P/B, P/E, and Div Yield percentage, VIX. \n",
      "\n",
      "\n",
      "##**Data Source**\n",
      "For the gold price - https://gold.org\n",
      "For stock indices - https://www.niftyindices.com/reports/historical-data\n",
      "name 'detect' is not defined Language is not detected: dougbr/movie-recommendation-with-movielens\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-with-movielens\n",
      "name 'detect' is not defined Language is not detected: dougbr\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation with Movielens\n",
      "name 'detect' is not defined Language is not detected: Using the Graph Database Neo4j\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-with-movielens\n",
      "name 'detect' is not defined Language is not detected: dougbr\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation with Movielens\n",
      "name 'detect' is not defined Language is not detected: Using the Graph Database Neo4j\n",
      "name 'detect' is not defined Language is not detected: prasad22/global-economy-indicators\n",
      "name 'detect' is not defined Language is not detected: global-economy-indicators\n",
      "name 'detect' is not defined Language is not detected: prasad22\n",
      "name 'detect' is not defined Language is not detected: Global Economy Indicators\n",
      "name 'detect' is not defined Language is not detected: World finance indicators data from 1970-2021\n",
      "name 'detect' is not defined Language is not detected: ## Data Set Information:\n",
      "\n",
      "The dataset is compiled from the **`National Accounts Main Aggregates Database`** that presents a series of analytical national accounts tables from 1970 onwards for more than 200 countries and areas of the world. It is the product of a global cooperation effort between the Economic Statistics Branch of the United Nations Statistics Division, international statistical agencies, and the national statistical services of these countries and is developed in accordance with the recommendation of the Statistical Commission at its first session in 1947 that the Statistics Division should publish regularly the most recent available data on national accounts for as many countries and areas as possible. \n",
      "\n",
      "This dataset can be used to perform clustering, regression, and time series tasks.\n",
      " \n",
      "name 'detect' is not defined Language is not detected: global-economy-indicators\n",
      "name 'detect' is not defined Language is not detected: prasad22\n",
      "name 'detect' is not defined Language is not detected: Global Economy Indicators\n",
      "name 'detect' is not defined Language is not detected: World finance indicators data from 1970-2021\n",
      "name 'detect' is not defined Language is not detected: ## Data Set Information:\n",
      "\n",
      "The dataset is compiled from the **`National Accounts Main Aggregates Database`** that presents a series of analytical national accounts tables from 1970 onwards for more than 200 countries and areas of the world. It is the product of a global cooperation effort between the Economic Statistics Branch of the United Nations Statistics Division, international statistical agencies, and the national statistical services of these countries and is developed in accordance with the recommendation of the Statistical Commission at its first session in 1947 that the Statistics Division should publish regularly the most recent available data on national accounts for as many countries and areas as possible. \n",
      "\n",
      "This dataset can be used to perform clustering, regression, and time series tasks.\n",
      " \n",
      "name 'detect' is not defined Language is not detected: yasirabdaali/tmdb-8500-top-rated-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: tmdb-8500-top-rated-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: TMDb (8500) top rated movies dataset\n",
      "name 'detect' is not defined Language is not detected: Contains data of top 8560 TMDb movies.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the top 8500 TMDb movies including the id, title, release date,avg vote,vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "You can use this data to make movies recommendation systems furthermore you can get the postures of the movies using the id and display it in your website.\n",
      "name 'detect' is not defined Language is not detected: tmdb-8500-top-rated-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: TMDb (8500) top rated movies dataset\n",
      "name 'detect' is not defined Language is not detected: Contains data of top 8560 TMDb movies.\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the top 8500 TMDb movies including the id, title, release date,avg vote,vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "You can use this data to make movies recommendation systems furthermore you can get the postures of the movies using the id and display it in your website.\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali/stack-overflow-developer-survey-20112022\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survey-20112022\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survey 2011-2022\n",
      "name 'detect' is not defined Language is not detected: Dataset of the Stack Overflow Developer Survey from 2011 to 2022\n",
      "name 'detect' is not defined Language is not detected: This dataset has the  The Stack Overflow Developer Survey data from 2011 to 2022.\n",
      "Although the Stack Overflow Survey datasets are already on kaggle except the the data of the\n",
      "year 2022. But i have combined all of them together for your convenience.\n",
      "Hope you will enjoy the Dataset.\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: stack-overflow-developer-survey-20112022\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: Stack Overflow Developer Survey 2011-2022\n",
      "name 'detect' is not defined Language is not detected: Dataset of the Stack Overflow Developer Survey from 2011 to 2022\n",
      "name 'detect' is not defined Language is not detected: This dataset has the  The Stack Overflow Developer Survey data from 2011 to 2022.\n",
      "Although the Stack Overflow Survey datasets are already on kaggle except the the data of the\n",
      "year 2022. But i have combined all of them together for your convenience.\n",
      "Hope you will enjoy the Dataset.\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali/tmdb-20000-trending-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: tmdb-20000-trending-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: TMDb 20000 Trending Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset of 20k Movies Trending on TMDb\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the 20000 movies Trending  on TMDb including the id, title, release date,avg vote,vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "You can use this data to make movies recommendation systems furthermore you can get the postures of the movies using the id and display it in your website.\n",
      "Here is what you can do using this dataset:\n",
      "1. Get the genres using the genre id from TMDb database and replace the genre id with genre(text)\n",
      "2. Get the credits of the movies using the movie id from TMDb database \n",
      "3. Create a movie recommendation system based on the genre,overview, actors and director.\n",
      "name 'detect' is not defined Language is not detected: tmdb-20000-trending-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: yasirabdaali\n",
      "name 'detect' is not defined Language is not detected: TMDb 20000 Trending Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset of 20k Movies Trending on TMDb\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the 20000 movies Trending  on TMDb including the id, title, release date,avg vote,vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "You can use this data to make movies recommendation systems furthermore you can get the postures of the movies using the id and display it in your website.\n",
      "Here is what you can do using this dataset:\n",
      "1. Get the genres using the genre id from TMDb database and replace the genre id with genre(text)\n",
      "2. Get the credits of the movies using the movie id from TMDb database \n",
      "3. Create a movie recommendation system based on the genre,overview, actors and director.\n",
      "name 'detect' is not defined Language is not detected: shubham2703/recipe-review-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: recipe-review-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: shubham2703\n",
      "name 'detect' is not defined Language is not detected: User Recipe Rating\n",
      "name 'detect' is not defined Language is not detected: Unveiling Culinary Insights: Exploring Recipe Reviews and User Interactions\n",
      "name 'detect' is not defined Language is not detected: 🌟 **User Recipe Rating Dataset Overview** 🌟\n",
      "\n",
      "The \"User Recipe Rating\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as:\n",
      "\n",
      "- Recipe name 🍲\n",
      "- Ranking on the top 100 recipes list 🏆\n",
      "- Unique recipe code 🆔\n",
      "- User details:\n",
      "  - User ID 👤\n",
      "  - User name 🧑‍🍳\n",
      "  - Internal user reputation score ⭐\n",
      "\n",
      "Each review comment is uniquely identified with a comment ID and comes with additional attributes, including:\n",
      "\n",
      "- Creation timestamp 🕒\n",
      "- Reply count 💬\n",
      "- Number of up-votes and down-votes received 👍👎\n",
      "\n",
      "Users' sentiment towards recipes is quantified on a 1 to 5-star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in:\n",
      "\n",
      "- Sentiment analysis 📊\n",
      "- User behavior analysis 🤔\n",
      "- Recipe recommendation systems 🍽️\n",
      "- And more! 🚀\n",
      "\n",
      "It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain. 🌐\n",
      "name 'detect' is not defined Language is not detected: recipe-review-and-user-feedback-dataset\n",
      "name 'detect' is not defined Language is not detected: shubham2703\n",
      "name 'detect' is not defined Language is not detected: User Recipe Rating\n",
      "name 'detect' is not defined Language is not detected: Unveiling Culinary Insights: Exploring Recipe Reviews and User Interactions\n",
      "name 'detect' is not defined Language is not detected: 🌟 **User Recipe Rating Dataset Overview** 🌟\n",
      "\n",
      "The \"User Recipe Rating\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as:\n",
      "\n",
      "- Recipe name 🍲\n",
      "- Ranking on the top 100 recipes list 🏆\n",
      "- Unique recipe code 🆔\n",
      "- User details:\n",
      "  - User ID 👤\n",
      "  - User name 🧑‍🍳\n",
      "  - Internal user reputation score ⭐\n",
      "\n",
      "Each review comment is uniquely identified with a comment ID and comes with additional attributes, including:\n",
      "\n",
      "- Creation timestamp 🕒\n",
      "- Reply count 💬\n",
      "- Number of up-votes and down-votes received 👍👎\n",
      "\n",
      "Users' sentiment towards recipes is quantified on a 1 to 5-star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in:\n",
      "\n",
      "- Sentiment analysis 📊\n",
      "- User behavior analysis 🤔\n",
      "- Recipe recommendation systems 🍽️\n",
      "- And more! 🚀\n",
      "\n",
      "It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain. 🌐\n",
      "name 'detect' is not defined Language is not detected: reverie5/womenintheloop-data-science-hackathon\n",
      "name 'detect' is not defined Language is not detected: womenintheloop-data-science-hackathon\n",
      "name 'detect' is not defined Language is not detected: reverie5\n",
      "name 'detect' is not defined Language is not detected: Women-in-the-loop Data Science Hackathon\n",
      "name 'detect' is not defined Language is not detected: LearnX Sales Forecasting\n",
      "name 'detect' is not defined Language is not detected: ### LearnX Sales Forecasting\n",
      "\n",
      "LearnX is an online learning platform aimed at professionals and students. LearnX serves as a market place that allows instructors to build online courses on topics of their expertise which is later published after due diligence by the LearnX team. The platform covers a wide variety of topics including Development, Business, Finance & Accounting & Software Marketing and so on\n",
      "\n",
      "Effective forecasting for course sales gives essential insight into upcoming cash flow meaning business can more accurately plan the budget to pay instructors and other operational costs and invest in the expansion of the business.\n",
      "\n",
      "Sales data for more than 2 years from 600 courses of LearnX's top domains is available along with information on\n",
      "\n",
      "    * Competition in the market for each course\n",
      "    * Course Type (Course/Program/Degree)\n",
      "    * Holiday Information for each day\n",
      "    * User Traffic on Course Page for each day\n",
      "\n",
      "Your task is to predict the course sales for each course in the test set for the next 60 days\n",
      "\n",
      "### Evaluation\n",
      "The evaluation metric for this competition is 1000*RMSLE where RMSLE is Root of Mean Squared Logarithmic Error across all entries in the test set.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: womenintheloop-data-science-hackathon\n",
      "name 'detect' is not defined Language is not detected: reverie5\n",
      "name 'detect' is not defined Language is not detected: Women-in-the-loop Data Science Hackathon\n",
      "name 'detect' is not defined Language is not detected: LearnX Sales Forecasting\n",
      "name 'detect' is not defined Language is not detected: ### LearnX Sales Forecasting\n",
      "\n",
      "LearnX is an online learning platform aimed at professionals and students. LearnX serves as a market place that allows instructors to build online courses on topics of their expertise which is later published after due diligence by the LearnX team. The platform covers a wide variety of topics including Development, Business, Finance & Accounting & Software Marketing and so on\n",
      "\n",
      "Effective forecasting for course sales gives essential insight into upcoming cash flow meaning business can more accurately plan the budget to pay instructors and other operational costs and invest in the expansion of the business.\n",
      "\n",
      "Sales data for more than 2 years from 600 courses of LearnX's top domains is available along with information on\n",
      "\n",
      "    * Competition in the market for each course\n",
      "    * Course Type (Course/Program/Degree)\n",
      "    * Holiday Information for each day\n",
      "    * User Traffic on Course Page for each day\n",
      "\n",
      "Your task is to predict the course sales for each course in the test set for the next 60 days\n",
      "\n",
      "### Evaluation\n",
      "The evaluation metric for this competition is 1000*RMSLE where RMSLE is Root of Mean Squared Logarithmic Error across all entries in the test set.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: shubhambathwal/flight-price-prediction\n",
      "name 'detect' is not defined Language is not detected: flight-price-prediction\n",
      "name 'detect' is not defined Language is not detected: shubhambathwal\n",
      "name 'detect' is not defined Language is not detected: Flight Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict Fllight Price, practise feature engineering, implement ensemble models\n",
      "name 'detect' is not defined Language is not detected: ###INTRODUCTION\n",
      "The objective of the study is to analyse the flight booking dataset obtained from “Ease My Trip” website and to conduct various statistical hypothesis tests in order to get meaningful information from it. The 'Linear Regression' statistical algorithm would be used to train the dataset and predict a continuous target variable. 'Easemytrip' is an internet platform for booking flight tickets, and hence a platform that potential passengers use to buy tickets. A thorough study of the data will aid in the discovery of valuable insights that will be of enormous value to passengers.\n",
      "\n",
      "###Research Questions\n",
      "The aim of our study is to answer the below research questions:\n",
      "a) Does price vary with Airlines?\n",
      "b) How is the price affected when tickets are bought in just 1 or 2 days before departure?\n",
      "c) Does ticket price change based on the departure time and arrival time?\n",
      "d) How the price changes with change in Source and Destination?\n",
      "e) How does the ticket price vary between Economy and Business class?\n",
      "\n",
      "\n",
      "###DATA COLLECTION AND METHODOLOGY\n",
      "Octoparse scraping tool was used to extract data from the website. Data was collected in two parts: one for economy class tickets and another for business class tickets. A total of 300261 distinct flight booking options was extracted from the site. Data was collected for 50 days, from February 11th to March 31st, 2022.\n",
      "Data source was secondary data and was collected from Ease my trip website.\n",
      "\n",
      "\n",
      "###DATASET\n",
      "Dataset contains information about flight booking options from the website Easemytrip for flight travel between India's top 6 metro cities. There are 300261 datapoints and 11 features in the cleaned dataset.\n",
      "\n",
      "###FEATURES\n",
      "The various features of the cleaned dataset are explained below:\n",
      "1) Airline: The name of the airline company is stored in the airline column. It is a categorical feature having 6 different airlines.\n",
      "2) Flight: Flight stores information regarding the plane's flight code. It is a categorical feature.\n",
      "3) Source City: City from which the flight takes off. It is a categorical feature having 6 unique cities.\n",
      "4) Departure Time: This is a derived categorical feature obtained created by grouping time periods into bins. It stores information about the departure time and have 6 unique time labels.\n",
      "5) Stops: A categorical feature with 3 distinct values that stores the number of stops between the source and destination cities.\n",
      "6) Arrival Time: This is a derived categorical feature created by grouping time intervals into bins. It has six distinct time labels and keeps information about the arrival time.\n",
      "7) Destination City: City where the flight will land. It is a categorical feature having 6 unique cities.\n",
      "8) Class: A categorical feature that contains information on seat class; it has two distinct values: Business and Economy.\n",
      "9) Duration: A continuous feature that displays the overall amount of time it takes to travel between cities in hours.\n",
      "10)Days Left: This is a derived characteristic that is calculated by subtracting the trip date by the booking date.\n",
      "11) Price: Target variable stores information of the ticket price.\n",
      "\n",
      "===================To boost learning, try to create an end-to-end project using the dataset.==================================\n",
      "name 'detect' is not defined Language is not detected: flight-price-prediction\n",
      "name 'detect' is not defined Language is not detected: shubhambathwal\n",
      "name 'detect' is not defined Language is not detected: Flight Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict Fllight Price, practise feature engineering, implement ensemble models\n",
      "name 'detect' is not defined Language is not detected: ###INTRODUCTION\n",
      "The objective of the study is to analyse the flight booking dataset obtained from “Ease My Trip” website and to conduct various statistical hypothesis tests in order to get meaningful information from it. The 'Linear Regression' statistical algorithm would be used to train the dataset and predict a continuous target variable. 'Easemytrip' is an internet platform for booking flight tickets, and hence a platform that potential passengers use to buy tickets. A thorough study of the data will aid in the discovery of valuable insights that will be of enormous value to passengers.\n",
      "\n",
      "###Research Questions\n",
      "The aim of our study is to answer the below research questions:\n",
      "a) Does price vary with Airlines?\n",
      "b) How is the price affected when tickets are bought in just 1 or 2 days before departure?\n",
      "c) Does ticket price change based on the departure time and arrival time?\n",
      "d) How the price changes with change in Source and Destination?\n",
      "e) How does the ticket price vary between Economy and Business class?\n",
      "\n",
      "\n",
      "###DATA COLLECTION AND METHODOLOGY\n",
      "Octoparse scraping tool was used to extract data from the website. Data was collected in two parts: one for economy class tickets and another for business class tickets. A total of 300261 distinct flight booking options was extracted from the site. Data was collected for 50 days, from February 11th to March 31st, 2022.\n",
      "Data source was secondary data and was collected from Ease my trip website.\n",
      "\n",
      "\n",
      "###DATASET\n",
      "Dataset contains information about flight booking options from the website Easemytrip for flight travel between India's top 6 metro cities. There are 300261 datapoints and 11 features in the cleaned dataset.\n",
      "\n",
      "###FEATURES\n",
      "The various features of the cleaned dataset are explained below:\n",
      "1) Airline: The name of the airline company is stored in the airline column. It is a categorical feature having 6 different airlines.\n",
      "2) Flight: Flight stores information regarding the plane's flight code. It is a categorical feature.\n",
      "3) Source City: City from which the flight takes off. It is a categorical feature having 6 unique cities.\n",
      "4) Departure Time: This is a derived categorical feature obtained created by grouping time periods into bins. It stores information about the departure time and have 6 unique time labels.\n",
      "5) Stops: A categorical feature with 3 distinct values that stores the number of stops between the source and destination cities.\n",
      "6) Arrival Time: This is a derived categorical feature created by grouping time intervals into bins. It has six distinct time labels and keeps information about the arrival time.\n",
      "7) Destination City: City where the flight will land. It is a categorical feature having 6 unique cities.\n",
      "8) Class: A categorical feature that contains information on seat class; it has two distinct values: Business and Economy.\n",
      "9) Duration: A continuous feature that displays the overall amount of time it takes to travel between cities in hours.\n",
      "10)Days Left: This is a derived characteristic that is calculated by subtracting the trip date by the booking date.\n",
      "11) Price: Target variable stores information of the ticket price.\n",
      "\n",
      "===================To boost learning, try to create an end-to-end project using the dataset.==================================\n",
      "name 'detect' is not defined Language is not detected: brllrb/uber-and-lyft-dataset-boston-ma\n",
      "name 'detect' is not defined Language is not detected: uber-and-lyft-dataset-boston-ma\n",
      "name 'detect' is not defined Language is not detected: brllrb\n",
      "name 'detect' is not defined Language is not detected: Uber and Lyft Dataset Boston, MA\n",
      "name 'detect' is not defined Language is not detected: From 11-26-2018 to 12-18-2018\n",
      "name 'detect' is not defined Language is not detected: ### Uber vs Lyft\n",
      "\n",
      "This is a very beginner-friendly dataset. It does contain a lot of NA values. It is a good dataset if you want to use a Linear Regression Model to see the pattern between different predectors such as `hour` and `price`.\n",
      "\n",
      "A really amazing part of this dataset is that I have included the corresponding `weather data` for that `hour` with a short summary of the `weather`. Other important factors are `temperature`, `wind`, and `sunset`. \n",
      "name 'detect' is not defined Language is not detected: uber-and-lyft-dataset-boston-ma\n",
      "name 'detect' is not defined Language is not detected: brllrb\n",
      "name 'detect' is not defined Language is not detected: Uber and Lyft Dataset Boston, MA\n",
      "name 'detect' is not defined Language is not detected: From 11-26-2018 to 12-18-2018\n",
      "name 'detect' is not defined Language is not detected: ### Uber vs Lyft\n",
      "\n",
      "This is a very beginner-friendly dataset. It does contain a lot of NA values. It is a good dataset if you want to use a Linear Regression Model to see the pattern between different predectors such as `hour` and `price`.\n",
      "\n",
      "A really amazing part of this dataset is that I have included the corresponding `weather data` for that `hour` with a short summary of the `weather`. Other important factors are `temperature`, `wind`, and `sunset`. \n",
      "name 'detect' is not defined Language is not detected: u201411546/data-sets-cold-chain-milk\n",
      "name 'detect' is not defined Language is not detected: data-sets-cold-chain-milk\n",
      "name 'detect' is not defined Language is not detected: u201411546\n",
      "name 'detect' is not defined Language is not detected: Data Sets Cold Chain Milk\n",
      "name 'detect' is not defined Language is not detected:  Data of temperature and time in the cold chain of milk.\n",
      "name 'detect' is not defined Language is not detected: data-sets-cold-chain-milk\n",
      "name 'detect' is not defined Language is not detected: u201411546\n",
      "name 'detect' is not defined Language is not detected: Data Sets Cold Chain Milk\n",
      "name 'detect' is not defined Language is not detected:  Data of temperature and time in the cold chain of milk.\n",
      "name 'detect' is not defined Language is not detected: ulrikthygepedersen/airlines-delay\n",
      "name 'detect' is not defined Language is not detected: airlines-delay\n",
      "name 'detect' is not defined Language is not detected: ulrikthygepedersen\n",
      "name 'detect' is not defined Language is not detected: Airlines Delay\n",
      "name 'detect' is not defined Language is not detected: Can you predict when a flight in delayed?\n",
      "name 'detect' is not defined Language is not detected: Airlines Dataset Inspired in the regression dataset from Elena Ikonomovska. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.\n",
      "name 'detect' is not defined Language is not detected: airlines-delay\n",
      "name 'detect' is not defined Language is not detected: ulrikthygepedersen\n",
      "name 'detect' is not defined Language is not detected: Airlines Delay\n",
      "name 'detect' is not defined Language is not detected: Can you predict when a flight in delayed?\n",
      "name 'detect' is not defined Language is not detected: Airlines Dataset Inspired in the regression dataset from Elena Ikonomovska. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.\n",
      "name 'detect' is not defined Language is not detected: rajatraj0502/linkedin-job-2023\n",
      "name 'detect' is not defined Language is not detected: linkedin-job-2023\n",
      "name 'detect' is not defined Language is not detected: rajatraj0502\n",
      "name 'detect' is not defined Language is not detected: LinkedIn Job Postings Dataset\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Dataset of Job Postings and Companies on LinkedIn\n",
      "name 'detect' is not defined Language is not detected: # LinkedIn Job Postings Dataset\n",
      "\n",
      "## Description\n",
      "This dataset contains information about job postings on LinkedIn. The data is divided into several files, each containing different aspects of the job postings:\n",
      "\n",
      "1. `job_postings.csv`: This file contains detailed information about each job posting, including the job title, description, salary, work type, location, and more.\n",
      "2. `companies.csv`: This file contains detailed information about each company that posted a job, including the company name, website, description, size, location, and more.\n",
      "3. `company_industries.csv`: This file contains the industries associated with each company.\n",
      "4. `company_specialities.csv`: This file contains the specialties associated with each company.\n",
      "5. `employee_counts.csv`: This file contains the employee and follower counts for each company.\n",
      "6. `benefits.csv`: This file contains the benefits associated with each job.\n",
      "7. `job_industries.csv`: This file contains the industries associated with each job.\n",
      "8. `job_skills.csv`: This file contains the skills associated with each job.\n",
      "\n",
      "## Usage\n",
      "This dataset can be used for various purposes such as:\n",
      "- Analyzing the job market\n",
      "- Analyzing company trends\n",
      "- Analyzing salary trends\n",
      "- Building a job recommendation system\n",
      "- Natural Language Processing (NLP) tasks such as keyword extraction, topic modeling, etc.\n",
      "\n",
      "## Acknowledgements\n",
      "This dataset was collected from LinkedIn. Please note that the data may be subject to LinkedIn's terms of use.\n",
      "\n",
      "## License\n",
      "This dataset is released under the [Open Database License (ODbL)](https://opendatacommons.org/licenses/odbl/1.0/).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: linkedin-job-2023\n",
      "name 'detect' is not defined Language is not detected: rajatraj0502\n",
      "name 'detect' is not defined Language is not detected: LinkedIn Job Postings Dataset\n",
      "name 'detect' is not defined Language is not detected: A Comprehensive Dataset of Job Postings and Companies on LinkedIn\n",
      "name 'detect' is not defined Language is not detected: # LinkedIn Job Postings Dataset\n",
      "\n",
      "## Description\n",
      "This dataset contains information about job postings on LinkedIn. The data is divided into several files, each containing different aspects of the job postings:\n",
      "\n",
      "1. `job_postings.csv`: This file contains detailed information about each job posting, including the job title, description, salary, work type, location, and more.\n",
      "2. `companies.csv`: This file contains detailed information about each company that posted a job, including the company name, website, description, size, location, and more.\n",
      "3. `company_industries.csv`: This file contains the industries associated with each company.\n",
      "4. `company_specialities.csv`: This file contains the specialties associated with each company.\n",
      "5. `employee_counts.csv`: This file contains the employee and follower counts for each company.\n",
      "6. `benefits.csv`: This file contains the benefits associated with each job.\n",
      "7. `job_industries.csv`: This file contains the industries associated with each job.\n",
      "8. `job_skills.csv`: This file contains the skills associated with each job.\n",
      "\n",
      "## Usage\n",
      "This dataset can be used for various purposes such as:\n",
      "- Analyzing the job market\n",
      "- Analyzing company trends\n",
      "- Analyzing salary trends\n",
      "- Building a job recommendation system\n",
      "- Natural Language Processing (NLP) tasks such as keyword extraction, topic modeling, etc.\n",
      "\n",
      "## Acknowledgements\n",
      "This dataset was collected from LinkedIn. Please note that the data may be subject to LinkedIn's terms of use.\n",
      "\n",
      "## License\n",
      "This dataset is released under the [Open Database License (ODbL)](https://opendatacommons.org/licenses/odbl/1.0/).\n",
      "\n",
      "name 'detect' is not defined Language is not detected: deependraverma13/book-rental-recommendation-ai-ml-project\n",
      "name 'detect' is not defined Language is not detected: book-rental-recommendation-ai-ml-project\n",
      "name 'detect' is not defined Language is not detected: deependraverma13\n",
      "name 'detect' is not defined Language is not detected: book-rental-recommendation-AI-ML-Project\n",
      "name 'detect' is not defined Language is not detected: Book Rental Recommendation Machine Leaning Capstone Project - Simplilearn\n",
      "name 'detect' is not defined Language is not detected: Description\n",
      "\n",
      "Book Rent is the largest online and offline book rental chain in India. They provide books of various genres, such as thrillers, mysteries, romances, and science fiction. The company charges a fixed rental fee for a book per month. Lately, the company has been losing its user base. The main reason for this is that users are not able to choose the right books for themselves. The company wants to solve this problem and increase its revenue and profit. \n",
      "name 'detect' is not defined Language is not detected: book-rental-recommendation-ai-ml-project\n",
      "name 'detect' is not defined Language is not detected: deependraverma13\n",
      "name 'detect' is not defined Language is not detected: book-rental-recommendation-AI-ML-Project\n",
      "name 'detect' is not defined Language is not detected: Book Rental Recommendation Machine Leaning Capstone Project - Simplilearn\n",
      "name 'detect' is not defined Language is not detected: Description\n",
      "\n",
      "Book Rent is the largest online and offline book rental chain in India. They provide books of various genres, such as thrillers, mysteries, romances, and science fiction. The company charges a fixed rental fee for a book per month. Lately, the company has been losing its user base. The main reason for this is that users are not able to choose the right books for themselves. The company wants to solve this problem and increase its revenue and profit. \n",
      "name 'detect' is not defined Language is not detected: deependraverma13/real-estate-banking-ai-capstone-project\n",
      "name 'detect' is not defined Language is not detected: real-estate-banking-ai-capstone-project\n",
      "name 'detect' is not defined Language is not detected: deependraverma13\n",
      "name 'detect' is not defined Language is not detected: Real estate Banking - AI Capstone Project\n",
      "name 'detect' is not defined Language is not detected: Real Estate-mortgage-backed securities, geographic business investment analysis\n",
      "name 'detect' is not defined Language is not detected: DESCRIPTION\n",
      "\n",
      "A banking institution requires actionable insights into mortgage-backed securities, geographic business investment, and real estate analysis. \n",
      "The mortgage bank would like to identify potential monthly mortgage expenses for each region based on monthly family income and rental of the real estate.\n",
      "A statistical model needs to be created to predict the potential demand in dollars amount of loan for each of the region in the USA. Also, there is a need to create a dashboard which would refresh periodically post data retrieval from the agencies.\n",
      "The dashboard must demonstrate relationships and trends for the key metrics as follows: number of loans, average rental income, monthly mortgage and owner’s cost, family income vs mortgage cost comparison across different regions. The metrics described here do not limit the dashboard to these few.\n",
      "Dataset Description\n",
      "\n",
      " \n",
      "\n",
      "Variables\n",
      "\n",
      "Description\n",
      "Second mortgage\tHouseholds with a second mortgage statistics\n",
      "Home equity\tHouseholds with a home equity loan statistics\n",
      "Debt\tHouseholds with any type of debt statistics\n",
      "Mortgage Costs\tStatistics regarding mortgage payments, home equity loans, utilities, and property taxes\n",
      "Home Owner Costs\tSum of utilities, and property taxes statistics\n",
      "Gross Rent\tContract rent plus the estimated average monthly cost of utility features\n",
      "High school Graduation\tHigh school graduation statistics\n",
      "Population Demographics\tPopulation demographics statistics\n",
      "Age Demographics\tAge demographic statistics\n",
      "Household Income\tTotal income of people residing in the household\n",
      "Family Income\tTotal income of people related to the householder\n",
      "Project Task: Week 1\n",
      "\n",
      "Data Import and Preparation:\n",
      "\n",
      "Import data. \n",
      "\n",
      "Figure out the primary key and look for the requirement of indexing.\n",
      "\n",
      "Gauge the fill rate of the variables and devise plans for missing value treatment. Please explain explicitly the reason for the treatment chosen for each variable.\n",
      "\n",
      "Exploratory Data Analysis (EDA):\n",
      "\n",
      "Perform debt analysis. You may take the following steps:\n",
      "\n",
      "Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map. You may keep the upper limit for the percent of households with a second mortgage to 50 percent\n",
      "\n",
      "Use the following bad debt equation:\n",
      "\n",
      "Bad Debt = P (Second Mortgage ∩ Home Equity Loan)\n",
      "Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage\n",
      "Create pie charts  to show overall debt and bad debt\n",
      "\n",
      "Create Box and whisker plot and analyze the distribution for 2nd mortgage, home equity, good debt, and bad debt for different cities\n",
      "\n",
      "Create a collated income distribution chart for family income, house hold income, and remaining income\n",
      "\n",
      "Perform EDA and come out with insights into population density and age. You may have to derive new fields (make sure to weight averages for accurate measurements): \n",
      "\n",
      "Use pop and ALand variables to create a new field called population density\n",
      "\n",
      "Use male_age_median, female_age_median, male_pop, and female_pop to create a new field called median age\n",
      "\n",
      "Visualize the findings using appropriate chart type\n",
      "\n",
      "Create bins for population into a new variable by selecting appropriate class interval so that the number of categories don’t exceed 5 for the ease of analysis.\n",
      "\n",
      "Analyze the married, separated, and divorced population for these population brackets\n",
      "\n",
      "Visualize using appropriate chart type\n",
      "\n",
      "Please detail your observations for rent as a percentage of income at an overall level, and for different states.\n",
      "\n",
      "Perform correlation analysis for all the relevant variables by creating a heatmap. Describe your findings.\n",
      "\n",
      " \n",
      "\n",
      "Project Task: Week 2\n",
      "\n",
      "Data Pre-processing:\n",
      "\n",
      "The economic multivariate data has a significant number of measured variables. The goal is to find where the measured variables depend on a number of smaller unobserved common factors or latent variables. \n",
      "\n",
      "Each variable is assumed to be dependent upon a linear combination of the common factors, and the coefficients are known as loadings. Each measured variable also includes a component due to independent random variability, known as “specific variance” because it is specific to one variable. Obtain the common factors and then plot the loadings. Use factor analysis to find latent variables in our dataset and gain insight into the linear relationships in the data. \n",
      "\n",
      "       Following are the list of latent variables:\n",
      "\n",
      "Highschool graduation rates\n",
      "\n",
      "Median population age\n",
      "\n",
      "Second mortgage statistics\n",
      "\n",
      "Percent own\n",
      "\n",
      "Bad debt expense\n",
      "\n",
      " \n",
      "\n",
      "Data Modeling :\n",
      "\n",
      "Build a linear Regression model to predict the total monthly expenditure for home mortgages loan. \n",
      "\n",
      "       Please refer deplotment_RE.xlsx. Column hc_mortgage_mean is predicted variable. This is the mean monthly mortgage and owner costs of specified geographical location.\n",
      "\n",
      "       Note: Exclude loans from prediction model which have NaN (Not a Number) values for hc_mortgage_mean. \n",
      "\n",
      "       a) Run a model at a Nation level. If the accuracy levels and R square are not satisfactory proceed to below step.\n",
      "\n",
      "       b) Run another model at State level. There are 52 states in USA.\n",
      "\n",
      "       c) Keep below considerations while building a linear regression model:\n",
      "\n",
      "Variables should have significant impact on predicting Monthly mortgage and owner costs\n",
      "\n",
      "Utilize all predictor variable to start with initial hypothesis\n",
      "\n",
      "R square of 60 percent and above should be achieved\n",
      "\n",
      "Ensure Multi-collinearity does not exist in dependent variables\n",
      "\n",
      "Test if predicted variable is normally distributed\n",
      "name 'detect' is not defined Language is not detected: real-estate-banking-ai-capstone-project\n",
      "name 'detect' is not defined Language is not detected: deependraverma13\n",
      "name 'detect' is not defined Language is not detected: Real estate Banking - AI Capstone Project\n",
      "name 'detect' is not defined Language is not detected: Real Estate-mortgage-backed securities, geographic business investment analysis\n",
      "name 'detect' is not defined Language is not detected: DESCRIPTION\n",
      "\n",
      "A banking institution requires actionable insights into mortgage-backed securities, geographic business investment, and real estate analysis. \n",
      "The mortgage bank would like to identify potential monthly mortgage expenses for each region based on monthly family income and rental of the real estate.\n",
      "A statistical model needs to be created to predict the potential demand in dollars amount of loan for each of the region in the USA. Also, there is a need to create a dashboard which would refresh periodically post data retrieval from the agencies.\n",
      "The dashboard must demonstrate relationships and trends for the key metrics as follows: number of loans, average rental income, monthly mortgage and owner’s cost, family income vs mortgage cost comparison across different regions. The metrics described here do not limit the dashboard to these few.\n",
      "Dataset Description\n",
      "\n",
      " \n",
      "\n",
      "Variables\n",
      "\n",
      "Description\n",
      "Second mortgage\tHouseholds with a second mortgage statistics\n",
      "Home equity\tHouseholds with a home equity loan statistics\n",
      "Debt\tHouseholds with any type of debt statistics\n",
      "Mortgage Costs\tStatistics regarding mortgage payments, home equity loans, utilities, and property taxes\n",
      "Home Owner Costs\tSum of utilities, and property taxes statistics\n",
      "Gross Rent\tContract rent plus the estimated average monthly cost of utility features\n",
      "High school Graduation\tHigh school graduation statistics\n",
      "Population Demographics\tPopulation demographics statistics\n",
      "Age Demographics\tAge demographic statistics\n",
      "Household Income\tTotal income of people residing in the household\n",
      "Family Income\tTotal income of people related to the householder\n",
      "Project Task: Week 1\n",
      "\n",
      "Data Import and Preparation:\n",
      "\n",
      "Import data. \n",
      "\n",
      "Figure out the primary key and look for the requirement of indexing.\n",
      "\n",
      "Gauge the fill rate of the variables and devise plans for missing value treatment. Please explain explicitly the reason for the treatment chosen for each variable.\n",
      "\n",
      "Exploratory Data Analysis (EDA):\n",
      "\n",
      "Perform debt analysis. You may take the following steps:\n",
      "\n",
      "Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map. You may keep the upper limit for the percent of households with a second mortgage to 50 percent\n",
      "\n",
      "Use the following bad debt equation:\n",
      "\n",
      "Bad Debt = P (Second Mortgage ∩ Home Equity Loan)\n",
      "Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage\n",
      "Create pie charts  to show overall debt and bad debt\n",
      "\n",
      "Create Box and whisker plot and analyze the distribution for 2nd mortgage, home equity, good debt, and bad debt for different cities\n",
      "\n",
      "Create a collated income distribution chart for family income, house hold income, and remaining income\n",
      "\n",
      "Perform EDA and come out with insights into population density and age. You may have to derive new fields (make sure to weight averages for accurate measurements): \n",
      "\n",
      "Use pop and ALand variables to create a new field called population density\n",
      "\n",
      "Use male_age_median, female_age_median, male_pop, and female_pop to create a new field called median age\n",
      "\n",
      "Visualize the findings using appropriate chart type\n",
      "\n",
      "Create bins for population into a new variable by selecting appropriate class interval so that the number of categories don’t exceed 5 for the ease of analysis.\n",
      "\n",
      "Analyze the married, separated, and divorced population for these population brackets\n",
      "\n",
      "Visualize using appropriate chart type\n",
      "\n",
      "Please detail your observations for rent as a percentage of income at an overall level, and for different states.\n",
      "\n",
      "Perform correlation analysis for all the relevant variables by creating a heatmap. Describe your findings.\n",
      "\n",
      " \n",
      "\n",
      "Project Task: Week 2\n",
      "\n",
      "Data Pre-processing:\n",
      "\n",
      "The economic multivariate data has a significant number of measured variables. The goal is to find where the measured variables depend on a number of smaller unobserved common factors or latent variables. \n",
      "\n",
      "Each variable is assumed to be dependent upon a linear combination of the common factors, and the coefficients are known as loadings. Each measured variable also includes a component due to independent random variability, known as “specific variance” because it is specific to one variable. Obtain the common factors and then plot the loadings. Use factor analysis to find latent variables in our dataset and gain insight into the linear relationships in the data. \n",
      "\n",
      "       Following are the list of latent variables:\n",
      "\n",
      "Highschool graduation rates\n",
      "\n",
      "Median population age\n",
      "\n",
      "Second mortgage statistics\n",
      "\n",
      "Percent own\n",
      "\n",
      "Bad debt expense\n",
      "\n",
      " \n",
      "\n",
      "Data Modeling :\n",
      "\n",
      "Build a linear Regression model to predict the total monthly expenditure for home mortgages loan. \n",
      "\n",
      "       Please refer deplotment_RE.xlsx. Column hc_mortgage_mean is predicted variable. This is the mean monthly mortgage and owner costs of specified geographical location.\n",
      "\n",
      "       Note: Exclude loans from prediction model which have NaN (Not a Number) values for hc_mortgage_mean. \n",
      "\n",
      "       a) Run a model at a Nation level. If the accuracy levels and R square are not satisfactory proceed to below step.\n",
      "\n",
      "       b) Run another model at State level. There are 52 states in USA.\n",
      "\n",
      "       c) Keep below considerations while building a linear regression model:\n",
      "\n",
      "Variables should have significant impact on predicting Monthly mortgage and owner costs\n",
      "\n",
      "Utilize all predictor variable to start with initial hypothesis\n",
      "\n",
      "R square of 60 percent and above should be achieved\n",
      "\n",
      "Ensure Multi-collinearity does not exist in dependent variables\n",
      "\n",
      "Test if predicted variable is normally distributed\n",
      "name 'detect' is not defined Language is not detected: zakariaeyoussefi/barcelona-airbnb-listings-inside-airbnb\n",
      "name 'detect' is not defined Language is not detected: barcelona-airbnb-listings-inside-airbnb\n",
      "name 'detect' is not defined Language is not detected: zakariaeyoussefi\n",
      "name 'detect' is not defined Language is not detected: Barcelona Airbnb listings - Inside Airbnb\n",
      "name 'detect' is not defined Language is not detected: Uncovering Insights: Exploring the Barcelona Airbnb Experience\n",
      "name 'detect' is not defined Language is not detected: From the project website: http://insideairbnb.com/about/\n",
      "\n",
      "Inside Airbnb is a mission driven project that provides data and advocacy about Airbnb's impact on residential communities.\n",
      "\n",
      "We work towards a vision where data and information empower communities to understand, decide and control the role of renting residential homes to tourists.\n",
      "\n",
      "***this data set contains two csv file:***\n",
      "\n",
      "   - barcelona_listings.csv: This file contains the raw, unmodified data sourced directly from the provider. It includes all the original columns and records without any alterations.\n",
      "\n",
      "   - Cleaned_airbnb_barcelona.csv: This file contains cleaned and processed data, derived from the raw data. It has undergone column selection, data cleansing, and transformation to ensure data quality and consistency. This file is intended for further analysis and modeling, providing a more focused and reliable dataset.\n",
      "\n",
      "\n",
      "Columns: \n",
      "\n",
      "**id:** A unique identifier for each Airbnb listing.\n",
      "\n",
      "**listing_url:** The URL link to the Airbnb listing page.\n",
      "\n",
      "**name:** The name or title given to the listing by the host.\n",
      "\n",
      "**summary:** A short summary or description of the listing, highlighting key features.\n",
      "\n",
      "**space:** Details about the space offered, including any unique features or amenities.\n",
      "\n",
      "**description:** A more detailed description provided by the host, covering all aspects of the listing.\n",
      "\n",
      "**neighborhood_overview:** An overview of the neighborhood, including local attractions and amenities.\n",
      "\n",
      "**access:** Information about accessing the property, such as check-in instructions.\n",
      "\n",
      "**picture_url:** The URL link to the listing's photo(s).\n",
      "\n",
      "**host_id:** A unique identifier for the host of the listing.\n",
      "\n",
      "**host_url:** The URL link to the host's Airbnb profile page.\n",
      "\n",
      "**host_name:** The name of the host as displayed on Airbnb.\n",
      "\n",
      "**host_since:** The date the host joined Airbnb.\n",
      "\n",
      "**host_response_time:** The host's response time category (e.g., within an hour, within a day).\n",
      "\n",
      "**host_response_rate:** The percentage of time the host responds to messages.\n",
      "\n",
      "**host_is_superhost:** Indicates whether the host has superhost status.\n",
      "\n",
      "**host_picture_url:** The URL link to the host's profile picture.\n",
      "\n",
      "**host_neighbourhood:** The neighborhood where the host is located.\n",
      "\n",
      "**host_listings_count:** The number of listings the host has on Airbnb.\n",
      "\n",
      "**host_verifications:** Verifications completed by the host (e.g., email, phone, ID).\n",
      "\n",
      "**host_has_profile_pic:** Indicates whether the host has a profile picture.\n",
      "\n",
      "**host_identity_verified:** Indicates whether the host's identity has been verified.\n",
      "\n",
      "**street:** The street address of the listing (partial for privacy).\n",
      "\n",
      "**neighbourhood:** The name of the neighborhood where the listing is located.\n",
      "\n",
      "**neighbourhood_cleansed:** A standardized version of the neighborhood name.\n",
      "\n",
      "**neighbourhood_group_cleansed:** A broader category of the neighborhood (e.g., borough or district).\n",
      "\n",
      "**city:** The city where the listing is located.\n",
      "\n",
      "**zipcode:** The postal code or zip code of the listing location.\n",
      "\n",
      "**country:** The country where the listing is located.\n",
      "\n",
      "**latitude and longitude:** Geographic coordinates of the listing.\n",
      "\n",
      "**is_location_exact:** Indicates whether the listed location is exact or approximate.\n",
      "\n",
      "**property_type:** The type of property (e.g., apartment, house, villa).\n",
      "\n",
      "**room_type:** The type of room (e.g., entire home, private room, shared room).\n",
      "\n",
      "**accommodates:** The maximum number of guests the listing can accommodate.\n",
      "\n",
      "**bathrooms, bedrooms, and beds:** The number of bathrooms, bedrooms, and beds in the listing.\n",
      "\n",
      "**amenities:** A list of amenities and features offered by the listing.\n",
      "\n",
      "**square_feet:** The square footage or size of the listing.\n",
      "\n",
      "**price:** The nightly price of the listing.\n",
      "\n",
      "**cleaning_fee:** Any additional cleaning fee charged by the host.\n",
      "\n",
      "**minimum_nights and maximum_nights:** The minimum and maximum number of nights guests can book.\n",
      "\n",
      "**has_availability:** Indicates whether the listing is available for booking.\n",
      "\n",
      "**availability_X:** Columns indicating availability for the next 30, 60, 90, and 365 days.\n",
      "\n",
      "**number_of_reviews:** The total number of reviews the listing has received.\n",
      "\n",
      "**number_of_reviews_ltm:** The number of reviews received in the last 12 months.\n",
      "\n",
      "**first_review and last_review:** Dates of the first and last reviews.\n",
      "\n",
      "**review_scores_X:** Columns indicating review scores for various aspects (accuracy, cleanliness, etc.).\n",
      "\n",
      "**instant_bookable:** Indicates whether the listing can be booked instantly.\n",
      "\n",
      "**reviews_per_month:** The average number of reviews received per month.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: barcelona-airbnb-listings-inside-airbnb\n",
      "name 'detect' is not defined Language is not detected: zakariaeyoussefi\n",
      "name 'detect' is not defined Language is not detected: Barcelona Airbnb listings - Inside Airbnb\n",
      "name 'detect' is not defined Language is not detected: Uncovering Insights: Exploring the Barcelona Airbnb Experience\n",
      "name 'detect' is not defined Language is not detected: From the project website: http://insideairbnb.com/about/\n",
      "\n",
      "Inside Airbnb is a mission driven project that provides data and advocacy about Airbnb's impact on residential communities.\n",
      "\n",
      "We work towards a vision where data and information empower communities to understand, decide and control the role of renting residential homes to tourists.\n",
      "\n",
      "***this data set contains two csv file:***\n",
      "\n",
      "   - barcelona_listings.csv: This file contains the raw, unmodified data sourced directly from the provider. It includes all the original columns and records without any alterations.\n",
      "\n",
      "   - Cleaned_airbnb_barcelona.csv: This file contains cleaned and processed data, derived from the raw data. It has undergone column selection, data cleansing, and transformation to ensure data quality and consistency. This file is intended for further analysis and modeling, providing a more focused and reliable dataset.\n",
      "\n",
      "\n",
      "Columns: \n",
      "\n",
      "**id:** A unique identifier for each Airbnb listing.\n",
      "\n",
      "**listing_url:** The URL link to the Airbnb listing page.\n",
      "\n",
      "**name:** The name or title given to the listing by the host.\n",
      "\n",
      "**summary:** A short summary or description of the listing, highlighting key features.\n",
      "\n",
      "**space:** Details about the space offered, including any unique features or amenities.\n",
      "\n",
      "**description:** A more detailed description provided by the host, covering all aspects of the listing.\n",
      "\n",
      "**neighborhood_overview:** An overview of the neighborhood, including local attractions and amenities.\n",
      "\n",
      "**access:** Information about accessing the property, such as check-in instructions.\n",
      "\n",
      "**picture_url:** The URL link to the listing's photo(s).\n",
      "\n",
      "**host_id:** A unique identifier for the host of the listing.\n",
      "\n",
      "**host_url:** The URL link to the host's Airbnb profile page.\n",
      "\n",
      "**host_name:** The name of the host as displayed on Airbnb.\n",
      "\n",
      "**host_since:** The date the host joined Airbnb.\n",
      "\n",
      "**host_response_time:** The host's response time category (e.g., within an hour, within a day).\n",
      "\n",
      "**host_response_rate:** The percentage of time the host responds to messages.\n",
      "\n",
      "**host_is_superhost:** Indicates whether the host has superhost status.\n",
      "\n",
      "**host_picture_url:** The URL link to the host's profile picture.\n",
      "\n",
      "**host_neighbourhood:** The neighborhood where the host is located.\n",
      "\n",
      "**host_listings_count:** The number of listings the host has on Airbnb.\n",
      "\n",
      "**host_verifications:** Verifications completed by the host (e.g., email, phone, ID).\n",
      "\n",
      "**host_has_profile_pic:** Indicates whether the host has a profile picture.\n",
      "\n",
      "**host_identity_verified:** Indicates whether the host's identity has been verified.\n",
      "\n",
      "**street:** The street address of the listing (partial for privacy).\n",
      "\n",
      "**neighbourhood:** The name of the neighborhood where the listing is located.\n",
      "\n",
      "**neighbourhood_cleansed:** A standardized version of the neighborhood name.\n",
      "\n",
      "**neighbourhood_group_cleansed:** A broader category of the neighborhood (e.g., borough or district).\n",
      "\n",
      "**city:** The city where the listing is located.\n",
      "\n",
      "**zipcode:** The postal code or zip code of the listing location.\n",
      "\n",
      "**country:** The country where the listing is located.\n",
      "\n",
      "**latitude and longitude:** Geographic coordinates of the listing.\n",
      "\n",
      "**is_location_exact:** Indicates whether the listed location is exact or approximate.\n",
      "\n",
      "**property_type:** The type of property (e.g., apartment, house, villa).\n",
      "\n",
      "**room_type:** The type of room (e.g., entire home, private room, shared room).\n",
      "\n",
      "**accommodates:** The maximum number of guests the listing can accommodate.\n",
      "\n",
      "**bathrooms, bedrooms, and beds:** The number of bathrooms, bedrooms, and beds in the listing.\n",
      "\n",
      "**amenities:** A list of amenities and features offered by the listing.\n",
      "\n",
      "**square_feet:** The square footage or size of the listing.\n",
      "\n",
      "**price:** The nightly price of the listing.\n",
      "\n",
      "**cleaning_fee:** Any additional cleaning fee charged by the host.\n",
      "\n",
      "**minimum_nights and maximum_nights:** The minimum and maximum number of nights guests can book.\n",
      "\n",
      "**has_availability:** Indicates whether the listing is available for booking.\n",
      "\n",
      "**availability_X:** Columns indicating availability for the next 30, 60, 90, and 365 days.\n",
      "\n",
      "**number_of_reviews:** The total number of reviews the listing has received.\n",
      "\n",
      "**number_of_reviews_ltm:** The number of reviews received in the last 12 months.\n",
      "\n",
      "**first_review and last_review:** Dates of the first and last reviews.\n",
      "\n",
      "**review_scores_X:** Columns indicating review scores for various aspects (accuracy, cleanliness, etc.).\n",
      "\n",
      "**instant_bookable:** Indicates whether the listing can be booked instantly.\n",
      "\n",
      "**reviews_per_month:** The average number of reviews received per month.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: lucamassaron/30-days-of-ml\n",
      "name 'detect' is not defined Language is not detected: 30-days-of-ml\n",
      "name 'detect' is not defined Language is not detected: lucamassaron\n",
      "name 'detect' is not defined Language is not detected: 30 Days of ML\n",
      "name 'detect' is not defined Language is not detected: Apply what you've learned! Predict values in a regression task\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The data relative to the Kaggle learning competition 30 Days of ML (https://www.kaggle.com/thirty-days-of-ml) cannot be downloaded by Kagglers who have not initially participated to it. now you can download it from here and use it for testing the many tutorials and notebooks available from the learning competition.\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data comes from a Kaggle competition, 30 Days of ML (https://www.kaggle.com/c/30-days-of-ml).\n",
      "name 'detect' is not defined Language is not detected: 30-days-of-ml\n",
      "name 'detect' is not defined Language is not detected: lucamassaron\n",
      "name 'detect' is not defined Language is not detected: 30 Days of ML\n",
      "name 'detect' is not defined Language is not detected: Apply what you've learned! Predict values in a regression task\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The data relative to the Kaggle learning competition 30 Days of ML (https://www.kaggle.com/thirty-days-of-ml) cannot be downloaded by Kagglers who have not initially participated to it. now you can download it from here and use it for testing the many tutorials and notebooks available from the learning competition.\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "The data comes from a Kaggle competition, 30 Days of ML (https://www.kaggle.com/c/30-days-of-ml).\n",
      "name 'detect' is not defined Language is not detected: kshitizregmi/jobs-and-job-description\n",
      "name 'detect' is not defined Language is not detected: jobs-and-job-description\n",
      "name 'detect' is not defined Language is not detected: kshitizregmi\n",
      "name 'detect' is not defined Language is not detected: Job Title and Job Description Dataset\n",
      "name 'detect' is not defined Language is not detected: Job Recommendation System Dataset\n",
      "name 'detect' is not defined Language is not detected: # Dataset Description:\n",
      "\n",
      "**Title:** Job Descriptions Dataset\n",
      "\n",
      "**Description:**\n",
      "This dataset comprises a collection of job titles paired with detailed job descriptions. You can use this dataset for natural language processing (NLP) tasks, offering opportunities for recommendation systems,  text classification, information retrieval, and semantic search.\n",
      "\n",
      "**Content:**\n",
      "- **Columns**:\n",
      "  - `Job Title`: Job titles associated with each description.\n",
      "  - `Job Description`: Comprehensive job descriptions outlining responsibilities and qualifications.\n",
      "\n",
      "**Potential Uses:**\n",
      "- Recommendation system\n",
      "- Text classification\n",
      "- Information retrieval\n",
      "- Semantic search/analysis\n",
      "\n",
      "**Data Source:**\n",
      "The dataset is sourced from reputable job listing platforms and represents diverse industries and job roles. Credits: glassdoor, merojob.com, indeed.com, etc.\n",
      "\n",
      "**Format:**\n",
      "CSV format with job titles and descriptions.\n",
      "\n",
      "**License:**\n",
      "This dataset is provided under the CC0: Public Domain License. Researchers are encouraged to explore and utilize this dataset responsibly for research and educational purposes only.\n",
      "name 'detect' is not defined Language is not detected: jobs-and-job-description\n",
      "name 'detect' is not defined Language is not detected: kshitizregmi\n",
      "name 'detect' is not defined Language is not detected: Job Title and Job Description Dataset\n",
      "name 'detect' is not defined Language is not detected: Job Recommendation System Dataset\n",
      "name 'detect' is not defined Language is not detected: # Dataset Description:\n",
      "\n",
      "**Title:** Job Descriptions Dataset\n",
      "\n",
      "**Description:**\n",
      "This dataset comprises a collection of job titles paired with detailed job descriptions. You can use this dataset for natural language processing (NLP) tasks, offering opportunities for recommendation systems,  text classification, information retrieval, and semantic search.\n",
      "\n",
      "**Content:**\n",
      "- **Columns**:\n",
      "  - `Job Title`: Job titles associated with each description.\n",
      "  - `Job Description`: Comprehensive job descriptions outlining responsibilities and qualifications.\n",
      "\n",
      "**Potential Uses:**\n",
      "- Recommendation system\n",
      "- Text classification\n",
      "- Information retrieval\n",
      "- Semantic search/analysis\n",
      "\n",
      "**Data Source:**\n",
      "The dataset is sourced from reputable job listing platforms and represents diverse industries and job roles. Credits: glassdoor, merojob.com, indeed.com, etc.\n",
      "\n",
      "**Format:**\n",
      "CSV format with job titles and descriptions.\n",
      "\n",
      "**License:**\n",
      "This dataset is provided under the CC0: Public Domain License. Researchers are encouraged to explore and utilize this dataset responsibly for research and educational purposes only.\n",
      "name 'detect' is not defined Language is not detected: jassican/hacklive-3-guided-hackathon-nlp\n",
      "name 'detect' is not defined Language is not detected: hacklive-3-guided-hackathon-nlp\n",
      "name 'detect' is not defined Language is not detected: jassican\n",
      "name 'detect' is not defined Language is not detected: HackLive 3: Guided Hackathon - NLP\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya Problem \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Topic Modeling for Research Articles 2.0\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      "\n",
      "\n",
      "### Content\n",
      "id                                                Unique ID for each article\n",
      "ABSTRACT\t                          Abstract of the research article\n",
      "Computer Science                   Whether article belongs to topic computer science (1/0)\n",
      "Mathematics\t                          Whether article belongs to topic Mathematics (1/0)\n",
      "Physics\t                                  Whether article belongs to topic physics (1/0)\n",
      "\n",
      "Statistics\tWhether article belongs to topic Statistics (1/0)\n",
      "Tags\t(TARGET) There are 25 columns of possible tags with (1/0) :\n",
      "1 : if article belongs to that tag\n",
      "0 : if article doesn't belong to that tag\n",
      " \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Analytics Vidhya dataset\n",
      "\n",
      "#### Link to Competition\n",
      "https://datahack.analyticsvidhya.com/contest/hacklive-3-guided-hackathon-text-classification/#LeaderBoard\n",
      "name 'detect' is not defined Language is not detected: hacklive-3-guided-hackathon-nlp\n",
      "name 'detect' is not defined Language is not detected: jassican\n",
      "name 'detect' is not defined Language is not detected: HackLive 3: Guided Hackathon - NLP\n",
      "name 'detect' is not defined Language is not detected: Analytics Vidhya Problem \n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Topic Modeling for Research Articles 2.0\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. \n",
      "\n",
      "Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n",
      "\n",
      "Given the abstracts for a set of research articles, predict the tags for each article included in the test set. \n",
      "Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Mathematics\n",
      "\n",
      "3. Physics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "List of possible tags are as follows:\n",
      "\n",
      "[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n",
      "\n",
      "\n",
      "### Content\n",
      "id                                                Unique ID for each article\n",
      "ABSTRACT\t                          Abstract of the research article\n",
      "Computer Science                   Whether article belongs to topic computer science (1/0)\n",
      "Mathematics\t                          Whether article belongs to topic Mathematics (1/0)\n",
      "Physics\t                                  Whether article belongs to topic physics (1/0)\n",
      "\n",
      "Statistics\tWhether article belongs to topic Statistics (1/0)\n",
      "Tags\t(TARGET) There are 25 columns of possible tags with (1/0) :\n",
      "1 : if article belongs to that tag\n",
      "0 : if article doesn't belong to that tag\n",
      " \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Analytics Vidhya dataset\n",
      "\n",
      "#### Link to Competition\n",
      "https://datahack.analyticsvidhya.com/contest/hacklive-3-guided-hackathon-text-classification/#LeaderBoard\n",
      "name 'detect' is not defined Language is not detected: ahsanaseer/top-rated-tmdb-movies-10k\n",
      "name 'detect' is not defined Language is not detected: top-rated-tmdb-movies-10k\n",
      "name 'detect' is not defined Language is not detected: ahsanaseer\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset of 10k top rated TMDB movies for text preprocessing (NLP)\n",
      "name 'detect' is not defined Language is not detected: # **Context**\n",
      "This is a data set of 10k top-rated TMDB movies till 26-July-2022. \n",
      "\n",
      "# **Content**\n",
      "The Dataset contains the following things:\n",
      "1. ID: Movie ID number on the website.\n",
      "2. title: Movie name\n",
      "3. genre: Movie genre (crime, adventure, etc.)\n",
      "4. original_language: Original language in which the movie is released\n",
      "5. overview: Summary of the movie\n",
      "6. popularity: Movie Popularity\n",
      "7. release_date: Movie release date\n",
      "8. vote_average: Movie vote average\n",
      "9. vote_count: Movie vote count\n",
      "\n",
      "# **Inspiration**\n",
      "This was assembled to perform an extensive **Text Preprocessing/cleansing (NLP)** on Movie Data. But this can also be used in Building Content-Based and Collaborative Filtering Based **Recommendation Engines**.\n",
      "name 'detect' is not defined Language is not detected: top-rated-tmdb-movies-10k\n",
      "name 'detect' is not defined Language is not detected: ahsanaseer\n",
      "name 'detect' is not defined Language is not detected: TMDB Movies Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset of 10k top rated TMDB movies for text preprocessing (NLP)\n",
      "name 'detect' is not defined Language is not detected: # **Context**\n",
      "This is a data set of 10k top-rated TMDB movies till 26-July-2022. \n",
      "\n",
      "# **Content**\n",
      "The Dataset contains the following things:\n",
      "1. ID: Movie ID number on the website.\n",
      "2. title: Movie name\n",
      "3. genre: Movie genre (crime, adventure, etc.)\n",
      "4. original_language: Original language in which the movie is released\n",
      "5. overview: Summary of the movie\n",
      "6. popularity: Movie Popularity\n",
      "7. release_date: Movie release date\n",
      "8. vote_average: Movie vote average\n",
      "9. vote_count: Movie vote count\n",
      "\n",
      "# **Inspiration**\n",
      "This was assembled to perform an extensive **Text Preprocessing/cleansing (NLP)** on Movie Data. But this can also be used in Building Content-Based and Collaborative Filtering Based **Recommendation Engines**.\n",
      "name 'detect' is not defined Language is not detected: ericpierce/new-york-housing-zillow-api\n",
      "name 'detect' is not defined Language is not detected: new-york-housing-zillow-api\n",
      "name 'detect' is not defined Language is not detected: ericpierce\n",
      "name 'detect' is not defined Language is not detected: New York Housing - Zillow API\n",
      "name 'detect' is not defined Language is not detected: 75,629 Housing Records collected on 1/20/2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Housing in New York during COVID! What an interesting dataset to tackle! \n",
      "\n",
      "WARNING: This data is \"raw\" but contains some highly interesting and predictive information, so long as you're able to extract it.\n",
      "\n",
      "### Content\n",
      "\n",
      "This data was collected on 1/20/21 and consists of 75,629 housing listings on Zillow.com using Zillow's API. \n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset includes a variety of datapoints for use in regression, predictive modeling, NLP (home description), Computer Vision (housing image links)\n",
      "name 'detect' is not defined Language is not detected: new-york-housing-zillow-api\n",
      "name 'detect' is not defined Language is not detected: ericpierce\n",
      "name 'detect' is not defined Language is not detected: New York Housing - Zillow API\n",
      "name 'detect' is not defined Language is not detected: 75,629 Housing Records collected on 1/20/2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Housing in New York during COVID! What an interesting dataset to tackle! \n",
      "\n",
      "WARNING: This data is \"raw\" but contains some highly interesting and predictive information, so long as you're able to extract it.\n",
      "\n",
      "### Content\n",
      "\n",
      "This data was collected on 1/20/21 and consists of 75,629 housing listings on Zillow.com using Zillow's API. \n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset includes a variety of datapoints for use in regression, predictive modeling, NLP (home description), Computer Vision (housing image links)\n",
      "name 'detect' is not defined Language is not detected: shashwatwork/christmas-gift-price-prediction\n",
      "name 'detect' is not defined Language is not detected: christmas-gift-price-prediction\n",
      "name 'detect' is not defined Language is not detected: shashwatwork\n",
      "name 'detect' is not defined Language is not detected: Christmas Gift Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict the Gift Price using Machine learning\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The main goal of online retailers is to increase the desirability and the value of the products. To achieve this goal, various promotional techniques are planned. Among these techniques, offering promotions and special offers to the customers is an effective method of driving ancillary traffic to the site, acquiring new customers, and growing the revenue. These advancements can likewise be utilized to encourage new visitors to become loyal customers.\n",
      "\n",
      "### Content\n",
      "\n",
      "The data provided is a combination of multiple categories and dates related to gifts.\n",
      "\n",
      "#### Data description\n",
      "\n",
      "* gift_id\t\n",
      "Unique ID of gift\n",
      "* gift_type\t\n",
      "Type of gift (clothes/perfumes/etc.)\n",
      "* gift_category\t\n",
      "Category to which the gift belongs under that gift type\n",
      "* gift_cluster\t\n",
      "Type of industry the gift belongs\n",
      "* instock_date\t\n",
      "Date of arrival of stock\n",
      "* stock_update_date\t\n",
      "The date on which the stock was updated\n",
      "* lsg_1 - lsg_6\t\n",
      "Anonymized variables related to gift\n",
      "* uk_date1, uk_date2\t\n",
      "Buyer related dates\n",
      "* is_discounted\t\n",
      "Shows whether the discount is applicable on the gift\n",
      "* volumes\t\n",
      "Number of packages bought\n",
      "* price\t\n",
      "The total price\n",
      "\n",
      "### Acknowledgements\n",
      "Source of Dataset can be found [here.](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/practice-problems/machine-learning/predict-the-price-of-gifts-0a33c38e/)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: christmas-gift-price-prediction\n",
      "name 'detect' is not defined Language is not detected: shashwatwork\n",
      "name 'detect' is not defined Language is not detected: Christmas Gift Price Prediction\n",
      "name 'detect' is not defined Language is not detected: Predict the Gift Price using Machine learning\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "The main goal of online retailers is to increase the desirability and the value of the products. To achieve this goal, various promotional techniques are planned. Among these techniques, offering promotions and special offers to the customers is an effective method of driving ancillary traffic to the site, acquiring new customers, and growing the revenue. These advancements can likewise be utilized to encourage new visitors to become loyal customers.\n",
      "\n",
      "### Content\n",
      "\n",
      "The data provided is a combination of multiple categories and dates related to gifts.\n",
      "\n",
      "#### Data description\n",
      "\n",
      "* gift_id\t\n",
      "Unique ID of gift\n",
      "* gift_type\t\n",
      "Type of gift (clothes/perfumes/etc.)\n",
      "* gift_category\t\n",
      "Category to which the gift belongs under that gift type\n",
      "* gift_cluster\t\n",
      "Type of industry the gift belongs\n",
      "* instock_date\t\n",
      "Date of arrival of stock\n",
      "* stock_update_date\t\n",
      "The date on which the stock was updated\n",
      "* lsg_1 - lsg_6\t\n",
      "Anonymized variables related to gift\n",
      "* uk_date1, uk_date2\t\n",
      "Buyer related dates\n",
      "* is_discounted\t\n",
      "Shows whether the discount is applicable on the gift\n",
      "* volumes\t\n",
      "Number of packages bought\n",
      "* price\t\n",
      "The total price\n",
      "\n",
      "### Acknowledgements\n",
      "Source of Dataset can be found [here.](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/practice-problems/machine-learning/predict-the-price-of-gifts-0a33c38e/)\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda/korean-drama-2015-23-actor-and-reviewmydramalist\n",
      "name 'detect' is not defined Language is not detected: korean-drama-2015-23-actor-and-reviewmydramalist\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda\n",
      "name 'detect' is not defined Language is not detected: Korean Drama from 2015-2023 with Actors & Reviews \n",
      "name 'detect' is not defined Language is not detected: 🔥Spinoff from \"Top 100 Korean Drama (MyDramaList)\" Dataset💯 but MORE DATA!\n",
      "name 'detect' is not defined Language is not detected: ### 😍NEW Korean Drama Dataset - spinoff of ***Top 100 Korean Drama (MyDramaList)*** 🥳<br>\n",
      "Go checkout the dataset using this link: https://www.kaggle.com/datasets/chanoncharuchinda/top-100-korean-drama-mydramalist\n",
      "\n",
      "**IMPORTANT NOTE:** *I made these datasets for **educational purposes**. I want people who **share similar interest and obsession in Korean Drama** to have access to this data to begin their exploration. Have fun!* Also, working with these two datasets simultaneously might be helpful and worth exploring.\n",
      "\n",
      "#### Project and Data Information\n",
      "A while back, I worked on a Dataset called Top 100 Korean Drama (MyDramaList). I decided to work on similar project again. For this project, I have generated three files:\n",
      "1. **korean_drama.csv**: 1752 Korean drama to be explored\n",
      "2. **recommendations.csv**: table contains the recommendation given to different drama. The recommendation is a product of the MyDramaList community.\n",
      "3. **review.csv**: review given to the drama from users on the website\n",
      "4. **wiki_actor.csv**: information regarding the actors (mainly the drama they were part of and their role)\n",
      "\n",
      "**DISCLAIMER**: I have removed the username of the reviewers from the dataset. I used a generated `user_id` field instead for easier partition.\n",
      "\n",
      "#### What is MyDramaList.com\n",
      "According to mydramalist.com, \"MyDramaList.com is a community-based project which provides Asian drama & movie fans\". On the website, fans can \"create their very own drama watchlists, rate dramas and films, write reviews\" and many more engaging activities. This dataset is different from the previous one as there are more collection of data on Korean drama, reviews, and actors. \n",
      "\n",
      "#### Acknowledgements\n",
      "This data is taken from the website https://mydramalist.com/shows/top_korean_dramas?page=1. This is my first time doing web scrapping. I wouldn't be able to do it without the help of StackOverflow and YouTube.\n",
      "\n",
      "#### Inspiration\n",
      "Similar inspiration to the previous dataset: \"I have been a huge fan of Korean Drama and K pop since high-school. It is fun to integrate what I love with my interest toward data science\". \n",
      "\n",
      "#### Recommendation and Suggestion\n",
      "If you would like to get more data on Korean drama, please let me know. I can get more information on drama that is prior to 2015 as well. Please leave any recommendation and suggestion (can't stop learning!). Thank you in advance!\n",
      "\n",
      "The possibility is endless…keep striving!\n",
      "name 'detect' is not defined Language is not detected: korean-drama-2015-23-actor-and-reviewmydramalist\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda\n",
      "name 'detect' is not defined Language is not detected: Korean Drama from 2015-2023 with Actors & Reviews \n",
      "name 'detect' is not defined Language is not detected: 🔥Spinoff from \"Top 100 Korean Drama (MyDramaList)\" Dataset💯 but MORE DATA!\n",
      "name 'detect' is not defined Language is not detected: ### 😍NEW Korean Drama Dataset - spinoff of ***Top 100 Korean Drama (MyDramaList)*** 🥳<br>\n",
      "Go checkout the dataset using this link: https://www.kaggle.com/datasets/chanoncharuchinda/top-100-korean-drama-mydramalist\n",
      "\n",
      "**IMPORTANT NOTE:** *I made these datasets for **educational purposes**. I want people who **share similar interest and obsession in Korean Drama** to have access to this data to begin their exploration. Have fun!* Also, working with these two datasets simultaneously might be helpful and worth exploring.\n",
      "\n",
      "#### Project and Data Information\n",
      "A while back, I worked on a Dataset called Top 100 Korean Drama (MyDramaList). I decided to work on similar project again. For this project, I have generated three files:\n",
      "1. **korean_drama.csv**: 1752 Korean drama to be explored\n",
      "2. **recommendations.csv**: table contains the recommendation given to different drama. The recommendation is a product of the MyDramaList community.\n",
      "3. **review.csv**: review given to the drama from users on the website\n",
      "4. **wiki_actor.csv**: information regarding the actors (mainly the drama they were part of and their role)\n",
      "\n",
      "**DISCLAIMER**: I have removed the username of the reviewers from the dataset. I used a generated `user_id` field instead for easier partition.\n",
      "\n",
      "#### What is MyDramaList.com\n",
      "According to mydramalist.com, \"MyDramaList.com is a community-based project which provides Asian drama & movie fans\". On the website, fans can \"create their very own drama watchlists, rate dramas and films, write reviews\" and many more engaging activities. This dataset is different from the previous one as there are more collection of data on Korean drama, reviews, and actors. \n",
      "\n",
      "#### Acknowledgements\n",
      "This data is taken from the website https://mydramalist.com/shows/top_korean_dramas?page=1. This is my first time doing web scrapping. I wouldn't be able to do it without the help of StackOverflow and YouTube.\n",
      "\n",
      "#### Inspiration\n",
      "Similar inspiration to the previous dataset: \"I have been a huge fan of Korean Drama and K pop since high-school. It is fun to integrate what I love with my interest toward data science\". \n",
      "\n",
      "#### Recommendation and Suggestion\n",
      "If you would like to get more data on Korean drama, please let me know. I can get more information on drama that is prior to 2015 as well. Please leave any recommendation and suggestion (can't stop learning!). Thank you in advance!\n",
      "\n",
      "The possibility is endless…keep striving!\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda/thai-and-japanese-drama-vs-korean-drama-dominance\n",
      "name 'detect' is not defined Language is not detected: thai-and-japanese-drama-vs-korean-drama-dominance\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda\n",
      "name 'detect' is not defined Language is not detected: Korean Drama Dominance vs. Thai and Japanese Drama\n",
      "name 'detect' is not defined Language is not detected: Drama information for South Korean Drama, Lakorn Thai, and Japanese Drama\n",
      "name 'detect' is not defined Language is not detected: ### 😍NEW Drama Dataset From South Korea, Lakorn Thai, and Japanese Drama<br><br>\n",
      "\n",
      "Go checkout my other datasets on Kaggle using this link: \n",
      "&gt; https://www.kaggle.com/datasets/chanoncharuchinda/top-100-korean-drama-mydramalist\n",
      "&gt; https://www.kaggle.com/datasets/chanoncharuchinda/korean-drama-2015-23-actor-and-reviewmydramalist\n",
      "\n",
      "**IMPORTANT NOTE:** *I made these datasets for **educational purposes**. I want people who **share similar interest and obsession in Korean Drama** to have access to this data to begin their exploration. Have fun!* Also, working with these two datasets simultaneously might be helpful and worth exploring.\n",
      "\n",
      "### Example Notebook\n",
      "The Example Notebook with Data Analysis and Data Visualization will be provided after the assignment is completed. For now, it would be just how to download the data and small descriptions. \n",
      "\n",
      "#### Project and Data Information\n",
      "This section will be updated later once I get more freetime\n",
      "\n",
      "**DISCLAIMER**: I have removed the username of the reviewers from the dataset. I used a generated `user_id` field instead for easier partition.\n",
      "\n",
      "#### What is MyDramaList.com\n",
      "According to mydramalist.com, \"MyDramaList.com is a community-based project which provides Asian drama & movie fans\". On the website, fans can \"create their very own drama watchlists, rate dramas and films, write reviews\" and many more engaging activities. This dataset is different from the previous one as there are more collection of data on Korean drama, reviews, and actors. \n",
      "\n",
      "#### Acknowledgements\n",
      "This data is taken from the website https://mydramalist.com/shows/top_korean_dramas?page=1. This is my first time doing web scrapping. I wouldn't be able to do it without the help of StackOverflow and YouTube.\n",
      "\n",
      "#### Inspiration\n",
      "Initially, this was personal project, but I decided to use this data for my university project in Data Mining and Machine Learning Class.\n",
      "\n",
      "Additionally, \"I have been a huge fan of Korean Drama and K pop since high-school. It is fun to integrate what I love with my interest toward data science\". \n",
      "\n",
      "#### Recommendation and Suggestion\n",
      "If you would like to get more data on Korean drama, please let me know. I can get more information on drama that is prior to 2015 as well. Please leave any recommendation and suggestion (can't stop learning!). Thank you in advance!\n",
      "\n",
      "The possibility is endless…keep striving!\n",
      "name 'detect' is not defined Language is not detected: thai-and-japanese-drama-vs-korean-drama-dominance\n",
      "name 'detect' is not defined Language is not detected: chanoncharuchinda\n",
      "name 'detect' is not defined Language is not detected: Korean Drama Dominance vs. Thai and Japanese Drama\n",
      "name 'detect' is not defined Language is not detected: Drama information for South Korean Drama, Lakorn Thai, and Japanese Drama\n",
      "name 'detect' is not defined Language is not detected: ### 😍NEW Drama Dataset From South Korea, Lakorn Thai, and Japanese Drama<br><br>\n",
      "\n",
      "Go checkout my other datasets on Kaggle using this link: \n",
      "&gt; https://www.kaggle.com/datasets/chanoncharuchinda/top-100-korean-drama-mydramalist\n",
      "&gt; https://www.kaggle.com/datasets/chanoncharuchinda/korean-drama-2015-23-actor-and-reviewmydramalist\n",
      "\n",
      "**IMPORTANT NOTE:** *I made these datasets for **educational purposes**. I want people who **share similar interest and obsession in Korean Drama** to have access to this data to begin their exploration. Have fun!* Also, working with these two datasets simultaneously might be helpful and worth exploring.\n",
      "\n",
      "### Example Notebook\n",
      "The Example Notebook with Data Analysis and Data Visualization will be provided after the assignment is completed. For now, it would be just how to download the data and small descriptions. \n",
      "\n",
      "#### Project and Data Information\n",
      "This section will be updated later once I get more freetime\n",
      "\n",
      "**DISCLAIMER**: I have removed the username of the reviewers from the dataset. I used a generated `user_id` field instead for easier partition.\n",
      "\n",
      "#### What is MyDramaList.com\n",
      "According to mydramalist.com, \"MyDramaList.com is a community-based project which provides Asian drama & movie fans\". On the website, fans can \"create their very own drama watchlists, rate dramas and films, write reviews\" and many more engaging activities. This dataset is different from the previous one as there are more collection of data on Korean drama, reviews, and actors. \n",
      "\n",
      "#### Acknowledgements\n",
      "This data is taken from the website https://mydramalist.com/shows/top_korean_dramas?page=1. This is my first time doing web scrapping. I wouldn't be able to do it without the help of StackOverflow and YouTube.\n",
      "\n",
      "#### Inspiration\n",
      "Initially, this was personal project, but I decided to use this data for my university project in Data Mining and Machine Learning Class.\n",
      "\n",
      "Additionally, \"I have been a huge fan of Korean Drama and K pop since high-school. It is fun to integrate what I love with my interest toward data science\". \n",
      "\n",
      "#### Recommendation and Suggestion\n",
      "If you would like to get more data on Korean drama, please let me know. I can get more information on drama that is prior to 2015 as well. Please leave any recommendation and suggestion (can't stop learning!). Thank you in advance!\n",
      "\n",
      "The possibility is endless…keep striving!\n",
      "name 'detect' is not defined Language is not detected: ravi20076/aes2024ancillary\n",
      "name 'detect' is not defined Language is not detected: aes2024ancillary\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: AES2024|Ancillary\n",
      "name 'detect' is not defined Language is not detected: Ancillary files for the AES competition- Apr-July2024\n",
      "name 'detect' is not defined Language is not detected: This dataset is my public storage for the below competition- \n",
      "[Learning Agency Lab - Automated Essay Scoring 2.0](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2)<br>\n",
      "\n",
      "I intend to store the below herewith-\n",
      "1. Model objects\n",
      "2. Feature importance data \n",
      "3. Interim training files for repeated usage\n",
      "4. Vectorizer\n",
      "5. Any other files considered suitable for inferencing\n",
      "\n",
      "Wishing you all the best for the competition!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aes2024ancillary\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: AES2024|Ancillary\n",
      "name 'detect' is not defined Language is not detected: Ancillary files for the AES competition- Apr-July2024\n",
      "name 'detect' is not defined Language is not detected: This dataset is my public storage for the below competition- \n",
      "[Learning Agency Lab - Automated Essay Scoring 2.0](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2)<br>\n",
      "\n",
      "I intend to store the below herewith-\n",
      "1. Model objects\n",
      "2. Feature importance data \n",
      "3. Interim training files for repeated usage\n",
      "4. Vectorizer\n",
      "5. Any other files considered suitable for inferencing\n",
      "\n",
      "Wishing you all the best for the competition!\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ravi20076/writingqualitymemoryreduction\n",
      "name 'detect' is not defined Language is not detected: writingqualitymemoryreduction\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: WritingQuality|MemoryReduction\n",
      "name 'detect' is not defined Language is not detected: Memory reduced training data for the Writing Process-Writing Quality competition\n",
      "name 'detect' is not defined Language is not detected: This is a memory reduced dataset for the **Writing Process-Writing Quality competition**. I encoded text columns into np.int8 type and binned categories with extremely low occurrences into a common bin. I also down-casted certain columns in the data based on their min-max values to save memory. I have saved the train-logs data in a binary format and the encoded text strings and their categories too as one may need them while inferring on the test data. \n",
      "This is also available in my [baseline data prep kernel](https://www.kaggle.com/code/ravi20076/lwpwq-preprocesseddataset). \n",
      "We will use this data as input for all our future steps including EDA, model development and inference development. We hope not to fall prey to memory errors using such an approach. \n",
      "All the best for the competition!\n",
      "name 'detect' is not defined Language is not detected: writingqualitymemoryreduction\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: WritingQuality|MemoryReduction\n",
      "name 'detect' is not defined Language is not detected: Memory reduced training data for the Writing Process-Writing Quality competition\n",
      "name 'detect' is not defined Language is not detected: This is a memory reduced dataset for the **Writing Process-Writing Quality competition**. I encoded text columns into np.int8 type and binned categories with extremely low occurrences into a common bin. I also down-casted certain columns in the data based on their min-max values to save memory. I have saved the train-logs data in a binary format and the encoded text strings and their categories too as one may need them while inferring on the test data. \n",
      "This is also available in my [baseline data prep kernel](https://www.kaggle.com/code/ravi20076/lwpwq-preprocesseddataset). \n",
      "We will use this data as input for all our future steps including EDA, model development and inference development. We hope not to fall prey to memory errors using such an approach. \n",
      "All the best for the competition!\n",
      "name 'detect' is not defined Language is not detected: ravi20076/enefittraindata\n",
      "name 'detect' is not defined Language is not detected: enefittraindata\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: Enefit|TrainData\n",
      "name 'detect' is not defined Language is not detected: Train data for Enefit competition with secondary features in parquet\n",
      "name 'detect' is not defined Language is not detected: This is the pre-processed dataset for the Enefit competition. This dataset inputs the train data from the competition, builds secondary features and outputs it here. \n",
      "We will develop this dataset as we move along in the competition. This data is used as input to the model development and inferencing kernel. <br>\n",
      "Preprocessing kernel link- https://www.kaggle.com/code/ravi20076/enefit-traindataprep\n",
      "name 'detect' is not defined Language is not detected: enefittraindata\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: Enefit|TrainData\n",
      "name 'detect' is not defined Language is not detected: Train data for Enefit competition with secondary features in parquet\n",
      "name 'detect' is not defined Language is not detected: This is the pre-processed dataset for the Enefit competition. This dataset inputs the train data from the competition, builds secondary features and outputs it here. \n",
      "We will develop this dataset as we move along in the competition. This data is used as input to the model development and inferencing kernel. <br>\n",
      "Preprocessing kernel link- https://www.kaggle.com/code/ravi20076/enefit-traindataprep\n",
      "name 'detect' is not defined Language is not detected: ravi20076/writingqualityessaycollation\n",
      "name 'detect' is not defined Language is not detected: writingqualityessaycollation\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: WritingQuality|EssayCollation\n",
      "name 'detect' is not defined Language is not detected: Construction of essays from the competition training data using text change\n",
      "name 'detect' is not defined Language is not detected: This is a supplementary dataset created from the competition training data using the text change column.\n",
      "This data contains ids and constructed essays across text change column including the old and new text for replacement and input text otherwise. This could be used for EDA and any supplementary analysis as deemed suitable. \n",
      "Best regards.\n",
      "P.S.- competition link- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality\n",
      "name 'detect' is not defined Language is not detected: writingqualityessaycollation\n",
      "name 'detect' is not defined Language is not detected: ravi20076\n",
      "name 'detect' is not defined Language is not detected: WritingQuality|EssayCollation\n",
      "name 'detect' is not defined Language is not detected: Construction of essays from the competition training data using text change\n",
      "name 'detect' is not defined Language is not detected: This is a supplementary dataset created from the competition training data using the text change column.\n",
      "This data contains ids and constructed essays across text change column including the old and new text for replacement and input text otherwise. This could be used for EDA and any supplementary analysis as deemed suitable. \n",
      "Best regards.\n",
      "P.S.- competition link- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality\n",
      "name 'detect' is not defined Language is not detected: sheemazain/netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: sheemazain\n",
      "name 'detect' is not defined Language is not detected: Netflix Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: As of my last update, Netflix doesn't publicly release a comprehensive dataset of its movies and TV shows. However, there are various third-party datasets available that have been compiled by enthusiasts and researchers. These datasets typically include information such as titles, genres, release years, ratings, and sometimes even viewer reviews.\n",
      "\n",
      "One popular dataset is the \"Netflix Prize\" dataset, which was released by Netflix for a competition aimed at improving its recommendation algorithm. However, this dataset is quite old and may not include more recent additions to the Netflix library.\n",
      "\n",
      "Alternatively, you can use web scraping techniques to extract data from Netflix's website, but keep in mind that this may violate their terms of service.\n",
      "\n",
      "If you're interested in exploring Netflix data for research or analysis purposes, I recommend checking out platforms like Kaggle, where you might find user-contributed datasets and kernels analyzing Netflix content. Just ensure that you're complying with any usage restrictions and respecting copyright laws when using such data.\n",
      "name 'detect' is not defined Language is not detected: netflix-movies-and-tv-shows\n",
      "name 'detect' is not defined Language is not detected: sheemazain\n",
      "name 'detect' is not defined Language is not detected: Netflix Movies and TV Shows\n",
      "name 'detect' is not defined Language is not detected: As of my last update, Netflix doesn't publicly release a comprehensive dataset of its movies and TV shows. However, there are various third-party datasets available that have been compiled by enthusiasts and researchers. These datasets typically include information such as titles, genres, release years, ratings, and sometimes even viewer reviews.\n",
      "\n",
      "One popular dataset is the \"Netflix Prize\" dataset, which was released by Netflix for a competition aimed at improving its recommendation algorithm. However, this dataset is quite old and may not include more recent additions to the Netflix library.\n",
      "\n",
      "Alternatively, you can use web scraping techniques to extract data from Netflix's website, but keep in mind that this may violate their terms of service.\n",
      "\n",
      "If you're interested in exploring Netflix data for research or analysis purposes, I recommend checking out platforms like Kaggle, where you might find user-contributed datasets and kernels analyzing Netflix content. Just ensure that you're complying with any usage restrictions and respecting copyright laws when using such data.\n",
      "name 'detect' is not defined Language is not detected: yugagrawal95/movie-data\n",
      "name 'detect' is not defined Language is not detected: movie-data\n",
      "name 'detect' is not defined Language is not detected: yugagrawal95\n",
      "name 'detect' is not defined Language is not detected: Movie Data\n",
      "name 'detect' is not defined Language is not detected: Movie Data set for recommendation\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "There's a story behind every dataset and here's your opportunity to share yours.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: movie-data\n",
      "name 'detect' is not defined Language is not detected: yugagrawal95\n",
      "name 'detect' is not defined Language is not detected: Movie Data\n",
      "name 'detect' is not defined Language is not detected: Movie Data set for recommendation\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "There's a story behind every dataset and here's your opportunity to share yours.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Your data will be in front of the world's largest data science community. What questions do you want to see answered?\n",
      "name 'detect' is not defined Language is not detected: dp5995/gym-exercise-mediapipe-33-landmarks\n",
      "name 'detect' is not defined Language is not detected: gym-exercise-mediapipe-33-landmarks\n",
      "name 'detect' is not defined Language is not detected: dp5995\n",
      "name 'detect' is not defined Language is not detected: Multi-Class Exercise Poses for Human Skeleton\n",
      "name 'detect' is not defined Language is not detected: An In-Depth Analysis of Human Skeleton Movements in Exercise Poses\n",
      "name 'detect' is not defined Language is not detected: This comprehensive exercise pose dataset consists of 2701 rows and 133 columns, providing valuable insights into human skeleton movements during various exercises. With a focus on analyzing different exercise poses, the dataset encompasses seven distinct classes: Rest, Left Bicep, Right Bicep, Left Shoulder, Right Shoulder, Left Tricep, and Right Tricep.\n",
      "\n",
      "Each row corresponds to a specific exercise, while the columns represent different aspects of the human skeleton model. The dataset captures the coordinates (X, Y, Z), and visibility values of 33 landmarks, resulting in a total of 132 values per exercise. These landmarks serve as key reference points to evaluate body positions and movements accurately.\n",
      "\n",
      "The dataset is invaluable for researchers, data scientists, and fitness enthusiasts seeking to understand human skeleton kinetics during exercise routines. It enables comprehensive analysis of body posture, movement patterns, and joint angles, facilitating in-depth insights into exercise performance and form.\n",
      "\n",
      "This dataset is an ideal resource for machine learning enthusiasts who wish to develop and evaluate models for exercise pose recognition, gesture analysis, and exercise tracking systems. The rich annotation and class labels make it suitable for tasks such as activity recognition, pose estimation, and exercise recommendation systems.\n",
      "\n",
      "By contributing this dataset to Kaggle, I aim to foster collaboration, knowledge sharing, and further advancements in the analysis of human skeleton movements. Researchers and data scientists can leverage this dataset to devise innovative approaches and algorithms to improve fitness monitoring, performance tracking, and personalized exercise guidance.\n",
      "name 'detect' is not defined Language is not detected: gym-exercise-mediapipe-33-landmarks\n",
      "name 'detect' is not defined Language is not detected: dp5995\n",
      "name 'detect' is not defined Language is not detected: Multi-Class Exercise Poses for Human Skeleton\n",
      "name 'detect' is not defined Language is not detected: An In-Depth Analysis of Human Skeleton Movements in Exercise Poses\n",
      "name 'detect' is not defined Language is not detected: This comprehensive exercise pose dataset consists of 2701 rows and 133 columns, providing valuable insights into human skeleton movements during various exercises. With a focus on analyzing different exercise poses, the dataset encompasses seven distinct classes: Rest, Left Bicep, Right Bicep, Left Shoulder, Right Shoulder, Left Tricep, and Right Tricep.\n",
      "\n",
      "Each row corresponds to a specific exercise, while the columns represent different aspects of the human skeleton model. The dataset captures the coordinates (X, Y, Z), and visibility values of 33 landmarks, resulting in a total of 132 values per exercise. These landmarks serve as key reference points to evaluate body positions and movements accurately.\n",
      "\n",
      "The dataset is invaluable for researchers, data scientists, and fitness enthusiasts seeking to understand human skeleton kinetics during exercise routines. It enables comprehensive analysis of body posture, movement patterns, and joint angles, facilitating in-depth insights into exercise performance and form.\n",
      "\n",
      "This dataset is an ideal resource for machine learning enthusiasts who wish to develop and evaluate models for exercise pose recognition, gesture analysis, and exercise tracking systems. The rich annotation and class labels make it suitable for tasks such as activity recognition, pose estimation, and exercise recommendation systems.\n",
      "\n",
      "By contributing this dataset to Kaggle, I aim to foster collaboration, knowledge sharing, and further advancements in the analysis of human skeleton movements. Researchers and data scientists can leverage this dataset to devise innovative approaches and algorithms to improve fitness monitoring, performance tracking, and personalized exercise guidance.\n",
      "name 'detect' is not defined Language is not detected: amolbhone/lead-score-case-study\n",
      "name 'detect' is not defined Language is not detected: lead-score-case-study\n",
      "name 'detect' is not defined Language is not detected: amolbhone\n",
      "name 'detect' is not defined Language is not detected: Lead Score - Upgrad Case study about X Education\n",
      "name 'detect' is not defined Language is not detected: Lead Score - Case Study\n",
      "name 'detect' is not defined Language is not detected: **Problem Statement**\n",
      "\n",
      "An X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the\n",
      "courses land on their website and browse for courses.\n",
      "The company markets its courses on several websites and search engines like Google. Once these people land on the website,\n",
      "they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form\n",
      "providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads\n",
      "through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc.\n",
      "Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is\n",
      "around 30%.\n",
      "Now, although X Education gets a lot of leads, its lead conversion rate is very poor. To make this process more efficient, the\n",
      "company wishes to identify the most potential leads, also known as ‘Hot Leads’.\n",
      "The company requires you to build a model wherein need to assign a lead score to each of the leads such that the customers\n",
      "with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion\n",
      "chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n",
      "\n",
      "\n",
      "**Goals of the Case Study**\n",
      "\n",
      "There are quite a few goals for this case study.\n",
      "1.\n",
      "Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the\n",
      "company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a\n",
      "lower score would mean that the lead is cold and will mostly not get converted.\n",
      "2.\n",
      "There are some more problems presented by the company which your model should be able to adjust to if the company's\n",
      "requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc\n",
      "file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in yo ur final\n",
      "PPT where you'll make recommendations.\n",
      "name 'detect' is not defined Language is not detected: lead-score-case-study\n",
      "name 'detect' is not defined Language is not detected: amolbhone\n",
      "name 'detect' is not defined Language is not detected: Lead Score - Upgrad Case study about X Education\n",
      "name 'detect' is not defined Language is not detected: Lead Score - Case Study\n",
      "name 'detect' is not defined Language is not detected: **Problem Statement**\n",
      "\n",
      "An X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the\n",
      "courses land on their website and browse for courses.\n",
      "The company markets its courses on several websites and search engines like Google. Once these people land on the website,\n",
      "they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form\n",
      "providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads\n",
      "through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc.\n",
      "Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is\n",
      "around 30%.\n",
      "Now, although X Education gets a lot of leads, its lead conversion rate is very poor. To make this process more efficient, the\n",
      "company wishes to identify the most potential leads, also known as ‘Hot Leads’.\n",
      "The company requires you to build a model wherein need to assign a lead score to each of the leads such that the customers\n",
      "with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion\n",
      "chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n",
      "\n",
      "\n",
      "**Goals of the Case Study**\n",
      "\n",
      "There are quite a few goals for this case study.\n",
      "1.\n",
      "Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the\n",
      "company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a\n",
      "lower score would mean that the lead is cold and will mostly not get converted.\n",
      "2.\n",
      "There are some more problems presented by the company which your model should be able to adjust to if the company's\n",
      "requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc\n",
      "file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in yo ur final\n",
      "PPT where you'll make recommendations.\n",
      "name 'detect' is not defined Language is not detected: nowke9/ipldata\n",
      "name 'detect' is not defined Language is not detected: ipldata\n",
      "name 'detect' is not defined Language is not detected: nowke9\n",
      "name 'detect' is not defined Language is not detected: Indian Premier League 2008-2019\n",
      "name 'detect' is not defined Language is not detected: Ball-by-ball Indian Premier League (IPL) cricket dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Indian Premier League (IPL) is a Twenty20 cricket format league in India. It is usually played in April and May every year. As of 2019, the title sponsor of the game is Vivo. The league was founded by Board of Control for Cricket India (BCCI) in 2008. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      " - Data till Season 11 (2008 - 2019)\n",
      " - `matches.csv` - Match by match data\n",
      " - `deliveries.csv` - Ball by ball data\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      " - Data source from 2008-2017 - [CricSheet.org][1] and [Manas - Kaggle][2]\n",
      " - Data source for 2018-2019 - [IPL T20 - Official website][3]\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Draw analysis, player/team performance, apply and learn statistical methods on real data\n",
      "\n",
      "### Kernels\n",
      "**Statistics**\n",
      "\n",
      "* [Summarizing quantitative data (mean, median, std. deviation, percentile, box plots etc.)](https://www.kaggle.com/nowke9/statistics-1-summarizing-quantitative-data)\n",
      "* [Distributions - Cumulative relative frequency, Normal distribution, PDF, Z-score, empirical rule, binomial distribution, Bernoulli distribution](https://www.kaggle.com/nowke9/statistics-2-distributions/)\n",
      "* [Bivariate data - Scatter plot, Correlation, Covariance, Least square regression, R-Squared, Root mean square error](https://www.kaggle.com/nowke9/statistics-3-bivariate-data)\n",
      "\n",
      "  [1]: https://cricsheet.org/ \"CricSheet\"\n",
      "  [2]: https://www.kaggle.com/manasgarg/ipl \"Kaggle - Indian Premier League (Cricket)\"\n",
      "  [3]: https://www.iplt20.com/ \"IPLT20.com\"\n",
      "name 'detect' is not defined Language is not detected: ipldata\n",
      "name 'detect' is not defined Language is not detected: nowke9\n",
      "name 'detect' is not defined Language is not detected: Indian Premier League 2008-2019\n",
      "name 'detect' is not defined Language is not detected: Ball-by-ball Indian Premier League (IPL) cricket dataset\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Indian Premier League (IPL) is a Twenty20 cricket format league in India. It is usually played in April and May every year. As of 2019, the title sponsor of the game is Vivo. The league was founded by Board of Control for Cricket India (BCCI) in 2008. \n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      " - Data till Season 11 (2008 - 2019)\n",
      " - `matches.csv` - Match by match data\n",
      " - `deliveries.csv` - Ball by ball data\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      " - Data source from 2008-2017 - [CricSheet.org][1] and [Manas - Kaggle][2]\n",
      " - Data source for 2018-2019 - [IPL T20 - Official website][3]\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Draw analysis, player/team performance, apply and learn statistical methods on real data\n",
      "\n",
      "### Kernels\n",
      "**Statistics**\n",
      "\n",
      "* [Summarizing quantitative data (mean, median, std. deviation, percentile, box plots etc.)](https://www.kaggle.com/nowke9/statistics-1-summarizing-quantitative-data)\n",
      "* [Distributions - Cumulative relative frequency, Normal distribution, PDF, Z-score, empirical rule, binomial distribution, Bernoulli distribution](https://www.kaggle.com/nowke9/statistics-2-distributions/)\n",
      "* [Bivariate data - Scatter plot, Correlation, Covariance, Least square regression, R-Squared, Root mean square error](https://www.kaggle.com/nowke9/statistics-3-bivariate-data)\n",
      "\n",
      "  [1]: https://cricsheet.org/ \"CricSheet\"\n",
      "  [2]: https://www.kaggle.com/manasgarg/ipl \"Kaggle - Indian Premier League (Cricket)\"\n",
      "  [3]: https://www.iplt20.com/ \"IPLT20.com\"\n",
      "name 'detect' is not defined Language is not detected: paakhim10/tweets-and-engagement-metrics\n",
      "name 'detect' is not defined Language is not detected: tweets-and-engagement-metrics\n",
      "name 'detect' is not defined Language is not detected: paakhim10\n",
      "name 'detect' is not defined Language is not detected: Tweets and Engagement Metrics\n",
      "name 'detect' is not defined Language is not detected: Twitter Data containing temporal, geographical, and general tweet metrics.\n",
      "name 'detect' is not defined Language is not detected: ### This is the data wrangled and cleaned from an existing dataset, and it is the output of my notebook: [https://www.kaggle.com/code/paakhim10/analyzing-and-classifying-twitter-sentiments](url)\n",
      "\n",
      "####You can use this dataset for:\n",
      "\n",
      "&gt;• Machine Learning\n",
      "1. Sentiment analysis\n",
      "2. Spam detection\n",
      "3. Text classification\n",
      "4. Trend identification (temporal and geographical)\n",
      "5. Building recommendation systems\n",
      "\n",
      "&gt;• Exploratory Data Analysis\n",
      "1. Geospatial or temporal mapping\n",
      "2. Hashtags Trend Detection\n",
      "3. User Engagement Metrics\n",
      "\n",
      "##### Credits:\n",
      "If you use this dataset in your research, please credit the original authors.\n",
      "If you use this dataset in your research, please credit [https://data.world/kjensen18](url)\n",
      "name 'detect' is not defined Language is not detected: tweets-and-engagement-metrics\n",
      "name 'detect' is not defined Language is not detected: paakhim10\n",
      "name 'detect' is not defined Language is not detected: Tweets and Engagement Metrics\n",
      "name 'detect' is not defined Language is not detected: Twitter Data containing temporal, geographical, and general tweet metrics.\n",
      "name 'detect' is not defined Language is not detected: ### This is the data wrangled and cleaned from an existing dataset, and it is the output of my notebook: [https://www.kaggle.com/code/paakhim10/analyzing-and-classifying-twitter-sentiments](url)\n",
      "\n",
      "####You can use this dataset for:\n",
      "\n",
      "&gt;• Machine Learning\n",
      "1. Sentiment analysis\n",
      "2. Spam detection\n",
      "3. Text classification\n",
      "4. Trend identification (temporal and geographical)\n",
      "5. Building recommendation systems\n",
      "\n",
      "&gt;• Exploratory Data Analysis\n",
      "1. Geospatial or temporal mapping\n",
      "2. Hashtags Trend Detection\n",
      "3. User Engagement Metrics\n",
      "\n",
      "##### Credits:\n",
      "If you use this dataset in your research, please credit the original authors.\n",
      "If you use this dataset in your research, please credit [https://data.world/kjensen18](url)\n",
      "name 'detect' is not defined Language is not detected: tsaustin/us-used-car-sales-data\n",
      "name 'detect' is not defined Language is not detected: us-used-car-sales-data\n",
      "name 'detect' is not defined Language is not detected: tsaustin\n",
      "name 'detect' is not defined Language is not detected: US used car sales data\n",
      "name 'detect' is not defined Language is not detected: Used car sales data with car make, model, year and more\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This is a data set for used car sales in the US. In total ~160k sales records over a period of 20 months in 2019 and 2020. There won't be any more updates to this dataset as eBay stopped providing full ZIP codes.\n",
      "\n",
      "### Content\n",
      "Each sample contains information about a used car sale, like selling price, location, details about the car (Make, model, year, mileage, etc). \n",
      " \n",
      "The data was scraped from eBay. I tried to filter the data when updating, meaning that if the same seller sells the same car again I'll remove the previous sale as it was most likely not a successful sale (Do you know that [eBay bids on cars are non binding][1]?)\n",
      "\n",
      "### Acknowledgements\n",
      "Header Image Credit: [Laitr Keiows \\[CC BY 3.0 (https://creativecommons.org/licenses/by/3.0)\\]][2]\n",
      "\n",
      "### Inspiration\n",
      "I'm curious what the community can do with this data to identify trends or even predict a fair price for a used car.\n",
      "\n",
      "\n",
      "  [1]: https://www.ebay.com/help/policies/rules-policies-buyers/nonbinding-bid-policy?id=4228\n",
      "  [2]: https://commons.wikimedia.org/wiki/File:Parking_lot_at_HAA_Kobe.jpg#filelinks\n",
      "name 'detect' is not defined Language is not detected: us-used-car-sales-data\n",
      "name 'detect' is not defined Language is not detected: tsaustin\n",
      "name 'detect' is not defined Language is not detected: US used car sales data\n",
      "name 'detect' is not defined Language is not detected: Used car sales data with car make, model, year and more\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This is a data set for used car sales in the US. In total ~160k sales records over a period of 20 months in 2019 and 2020. There won't be any more updates to this dataset as eBay stopped providing full ZIP codes.\n",
      "\n",
      "### Content\n",
      "Each sample contains information about a used car sale, like selling price, location, details about the car (Make, model, year, mileage, etc). \n",
      " \n",
      "The data was scraped from eBay. I tried to filter the data when updating, meaning that if the same seller sells the same car again I'll remove the previous sale as it was most likely not a successful sale (Do you know that [eBay bids on cars are non binding][1]?)\n",
      "\n",
      "### Acknowledgements\n",
      "Header Image Credit: [Laitr Keiows \\[CC BY 3.0 (https://creativecommons.org/licenses/by/3.0)\\]][2]\n",
      "\n",
      "### Inspiration\n",
      "I'm curious what the community can do with this data to identify trends or even predict a fair price for a used car.\n",
      "\n",
      "\n",
      "  [1]: https://www.ebay.com/help/policies/rules-policies-buyers/nonbinding-bid-policy?id=4228\n",
      "  [2]: https://commons.wikimedia.org/wiki/File:Parking_lot_at_HAA_Kobe.jpg#filelinks\n",
      "name 'detect' is not defined Language is not detected: soundslikedata/flu-shot-learning\n",
      "name 'detect' is not defined Language is not detected: flu-shot-learning\n",
      "name 'detect' is not defined Language is not detected: soundslikedata\n",
      "name 'detect' is not defined Language is not detected: Flu Shot Learning\n",
      "name 'detect' is not defined Language is not detected: HOSTED BY DRIVENDATA\n",
      "name 'detect' is not defined Language is not detected: ---\n",
      "\n",
      "The data for this competition comes from the National 2009 H1N1 Flu Survey (NHFS).\n",
      "\n",
      "In their own words:\n",
      "\n",
      "&gt; The National 2009 H1N1 Flu Survey (NHFS) was sponsored by the National Center for Immunization and Respiratory Diseases (NCIRD) and conducted jointly by NCIRD and the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC). The NHFS was a list-assisted random-digit-dialing telephone survey of households, designed to monitor influenza immunization coverage in the 2009-10 season.\n",
      "&gt; \n",
      "&gt; The target population for the NHFS was all persons 6 months or older living in the United States at the time of the interview. Data from the NHFS were used to produce timely estimates of vaccination coverage rates for both the monovalent pH1N1 and trivalent seasonal influenza vaccines.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "  \n",
      "\n",
      "The NHFS was conducted between October 2009 and June 2010. It was one-time survey designed specifically to monitor vaccination during the 2009-2010 flu season in response to the 2009 H1N1 pandemic. The CDC has other ongoing programs for annual phone surveys that continue to monitor seasonal flu vaccination.\n",
      "\n",
      "## Data use restrictions\n",
      "\n",
      "The source dataset comes with the following data use restrictions:\n",
      "\n",
      "&gt; The Public Health Service Act (Section 308(d)) provides that the data collected by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC), may be used only for the purpose of health statistical reporting and analysis.\n",
      "&gt; \n",
      "&gt; Any effort to determine the identity of any reported case is prohibited by this law.\n",
      "&gt; \n",
      "&gt; NCHS does all it can to ensure that the identity of data subjects cannot be disclosed. All direct identifiers, as well as any characteristics that might lead to identification, are omitted from the data files. Any intentional identification or disclosure of a person or establishment violates the assurances of confidentiality given to the providers of the information.\n",
      "&gt; \n",
      "&gt; Therefore, users will:\n",
      "&gt; \n",
      "&gt; 1.  Use the data in these data files for statistical reporting and analysis only.\n",
      "&gt; 2.  Make no use of the identity of any person or establishment discovered inadvertently and advise the Director, NCHS, of any such discovery (1 (800) 232-4636).\n",
      "&gt; 3.  Not link these data files with individually identifiable data from other NCHS or non-NCHS data files.\n",
      "&gt; \n",
      "&gt; By using these data, you signify your agreement to comply with the above requirements.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "-   [U.S. National 2009 H1N1 Flu Survey (NHFS)](https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1)\n",
      "-   [U.S. National Immunization Surveys (NIS)](https://www.cdc.gov/vaccines/imz-managers/nis/about.html)\n",
      "-   [2009 H1N1 Pandemic (H1N1pdm09 virus)](https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html), by the U.S. CDC\n",
      "-   [About Flu](https://www.cdc.gov/flu/about/index.html), by the U.S. CDC\n",
      "-   [Key Facts About Seasonal Flu Vaccine](https://www.cdc.gov/flu/prevent/keyfacts.htm), by the U.S. CDC\n",
      "\n",
      "Data is provided courtesy of the United States [National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm).\n",
      "\n",
      "U.S. Department of Health and Human Services (DHHS). National Center for Health Statistics. The National 2009 H1N1 Flu Survey. Hyattsville, MD: Centers for Disease Control and Prevention, 2012.\n",
      "\n",
      "Images courtesy of the [U.S. Navy](https://www.flickr.com/photos/navcent/15607260325/) and the [Fort Meade Public Affairs Office](https://www.flickr.com/photos/ftmeade/15242740638/) via Flickr under the [CC BY 2.0 license](https://creativecommons.org/licenses/by/2.0/legalcode).\n",
      "\n",
      "The data for this competition comes from the National 2009 H1N1 Flu Survey (NHFS).\n",
      "\n",
      "In their own words:\n",
      "\n",
      "&gt; The National 2009 H1N1 Flu Survey (NHFS) was sponsored by the National Center for Immunization and Respiratory Diseases (NCIRD) and conducted jointly by NCIRD and the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC). The NHFS was a list-assisted random-digit-dialing telephone survey of households, designed to monitor influenza immunization coverage in the 2009-10 season.\n",
      "&gt; \n",
      "&gt; The target population for the NHFS was all persons 6 months or older living in the United States at the time of the interview. Data from the NHFS were used to produce timely estimates of vaccination coverage rates for both the monovalent pH1N1 and trivalent seasonal influenza vaccines.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "  \n",
      "\n",
      "The NHFS was conducted between October 2009 and June 2010. It was one-time survey designed specifically to monitor vaccination during the 2009-2010 flu season in response to the 2009 H1N1 pandemic. The CDC has other ongoing programs for annual phone surveys that continue to monitor seasonal flu vaccination.\n",
      "\n",
      "## Data use restrictions\n",
      "\n",
      "The source dataset comes with the following data use restrictions:\n",
      "\n",
      "&gt; The Public Health Service Act (Section 308(d)) provides that the data collected by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC), may be used only for the purpose of health statistical reporting and analysis.\n",
      "&gt; \n",
      "&gt; Any effort to determine the identity of any reported case is prohibited by this law.\n",
      "&gt; \n",
      "&gt; NCHS does all it can to ensure that the identity of data subjects cannot be disclosed. All direct identifiers, as well as any characteristics that might lead to identification, are omitted from the data files. Any intentional identification or disclosure of a person or establishment violates the assurances of confidentiality given to the providers of the information.\n",
      "&gt; \n",
      "&gt; Therefore, users will:\n",
      "&gt; \n",
      "&gt; 1.  Use the data in these data files for statistical reporting and analysis only.\n",
      "&gt; 2.  Make no use of the identity of any person or establishment discovered inadvertently and advise the Director, NCHS, of any such discovery (1 (800) 232-4636).\n",
      "&gt; 3.  Not link these data files with individually identifiable data from other NCHS or non-NCHS data files.\n",
      "&gt; \n",
      "&gt; By using these data, you signify your agreement to comply with the above requirements.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "-   [U.S. National 2009 H1N1 Flu Survey (NHFS)](https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1)\n",
      "-   [U.S. National Immunization Surveys (NIS)](https://www.cdc.gov/vaccines/imz-managers/nis/about.html)\n",
      "-   [2009 H1N1 Pandemic (H1N1pdm09 virus)](https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html), by the U.S. CDC\n",
      "-   [About Flu](https://www.cdc.gov/flu/about/index.html), by the U.S. CDC\n",
      "-   [Key Facts About Seasonal Flu Vaccine](https://www.cdc.gov/flu/prevent/keyfacts.htm), by the U.S. CDC\n",
      "name 'detect' is not defined Language is not detected: flu-shot-learning\n",
      "name 'detect' is not defined Language is not detected: soundslikedata\n",
      "name 'detect' is not defined Language is not detected: Flu Shot Learning\n",
      "name 'detect' is not defined Language is not detected: HOSTED BY DRIVENDATA\n",
      "name 'detect' is not defined Language is not detected: ---\n",
      "\n",
      "The data for this competition comes from the National 2009 H1N1 Flu Survey (NHFS).\n",
      "\n",
      "In their own words:\n",
      "\n",
      "&gt; The National 2009 H1N1 Flu Survey (NHFS) was sponsored by the National Center for Immunization and Respiratory Diseases (NCIRD) and conducted jointly by NCIRD and the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC). The NHFS was a list-assisted random-digit-dialing telephone survey of households, designed to monitor influenza immunization coverage in the 2009-10 season.\n",
      "&gt; \n",
      "&gt; The target population for the NHFS was all persons 6 months or older living in the United States at the time of the interview. Data from the NHFS were used to produce timely estimates of vaccination coverage rates for both the monovalent pH1N1 and trivalent seasonal influenza vaccines.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "  \n",
      "\n",
      "The NHFS was conducted between October 2009 and June 2010. It was one-time survey designed specifically to monitor vaccination during the 2009-2010 flu season in response to the 2009 H1N1 pandemic. The CDC has other ongoing programs for annual phone surveys that continue to monitor seasonal flu vaccination.\n",
      "\n",
      "## Data use restrictions\n",
      "\n",
      "The source dataset comes with the following data use restrictions:\n",
      "\n",
      "&gt; The Public Health Service Act (Section 308(d)) provides that the data collected by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC), may be used only for the purpose of health statistical reporting and analysis.\n",
      "&gt; \n",
      "&gt; Any effort to determine the identity of any reported case is prohibited by this law.\n",
      "&gt; \n",
      "&gt; NCHS does all it can to ensure that the identity of data subjects cannot be disclosed. All direct identifiers, as well as any characteristics that might lead to identification, are omitted from the data files. Any intentional identification or disclosure of a person or establishment violates the assurances of confidentiality given to the providers of the information.\n",
      "&gt; \n",
      "&gt; Therefore, users will:\n",
      "&gt; \n",
      "&gt; 1.  Use the data in these data files for statistical reporting and analysis only.\n",
      "&gt; 2.  Make no use of the identity of any person or establishment discovered inadvertently and advise the Director, NCHS, of any such discovery (1 (800) 232-4636).\n",
      "&gt; 3.  Not link these data files with individually identifiable data from other NCHS or non-NCHS data files.\n",
      "&gt; \n",
      "&gt; By using these data, you signify your agreement to comply with the above requirements.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "-   [U.S. National 2009 H1N1 Flu Survey (NHFS)](https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1)\n",
      "-   [U.S. National Immunization Surveys (NIS)](https://www.cdc.gov/vaccines/imz-managers/nis/about.html)\n",
      "-   [2009 H1N1 Pandemic (H1N1pdm09 virus)](https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html), by the U.S. CDC\n",
      "-   [About Flu](https://www.cdc.gov/flu/about/index.html), by the U.S. CDC\n",
      "-   [Key Facts About Seasonal Flu Vaccine](https://www.cdc.gov/flu/prevent/keyfacts.htm), by the U.S. CDC\n",
      "\n",
      "Data is provided courtesy of the United States [National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm).\n",
      "\n",
      "U.S. Department of Health and Human Services (DHHS). National Center for Health Statistics. The National 2009 H1N1 Flu Survey. Hyattsville, MD: Centers for Disease Control and Prevention, 2012.\n",
      "\n",
      "Images courtesy of the [U.S. Navy](https://www.flickr.com/photos/navcent/15607260325/) and the [Fort Meade Public Affairs Office](https://www.flickr.com/photos/ftmeade/15242740638/) via Flickr under the [CC BY 2.0 license](https://creativecommons.org/licenses/by/2.0/legalcode).\n",
      "\n",
      "The data for this competition comes from the National 2009 H1N1 Flu Survey (NHFS).\n",
      "\n",
      "In their own words:\n",
      "\n",
      "&gt; The National 2009 H1N1 Flu Survey (NHFS) was sponsored by the National Center for Immunization and Respiratory Diseases (NCIRD) and conducted jointly by NCIRD and the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC). The NHFS was a list-assisted random-digit-dialing telephone survey of households, designed to monitor influenza immunization coverage in the 2009-10 season.\n",
      "&gt; \n",
      "&gt; The target population for the NHFS was all persons 6 months or older living in the United States at the time of the interview. Data from the NHFS were used to produce timely estimates of vaccination coverage rates for both the monovalent pH1N1 and trivalent seasonal influenza vaccines.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "  \n",
      "\n",
      "The NHFS was conducted between October 2009 and June 2010. It was one-time survey designed specifically to monitor vaccination during the 2009-2010 flu season in response to the 2009 H1N1 pandemic. The CDC has other ongoing programs for annual phone surveys that continue to monitor seasonal flu vaccination.\n",
      "\n",
      "## Data use restrictions\n",
      "\n",
      "The source dataset comes with the following data use restrictions:\n",
      "\n",
      "&gt; The Public Health Service Act (Section 308(d)) provides that the data collected by the National Center for Health Statistics (NCHS), Centers for Disease Control and Prevention (CDC), may be used only for the purpose of health statistical reporting and analysis.\n",
      "&gt; \n",
      "&gt; Any effort to determine the identity of any reported case is prohibited by this law.\n",
      "&gt; \n",
      "&gt; NCHS does all it can to ensure that the identity of data subjects cannot be disclosed. All direct identifiers, as well as any characteristics that might lead to identification, are omitted from the data files. Any intentional identification or disclosure of a person or establishment violates the assurances of confidentiality given to the providers of the information.\n",
      "&gt; \n",
      "&gt; Therefore, users will:\n",
      "&gt; \n",
      "&gt; 1.  Use the data in these data files for statistical reporting and analysis only.\n",
      "&gt; 2.  Make no use of the identity of any person or establishment discovered inadvertently and advise the Director, NCHS, of any such discovery (1 (800) 232-4636).\n",
      "&gt; 3.  Not link these data files with individually identifiable data from other NCHS or non-NCHS data files.\n",
      "&gt; \n",
      "&gt; By using these data, you signify your agreement to comply with the above requirements.\n",
      "\n",
      "[National 2009 H1N1 Flu Survey Public-Use Data File Readme](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nhfs/nhfspuf_readme.txt)\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "-   [U.S. National 2009 H1N1 Flu Survey (NHFS)](https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1)\n",
      "-   [U.S. National Immunization Surveys (NIS)](https://www.cdc.gov/vaccines/imz-managers/nis/about.html)\n",
      "-   [2009 H1N1 Pandemic (H1N1pdm09 virus)](https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html), by the U.S. CDC\n",
      "-   [About Flu](https://www.cdc.gov/flu/about/index.html), by the U.S. CDC\n",
      "-   [Key Facts About Seasonal Flu Vaccine](https://www.cdc.gov/flu/prevent/keyfacts.htm), by the U.S. CDC\n",
      "name 'detect' is not defined Language is not detected: manashjyotiborah/top-10000-movies-hosted-on-tmdb\n",
      "name 'detect' is not defined Language is not detected: top-10000-movies-hosted-on-tmdb\n",
      "name 'detect' is not defined Language is not detected: manashjyotiborah\n",
      "name 'detect' is not defined Language is not detected: Popular 10,000 movies listed on TMDb (latest)\n",
      "name 'detect' is not defined Language is not detected: Popular 10000 English movies (latest)\n",
      "name 'detect' is not defined Language is not detected: **Context**: \n",
      "\n",
      "These files contain TMDb datasets of popular 10000 English movies. Key data points of the datasets are: cast, crew, keywords, genre, overview, popularity, vote_average, vote_count, title etc.\n",
      "\n",
      "**Acknowledegements**:\n",
      "\n",
      "The cast, crew, keywords, and movie informations have been collected from the TMDb using the public API through web-scraping. Additional information regarding movies can be accessed through the public API on the TMDb website. However, these datasets contain only the relevant columns that are required for building recommendation systems. \n",
      "\n",
      "If you need an extra feature/column, feel free to post it on the discussion tab.\n",
      "\n",
      "**Useful for**:\n",
      "\n",
      "* building recommendation systems based on *content-based*, *collaborative filtering based* or *hybrid* approach.\n",
      "\n",
      "Please consider an **UPVOTE** if you found it useful.\n",
      "name 'detect' is not defined Language is not detected: top-10000-movies-hosted-on-tmdb\n",
      "name 'detect' is not defined Language is not detected: manashjyotiborah\n",
      "name 'detect' is not defined Language is not detected: Popular 10,000 movies listed on TMDb (latest)\n",
      "name 'detect' is not defined Language is not detected: Popular 10000 English movies (latest)\n",
      "name 'detect' is not defined Language is not detected: **Context**: \n",
      "\n",
      "These files contain TMDb datasets of popular 10000 English movies. Key data points of the datasets are: cast, crew, keywords, genre, overview, popularity, vote_average, vote_count, title etc.\n",
      "\n",
      "**Acknowledegements**:\n",
      "\n",
      "The cast, crew, keywords, and movie informations have been collected from the TMDb using the public API through web-scraping. Additional information regarding movies can be accessed through the public API on the TMDb website. However, these datasets contain only the relevant columns that are required for building recommendation systems. \n",
      "\n",
      "If you need an extra feature/column, feel free to post it on the discussion tab.\n",
      "\n",
      "**Useful for**:\n",
      "\n",
      "* building recommendation systems based on *content-based*, *collaborative filtering based* or *hybrid* approach.\n",
      "\n",
      "Please consider an **UPVOTE** if you found it useful.\n",
      "name 'detect' is not defined Language is not detected: vaipant/anime-data-from-1970-to-2024\n",
      "name 'detect' is not defined Language is not detected: anime-data-from-1970-to-2024\n",
      "name 'detect' is not defined Language is not detected: vaipant\n",
      "name 'detect' is not defined Language is not detected: Anime Data from 1970 to 2024\n",
      "name 'detect' is not defined Language is not detected: Explore Anime Evolution: Comprehensive Dataset and Scraping Script (1970-2024)\n",
      "name 'detect' is not defined Language is not detected: This dataset contains a comprehensive collection of anime titles spanning the years 1970 to 2024. The data was collected from MyAnimeList using web scraping techniques. It includes essential information about each anime, such as its unique ID, title, genre, description, studio, release year, and user ratings. The dataset offers a valuable resource for exploring the evolution of anime over the decades and understanding trends in the industry. Researchers, anime enthusiasts, and data analysts can use this dataset to analyze various aspects of anime production and consumption, including popular genres, top-rated studios, and changes in audience preferences over time. The dataset is presented in a CSV format and is suitable for a wide range of data analysis and machine learning applications.\n",
      "\n",
      "## Columns in the data.json :\n",
      "mal_id: Unique identifier for the anime entry.\n",
      "titles: List of titles associated with the anime. In this case, \"Attack No.1\".\n",
      "type: Type of the anime, e.g., TV series.\n",
      "source: Source material of the anime, here it's based on a manga.\n",
      "episodes: Number of episodes in the anime (104 in this case).\n",
      "rating: Audience rating category, PG-13 in this example.\n",
      "score: Average score given to the anime by users.\n",
      "scored_by: Number of users who have scored the anime.\n",
      "rank: Ranking of the anime based on score or popularity.\n",
      "popularity: Popularity ranking of the anime.\n",
      "members: Number of members who have added this anime to their list.\n",
      "favorites: Number of users who have favorited this anime.\n",
      "synopsis: Plot summary or synopsis of the anime.\n",
      "studios: Production studio responsible for creating the anime.\n",
      "genres: List of genres the anime belongs to (e.g., Drama, Sports).\n",
      "themes: List of themes present in the anime (e.g., Team Sports).\n",
      "\n",
      "## Columns in the  user_recommendation.csv :\n",
      "mal_id: This column represents the ID of an anime that users have watched or interacted with.\n",
      "mal_id_recomm: This column lists the IDs of anime recommended by users for a specific mal_id.\n",
      "votes: The votes column indicates the number of votes or recommendations given by users for the recommendation of mal_id_recomm for mal_id.\n",
      "\n",
      "\n",
      "The dataset is ready for exploration, analysis, and visualization to uncover insights into the world of anime and its dynamic landscape.\n",
      "name 'detect' is not defined Language is not detected: anime-data-from-1970-to-2024\n",
      "name 'detect' is not defined Language is not detected: vaipant\n",
      "name 'detect' is not defined Language is not detected: Anime Data from 1970 to 2024\n",
      "name 'detect' is not defined Language is not detected: Explore Anime Evolution: Comprehensive Dataset and Scraping Script (1970-2024)\n",
      "name 'detect' is not defined Language is not detected: This dataset contains a comprehensive collection of anime titles spanning the years 1970 to 2024. The data was collected from MyAnimeList using web scraping techniques. It includes essential information about each anime, such as its unique ID, title, genre, description, studio, release year, and user ratings. The dataset offers a valuable resource for exploring the evolution of anime over the decades and understanding trends in the industry. Researchers, anime enthusiasts, and data analysts can use this dataset to analyze various aspects of anime production and consumption, including popular genres, top-rated studios, and changes in audience preferences over time. The dataset is presented in a CSV format and is suitable for a wide range of data analysis and machine learning applications.\n",
      "\n",
      "## Columns in the data.json :\n",
      "mal_id: Unique identifier for the anime entry.\n",
      "titles: List of titles associated with the anime. In this case, \"Attack No.1\".\n",
      "type: Type of the anime, e.g., TV series.\n",
      "source: Source material of the anime, here it's based on a manga.\n",
      "episodes: Number of episodes in the anime (104 in this case).\n",
      "rating: Audience rating category, PG-13 in this example.\n",
      "score: Average score given to the anime by users.\n",
      "scored_by: Number of users who have scored the anime.\n",
      "rank: Ranking of the anime based on score or popularity.\n",
      "popularity: Popularity ranking of the anime.\n",
      "members: Number of members who have added this anime to their list.\n",
      "favorites: Number of users who have favorited this anime.\n",
      "synopsis: Plot summary or synopsis of the anime.\n",
      "studios: Production studio responsible for creating the anime.\n",
      "genres: List of genres the anime belongs to (e.g., Drama, Sports).\n",
      "themes: List of themes present in the anime (e.g., Team Sports).\n",
      "\n",
      "## Columns in the  user_recommendation.csv :\n",
      "mal_id: This column represents the ID of an anime that users have watched or interacted with.\n",
      "mal_id_recomm: This column lists the IDs of anime recommended by users for a specific mal_id.\n",
      "votes: The votes column indicates the number of votes or recommendations given by users for the recommendation of mal_id_recomm for mal_id.\n",
      "\n",
      "\n",
      "The dataset is ready for exploration, analysis, and visualization to uncover insights into the world of anime and its dynamic landscape.\n",
      "name 'detect' is not defined Language is not detected: raminhuseyn/airline-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: airline-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: raminhuseyn\n",
      "name 'detect' is not defined Language is not detected: Airline Customer Satisfaction\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction of an airline customers.\n",
      "name 'detect' is not defined Language is not detected: The dataset provides insights into customer satisfaction levels within an undisclosed airline company. While the specific airline name is withheld, the dataset is rich in information, containing **22 columns and 129,880 rows**. It aims to predict whether future customers will be satisfied based on various parameters included in the dataset.\n",
      "\n",
      "The columns likely cover a range of factors that influence customer satisfaction, such as flight punctuality, service quality, and so. By analyzing this dataset, airlines can gain valuable insights into the factors that contribute to customer satisfaction and tailor their services accordingly to enhance the overall customer experience.\n",
      "name 'detect' is not defined Language is not detected: airline-customer-satisfaction\n",
      "name 'detect' is not defined Language is not detected: raminhuseyn\n",
      "name 'detect' is not defined Language is not detected: Airline Customer Satisfaction\n",
      "name 'detect' is not defined Language is not detected: Customer satisfaction of an airline customers.\n",
      "name 'detect' is not defined Language is not detected: The dataset provides insights into customer satisfaction levels within an undisclosed airline company. While the specific airline name is withheld, the dataset is rich in information, containing **22 columns and 129,880 rows**. It aims to predict whether future customers will be satisfied based on various parameters included in the dataset.\n",
      "\n",
      "The columns likely cover a range of factors that influence customer satisfaction, such as flight punctuality, service quality, and so. By analyzing this dataset, airlines can gain valuable insights into the factors that contribute to customer satisfaction and tailor their services accordingly to enhance the overall customer experience.\n",
      "name 'detect' is not defined Language is not detected: mmichelli/cirrus-cumulus-stratus-nimbus-ccsn-database\n",
      "name 'detect' is not defined Language is not detected: cirrus-cumulus-stratus-nimbus-ccsn-database\n",
      "name 'detect' is not defined Language is not detected: mmichelli\n",
      "name 'detect' is not defined Language is not detected: Cirrus Cumulus Stratus Nimbus (CCSN) Database\n",
      "name 'detect' is not defined Language is not detected: The CCSN dataset contains 2543 cloud images.\n",
      "name 'detect' is not defined Language is not detected: The CCSN dataset contains 2543 cloud images. According to the World Meteorological Organization’s genera-based classification recommendation, we divide into 11 different categories： Ac, Sc, Ns, Cu, Ci, Cc, Cb, As, Ct, Cs, St. It is worth noting that contrails have consideration in our dataset. \n",
      "\n",
      "Ci = cirrus; Cs = cirrostratus; Cc = cirrocumulus; Ac = altocumulus; As = altostratus; Cu = cumulus; Cb = cumulonimbus; Ns = nimbostratus; Sc = stratocumulus; St = stratus; Ct = contrail.\n",
      "\n",
      "\n",
      "[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD]( https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD)\n",
      "name 'detect' is not defined Language is not detected: cirrus-cumulus-stratus-nimbus-ccsn-database\n",
      "name 'detect' is not defined Language is not detected: mmichelli\n",
      "name 'detect' is not defined Language is not detected: Cirrus Cumulus Stratus Nimbus (CCSN) Database\n",
      "name 'detect' is not defined Language is not detected: The CCSN dataset contains 2543 cloud images.\n",
      "name 'detect' is not defined Language is not detected: The CCSN dataset contains 2543 cloud images. According to the World Meteorological Organization’s genera-based classification recommendation, we divide into 11 different categories： Ac, Sc, Ns, Cu, Ci, Cc, Cb, As, Ct, Cs, St. It is worth noting that contrails have consideration in our dataset. \n",
      "\n",
      "Ci = cirrus; Cs = cirrostratus; Cc = cirrocumulus; Ac = altocumulus; As = altostratus; Cu = cumulus; Cb = cumulonimbus; Ns = nimbostratus; Sc = stratocumulus; St = stratus; Ct = contrail.\n",
      "\n",
      "\n",
      "[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD]( https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD)\n",
      "name 'detect' is not defined Language is not detected: chahilchoudhary/top-11639-movies-dataset-with-genres-mapped-from\n",
      "name 'detect' is not defined Language is not detected: top-11639-movies-dataset-with-genres-mapped-from\n",
      "name 'detect' is not defined Language is not detected: chahilchoudhary\n",
      "name 'detect' is not defined Language is not detected: Top 11639 Movies Dataset with Genres Mapped from\n",
      "name 'detect' is not defined Language is not detected: Dataset of top 11639 movies which can be use for the movie Recommendation system\n",
      "name 'detect' is not defined Language is not detected: This dataset represents a collection of movies, each listed with their respective titles, overviews, and genres. Originally, the genres were provided as numeric IDs, which have been meticulously mapped to their corresponding genre names for better readability and usability. This mapping adheres to a predefined genre identification key which includes a wide range of genres from Action to Western.\n",
      "\n",
      "The dataset has been processed to replace the numeric genre IDs with actual genre names, making it more accessible for analysis, visualization, and machine learning purposes. The updated structure facilitates easier exploration and understanding of the movie genres without needing to refer to external sources for ID decoding.\n",
      "\n",
      "This dataset is suitable for anyone interested in film categorization, industry trend analysis, or content-based recommendation systems. It provides a foundational base for exploratory data analysis and can be used to draw insights about genre popularity, frequency, and distribution. Furthermore, the textual data in the 'overview' column can be utilized for natural language processing tasks such as sentiment analysis, keyword extraction, or content summarization.\n",
      "name 'detect' is not defined Language is not detected: top-11639-movies-dataset-with-genres-mapped-from\n",
      "name 'detect' is not defined Language is not detected: chahilchoudhary\n",
      "name 'detect' is not defined Language is not detected: Top 11639 Movies Dataset with Genres Mapped from\n",
      "name 'detect' is not defined Language is not detected: Dataset of top 11639 movies which can be use for the movie Recommendation system\n",
      "name 'detect' is not defined Language is not detected: This dataset represents a collection of movies, each listed with their respective titles, overviews, and genres. Originally, the genres were provided as numeric IDs, which have been meticulously mapped to their corresponding genre names for better readability and usability. This mapping adheres to a predefined genre identification key which includes a wide range of genres from Action to Western.\n",
      "\n",
      "The dataset has been processed to replace the numeric genre IDs with actual genre names, making it more accessible for analysis, visualization, and machine learning purposes. The updated structure facilitates easier exploration and understanding of the movie genres without needing to refer to external sources for ID decoding.\n",
      "\n",
      "This dataset is suitable for anyone interested in film categorization, industry trend analysis, or content-based recommendation systems. It provides a foundational base for exploratory data analysis and can be used to draw insights about genre popularity, frequency, and distribution. Furthermore, the textual data in the 'overview' column can be utilized for natural language processing tasks such as sentiment analysis, keyword extraction, or content summarization.\n",
      "name 'detect' is not defined Language is not detected: mehtamala/institute-graduation-rate-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: institute-graduation-rate-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: mehtamala\n",
      "name 'detect' is not defined Language is not detected: Institute Graduation Rate Prediction Dataset EDM\n",
      "name 'detect' is not defined Language is not detected: IPEDS Dataset Designed For Predicting Institutes' Graduation rates.\n",
      "name 'detect' is not defined Language is not detected: \n",
      "Institute Graduation Rate Prediction Dataset is prepared from IPEDS[1] dataset by following\n",
      "proposed framework[2] by** Ms. Mala H. Mehta, Dr. N.C.Chauhan and Dr.Anu Gokhle** (Research Paper presented in ET2ECN-2021 International Conference). The paper will soon be published in Springer-Scopus Indexed publication.\n",
      "\n",
      "The dataset consists of total 143 features and 11319 records of 8 student batches (from 2004 to 2011). How many students have successfully graduated within stipulated time period? Can we do the prediction of that? If low graduation rates are known in advance, institute can take prior steps to avoid low graduation rates.\n",
      "\n",
      "**Cite this dataset as - Ms. Mala Mehta Bhatt, Dr. N.C.Chauhan, & Dr. Anu Gokhale. (2021). <i>Institute Graduation Rate Prediction Dataset</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/2914166**\n",
      "\n",
      "1 Objective\n",
      " 1.1 Context \n",
      " Education data mining (EDM) is a field related to generate useful,novel and actionable knowledge\n",
      "by applying miniing/ML algorithms on academic data. Knowledge generated could give unexpected\n",
      " benefit to education domain stakeholders.\n",
      "\n",
      "EDM also known sometimes as Learning Analytics has various branches to work. Two Major branches\n",
      " are: 1. Student Performance related study 2. Institute Performance related study.\n",
      "Much research is done on the first aspect, however, the second aspect is not touched much.\n",
      "\n",
      "This dataset is designed with aim of effectively predicting Institute Graduation Rates for Higher education\n",
      " institutions.\n",
      "\n",
      " 2 IPEDS [1] Dataset\n",
      "The National Centre for Education Statistics (NCES) is the primary federal entity for collecting,\n",
      "analyzing, and reporting data related to education in the United States and other nations.\n",
      "\n",
      " The Integrated Postsecondary Education Data System (IPEDS) surveys approximately 7,500 postsecondary\n",
      " institutions, including universities and colleges, as well as institutions offering technical and\n",
      "vocational education beyond the high school level. IPEDS, which began in 1986, replaced the Higher\n",
      " Education General Information Survey (HEGIS).\n",
      "\n",
      "IPEDS consists of nine integrated components that obtain information on who provides postsecondary\n",
      "education (institutions), who participates in it and completes it (students), what programs are\n",
      " offered and what programs are completed, and both the human and financial resources involved in the\n",
      "provision of institutionally-based postsecondary education.\n",
      "\n",
      "3 Approach\n",
      "3.1 Feature Selection\n",
      "IPEDS dataset is a big dataset consisting of many tables and many years' databases. A framework[2] was\n",
      "designed to extract IGR related features and data. By following this framework, final file was created.\n",
      "143 Features were selected out of which one is response variable.\n",
      " 3.1.1 Response Variable\n",
      "GBA4RTT - Graduation rate - bachelor's degree within 4 years\n",
      " 3.1.2 Predictor Variables\n",
      "142 Predictor/Independent features are identified. (meta data is uploaded.)\n",
      "\n",
      "3.2 Handling Missing Values\n",
      "Missing values are handled by applying statistical measure mean on each feature and the replacing\n",
      "missing values by them.\n",
      "3.3 Splitting into Train-Validation-Test sets\n",
      "Data is split into training and testing set with 80-20% ratio.\n",
      "3.4 Modeling\n",
      " AS Response variable considered in the study is a continuous variable. Regression Models are used\n",
      " to find the minimum error in prediction.\n",
      "4 models are considered: Multiple linear regression, Support vector regression, Decision tree regression,\n",
      "XGBoost regression\n",
      "4 Execution\n",
      "Execution process consists of below mentioned step by step procedure: 1. Preprocessing of data,\n",
      "2. Splitting the data in training and testing sets, 3. Applying the models, 4. Measuring MSE,RMSE,R2,\n",
      " Adjusted R2 and program's running time.\n",
      "5 Conclusion\n",
      "Mean Squared Error measured is considered here for comparison among 4 models. Minimum MSE is received in\n",
      "XGBoost regression algorithm followed by support vector regression, decision tree regression and multiple\n",
      " linear regression algorithms.\n",
      "Future Work\n",
      "Researchers could use the dataset for further analysis with different models, different dimensionality\n",
      " reduction techniques and education domain analysis.\n",
      " References\n",
      "[1] NCES, “National Center for Education Statistics”, Available at: https://nces.ed.gov/ipeds/use-the-data,\n",
      "Accessed at 2021.\n",
      "[2] \"A Dataset preparation framework for education data mining\" presented in 4th international conference on\n",
      " Emerging technology trends in electronics, communication and networking (ET2ECN-2021), SVNIT, Surat.\n",
      " Acknowledgements\n",
      "Thanks to NCES [1], for providing such huge open repository related to education available freely. I acknowledge\n",
      "all efforts put by Dr. N.C.Chauhan and Dr. Anu Gokhale in this work. Special Thanks to **Vinay Bhatt**, who found IPEDS repository for me, because of that only I was able to prepare this dataset. \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: institute-graduation-rate-prediction-dataset\n",
      "name 'detect' is not defined Language is not detected: mehtamala\n",
      "name 'detect' is not defined Language is not detected: Institute Graduation Rate Prediction Dataset EDM\n",
      "name 'detect' is not defined Language is not detected: IPEDS Dataset Designed For Predicting Institutes' Graduation rates.\n",
      "name 'detect' is not defined Language is not detected: \n",
      "Institute Graduation Rate Prediction Dataset is prepared from IPEDS[1] dataset by following\n",
      "proposed framework[2] by** Ms. Mala H. Mehta, Dr. N.C.Chauhan and Dr.Anu Gokhle** (Research Paper presented in ET2ECN-2021 International Conference). The paper will soon be published in Springer-Scopus Indexed publication.\n",
      "\n",
      "The dataset consists of total 143 features and 11319 records of 8 student batches (from 2004 to 2011). How many students have successfully graduated within stipulated time period? Can we do the prediction of that? If low graduation rates are known in advance, institute can take prior steps to avoid low graduation rates.\n",
      "\n",
      "**Cite this dataset as - Ms. Mala Mehta Bhatt, Dr. N.C.Chauhan, & Dr. Anu Gokhale. (2021). <i>Institute Graduation Rate Prediction Dataset</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/2914166**\n",
      "\n",
      "1 Objective\n",
      " 1.1 Context \n",
      " Education data mining (EDM) is a field related to generate useful,novel and actionable knowledge\n",
      "by applying miniing/ML algorithms on academic data. Knowledge generated could give unexpected\n",
      " benefit to education domain stakeholders.\n",
      "\n",
      "EDM also known sometimes as Learning Analytics has various branches to work. Two Major branches\n",
      " are: 1. Student Performance related study 2. Institute Performance related study.\n",
      "Much research is done on the first aspect, however, the second aspect is not touched much.\n",
      "\n",
      "This dataset is designed with aim of effectively predicting Institute Graduation Rates for Higher education\n",
      " institutions.\n",
      "\n",
      " 2 IPEDS [1] Dataset\n",
      "The National Centre for Education Statistics (NCES) is the primary federal entity for collecting,\n",
      "analyzing, and reporting data related to education in the United States and other nations.\n",
      "\n",
      " The Integrated Postsecondary Education Data System (IPEDS) surveys approximately 7,500 postsecondary\n",
      " institutions, including universities and colleges, as well as institutions offering technical and\n",
      "vocational education beyond the high school level. IPEDS, which began in 1986, replaced the Higher\n",
      " Education General Information Survey (HEGIS).\n",
      "\n",
      "IPEDS consists of nine integrated components that obtain information on who provides postsecondary\n",
      "education (institutions), who participates in it and completes it (students), what programs are\n",
      " offered and what programs are completed, and both the human and financial resources involved in the\n",
      "provision of institutionally-based postsecondary education.\n",
      "\n",
      "3 Approach\n",
      "3.1 Feature Selection\n",
      "IPEDS dataset is a big dataset consisting of many tables and many years' databases. A framework[2] was\n",
      "designed to extract IGR related features and data. By following this framework, final file was created.\n",
      "143 Features were selected out of which one is response variable.\n",
      " 3.1.1 Response Variable\n",
      "GBA4RTT - Graduation rate - bachelor's degree within 4 years\n",
      " 3.1.2 Predictor Variables\n",
      "142 Predictor/Independent features are identified. (meta data is uploaded.)\n",
      "\n",
      "3.2 Handling Missing Values\n",
      "Missing values are handled by applying statistical measure mean on each feature and the replacing\n",
      "missing values by them.\n",
      "3.3 Splitting into Train-Validation-Test sets\n",
      "Data is split into training and testing set with 80-20% ratio.\n",
      "3.4 Modeling\n",
      " AS Response variable considered in the study is a continuous variable. Regression Models are used\n",
      " to find the minimum error in prediction.\n",
      "4 models are considered: Multiple linear regression, Support vector regression, Decision tree regression,\n",
      "XGBoost regression\n",
      "4 Execution\n",
      "Execution process consists of below mentioned step by step procedure: 1. Preprocessing of data,\n",
      "2. Splitting the data in training and testing sets, 3. Applying the models, 4. Measuring MSE,RMSE,R2,\n",
      " Adjusted R2 and program's running time.\n",
      "5 Conclusion\n",
      "Mean Squared Error measured is considered here for comparison among 4 models. Minimum MSE is received in\n",
      "XGBoost regression algorithm followed by support vector regression, decision tree regression and multiple\n",
      " linear regression algorithms.\n",
      "Future Work\n",
      "Researchers could use the dataset for further analysis with different models, different dimensionality\n",
      " reduction techniques and education domain analysis.\n",
      " References\n",
      "[1] NCES, “National Center for Education Statistics”, Available at: https://nces.ed.gov/ipeds/use-the-data,\n",
      "Accessed at 2021.\n",
      "[2] \"A Dataset preparation framework for education data mining\" presented in 4th international conference on\n",
      " Emerging technology trends in electronics, communication and networking (ET2ECN-2021), SVNIT, Surat.\n",
      " Acknowledgements\n",
      "Thanks to NCES [1], for providing such huge open repository related to education available freely. I acknowledge\n",
      "all efforts put by Dr. N.C.Chauhan and Dr. Anu Gokhale in this work. Special Thanks to **Vinay Bhatt**, who found IPEDS repository for me, because of that only I was able to prepare this dataset. \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hyeonwooan/imdb-data-preprocessed-using-ml20m\n",
      "name 'detect' is not defined Language is not detected: imdb-data-preprocessed-using-ml20m\n",
      "name 'detect' is not defined Language is not detected: hyeonwooan\n",
      "name 'detect' is not defined Language is not detected: IMDB data preprocessed using ml-20m\n",
      "name 'detect' is not defined Language is not detected: the preprocessed data for a recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "It is data processed to make a movie recommendation system.\n",
      "\n",
      "About 90,000 movie data of IMDB and rating data of Movielens (ml-20m) were used, and actors and directors' features were extracted using Matrix Factorization based on user's rating. Also, in the case of the story, the names of people who can adversely affect the sentence analysis were removed using the pre-trained BERT model and only the noun words were left.\n",
      "\n",
      "However, before using the data, please consider the fact that a lot of the intuition of the author is included.\n",
      "\n",
      "### Content\n",
      "imdb\\_data.csv : \n",
      "  - Data scrapping from the IMDB site around April 2020.\n",
      "  - There are 91,515 target films.\n",
      "  - The preprocessed data(columns) is as follows.\n",
      "    + story : Eliminates adverse effects on technologies such as TF-IDF by removing person names using BERT and leaving only nouns.\n",
      "    + actors : To convert to a lower case and remove data polarization between films, only four main characters (mainly listed in sequence) were left.\n",
      "    + genre : Similar to actors, if you include multiple genres, only allow up to 3 genres to be included.\n",
      "    + actors\\_order : The indices of each actors in actors\\_vectors that result from Matrix Factorization. It is configured to be the same as the order written in actors.\n",
      "    + directors\\_order : Indices of each director in directors\\_vectors resulting from Matrix Factorization.\n",
      "    + popularity : Normalized value from 0 to 1 based on ranking according to votes by year.\n",
      "\n",
      "actors\\_vectors*.p, directors\\_vectors*.p : \n",
      "  - Vectors created using truncated SVD for each actor and director based on imdb data and rating data.\n",
      "  - The suffix of the file, such as \"\\_10, \\_20, ...\", indicates how many rating data for each user was used. For example, \"\\_10\" is a vector created using 10 rating data for each user.\n",
      "  - In the case of a file without a suffix, it is a vector generated by random numbers.\n",
      "  - If there is no evaluation for actors or directors, the average of all vectors with evaluation is substituted.\n",
      "\n",
      "imdb\\_rating\\_with\\_ml20.csv : \n",
      "  - Among the rating data of ml-20m, only the data of movies included in imdb\\_data.csv is filtered and imdb\\_id is added.\n",
      "  - [imdb_metadata](https://www.kaggle.com/rounakbanik/movie-recommender-systems/data?select=movies_metadata.csv) was used to join the two.\n",
      "\n",
      "### Acknowledgements\n",
      "MovieLens : It shares a lot of explicit feedback data about movies and plays a very key role in data processing and recommendation systems.\n",
      " - https://grouplens.org/datasets/movielens/20m/\n",
      "\n",
      "IMDB : A site where you can refer to a lot of information about movies. Many parts of the recommendation system were helped.\n",
      " - https://www.imdb.com/\n",
      "\n",
      "BERT : Helps remove person names from stories in IMDB data and stemming sentences.\n",
      " - https://github.com/kamalkraj/BERT-NER\n",
      "\n",
      "movies\\_metadata : Played an important role in combining MovieLens' movie's ID with IMDB's movie's ID.\n",
      " - https://www.kaggle.com/rounakbanik/movie-recommender-systems/data?select=movies\\_metadata.csv\n",
      "\n",
      "### Inspiration\n",
      "  - You can try building a recommender system.\n",
      "\n",
      "### Dataset License\n",
      " - [IMDB License](https://www.imdb.com/conditions?ref_=helpms_ih_gi_usedata)\n",
      " - [Movielens License](https://files.grouplens.org/datasets/movielens/ml-25m-README.html)\n",
      "name 'detect' is not defined Language is not detected: imdb-data-preprocessed-using-ml20m\n",
      "name 'detect' is not defined Language is not detected: hyeonwooan\n",
      "name 'detect' is not defined Language is not detected: IMDB data preprocessed using ml-20m\n",
      "name 'detect' is not defined Language is not detected: the preprocessed data for a recommendation system\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "It is data processed to make a movie recommendation system.\n",
      "\n",
      "About 90,000 movie data of IMDB and rating data of Movielens (ml-20m) were used, and actors and directors' features were extracted using Matrix Factorization based on user's rating. Also, in the case of the story, the names of people who can adversely affect the sentence analysis were removed using the pre-trained BERT model and only the noun words were left.\n",
      "\n",
      "However, before using the data, please consider the fact that a lot of the intuition of the author is included.\n",
      "\n",
      "### Content\n",
      "imdb\\_data.csv : \n",
      "  - Data scrapping from the IMDB site around April 2020.\n",
      "  - There are 91,515 target films.\n",
      "  - The preprocessed data(columns) is as follows.\n",
      "    + story : Eliminates adverse effects on technologies such as TF-IDF by removing person names using BERT and leaving only nouns.\n",
      "    + actors : To convert to a lower case and remove data polarization between films, only four main characters (mainly listed in sequence) were left.\n",
      "    + genre : Similar to actors, if you include multiple genres, only allow up to 3 genres to be included.\n",
      "    + actors\\_order : The indices of each actors in actors\\_vectors that result from Matrix Factorization. It is configured to be the same as the order written in actors.\n",
      "    + directors\\_order : Indices of each director in directors\\_vectors resulting from Matrix Factorization.\n",
      "    + popularity : Normalized value from 0 to 1 based on ranking according to votes by year.\n",
      "\n",
      "actors\\_vectors*.p, directors\\_vectors*.p : \n",
      "  - Vectors created using truncated SVD for each actor and director based on imdb data and rating data.\n",
      "  - The suffix of the file, such as \"\\_10, \\_20, ...\", indicates how many rating data for each user was used. For example, \"\\_10\" is a vector created using 10 rating data for each user.\n",
      "  - In the case of a file without a suffix, it is a vector generated by random numbers.\n",
      "  - If there is no evaluation for actors or directors, the average of all vectors with evaluation is substituted.\n",
      "\n",
      "imdb\\_rating\\_with\\_ml20.csv : \n",
      "  - Among the rating data of ml-20m, only the data of movies included in imdb\\_data.csv is filtered and imdb\\_id is added.\n",
      "  - [imdb_metadata](https://www.kaggle.com/rounakbanik/movie-recommender-systems/data?select=movies_metadata.csv) was used to join the two.\n",
      "\n",
      "### Acknowledgements\n",
      "MovieLens : It shares a lot of explicit feedback data about movies and plays a very key role in data processing and recommendation systems.\n",
      " - https://grouplens.org/datasets/movielens/20m/\n",
      "\n",
      "IMDB : A site where you can refer to a lot of information about movies. Many parts of the recommendation system were helped.\n",
      " - https://www.imdb.com/\n",
      "\n",
      "BERT : Helps remove person names from stories in IMDB data and stemming sentences.\n",
      " - https://github.com/kamalkraj/BERT-NER\n",
      "\n",
      "movies\\_metadata : Played an important role in combining MovieLens' movie's ID with IMDB's movie's ID.\n",
      " - https://www.kaggle.com/rounakbanik/movie-recommender-systems/data?select=movies\\_metadata.csv\n",
      "\n",
      "### Inspiration\n",
      "  - You can try building a recommender system.\n",
      "\n",
      "### Dataset License\n",
      " - [IMDB License](https://www.imdb.com/conditions?ref_=helpms_ih_gi_usedata)\n",
      " - [Movielens License](https://files.grouplens.org/datasets/movielens/ml-25m-README.html)\n",
      "name 'detect' is not defined Language is not detected: nikunjhemani/movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: nikunjhemani\n",
      "name 'detect' is not defined Language is not detected: Movie_Recommendation_Dataset\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: nikunjhemani\n",
      "name 'detect' is not defined Language is not detected: Movie_Recommendation_Dataset\n",
      "name 'detect' is not defined Language is not detected: abdullahorzan/moodify-dataset\n",
      "name 'detect' is not defined Language is not detected: moodify-dataset\n",
      "name 'detect' is not defined Language is not detected: abdullahorzan\n",
      "name 'detect' is not defined Language is not detected: 278k Emotion Labeled Spotify Songs\n",
      "name 'detect' is not defined Language is not detected: Approximately 278k songs from Spotify with labeled emotions by Moodify app.\n",
      "name 'detect' is not defined Language is not detected: # Moodify Dataset\n",
      "The main idea of Moodify project is to classify songs not only based on their lyrical and musical features, but also incorporating emotions, in order to provide users with more successful recommendation outputs. Moodify aims to categorize songs into four main emotions and utilize the similarities in musical features within these categories to offer users more effective recommendations, weighting them with emotions.\n",
      "\n",
      "In this perspective, we build LGBM model to predict emotions of songs and we use this dataset in our both test and recommendation phases. It contains nearly 278.000 songs from Spotify and all songs emotionally labeled. \n",
      "\n",
      "You can find more information about project and dataset in our project's GitHub repository.\n",
      "\n",
      "https://github.com/orzanai/Moodify\n",
      "\n",
      "## Additional information about variables:\n",
      "\n",
      "- Labels: {'sad': 0, 'happy': 1, 'energetic': 2, 'calm': 3}\n",
      "\n",
      "- Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
      "- Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
      "- Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
      "- Instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
      "- Liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.\n",
      "- Loudness: the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing the relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
      "- Speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audiobook, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
      "- Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
      "- Tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
      "name 'detect' is not defined Language is not detected: moodify-dataset\n",
      "name 'detect' is not defined Language is not detected: abdullahorzan\n",
      "name 'detect' is not defined Language is not detected: 278k Emotion Labeled Spotify Songs\n",
      "name 'detect' is not defined Language is not detected: Approximately 278k songs from Spotify with labeled emotions by Moodify app.\n",
      "name 'detect' is not defined Language is not detected: # Moodify Dataset\n",
      "The main idea of Moodify project is to classify songs not only based on their lyrical and musical features, but also incorporating emotions, in order to provide users with more successful recommendation outputs. Moodify aims to categorize songs into four main emotions and utilize the similarities in musical features within these categories to offer users more effective recommendations, weighting them with emotions.\n",
      "\n",
      "In this perspective, we build LGBM model to predict emotions of songs and we use this dataset in our both test and recommendation phases. It contains nearly 278.000 songs from Spotify and all songs emotionally labeled. \n",
      "\n",
      "You can find more information about project and dataset in our project's GitHub repository.\n",
      "\n",
      "https://github.com/orzanai/Moodify\n",
      "\n",
      "## Additional information about variables:\n",
      "\n",
      "- Labels: {'sad': 0, 'happy': 1, 'energetic': 2, 'calm': 3}\n",
      "\n",
      "- Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
      "- Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
      "- Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
      "- Instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
      "- Liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.\n",
      "- Loudness: the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing the relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
      "- Speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audiobook, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
      "- Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
      "- Tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
      "name 'detect' is not defined Language is not detected: ksavleen/movie-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: ksavleen\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation Datasets\n",
      "name 'detect' is not defined Language is not detected: Movie Ratings and Movie Details reviewed by different viewers.\n",
      "name 'detect' is not defined Language is not detected: Datasets have **movie titles** and details of **movie ratings** from **MovieLens** datasets \n",
      "It has details like Item ID, User ID, Movie Title, Ratings given by viewers on scale of 1 to 5 and timestamps\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation-dataset\n",
      "name 'detect' is not defined Language is not detected: ksavleen\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation Datasets\n",
      "name 'detect' is not defined Language is not detected: Movie Ratings and Movie Details reviewed by different viewers.\n",
      "name 'detect' is not defined Language is not detected: Datasets have **movie titles** and details of **movie ratings** from **MovieLens** datasets \n",
      "It has details like Item ID, User ID, Movie Title, Ratings given by viewers on scale of 1 to 5 and timestamps\n",
      "name 'detect' is not defined Language is not detected: joshjms/kawaii\n",
      "name 'detect' is not defined Language is not detected: kawaii\n",
      "name 'detect' is not defined Language is not detected: joshjms\n",
      "name 'detect' is not defined Language is not detected: Manga Dataset\n",
      "name 'detect' is not defined Language is not detected: All information on manga, almost 70,000 manga entries.\n",
      "name 'detect' is not defined Language is not detected: Contains all manga from MAL. Data retrieved from MAL API and Jikan API.\n",
      "\n",
      "`manga.csv` - Manga details\n",
      "`genre.csv` - Genre details\n",
      "`manga_genre.csv` - Genre of each manga\n",
      "`recommendations.csv` - Recommendation pairs of manga\n",
      "\n",
      "No need to credit me, all I did is scrape and take ❤️\n",
      "\n",
      "If I do violate any laws, or for any inquiries and extra information, please notify me in [REDACTED]\n",
      "\n",
      "I love you guys ❤️😍😋\n",
      "name 'detect' is not defined Language is not detected: kawaii\n",
      "name 'detect' is not defined Language is not detected: joshjms\n",
      "name 'detect' is not defined Language is not detected: Manga Dataset\n",
      "name 'detect' is not defined Language is not detected: All information on manga, almost 70,000 manga entries.\n",
      "name 'detect' is not defined Language is not detected: Contains all manga from MAL. Data retrieved from MAL API and Jikan API.\n",
      "\n",
      "`manga.csv` - Manga details\n",
      "`genre.csv` - Genre details\n",
      "`manga_genre.csv` - Genre of each manga\n",
      "`recommendations.csv` - Recommendation pairs of manga\n",
      "\n",
      "No need to credit me, all I did is scrape and take ❤️\n",
      "\n",
      "If I do violate any laws, or for any inquiries and extra information, please notify me in [REDACTED]\n",
      "\n",
      "I love you guys ❤️😍😋\n",
      "name 'detect' is not defined Language is not detected: nantonio/a-hotels-customers-dataset\n",
      "name 'detect' is not defined Language is not detected: a-hotels-customers-dataset\n",
      "name 'detect' is not defined Language is not detected: nantonio\n",
      "name 'detect' is not defined Language is not detected: A hotel's customers dataset\n",
      "name 'detect' is not defined Language is not detected: Personal, behavioral, demographic, and geographic data from a hotel in Lisbon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This real-world customer dataset with 31 variables describes 83,590 instances (customers) from a hotel in Lisbon, Portugal.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The data comprehends three full years of customer personal, behavioral, demographic, and geographical information.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Additional information on this dataset can be found in the article[ A Hotel's customers personal, behavioral, demographic, and geographic dataset from Lisbon, Portugal (2015-2018)](https://doi.org/10.1016/j.dib.2020.106583), written by Nuno Antonio, Ana de Almeida, and Luis Nunes for Data in Brief (online November 2020).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset can be used in data mining, machine learning, and other analytical field problems in the scope of data science. Due to its unit of analysis, it is a dataset especially suitable for building customer segmentation models, including clustering and RFM (Recency, Frequency, and Monetary value) models, but also be used in classification and regression problems.\n",
      "name 'detect' is not defined Language is not detected: a-hotels-customers-dataset\n",
      "name 'detect' is not defined Language is not detected: nantonio\n",
      "name 'detect' is not defined Language is not detected: A hotel's customers dataset\n",
      "name 'detect' is not defined Language is not detected: Personal, behavioral, demographic, and geographic data from a hotel in Lisbon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This real-world customer dataset with 31 variables describes 83,590 instances (customers) from a hotel in Lisbon, Portugal.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The data comprehends three full years of customer personal, behavioral, demographic, and geographical information.\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "Additional information on this dataset can be found in the article[ A Hotel's customers personal, behavioral, demographic, and geographic dataset from Lisbon, Portugal (2015-2018)](https://doi.org/10.1016/j.dib.2020.106583), written by Nuno Antonio, Ana de Almeida, and Luis Nunes for Data in Brief (online November 2020).\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "This dataset can be used in data mining, machine learning, and other analytical field problems in the scope of data science. Due to its unit of analysis, it is a dataset especially suitable for building customer segmentation models, including clustering and RFM (Recency, Frequency, and Monetary value) models, but also be used in classification and regression problems.\n",
      "name 'detect' is not defined Language is not detected: mustafayazici/goodbooks-10k-rating-and-description\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k-rating-and-description\n",
      "name 'detect' is not defined Language is not detected: mustafayazici\n",
      "name 'detect' is not defined Language is not detected: goodbooks_10k_rating_and_description\n",
      "name 'detect' is not defined Language is not detected: It is the only dataset with **user rating** and **book description** in Kaggle.\n",
      "\n",
      "This dataset has enough data to make the following recommendation systems.\n",
      "\n",
      "- Association Rule Learning\n",
      "- User Based Recommendation\n",
      "- Content Based Recommendation\n",
      "- Matrix Based Collaborative Modeling ( Matrix Factorization )\n",
      "\n",
      "\n",
      "**Highlight**\n",
      "- 6.000.000 rating\n",
      "- 4 columns for Content Based Recommendation\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: goodbooks-10k-rating-and-description\n",
      "name 'detect' is not defined Language is not detected: mustafayazici\n",
      "name 'detect' is not defined Language is not detected: goodbooks_10k_rating_and_description\n",
      "name 'detect' is not defined Language is not detected: It is the only dataset with **user rating** and **book description** in Kaggle.\n",
      "\n",
      "This dataset has enough data to make the following recommendation systems.\n",
      "\n",
      "- Association Rule Learning\n",
      "- User Based Recommendation\n",
      "- Content Based Recommendation\n",
      "- Matrix Based Collaborative Modeling ( Matrix Factorization )\n",
      "\n",
      "\n",
      "**Highlight**\n",
      "- 6.000.000 rating\n",
      "- 4 columns for Content Based Recommendation\n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ramjasmaurya/medias-cost-prediction-in-foodmart\n",
      "name 'detect' is not defined Language is not detected: medias-cost-prediction-in-foodmart\n",
      "name 'detect' is not defined Language is not detected: ramjasmaurya\n",
      "name 'detect' is not defined Language is not detected: Cost Prediction on acquiring Customers.\n",
      "name 'detect' is not defined Language is not detected: Cost to run a media campaign for US FoodMarts according to Income,Store Data\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6279218%2Fd60f31abd8bd38d5b0fa92cb1d7a4092%2Fimages-_23_.jpeg?generation=1662456998926514&alt=media)\n",
      "\n",
      "\n",
      "## PREDICT COST ON MEDIA CAMPAIGNS IN FOOD MART OF USA .\n",
      "## ON THE BASIS OF 60K CUSTOMERS INCOME ,PRODUCT,PROMOTION AND STORE FEATURES. \n",
      "\n",
      "\n",
      "**ABOUT FOODMART:**\n",
      "\n",
      "Food Mart (CFM) is a chain of convenience stores in the United States. The private company's headquarters are located in Mentor, Ohio, and there are currently approximately 325 stores located in the US. Convenient Food Mart operates on the franchise system.\n",
      "\n",
      " Food Mart was the nation's third-largest chain of convenience stores as of 1988.\n",
      "\n",
      "The NASDAQ exchange dropped Convenient Food Mart the same year when the company failed to meet financial reporting requirements.\n",
      "\n",
      "Carden & Cherry advertised Convenient Food Mart with the Ernest character in the 1980s.\n",
      "\n",
      "...KNOW MORE FROM WIKI .\n",
      "name 'detect' is not defined Language is not detected: medias-cost-prediction-in-foodmart\n",
      "name 'detect' is not defined Language is not detected: ramjasmaurya\n",
      "name 'detect' is not defined Language is not detected: Cost Prediction on acquiring Customers.\n",
      "name 'detect' is not defined Language is not detected: Cost to run a media campaign for US FoodMarts according to Income,Store Data\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6279218%2Fd60f31abd8bd38d5b0fa92cb1d7a4092%2Fimages-_23_.jpeg?generation=1662456998926514&alt=media)\n",
      "\n",
      "\n",
      "## PREDICT COST ON MEDIA CAMPAIGNS IN FOOD MART OF USA .\n",
      "## ON THE BASIS OF 60K CUSTOMERS INCOME ,PRODUCT,PROMOTION AND STORE FEATURES. \n",
      "\n",
      "\n",
      "**ABOUT FOODMART:**\n",
      "\n",
      "Food Mart (CFM) is a chain of convenience stores in the United States. The private company's headquarters are located in Mentor, Ohio, and there are currently approximately 325 stores located in the US. Convenient Food Mart operates on the franchise system.\n",
      "\n",
      " Food Mart was the nation's third-largest chain of convenience stores as of 1988.\n",
      "\n",
      "The NASDAQ exchange dropped Convenient Food Mart the same year when the company failed to meet financial reporting requirements.\n",
      "\n",
      "Carden & Cherry advertised Convenient Food Mart with the Ernest character in the 1980s.\n",
      "\n",
      "...KNOW MORE FROM WIKI .\n",
      "name 'detect' is not defined Language is not detected: kkqwerty/movies-tmdb\n",
      "name 'detect' is not defined Language is not detected: movies-tmdb\n",
      "name 'detect' is not defined Language is not detected: kkqwerty\n",
      "name 'detect' is not defined Language is not detected: Movies (TMDB)\n",
      "name 'detect' is not defined Language is not detected: Movies Recommendation\n",
      "name 'detect' is not defined Language is not detected: movies-tmdb\n",
      "name 'detect' is not defined Language is not detected: kkqwerty\n",
      "name 'detect' is not defined Language is not detected: Movies (TMDB)\n",
      "name 'detect' is not defined Language is not detected: Movies Recommendation\n",
      "name 'detect' is not defined Language is not detected: ipsita001/workation-price-prediction-challengemachinehack\n",
      "name 'detect' is not defined Language is not detected: workation-price-prediction-challengemachinehack\n",
      "name 'detect' is not defined Language is not detected: ipsita001\n",
      "name 'detect' is not defined Language is not detected: Workation Price Prediction Challenge-MachineHack\n",
      "name 'detect' is not defined Language is not detected: # Overview\n",
      "The new covid-era has provided a new way of living the work-life balance. We have seen a lot of different websites providing packages to work from different locations. From Kashmir to Kanyakumari, from Gujarat to Assam we have collected packages in and around India. It becomes really difficult to find the best place with all the amenities such as high-speed internet, a comfortable stay as well as within the budget. To solve the real-world problem of finding the best deals for a calm and enjoying workation trip. Workation is the best way to work at a remote location with a recreational and rejuvenating vacation for the team.\n",
      "\n",
      "In this competition, one has to use the knowledge of machine learning, deep learning, and model building to predict the price per person for your next workstation trip. The data has more than 18000+ rows of different packages with the details like start location, hotel type, cost per person, destination, Itinerary, and many more.\n",
      "\n",
      " \n",
      "\n",
      "## Attributes:\n",
      "\n",
      "Uniq Id - Unique Identifier per row sample\n",
      "Package Name - Name of the tour package\n",
      "Package Type - Type of the tour package\n",
      "Destination - A destination place\n",
      "Itinerary - complete itinerary\n",
      "Places Covered - covered places in the itinerary\n",
      "Travel Date - Date of travel\n",
      "Hotel Details - Details of the hotel stay\n",
      "Start City - Start place for the travel\n",
      "Airline - Flight details\n",
      "Flight Stops - Intermediate stops if any\n",
      "Meals - Inflight meals or services\n",
      "Sightseeing Places Covered - Itinerary details regarding sightseeing\n",
      "Cancellation Rules - Cancellation policy as per travel company\n",
      "Per Person Price - Price of the tour package per person (Target Column)\n",
      "\n",
      "Skills:\n",
      "\n",
      "Advanced Regression Modeling\n",
      "Feature Engineering, Ensemble Modeling\n",
      "Optimizing RMSLE(Root Mean Squared Log Error) as a metric to generalize well on unseen data\n",
      "name 'detect' is not defined Language is not detected: workation-price-prediction-challengemachinehack\n",
      "name 'detect' is not defined Language is not detected: ipsita001\n",
      "name 'detect' is not defined Language is not detected: Workation Price Prediction Challenge-MachineHack\n",
      "name 'detect' is not defined Language is not detected: # Overview\n",
      "The new covid-era has provided a new way of living the work-life balance. We have seen a lot of different websites providing packages to work from different locations. From Kashmir to Kanyakumari, from Gujarat to Assam we have collected packages in and around India. It becomes really difficult to find the best place with all the amenities such as high-speed internet, a comfortable stay as well as within the budget. To solve the real-world problem of finding the best deals for a calm and enjoying workation trip. Workation is the best way to work at a remote location with a recreational and rejuvenating vacation for the team.\n",
      "\n",
      "In this competition, one has to use the knowledge of machine learning, deep learning, and model building to predict the price per person for your next workstation trip. The data has more than 18000+ rows of different packages with the details like start location, hotel type, cost per person, destination, Itinerary, and many more.\n",
      "\n",
      " \n",
      "\n",
      "## Attributes:\n",
      "\n",
      "Uniq Id - Unique Identifier per row sample\n",
      "Package Name - Name of the tour package\n",
      "Package Type - Type of the tour package\n",
      "Destination - A destination place\n",
      "Itinerary - complete itinerary\n",
      "Places Covered - covered places in the itinerary\n",
      "Travel Date - Date of travel\n",
      "Hotel Details - Details of the hotel stay\n",
      "Start City - Start place for the travel\n",
      "Airline - Flight details\n",
      "Flight Stops - Intermediate stops if any\n",
      "Meals - Inflight meals or services\n",
      "Sightseeing Places Covered - Itinerary details regarding sightseeing\n",
      "Cancellation Rules - Cancellation policy as per travel company\n",
      "Per Person Price - Price of the tour package per person (Target Column)\n",
      "\n",
      "Skills:\n",
      "\n",
      "Advanced Regression Modeling\n",
      "Feature Engineering, Ensemble Modeling\n",
      "Optimizing RMSLE(Root Mean Squared Log Error) as a metric to generalize well on unseen data\n",
      "name 'detect' is not defined Language is not detected: jixiangruyi/predict-employee-left\n",
      "name 'detect' is not defined Language is not detected: predict-employee-left\n",
      "name 'detect' is not defined Language is not detected: jixiangruyi\n",
      "name 'detect' is not defined Language is not detected: predict_employee_left\n",
      "name 'detect' is not defined Language is not detected: Use Logistic regression and tree-based model to predict whether or not an employ\n",
      "name 'detect' is not defined Language is not detected: **PPP** solves most problems\n",
      "**P**roblem：  What’s likely to make the employee leave the company?\n",
      "**P**rocess：   Use logistic regression and tree-based model（decision tree and random forest）to predict\n",
      "**P**rocedure：PACE  Plan，Analyze，Construct，Evaluation\n",
      "name 'detect' is not defined Language is not detected: predict-employee-left\n",
      "name 'detect' is not defined Language is not detected: jixiangruyi\n",
      "name 'detect' is not defined Language is not detected: predict_employee_left\n",
      "name 'detect' is not defined Language is not detected: Use Logistic regression and tree-based model to predict whether or not an employ\n",
      "name 'detect' is not defined Language is not detected: **PPP** solves most problems\n",
      "**P**roblem：  What’s likely to make the employee leave the company?\n",
      "**P**rocess：   Use logistic regression and tree-based model（decision tree and random forest）to predict\n",
      "**P**rocedure：PACE  Plan，Analyze，Construct，Evaluation\n",
      "name 'detect' is not defined Language is not detected: yellowj4acket/real-estate-georgia\n",
      "name 'detect' is not defined Language is not detected: real-estate-georgia\n",
      "name 'detect' is not defined Language is not detected: yellowj4acket\n",
      "name 'detect' is not defined Language is not detected: Real Estate Georgia\n",
      "name 'detect' is not defined Language is not detected: Real Worlds Dataset for Georgia (US) for the first 6 months of 2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "For various project occasions, a comprehensive US listings dataset was created with a colleague. This dataset is a subset for the State of Georgia.\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset shows real estate listing for Georgia (US) for the first 6 months of 2021. Prices are listed in USD.\n",
      "\n",
      "### Acknowledgements and Citation\n",
      "\n",
      "This dataset is a co-production, thanks to my fellow mate Jordan for building a great comprehensive US dataset from which I could filter this.\n",
      "\n",
      "**Using this dataset requires citing the contributor.**\n",
      "\n",
      "### Credits for the Cover Image\n",
      "\n",
      "Thank you very much <a href=\"https://unsplash.com/@kriztheman?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Christopher Alvarenga</a> who provided this through <a href=\"https://unsplash.com/s/photos/atlanta?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
      "  \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: real-estate-georgia\n",
      "name 'detect' is not defined Language is not detected: yellowj4acket\n",
      "name 'detect' is not defined Language is not detected: Real Estate Georgia\n",
      "name 'detect' is not defined Language is not detected: Real Worlds Dataset for Georgia (US) for the first 6 months of 2021\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "For various project occasions, a comprehensive US listings dataset was created with a colleague. This dataset is a subset for the State of Georgia.\n",
      "\n",
      "### Content\n",
      "\n",
      "This dataset shows real estate listing for Georgia (US) for the first 6 months of 2021. Prices are listed in USD.\n",
      "\n",
      "### Acknowledgements and Citation\n",
      "\n",
      "This dataset is a co-production, thanks to my fellow mate Jordan for building a great comprehensive US dataset from which I could filter this.\n",
      "\n",
      "**Using this dataset requires citing the contributor.**\n",
      "\n",
      "### Credits for the Cover Image\n",
      "\n",
      "Thank you very much <a href=\"https://unsplash.com/@kriztheman?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Christopher Alvarenga</a> who provided this through <a href=\"https://unsplash.com/s/photos/atlanta?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
      "  \n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: pythonafroz/movies-recomandation\n",
      "name 'detect' is not defined Language is not detected: movies-recomandation\n",
      "name 'detect' is not defined Language is not detected: pythonafroz\n",
      "name 'detect' is not defined Language is not detected: Movies recommendation\n",
      "name 'detect' is not defined Language is not detected: movies-recomandation\n",
      "name 'detect' is not defined Language is not detected: pythonafroz\n",
      "name 'detect' is not defined Language is not detected: Movies recommendation\n",
      "name 'detect' is not defined Language is not detected: lapodini/british-airway-reviews\n",
      "name 'detect' is not defined Language is not detected: british-airway-reviews\n",
      "name 'detect' is not defined Language is not detected: lapodini\n",
      "name 'detect' is not defined Language is not detected: British Airways Reviews\n",
      "name 'detect' is not defined Language is not detected: Reviews and General Sentiment among Customers\n",
      "name 'detect' is not defined Language is not detected: This dataset was scrapped by the SkyTrax site: https://www.airlinequality.com/airline-reviews/british-airways.\n",
      "\n",
      "It contains 2 .cvs files:\n",
      "\n",
      "1. `reviews_data1.csv`: This is a collection of reviews text along with the `Recommended` column (is the company recommended by the user? YES or NO) and the `Verified` column (if the travel is verified. YES or NO)\n",
      "\n",
      "2. `ratings_data.csv`: This is a collection of rate points.\n",
      "\n",
      "## Data Dictionary\n",
      "` reviews_data1.csv`\n",
      "1) Verified: YES or NO\n",
      "2) Reviews: content of the review\n",
      "3) Recommended: YES or NO\n",
      "\n",
      "`ratings_data.csv`\n",
      "1) Aircraft: Type of aircraft.\n",
      "2) Type Of Traveler:\n",
      "                                - Couple Leisure\n",
      "                                - Solo Leisure\n",
      "                                - Business\n",
      "                                - Family Leisure\n",
      "3) Seat Type:\n",
      "                      - Economy Class\n",
      "                      - Business Class\n",
      "                      - Premium Economy\n",
      "                      - First Class\n",
      "4) Route: Flight route\n",
      "5) Date Flown: Date of the travel\n",
      "6) Seat Confort: 1 to 5 stars\n",
      "7) Cabin Staff: 1 to 5 Stars\n",
      "8) Food & Beverages: 1 to 5 stars\n",
      "9) Inflight Entertainment: 1 to 5 stars\n",
      "10) Ground Service: 1 to 5 stars\n",
      "11) Wifi & Connectivity: 1 to 5 stars\n",
      "12) Value For Money: 1 to 5 stars\n",
      "13) Recommended: YES or NO\n",
      "\n",
      "## Possible use\n",
      "Sentiment analysis on reviews to understand which words come from texts.\n",
      "Analysis of ratings to understand which feature contributes more to the recommendation. \n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: british-airway-reviews\n",
      "name 'detect' is not defined Language is not detected: lapodini\n",
      "name 'detect' is not defined Language is not detected: British Airways Reviews\n",
      "name 'detect' is not defined Language is not detected: Reviews and General Sentiment among Customers\n",
      "name 'detect' is not defined Language is not detected: This dataset was scrapped by the SkyTrax site: https://www.airlinequality.com/airline-reviews/british-airways.\n",
      "\n",
      "It contains 2 .cvs files:\n",
      "\n",
      "1. `reviews_data1.csv`: This is a collection of reviews text along with the `Recommended` column (is the company recommended by the user? YES or NO) and the `Verified` column (if the travel is verified. YES or NO)\n",
      "\n",
      "2. `ratings_data.csv`: This is a collection of rate points.\n",
      "\n",
      "## Data Dictionary\n",
      "` reviews_data1.csv`\n",
      "1) Verified: YES or NO\n",
      "2) Reviews: content of the review\n",
      "3) Recommended: YES or NO\n",
      "\n",
      "`ratings_data.csv`\n",
      "1) Aircraft: Type of aircraft.\n",
      "2) Type Of Traveler:\n",
      "                                - Couple Leisure\n",
      "                                - Solo Leisure\n",
      "                                - Business\n",
      "                                - Family Leisure\n",
      "3) Seat Type:\n",
      "                      - Economy Class\n",
      "                      - Business Class\n",
      "                      - Premium Economy\n",
      "                      - First Class\n",
      "4) Route: Flight route\n",
      "5) Date Flown: Date of the travel\n",
      "6) Seat Confort: 1 to 5 stars\n",
      "7) Cabin Staff: 1 to 5 Stars\n",
      "8) Food & Beverages: 1 to 5 stars\n",
      "9) Inflight Entertainment: 1 to 5 stars\n",
      "10) Ground Service: 1 to 5 stars\n",
      "11) Wifi & Connectivity: 1 to 5 stars\n",
      "12) Value For Money: 1 to 5 stars\n",
      "13) Recommended: YES or NO\n",
      "\n",
      "## Possible use\n",
      "Sentiment analysis on reviews to understand which words come from texts.\n",
      "Analysis of ratings to understand which feature contributes more to the recommendation. \n",
      "\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sezginildes/the-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: the-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: sezginildes\n",
      "name 'detect' is not defined Language is not detected: the_movies_dataset\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "These files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.\n",
      "\n",
      "This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.\n",
      "\n",
      "Content\n",
      "This dataset consists of the following files:\n",
      "\n",
      "movies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\n",
      "\n",
      "keywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.\n",
      "\n",
      "credits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.\n",
      "\n",
      "links.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.\n",
      "\n",
      "links_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.\n",
      "\n",
      "ratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies.\n",
      "\n",
      "The Full MovieLens Dataset consisting of 26 million ratings and 750,000 tag applications from 270,000 users on all the 45,000 movies in this dataset can be accessed here\n",
      "\n",
      "Acknowledgements\n",
      "This dataset is an ensemble of data collected from TMDB and GroupLens.\n",
      "The Movie Details, Credits and Keywords have been collected from the TMDB Open API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.\n",
      "\n",
      "The Movie Links and Ratings have been obtained from the Official GroupLens website. The files are a part of the dataset available here\n",
      "\n",
      "\n",
      "\n",
      "Inspiration\n",
      "This dataset was assembled as part of my second Capstone Project for Springboard's Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata in combination with MovieLens ratings to build various types of Recommender Systems.\n",
      "\n",
      "Both my notebooks are available as kernels with this dataset: The Story of Film and Movie Recommender Systems\n",
      "\n",
      "Some of the things you can do with this dataset:\n",
      "Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content Based and Collaborative Filtering Based Recommendation Engines.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: the-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: sezginildes\n",
      "name 'detect' is not defined Language is not detected: the_movies_dataset\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "These files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.\n",
      "\n",
      "This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.\n",
      "\n",
      "Content\n",
      "This dataset consists of the following files:\n",
      "\n",
      "movies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\n",
      "\n",
      "keywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.\n",
      "\n",
      "credits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.\n",
      "\n",
      "links.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.\n",
      "\n",
      "links_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.\n",
      "\n",
      "ratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies.\n",
      "\n",
      "The Full MovieLens Dataset consisting of 26 million ratings and 750,000 tag applications from 270,000 users on all the 45,000 movies in this dataset can be accessed here\n",
      "\n",
      "Acknowledgements\n",
      "This dataset is an ensemble of data collected from TMDB and GroupLens.\n",
      "The Movie Details, Credits and Keywords have been collected from the TMDB Open API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.\n",
      "\n",
      "The Movie Links and Ratings have been obtained from the Official GroupLens website. The files are a part of the dataset available here\n",
      "\n",
      "\n",
      "\n",
      "Inspiration\n",
      "This dataset was assembled as part of my second Capstone Project for Springboard's Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata in combination with MovieLens ratings to build various types of Recommender Systems.\n",
      "\n",
      "Both my notebooks are available as kernels with this dataset: The Story of Film and Movie Recommender Systems\n",
      "\n",
      "Some of the things you can do with this dataset:\n",
      "Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content Based and Collaborative Filtering Based Recommendation Engines.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: denizbilginn/tv-shows\n",
      "name 'detect' is not defined Language is not detected: tv-shows\n",
      "name 'detect' is not defined Language is not detected: denizbilginn\n",
      "name 'detect' is not defined Language is not detected: TV Shows\n",
      "name 'detect' is not defined Language is not detected: A full, 3NF database that about current TV Shows (Approximately 160K Shows)\n",
      "name 'detect' is not defined Language is not detected: Welcome to TV Shows database, the database includes information of approximately **160K shows**. The data of the database is **updated** in **Jan 2024**.\n",
      "\n",
      "I carefully pre-processed the database as *3NF*. I gather the dataset from Asaniczka's dataset and the I pre-processed it.\n",
      "https://www.kaggle.com/datasets/asaniczka/full-tmdb-tv-shows-dataset-2023-150k-shows\n",
      "\n",
      "You can use this database to research **human taste tendencies**, **AI applications** and more...\n",
      "\n",
      "There is a ER diagram, you can use **forward checking** to create whole database clearly. The ER diagram created in **MySQL**. After creating the database, you can upload CSV tables to the SQL tables.\n",
      "\n",
      "#### Interesting Task Ideas:\n",
      "1. Explore trends in TV show popularity based on vote count and average.\n",
      "2. Analyze TV show genres to identify the most popular genres or combinations of genres.\n",
      "3. Investigate the relationship between TV show ratings and the number of seasons and episodes.\n",
      "4. Build a recommendation system that suggests TV shows based on a user's favorite genres or languages.\n",
      "5. Predict the success of a TV show based on features like vote count, average, and popularity.\n",
      "6. Identify the most prolific TV show creators or production companies based on the number of shows they have created.\n",
      "7. Explore the distribution of TV show run times and investigate whether episode duration affects the overall ratings.\n",
      "8. Investigate TV show production trends across different countries and networks.\n",
      "9. Analyze the relationship between TV show language and popularity, and investigate the popularity of non-English shows.\n",
      "10. Track the status of TV shows (in production or not) and analyze their popularity over time.\n",
      "11. Develop a language analysis model to identify sentiment or themes from TV show overviews.\n",
      "\n",
      "Please feel free to ask any questions about the data in the discussion section.\n",
      "name 'detect' is not defined Language is not detected: tv-shows\n",
      "name 'detect' is not defined Language is not detected: denizbilginn\n",
      "name 'detect' is not defined Language is not detected: TV Shows\n",
      "name 'detect' is not defined Language is not detected: A full, 3NF database that about current TV Shows (Approximately 160K Shows)\n",
      "name 'detect' is not defined Language is not detected: Welcome to TV Shows database, the database includes information of approximately **160K shows**. The data of the database is **updated** in **Jan 2024**.\n",
      "\n",
      "I carefully pre-processed the database as *3NF*. I gather the dataset from Asaniczka's dataset and the I pre-processed it.\n",
      "https://www.kaggle.com/datasets/asaniczka/full-tmdb-tv-shows-dataset-2023-150k-shows\n",
      "\n",
      "You can use this database to research **human taste tendencies**, **AI applications** and more...\n",
      "\n",
      "There is a ER diagram, you can use **forward checking** to create whole database clearly. The ER diagram created in **MySQL**. After creating the database, you can upload CSV tables to the SQL tables.\n",
      "\n",
      "#### Interesting Task Ideas:\n",
      "1. Explore trends in TV show popularity based on vote count and average.\n",
      "2. Analyze TV show genres to identify the most popular genres or combinations of genres.\n",
      "3. Investigate the relationship between TV show ratings and the number of seasons and episodes.\n",
      "4. Build a recommendation system that suggests TV shows based on a user's favorite genres or languages.\n",
      "5. Predict the success of a TV show based on features like vote count, average, and popularity.\n",
      "6. Identify the most prolific TV show creators or production companies based on the number of shows they have created.\n",
      "7. Explore the distribution of TV show run times and investigate whether episode duration affects the overall ratings.\n",
      "8. Investigate TV show production trends across different countries and networks.\n",
      "9. Analyze the relationship between TV show language and popularity, and investigate the popularity of non-English shows.\n",
      "10. Track the status of TV shows (in production or not) and analyze their popularity over time.\n",
      "11. Develop a language analysis model to identify sentiment or themes from TV show overviews.\n",
      "\n",
      "Please feel free to ask any questions about the data in the discussion section.\n",
      "name 'detect' is not defined Language is not detected: liorasymone/music-genre-classification-project\n",
      "name 'detect' is not defined Language is not detected: music-genre-classification-project\n",
      "name 'detect' is not defined Language is not detected: liorasymone\n",
      "name 'detect' is not defined Language is not detected: Music Genre Classification - Individual Project\n",
      "name 'detect' is not defined Language is not detected: Classification, recommendation and visualisation.\n",
      "name 'detect' is not defined Language is not detected: This report investigates the effectiveness of different machine learning methods on classifying music into different genres & subgenres, explores the mathematical techniques used for feature extraction, discusses the social implications of music genre, and offers processes for music recommendation.\n",
      "name 'detect' is not defined Language is not detected: music-genre-classification-project\n",
      "name 'detect' is not defined Language is not detected: liorasymone\n",
      "name 'detect' is not defined Language is not detected: Music Genre Classification - Individual Project\n",
      "name 'detect' is not defined Language is not detected: Classification, recommendation and visualisation.\n",
      "name 'detect' is not defined Language is not detected: This report investigates the effectiveness of different machine learning methods on classifying music into different genres & subgenres, explores the mathematical techniques used for feature extraction, discusses the social implications of music genre, and offers processes for music recommendation.\n",
      "name 'detect' is not defined Language is not detected: mayurdesai88/tmdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: mayurdesai88\n",
      "name 'detect' is not defined Language is not detected: TMDB MOVIES DATASET\n",
      "name 'detect' is not defined Language is not detected: Its contain over 10000 movies data\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the over 10000 TMDB movies including the id, title, release date,avg vote, vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "This data set contains some null values as there are missing fields in the tmdb database. Thought it's good for a young analyst to deal with missing value and you can also use this data to make movies recommendation systems \n",
      "name 'detect' is not defined Language is not detected: tmdb-movies-dataset\n",
      "name 'detect' is not defined Language is not detected: mayurdesai88\n",
      "name 'detect' is not defined Language is not detected: TMDB MOVIES DATASET\n",
      "name 'detect' is not defined Language is not detected: Its contain over 10000 movies data\n",
      "name 'detect' is not defined Language is not detected: This dataset contains the data of the over 10000 TMDB movies including the id, title, release date,avg vote, vote count, overview and popularity, etc.\n",
      "This data was collected by using the TMDB API, requests, json and converted into a dataframe using pandas.\n",
      "This data set contains some null values as there are missing fields in the tmdb database. Thought it's good for a young analyst to deal with missing value and you can also use this data to make movies recommendation systems \n",
      "name 'detect' is not defined Language is not detected: juliobfad/aavegotchinft\n",
      "name 'detect' is not defined Language is not detected: aavegotchinft\n",
      "name 'detect' is not defined Language is not detected: juliobfad\n",
      "name 'detect' is not defined Language is not detected: Aavegotchi Gotchis & Wearable sales 2020-2022\n",
      "name 'detect' is not defined Language is not detected: Analyze the thriving NFT Polygon market leveraging on the Aavegotchi protocol.\n",
      "name 'detect' is not defined Language is not detected: #  Welcome to the Aavegotchi Rabbithole\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2Fb53949ac349111ab82666c0d00c391c1%2Fgotchisun.png?generation=1670934029339188&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aavegotchi NFT general info  : https://wiki.aavegotchi.com/\n",
      "\n",
      "Aavegotchi_sales.csv: Sales from gotchis market:\n",
      "\n",
      "- $GHST_price: price at which the gotchi was sold in the cryptocurrency $GHST\n",
      "- ID: ID of the gotchi\n",
      "- BRS: Base Rarity Score of the gotchi, it is calculated from its traits : \n",
      " if (number &lt; 50): \n",
      "       return 100 - number\n",
      " else: \n",
      "       return number + 1\n",
      "- HAUNT: Which haunt the gotchi belongs to, Haunt 1(Edition1): 10000, Haunt2: 15000\n",
      "- Wearables: The wearables gotchis were wearing the moment they sold, comes in a list which represents the body slot of the gotchi as follows : Body/Face/Eyes/Head/LHands/RHands/Pet/Background \n",
      "- Seller: Seller address\n",
      "- Buyer: Buyer address\n",
      "- MBRS: Modified Basic Rarity Score, takes into account the effects from the wearables, which modify the BRS of the gotchis\n",
      "- Wearables additional data: https://wiki.aavegotchi.com/en/wearables\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2Fb32b5cff479b3ebf7e9832d34668fc94%2Fgotchitraits2.png?generation=1670932622836036&alt=media)\n",
      "\n",
      "- nrgTrait: Energy trait of the gotchi.\n",
      "- aggTrait: Aggression trait of the gotchi.\n",
      "- spkTrait: Spookyness trait of the gotchi.\n",
      "- brnTrait: Brain trait of the gotchi\n",
      "- eysTrait: Eyeshape trait of the gotchi\n",
      "- eyecTrait: Eyecolour trait of the gotchi\n",
      "- Date: Sell day\n",
      "- $USD_price: Price of $GHST (daily mean) @ the day of the sell.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2F33ca17b185e8af3e5762561c39521fe8%2Fwearables.png?generation=1670932799509596&alt=media)\n",
      "\n",
      "Wearable_sales.csv: Sales from the wearables market\n",
      "- $GHST_price: price at which the wearable was sold in the cryptocurrency $GHST\n",
      "- WearID : Wearable ID, corresponds to a wearable name\n",
      "- Rarity: Rarity level of the wearable: {0:Common,1:Uncommon,2:Rare,3:Legendary,4:Mythical,5:Godlike}\n",
      "- Quantity: Number of the same ID wearable sold\n",
      "- Seller: Seller address\n",
      "- Buyer: Buyer address\n",
      "- Date: Sell day\n",
      "- $USD_price: Price of $GHST (daily mean) @ the day of the sell.\n",
      "\n",
      "GHST.csv: Daily close price of the $GHST cryptocurrency.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2F3290aca1a8afe80e33cb026686364e90%2Fghstprice.png?generation=1670932938413258&alt=media)\n",
      "\n",
      "Source: Coingecko API\n",
      "name 'detect' is not defined Language is not detected: aavegotchinft\n",
      "name 'detect' is not defined Language is not detected: juliobfad\n",
      "name 'detect' is not defined Language is not detected: Aavegotchi Gotchis & Wearable sales 2020-2022\n",
      "name 'detect' is not defined Language is not detected: Analyze the thriving NFT Polygon market leveraging on the Aavegotchi protocol.\n",
      "name 'detect' is not defined Language is not detected: #  Welcome to the Aavegotchi Rabbithole\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2Fb53949ac349111ab82666c0d00c391c1%2Fgotchisun.png?generation=1670934029339188&alt=media)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aavegotchi NFT general info  : https://wiki.aavegotchi.com/\n",
      "\n",
      "Aavegotchi_sales.csv: Sales from gotchis market:\n",
      "\n",
      "- $GHST_price: price at which the gotchi was sold in the cryptocurrency $GHST\n",
      "- ID: ID of the gotchi\n",
      "- BRS: Base Rarity Score of the gotchi, it is calculated from its traits : \n",
      " if (number &lt; 50): \n",
      "       return 100 - number\n",
      " else: \n",
      "       return number + 1\n",
      "- HAUNT: Which haunt the gotchi belongs to, Haunt 1(Edition1): 10000, Haunt2: 15000\n",
      "- Wearables: The wearables gotchis were wearing the moment they sold, comes in a list which represents the body slot of the gotchi as follows : Body/Face/Eyes/Head/LHands/RHands/Pet/Background \n",
      "- Seller: Seller address\n",
      "- Buyer: Buyer address\n",
      "- MBRS: Modified Basic Rarity Score, takes into account the effects from the wearables, which modify the BRS of the gotchis\n",
      "- Wearables additional data: https://wiki.aavegotchi.com/en/wearables\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2Fb32b5cff479b3ebf7e9832d34668fc94%2Fgotchitraits2.png?generation=1670932622836036&alt=media)\n",
      "\n",
      "- nrgTrait: Energy trait of the gotchi.\n",
      "- aggTrait: Aggression trait of the gotchi.\n",
      "- spkTrait: Spookyness trait of the gotchi.\n",
      "- brnTrait: Brain trait of the gotchi\n",
      "- eysTrait: Eyeshape trait of the gotchi\n",
      "- eyecTrait: Eyecolour trait of the gotchi\n",
      "- Date: Sell day\n",
      "- $USD_price: Price of $GHST (daily mean) @ the day of the sell.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2F33ca17b185e8af3e5762561c39521fe8%2Fwearables.png?generation=1670932799509596&alt=media)\n",
      "\n",
      "Wearable_sales.csv: Sales from the wearables market\n",
      "- $GHST_price: price at which the wearable was sold in the cryptocurrency $GHST\n",
      "- WearID : Wearable ID, corresponds to a wearable name\n",
      "- Rarity: Rarity level of the wearable: {0:Common,1:Uncommon,2:Rare,3:Legendary,4:Mythical,5:Godlike}\n",
      "- Quantity: Number of the same ID wearable sold\n",
      "- Seller: Seller address\n",
      "- Buyer: Buyer address\n",
      "- Date: Sell day\n",
      "- $USD_price: Price of $GHST (daily mean) @ the day of the sell.\n",
      "\n",
      "GHST.csv: Daily close price of the $GHST cryptocurrency.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F7796991%2F3290aca1a8afe80e33cb026686364e90%2Fghstprice.png?generation=1670932938413258&alt=media)\n",
      "\n",
      "Source: Coingecko API\n",
      "name 'detect' is not defined Language is not detected: ramzanzdemir/online-retail-gift-products\n",
      "name 'detect' is not defined Language is not detected: online-retail-gift-products\n",
      "name 'detect' is not defined Language is not detected: ramzanzdemir\n",
      "name 'detect' is not defined Language is not detected: Online Retail: Gift Products\n",
      "name 'detect' is not defined Language is not detected: Contains Gift Products\n",
      "name 'detect' is not defined Language is not detected: The data set named Online Retail II was obtained from a UK-based online store.\n",
      "Includes sales between 01/12/2009 - 09/12/2011.\n",
      "\n",
      "\n",
      "Variables\n",
      "-InvoiceNo: Invoice number. The unique number of each transaction, namely the invoice. Aborted operation if it starts with C.\n",
      "-StockCode: Product code. Unique number for each product.\n",
      "-Description: Product name\n",
      "-Quantity: Number of products. It expresses how many of the products on the invoices have been sold.\n",
      "-InvoiceDate: Invoice date and time.\n",
      "-UnitPrice: Product price (in GBP)\n",
      "-CustomerID: Unique customer number\n",
      "-Country: Country name. Country where the customer lives.\n",
      "name 'detect' is not defined Language is not detected: online-retail-gift-products\n",
      "name 'detect' is not defined Language is not detected: ramzanzdemir\n",
      "name 'detect' is not defined Language is not detected: Online Retail: Gift Products\n",
      "name 'detect' is not defined Language is not detected: Contains Gift Products\n",
      "name 'detect' is not defined Language is not detected: The data set named Online Retail II was obtained from a UK-based online store.\n",
      "Includes sales between 01/12/2009 - 09/12/2011.\n",
      "\n",
      "\n",
      "Variables\n",
      "-InvoiceNo: Invoice number. The unique number of each transaction, namely the invoice. Aborted operation if it starts with C.\n",
      "-StockCode: Product code. Unique number for each product.\n",
      "-Description: Product name\n",
      "-Quantity: Number of products. It expresses how many of the products on the invoices have been sold.\n",
      "-InvoiceDate: Invoice date and time.\n",
      "-UnitPrice: Product price (in GBP)\n",
      "-CustomerID: Unique customer number\n",
      "-Country: Country name. Country where the customer lives.\n",
      "name 'detect' is not defined Language is not detected: greenwing1985/housepricing\n",
      "name 'detect' is not defined Language is not detected: housepricing\n",
      "name 'detect' is not defined Language is not detected: greenwing1985\n",
      "name 'detect' is not defined Language is not detected: HousePricing\n",
      "name 'detect' is not defined Language is not detected: Computer generated Dataset for ML training for absolute beginners\n",
      "name 'detect' is not defined Language is not detected: This is database is generated by computer, to help the very bigger in the field of machine learning , who wish to practice python and different ML models.\n",
      "name 'detect' is not defined Language is not detected: housepricing\n",
      "name 'detect' is not defined Language is not detected: greenwing1985\n",
      "name 'detect' is not defined Language is not detected: HousePricing\n",
      "name 'detect' is not defined Language is not detected: Computer generated Dataset for ML training for absolute beginners\n",
      "name 'detect' is not defined Language is not detected: This is database is generated by computer, to help the very bigger in the field of machine learning , who wish to practice python and different ML models.\n",
      "name 'detect' is not defined Language is not detected: patricklford/covid-19\n",
      "name 'detect' is not defined Language is not detected: covid-19\n",
      "name 'detect' is not defined Language is not detected: patricklford\n",
      "name 'detect' is not defined Language is not detected: COVID-19 & the virus that causes it: SARS-CoV-2.\n",
      "name 'detect' is not defined Language is not detected: Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?\n",
      "name 'detect' is not defined Language is not detected: **Introduction:**\n",
      "\n",
      "Coronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n",
      "\n",
      "Influenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n",
      "\n",
      "Influenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n",
      "\n",
      "Coronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n",
      "\n",
      "While both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n",
      "\n",
      "Another project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https://www.kaggle.com/datasets/patricklford/flunet-global-influenza-programme-who)\n",
      "\n",
      "**Origins and Discovery:**\n",
      "\n",
      "Coronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n",
      "\n",
      "**Human Coronaviruses:**\n",
      "\n",
      "The first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n",
      "\n",
      "A table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https://docs.google.com/document/d/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE/edit?usp=sharing)\n",
      "\n",
      "**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n",
      "\n",
      "The emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n",
      "\n",
      "**Middle East Respiratory Syndrome (MERS):**\n",
      "\n",
      "In 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n",
      "\n",
      "**COVID-19 Pandemic:**\n",
      "\n",
      "The most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n",
      "\n",
      "**COVID-19 Origin:**\n",
      "\n",
      "The origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n",
      "- Zoonotic Spillover:\n",
      "  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n",
      "  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n",
      "  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https://www.weforum.org/agenda/2020/03/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins/)\n",
      "- Laboratory-related Incident:\n",
      "  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n",
      "  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n",
      "- Ongoing Investigations:\n",
      "  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https://www.thelancet.com/journals/lanmic/article/PIIS2666-5247(23)00074-5/fulltext)\n",
      "  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n",
      "  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https://www.reuters.com/world/china/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22/)\n",
      "- International Calls for Investigation:\n",
      "  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n",
      "  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n",
      "\n",
      "In summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https://www.cell.com/current-biology/fulltext/S0960-9822(24)00446-9)\n",
      "\n",
      "**Long COVID:**\n",
      "\n",
      "\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n",
      "\n",
      "Symptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n",
      "- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n",
      "- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n",
      "- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n",
      "- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n",
      "- Headaches: Recurring headaches or migraines.\n",
      "- Chest pain: Persistent chest discomfort or tightness.\n",
      "- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n",
      "- Heart palpitations: Awareness of irregular or rapid heartbeat.\n",
      "- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n",
      "- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n",
      "\n",
      "The exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n",
      "\n",
      "Long COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n",
      "\n",
      "Managing long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n",
      "\n",
      "Supportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https://impact.economist.com/perspectives/health/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n",
      "\n",
      "Research into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https://www.news-medical.net/news/20240218/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n",
      "\n",
      "**Excess Deaths:**\n",
      "\n",
      "Excess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\n",
      "Here's a breakdown:\n",
      "- What are \"normal circumstances\"?\n",
      "  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n",
      "- How is it measured?\n",
      "  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n",
      "  - The difference between these two figures represents the excess deaths.\n",
      "  - This can be expressed as a number or a percentage.\n",
      "- Why is it important?\n",
      "  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n",
      "  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n",
      "  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n",
      "- Examples:\n",
      "  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n",
      "  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n",
      "- Additional points:\n",
      "  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n",
      "  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n",
      "  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n",
      "\n",
      "**Data Visualisations.**\n",
      "\n",
      "**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\n",
      "A Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148518)\n",
      "\n",
      "Charts:\n",
      "- Total COVID Cases Over Time:\n",
      "  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n",
      "  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n",
      "  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n",
      "- Total COVID Deaths Over Time:\n",
      "  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n",
      "  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n",
      "  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n",
      "- Distribution of Excess Deaths:\n",
      "  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n",
      "  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n",
      "  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n",
      "- Excess Deaths in Individuals Aged 65 and Older Over Time:\n",
      "  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n",
      "  - The y-axis shows the number of deaths, while the x-axis shows the year.\n",
      "  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n",
      "\n",
      "It is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n",
      "\n",
      "**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: ED.csv - [link](http://rpubs.com/Paddy_5142/1152620)\n",
      "\n",
      "A Markdown document with the R code for data examination for the data set: ED.csv - [link](http://rpubs.com/Paddy_5142/1154622)\n",
      "\n",
      "A document that explains the R code for data examination: ED.csv - [link](https://docs.google.com/document/d/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA/edit?usp=sharing)\n",
      "\n",
      "The x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n",
      "- 0 to 44 years old (red).\n",
      "- 45 to 64 years old (green).\n",
      "- 65 and over (blue).\n",
      "\n",
      "The chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n",
      "\n",
      "It is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https://ourworldindata.org/excess-mortality-covid)\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148820)\n",
      "\n",
      "- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n",
      "  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n",
      "  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n",
      "\n",
      "- The tibble output summarises the data for each variable: \n",
      "  - The first column, \"variable\", shows the name of the variable. \n",
      "  - The second column, \"mean_value\", shows the mean value of the variable. \n",
      "  - The third column, \"median_value\", shows the median value of the variable. \n",
      "  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n",
      "  - The fifth column, \"min_value\", shows the minimum value of the variable. \n",
      "  - The sixth column, \"max_value\", shows the maximum value of the variable.\n",
      "\n",
      "As you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148809)\n",
      "\n",
      "- For the \"Cardiovascular Death Rate\" plot:\n",
      "  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n",
      "  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n",
      "- For the \"Diabetes Prevalence\" plot:\n",
      "  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n",
      "  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n",
      "  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1152194)\n",
      "\n",
      "Chart:\n",
      "- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n",
      "- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n",
      "\n",
      "Tibble Output:\n",
      "- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n",
      "- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n",
      "- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n",
      "\n",
      "**COVID-19 Transmission:**\n",
      "\n",
      "Coronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https://www.nature.com/articles/s41467-020-19393-6)\n",
      "- Respiratory Droplets: \n",
      "  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n",
      "- Aerosol Transmission: \n",
      "  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n",
      "- Contact Transmission: \n",
      "  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n",
      "\n",
      "While respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https://www.ecdc.europa.eu/en/infectious-disease-topics/z-disease-list/covid-19/facts/transmission-covid-19)\n",
      "\n",
      "**COVID-19: Airborne Transmission.**\n",
      "\n",
      "The understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n",
      "- Early Recognition of Respiratory Transmission: \n",
      "  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n",
      "- Emerging Evidence of Airborne Transmission: \n",
      "  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n",
      "- Scientific Studies and Investigations: \n",
      "  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n",
      "- Expert Consensus and Updated Guidance: \n",
      "  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n",
      "- Recognition by Health Authorities: \n",
      "  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n",
      "  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n",
      "\n",
      "It's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n",
      "\n",
      "Overall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n",
      "\n",
      "**The Effects of Temperature and Humidity on Transmission:**\n",
      "\n",
      "The role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n",
      "- Temperature:\n",
      "  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n",
      "  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n",
      "  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n",
      "- Humidity:\n",
      "  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n",
      "  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n",
      "   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n",
      "\n",
      "Overall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n",
      "\n",
      "**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n",
      "\n",
      "Emerging evidence supports a link between environmental factors—including air pollution and chemical exposures, climate, and the built environment—and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment–COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10044492/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n",
      "\n",
      "Carbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO₂ levels to reduce virus survival and minimise the risk of infection. [link](https://www.nature.com/articles/s41467-024-47777-5)\n",
      "\n",
      "A previous project of mine entitled *Global CO₂ Emissions*. [link](https://www.kaggle.com/datasets/patricklford/global-co-emissions)\n",
      "\n",
      "**Viral Load:** [link](https://www.webmd.com/covid/covid-viral-load)\n",
      "- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n",
      "- Disease Severity – It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n",
      "- Transmission – Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n",
      "- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n",
      "- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n",
      "- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n",
      "- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\n",
      "Individual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n",
      "- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n",
      "\n",
      "**Waste Water Testing: Potential for Early Detection of Viruses.**\n",
      "\n",
      "The testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https://www.who.int/news-room/commentaries/detail/status-of-environmental-surveillance-for-sars-cov-2-virus)\n",
      "\n",
      "Several studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7384408/)\n",
      "\n",
      "Reports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582)\n",
      "\n",
      "Overall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https://www.scientificamerican.com/article/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests/)\n",
      "\n",
      "**Vaccination:** \n",
      "\n",
      "Vaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n",
      "- Disease Prevention: \n",
      "  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n",
      "- Eradication and Control of Diseases: \n",
      "  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n",
      "- Protection of Vulnerable Populations: \n",
      "  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n",
      "- Reduction of Healthcare Costs: \n",
      "  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n",
      "- Prevention of Outbreaks and Pandemics: \n",
      "  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n",
      "- Safe and Effective: \n",
      "  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n",
      "\n",
      "In conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n",
      "\n",
      "**COVID-19 Vaccination:**\n",
      "\n",
      "The development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n",
      "- Global Scientific Collaboration:\n",
      "  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n",
      "  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n",
      "  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n",
      "- Speed of Research and Deployment:\n",
      "  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n",
      "  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n",
      "  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n",
      "  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https://www.nature.com/articles/d41586-024-01293-0)\n",
      "  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n",
      "- Disparities in Vaccine Availability:\n",
      "  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n",
      "  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n",
      "  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n",
      "\n",
      "Coronavirus (COVID-19) Vaccinations: [link](https://ourworldindata.org/covid-vaccinations)\n",
      "- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n",
      "- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n",
      "- 32.7% of people in low-income countries have received at least one dose.\n",
      "\n",
      "In conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n",
      "\n",
      "**Data Visualisations:** \n",
      "\n",
      "**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n",
      "\n",
      "**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1147668)\n",
      "\n",
      "Print out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n",
      "\n",
      "A correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n",
      "- Correlation: \n",
      "  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n",
      "- Negative Correlation:\n",
      "  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n",
      "- Interpretation: \n",
      "  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n",
      "- Strength of Correlation: \n",
      "  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n",
      "\n",
      "Here's a comparison of the two plots and some additional insights:\n",
      "- Similarities:\n",
      "  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n",
      "  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n",
      "  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n",
      "- Both plots highlight the limitations of drawing causal conclusions: \n",
      "  - Neither plot can definitively establish that vaccination causes lower death rates. \n",
      "  - Other factors may be influencing the observed relationships.\n",
      "- Differences:\n",
      "  - The line plot provides a clearer visual representation of the trend over time. \n",
      "  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n",
      "  - The scatter plot shows more individual country variation. \n",
      "  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n",
      "  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n",
      "- Additional insights:\n",
      "  - It's important to consider the time frame of the data. \n",
      "  - Both plots only show data up to a certain point in time. \n",
      "- Other factors besides vaccination rates could be influencing excess deaths:\n",
      "  - Demographics (e.g., age structure, population density).\n",
      "  - Socioeconomic factors (e.g., poverty, inequality).\n",
      "  - Healthcare access and quality.\n",
      "  - Public health measures (e.g., masking, lock downs).\n",
      "  - Non-COVID-19 related deaths.\n",
      "- Overall:\n",
      "  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n",
      "  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n",
      "\n",
      "**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1151351)\n",
      "\n",
      "Code explanation:\n",
      "- Load Libraries: \n",
      "  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n",
      "- Pre-processing:\n",
      "  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n",
      "  - Convert 'date' to Date: For accurate time-based analysis.\n",
      "- Rolling Averages:\n",
      "  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n",
      "- Correlation:\n",
      "  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n",
      "- Visualisation:\n",
      "  - Create a line plot with time ('date') on the x-axis.\n",
      "  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n",
      "  - Add informative labels and a clean theme.\n",
      "  - Include an annotation on the plot displaying the calculated correlation value.\n",
      "- Chart Correlation:\n",
      "  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n",
      "  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n",
      "- Simple Linear Regression Model:\n",
      "  - Output from the code.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n",
      "\n",
      "Explanation of the Linear Regression Call and Output:\n",
      "- Call:\n",
      "  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n",
      "  - lm(): The standard function in R to fit linear regression models.\n",
      "  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n",
      "  - data = data_filtered: Specifies the dataset used for the analysis.\n",
      "- Output (summary(model)):\n",
      "  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n",
      "  - Coefficients: The heart of the regression output.\n",
      "  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n",
      "  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n",
      "  - Std. Error: Variability around coefficient estimates.\n",
      "  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n",
      "  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n",
      "- Goodness-of-Fit:\n",
      "  - Residual standard error: Variability around the regression line (smaller is better).\n",
      "  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n",
      "  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n",
      "- Interpretation of Results:\n",
      "  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n",
      "  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n",
      "- Essential Caveats:\n",
      "  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n",
      "\n",
      "Comparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10120793/)\n",
      "\n",
      "No Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https://www.factcheck.org/2023/04/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online/)\n",
      "\n",
      "**Slow vs. Fast Mutating Viruses:**\n",
      "\n",
      "The rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n",
      "- Mutation Rate:\n",
      "  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n",
      "  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n",
      "- Impact on Vaccines:\n",
      "  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n",
      "  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n",
      "- COVID-19 as a Case Study:\n",
      "  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n",
      "  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n",
      "  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n",
      "   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n",
      "  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n",
      "  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n",
      "- Single vs. Multiple Vaccines:\n",
      "  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n",
      "  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n",
      "- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n",
      "\n",
      "The optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n",
      "\n",
      "**Immune Exhaustion:**\n",
      "\n",
      "There is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9167431/)\n",
      "- Immune exhaustion:\n",
      "  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n",
      "- Lack of convincing evidence: \n",
      "  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n",
      "- Specific concerns for older adults and immunocompromised individuals: \n",
      "  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n",
      "- Weighing benefits and risks:\n",
      "  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n",
      "- Importance of considering individual circumstances: \n",
      "  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n",
      "- Ongoing research:\n",
      "  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n",
      "  - As more data becomes available, our understanding of the risks and benefits may evolve.\n",
      "- Key takeaways:\n",
      "  - COVID-19 vaccination remains highly effective and safe for most individuals.\n",
      "  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n",
      "  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\n",
      "Consulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n",
      "- Current evidence does not support a link between excess deaths and over-vaccination:\n",
      "  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n",
      "  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n",
      "- Multiple factors contribute to excess deaths in elderly populations: \n",
      "  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n",
      "- Age-related decline in immune system function: \n",
      "  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https://immunityageing.biomedcentral.com/articles/10.1186/s12979-019-0164-9)\n",
      "\n",
      "Exhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9640209/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n",
      "\n",
      "**Excess Deaths are Increasing:**\n",
      "\n",
      "*UK’s pattern of excess deaths deserves ‘close scrutiny’*: Research Professional News -  [link](https://www.researchprofessionalnews.com/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n",
      "\n",
      "*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https://www.euromomo.eu/graphs-and-maps/)\n",
      "\n",
      "EUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https://ec.europa.eu/eurostat/cache/recovery-dashboard/)\n",
      "\n",
      "Some potential theories based on recent news and ongoing research. \n",
      "\n",
      "- Delayed and missed medical care: \n",
      "  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n",
      "- Long COVID: \n",
      "  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n",
      "- Mental health impacts: \n",
      "  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n",
      "- Economic disruptions: \n",
      "  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n",
      "- Toxification:\n",
      "  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https://docs.google.com/document/d/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8/edit?usp=sharing)\n",
      "- Climate change: \n",
      "  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n",
      "- War and conflict: \n",
      "  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n",
      "- Ageing populations:\n",
      "  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n",
      "- Data limitations and complexities:\n",
      "  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n",
      "  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n",
      "\n",
      "It's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n",
      "\n",
      "**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n",
      "\n",
      "Dr. Clare Craig of HART [link]( https://totalityofevidence.com/dr-clare-craig/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https://www.conservativewoman.co.uk/ignored-danger-signals/)\n",
      "- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n",
      "- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n",
      "\n",
      "Further Supporting Evidence:\n",
      "- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https://www.nature.com/articles/s41598-022-10928-z)\n",
      "- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8709364/)\n",
      "- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https://www.tga.gov.au/news/covid-19-vaccine-safety-reports/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n",
      "\n",
      "Important Considerations:\n",
      "- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n",
      "- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n",
      "\n",
      "Call to Action:\n",
      "\n",
      "The potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n",
      "\n",
      "Trends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https://hansard.parliament.uk/commons/2023-10-20/debates/69C5A514-9A04-4ED7-B56B-61A3D40E3226/TrendsInExcessDeaths)\n",
      "\n",
      "Excess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https://www.parallelparliament.co.uk/mp/andrew-bridgen/debate/2024-01-16/commons/westminster-hall/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n",
      "\n",
      "**COVID-19 Lock Downs:**\n",
      "- No Universal Definition of \"Lock down\": \n",
      "  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n",
      "- The specificity of Measures: \n",
      "  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n",
      "- Changing Restrictions: \n",
      "  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n",
      "\n",
      "The Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https://ourworldindata.org/covid-stringency-index)\n",
      "- The nine metrics used to calculate the Stringency Index are: \n",
      "  - School closures. \n",
      "  - Workplace closures. \n",
      "  - Cancellation of public events. \n",
      "  - Restrictions on public gatherings. \n",
      "  - Closures of public transport.\n",
      "  - Stay-at-home requirements. \n",
      "  - Public information campaigns. \n",
      "  - Restrictions on internal movements.  \n",
      "  - International travel controls.\n",
      "\n",
      "COVID-19 lock downs by country: Wikipedia - [link](https://en.wikipedia.org/wiki/COVID-19_lockdowns_by_country)\n",
      "\n",
      "**Understanding COVID-19 Isolation:**\n",
      "- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n",
      "- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https://www.webmd.com/covid/news/20240223/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n",
      "\n",
      "The Toll of Isolation:\n",
      "- Mental Health - Prolonged isolation can harm mental health, leading to:\n",
      "  - Anxiety and stress\n",
      "  - Depression\n",
      "  - Loneliness\n",
      "  - Difficulty sleeping\n",
      "- Strained Relationships:\n",
      "  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n",
      "  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n",
      "  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n",
      "\n",
      "Potential Impact of CDC Changes:\n",
      "- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n",
      "  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n",
      "  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n",
      "  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n",
      "  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n",
      "  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n",
      "\n",
      "Why the Change?:\n",
      "- The CDC likely grapples with these factors:\n",
      "  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n",
      "  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n",
      "  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n",
      "\n",
      "This change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n",
      "- It underscores the need for:\n",
      "  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n",
      "  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n",
      "  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n",
      "\n",
      "**Behavioural Manipulation of Viruses:**\n",
      "\n",
      "How some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n",
      "- Why Manipulate Behaviour?\n",
      "  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n",
      "- Examples of Viral Manipulation:\n",
      "  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n",
      "  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n",
      "  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n",
      "  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n",
      "\n",
      "How Manipulation Works.\n",
      " - The mechanisms are diverse and complex, but some common themes include:\n",
      "  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n",
      "  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal – which could be manipulated by the pathogen.\n",
      "  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n",
      "\n",
      "Important Notes:\n",
      "- Evolving Science: \n",
      "  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n",
      "- Not Every Virus: \n",
      "  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n",
      "- Ethical Considerations: \n",
      "  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n",
      "\n",
      "There's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n",
      "- Transmission Mechanisms: \n",
      "  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n",
      "  - It doesn't require specific behavioural changes in the host for successful transmission. \n",
      "  - Simply being in close proximity to others when infected puts them at risk.\n",
      "- Symptoms and Behaviour: \n",
      "  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n",
      "  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n",
      "- Focus of Research: \n",
      "  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n",
      "  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n",
      "- Long-Term Effects: \n",
      "  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n",
      "  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n",
      "- Social and Psychological Impacts: \n",
      "  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n",
      "\n",
      "While there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7175891/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n",
      "\n",
      "\n",
      "**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n",
      "- Close Confinement: \n",
      "  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n",
      "  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n",
      "- Reduced Genetic Diversity: \n",
      "  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n",
      "  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n",
      "- Weakened Immune Systems: \n",
      "  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n",
      "  - This makes the animals more susceptible to contracting and spreading diseases.\n",
      "- Antibiotic Overuse: \n",
      "  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n",
      "  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n",
      "- Increased Human-Animal Contact: \n",
      "  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n",
      "- Globalised Meat Trade: \n",
      "  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n",
      "\n",
      "The Spanish Flu - A Historical Example: [link](https://en.wikipedia.org/wiki/Spanish_flu)\n",
      "\n",
      "While the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n",
      "\n",
      "The Consequence:\n",
      "\n",
      "These factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans – sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n",
      "- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https://www.webmd.com/food-recipes/food-poisoning/news/20240510/cows-are-potential-spreaders-bird-flu-humans)\n",
      "- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https://www.nature.com/articles/d41586-024-01416-7)\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Coronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks – and the global pandemics they could ignite.\n",
      "\n",
      "\n",
      "Patrick Ford 🦇\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "*Immunity and the Connections of Mental Well Being.\n",
      "The Power of Food to Strengthen the Immune System, to Protect Us.* [link](https://www.kaggle.com/datasets/patricklford/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n",
      "\n",
      "*We did not weave the web of life !\n",
      "We are merely a strand in the web.* [link](https://www.kaggle.com/datasets/patricklford/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n",
      "- Global Demographic Shifts.\n",
      "- Inequality.\n",
      "- Climate change.\n",
      "- Resource depletion.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Theoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https://docs.google.com/document/d/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI/edit?usp=sharing)\n",
      "name 'detect' is not defined Language is not detected: covid-19\n",
      "name 'detect' is not defined Language is not detected: patricklford\n",
      "name 'detect' is not defined Language is not detected: COVID-19 & the virus that causes it: SARS-CoV-2.\n",
      "name 'detect' is not defined Language is not detected: Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?\n",
      "name 'detect' is not defined Language is not detected: **Introduction:**\n",
      "\n",
      "Coronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n",
      "\n",
      "Influenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n",
      "\n",
      "Influenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n",
      "\n",
      "Coronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n",
      "\n",
      "While both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n",
      "\n",
      "Another project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https://www.kaggle.com/datasets/patricklford/flunet-global-influenza-programme-who)\n",
      "\n",
      "**Origins and Discovery:**\n",
      "\n",
      "Coronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n",
      "\n",
      "**Human Coronaviruses:**\n",
      "\n",
      "The first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n",
      "\n",
      "A table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https://docs.google.com/document/d/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE/edit?usp=sharing)\n",
      "\n",
      "**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n",
      "\n",
      "The emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n",
      "\n",
      "**Middle East Respiratory Syndrome (MERS):**\n",
      "\n",
      "In 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n",
      "\n",
      "**COVID-19 Pandemic:**\n",
      "\n",
      "The most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n",
      "\n",
      "**COVID-19 Origin:**\n",
      "\n",
      "The origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n",
      "- Zoonotic Spillover:\n",
      "  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n",
      "  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n",
      "  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https://www.weforum.org/agenda/2020/03/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins/)\n",
      "- Laboratory-related Incident:\n",
      "  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n",
      "  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n",
      "- Ongoing Investigations:\n",
      "  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https://www.thelancet.com/journals/lanmic/article/PIIS2666-5247(23)00074-5/fulltext)\n",
      "  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n",
      "  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https://www.reuters.com/world/china/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22/)\n",
      "- International Calls for Investigation:\n",
      "  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n",
      "  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n",
      "\n",
      "In summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https://www.cell.com/current-biology/fulltext/S0960-9822(24)00446-9)\n",
      "\n",
      "**Long COVID:**\n",
      "\n",
      "\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n",
      "\n",
      "Symptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n",
      "- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n",
      "- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n",
      "- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n",
      "- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n",
      "- Headaches: Recurring headaches or migraines.\n",
      "- Chest pain: Persistent chest discomfort or tightness.\n",
      "- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n",
      "- Heart palpitations: Awareness of irregular or rapid heartbeat.\n",
      "- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n",
      "- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n",
      "\n",
      "The exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n",
      "\n",
      "Long COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n",
      "\n",
      "Managing long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n",
      "\n",
      "Supportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https://impact.economist.com/perspectives/health/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n",
      "\n",
      "Research into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https://www.news-medical.net/news/20240218/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n",
      "\n",
      "**Excess Deaths:**\n",
      "\n",
      "Excess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\n",
      "Here's a breakdown:\n",
      "- What are \"normal circumstances\"?\n",
      "  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n",
      "- How is it measured?\n",
      "  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n",
      "  - The difference between these two figures represents the excess deaths.\n",
      "  - This can be expressed as a number or a percentage.\n",
      "- Why is it important?\n",
      "  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n",
      "  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n",
      "  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n",
      "- Examples:\n",
      "  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n",
      "  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n",
      "- Additional points:\n",
      "  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n",
      "  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n",
      "  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n",
      "\n",
      "**Data Visualisations.**\n",
      "\n",
      "**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\n",
      "A Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148518)\n",
      "\n",
      "Charts:\n",
      "- Total COVID Cases Over Time:\n",
      "  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n",
      "  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n",
      "  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n",
      "- Total COVID Deaths Over Time:\n",
      "  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n",
      "  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n",
      "  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n",
      "- Distribution of Excess Deaths:\n",
      "  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n",
      "  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n",
      "  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n",
      "- Excess Deaths in Individuals Aged 65 and Older Over Time:\n",
      "  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n",
      "  - The y-axis shows the number of deaths, while the x-axis shows the year.\n",
      "  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n",
      "\n",
      "It is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n",
      "\n",
      "**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: ED.csv - [link](http://rpubs.com/Paddy_5142/1152620)\n",
      "\n",
      "A Markdown document with the R code for data examination for the data set: ED.csv - [link](http://rpubs.com/Paddy_5142/1154622)\n",
      "\n",
      "A document that explains the R code for data examination: ED.csv - [link](https://docs.google.com/document/d/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA/edit?usp=sharing)\n",
      "\n",
      "The x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n",
      "- 0 to 44 years old (red).\n",
      "- 45 to 64 years old (green).\n",
      "- 65 and over (blue).\n",
      "\n",
      "The chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n",
      "\n",
      "It is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https://ourworldindata.org/excess-mortality-covid)\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148820)\n",
      "\n",
      "- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n",
      "  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n",
      "  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n",
      "\n",
      "- The tibble output summarises the data for each variable: \n",
      "  - The first column, \"variable\", shows the name of the variable. \n",
      "  - The second column, \"mean_value\", shows the mean value of the variable. \n",
      "  - The third column, \"median_value\", shows the median value of the variable. \n",
      "  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n",
      "  - The fifth column, \"min_value\", shows the minimum value of the variable. \n",
      "  - The sixth column, \"max_value\", shows the maximum value of the variable.\n",
      "\n",
      "As you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1148809)\n",
      "\n",
      "- For the \"Cardiovascular Death Rate\" plot:\n",
      "  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n",
      "  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n",
      "- For the \"Diabetes Prevalence\" plot:\n",
      "  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n",
      "  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n",
      "  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n",
      "\n",
      "**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1152194)\n",
      "\n",
      "Chart:\n",
      "- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n",
      "- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n",
      "\n",
      "Tibble Output:\n",
      "- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n",
      "- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n",
      "- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n",
      "\n",
      "**COVID-19 Transmission:**\n",
      "\n",
      "Coronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https://www.nature.com/articles/s41467-020-19393-6)\n",
      "- Respiratory Droplets: \n",
      "  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n",
      "- Aerosol Transmission: \n",
      "  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n",
      "- Contact Transmission: \n",
      "  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n",
      "\n",
      "While respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https://www.ecdc.europa.eu/en/infectious-disease-topics/z-disease-list/covid-19/facts/transmission-covid-19)\n",
      "\n",
      "**COVID-19: Airborne Transmission.**\n",
      "\n",
      "The understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n",
      "- Early Recognition of Respiratory Transmission: \n",
      "  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n",
      "- Emerging Evidence of Airborne Transmission: \n",
      "  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n",
      "- Scientific Studies and Investigations: \n",
      "  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n",
      "- Expert Consensus and Updated Guidance: \n",
      "  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n",
      "- Recognition by Health Authorities: \n",
      "  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n",
      "  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n",
      "\n",
      "It's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n",
      "\n",
      "Overall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n",
      "\n",
      "**The Effects of Temperature and Humidity on Transmission:**\n",
      "\n",
      "The role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n",
      "- Temperature:\n",
      "  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n",
      "  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n",
      "  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n",
      "- Humidity:\n",
      "  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n",
      "  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n",
      "   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n",
      "\n",
      "Overall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n",
      "\n",
      "**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n",
      "\n",
      "Emerging evidence supports a link between environmental factors—including air pollution and chemical exposures, climate, and the built environment—and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment–COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10044492/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n",
      "\n",
      "Carbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO₂ levels to reduce virus survival and minimise the risk of infection. [link](https://www.nature.com/articles/s41467-024-47777-5)\n",
      "\n",
      "A previous project of mine entitled *Global CO₂ Emissions*. [link](https://www.kaggle.com/datasets/patricklford/global-co-emissions)\n",
      "\n",
      "**Viral Load:** [link](https://www.webmd.com/covid/covid-viral-load)\n",
      "- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n",
      "- Disease Severity – It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n",
      "- Transmission – Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n",
      "- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n",
      "- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n",
      "- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n",
      "- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\n",
      "Individual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n",
      "- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n",
      "\n",
      "**Waste Water Testing: Potential for Early Detection of Viruses.**\n",
      "\n",
      "The testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https://www.who.int/news-room/commentaries/detail/status-of-environmental-surveillance-for-sars-cov-2-virus)\n",
      "\n",
      "Several studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7384408/)\n",
      "\n",
      "Reports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582)\n",
      "\n",
      "Overall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https://www.scientificamerican.com/article/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests/)\n",
      "\n",
      "**Vaccination:** \n",
      "\n",
      "Vaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n",
      "- Disease Prevention: \n",
      "  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n",
      "- Eradication and Control of Diseases: \n",
      "  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n",
      "- Protection of Vulnerable Populations: \n",
      "  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n",
      "- Reduction of Healthcare Costs: \n",
      "  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n",
      "- Prevention of Outbreaks and Pandemics: \n",
      "  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n",
      "- Safe and Effective: \n",
      "  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n",
      "\n",
      "In conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n",
      "\n",
      "**COVID-19 Vaccination:**\n",
      "\n",
      "The development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n",
      "- Global Scientific Collaboration:\n",
      "  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n",
      "  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n",
      "  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n",
      "- Speed of Research and Deployment:\n",
      "  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n",
      "  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n",
      "  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n",
      "  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https://www.nature.com/articles/d41586-024-01293-0)\n",
      "  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n",
      "- Disparities in Vaccine Availability:\n",
      "  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n",
      "  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n",
      "  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n",
      "\n",
      "Coronavirus (COVID-19) Vaccinations: [link](https://ourworldindata.org/covid-vaccinations)\n",
      "- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n",
      "- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n",
      "- 32.7% of people in low-income countries have received at least one dose.\n",
      "\n",
      "In conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n",
      "\n",
      "**Data Visualisations:** \n",
      "\n",
      "**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n",
      "\n",
      "**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1147668)\n",
      "\n",
      "Print out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n",
      "\n",
      "A correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n",
      "- Correlation: \n",
      "  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n",
      "- Negative Correlation:\n",
      "  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n",
      "- Interpretation: \n",
      "  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n",
      "- Strength of Correlation: \n",
      "  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n",
      "\n",
      "Here's a comparison of the two plots and some additional insights:\n",
      "- Similarities:\n",
      "  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n",
      "  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n",
      "  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n",
      "- Both plots highlight the limitations of drawing causal conclusions: \n",
      "  - Neither plot can definitively establish that vaccination causes lower death rates. \n",
      "  - Other factors may be influencing the observed relationships.\n",
      "- Differences:\n",
      "  - The line plot provides a clearer visual representation of the trend over time. \n",
      "  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n",
      "  - The scatter plot shows more individual country variation. \n",
      "  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n",
      "  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n",
      "- Additional insights:\n",
      "  - It's important to consider the time frame of the data. \n",
      "  - Both plots only show data up to a certain point in time. \n",
      "- Other factors besides vaccination rates could be influencing excess deaths:\n",
      "  - Demographics (e.g., age structure, population density).\n",
      "  - Socioeconomic factors (e.g., poverty, inequality).\n",
      "  - Healthcare access and quality.\n",
      "  - Public health measures (e.g., masking, lock downs).\n",
      "  - Non-COVID-19 related deaths.\n",
      "- Overall:\n",
      "  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n",
      "  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n",
      "\n",
      "**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n",
      "\n",
      "A Markdown document with the R code for the above plot from the data: covid.csv - [link](http://rpubs.com/Paddy_5142/1151351)\n",
      "\n",
      "Code explanation:\n",
      "- Load Libraries: \n",
      "  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n",
      "- Pre-processing:\n",
      "  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n",
      "  - Convert 'date' to Date: For accurate time-based analysis.\n",
      "- Rolling Averages:\n",
      "  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n",
      "- Correlation:\n",
      "  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n",
      "- Visualisation:\n",
      "  - Create a line plot with time ('date') on the x-axis.\n",
      "  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n",
      "  - Add informative labels and a clean theme.\n",
      "  - Include an annotation on the plot displaying the calculated correlation value.\n",
      "- Chart Correlation:\n",
      "  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n",
      "  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n",
      "- Simple Linear Regression Model:\n",
      "  - Output from the code.\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n",
      "\n",
      "Explanation of the Linear Regression Call and Output:\n",
      "- Call:\n",
      "  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n",
      "  - lm(): The standard function in R to fit linear regression models.\n",
      "  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n",
      "  - data = data_filtered: Specifies the dataset used for the analysis.\n",
      "- Output (summary(model)):\n",
      "  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n",
      "  - Coefficients: The heart of the regression output.\n",
      "  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n",
      "  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n",
      "  - Std. Error: Variability around coefficient estimates.\n",
      "  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n",
      "  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n",
      "- Goodness-of-Fit:\n",
      "  - Residual standard error: Variability around the regression line (smaller is better).\n",
      "  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n",
      "  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n",
      "- Interpretation of Results:\n",
      "  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n",
      "  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n",
      "- Essential Caveats:\n",
      "  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n",
      "\n",
      "Comparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10120793/)\n",
      "\n",
      "No Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https://www.factcheck.org/2023/04/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online/)\n",
      "\n",
      "**Slow vs. Fast Mutating Viruses:**\n",
      "\n",
      "The rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n",
      "- Mutation Rate:\n",
      "  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n",
      "  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n",
      "- Impact on Vaccines:\n",
      "  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n",
      "  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n",
      "- COVID-19 as a Case Study:\n",
      "  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n",
      "  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n",
      "  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n",
      "   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n",
      "  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n",
      "  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n",
      "- Single vs. Multiple Vaccines:\n",
      "  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n",
      "  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n",
      "- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n",
      "\n",
      "The optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n",
      "\n",
      "**Immune Exhaustion:**\n",
      "\n",
      "There is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9167431/)\n",
      "- Immune exhaustion:\n",
      "  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n",
      "- Lack of convincing evidence: \n",
      "  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n",
      "- Specific concerns for older adults and immunocompromised individuals: \n",
      "  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n",
      "- Weighing benefits and risks:\n",
      "  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n",
      "- Importance of considering individual circumstances: \n",
      "  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n",
      "- Ongoing research:\n",
      "  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n",
      "  - As more data becomes available, our understanding of the risks and benefits may evolve.\n",
      "- Key takeaways:\n",
      "  - COVID-19 vaccination remains highly effective and safe for most individuals.\n",
      "  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n",
      "  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\n",
      "Consulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n",
      "- Current evidence does not support a link between excess deaths and over-vaccination:\n",
      "  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n",
      "  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n",
      "- Multiple factors contribute to excess deaths in elderly populations: \n",
      "  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n",
      "- Age-related decline in immune system function: \n",
      "  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https://immunityageing.biomedcentral.com/articles/10.1186/s12979-019-0164-9)\n",
      "\n",
      "Exhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9640209/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n",
      "\n",
      "**Excess Deaths are Increasing:**\n",
      "\n",
      "*UK’s pattern of excess deaths deserves ‘close scrutiny’*: Research Professional News -  [link](https://www.researchprofessionalnews.com/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n",
      "\n",
      "*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https://www.euromomo.eu/graphs-and-maps/)\n",
      "\n",
      "EUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https://ec.europa.eu/eurostat/cache/recovery-dashboard/)\n",
      "\n",
      "Some potential theories based on recent news and ongoing research. \n",
      "\n",
      "- Delayed and missed medical care: \n",
      "  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n",
      "- Long COVID: \n",
      "  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n",
      "- Mental health impacts: \n",
      "  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n",
      "- Economic disruptions: \n",
      "  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n",
      "- Toxification:\n",
      "  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https://docs.google.com/document/d/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8/edit?usp=sharing)\n",
      "- Climate change: \n",
      "  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n",
      "- War and conflict: \n",
      "  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n",
      "- Ageing populations:\n",
      "  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n",
      "- Data limitations and complexities:\n",
      "  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n",
      "  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n",
      "\n",
      "It's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n",
      "\n",
      "**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n",
      "\n",
      "Dr. Clare Craig of HART [link]( https://totalityofevidence.com/dr-clare-craig/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https://www.conservativewoman.co.uk/ignored-danger-signals/)\n",
      "- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n",
      "- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n",
      "\n",
      "Further Supporting Evidence:\n",
      "- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https://www.nature.com/articles/s41598-022-10928-z)\n",
      "- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8709364/)\n",
      "- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https://www.tga.gov.au/news/covid-19-vaccine-safety-reports/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n",
      "\n",
      "Important Considerations:\n",
      "- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n",
      "- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n",
      "\n",
      "Call to Action:\n",
      "\n",
      "The potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n",
      "\n",
      "Trends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https://hansard.parliament.uk/commons/2023-10-20/debates/69C5A514-9A04-4ED7-B56B-61A3D40E3226/TrendsInExcessDeaths)\n",
      "\n",
      "Excess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https://www.parallelparliament.co.uk/mp/andrew-bridgen/debate/2024-01-16/commons/westminster-hall/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n",
      "\n",
      "**COVID-19 Lock Downs:**\n",
      "- No Universal Definition of \"Lock down\": \n",
      "  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n",
      "- The specificity of Measures: \n",
      "  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n",
      "- Changing Restrictions: \n",
      "  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n",
      "\n",
      "The Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https://ourworldindata.org/covid-stringency-index)\n",
      "- The nine metrics used to calculate the Stringency Index are: \n",
      "  - School closures. \n",
      "  - Workplace closures. \n",
      "  - Cancellation of public events. \n",
      "  - Restrictions on public gatherings. \n",
      "  - Closures of public transport.\n",
      "  - Stay-at-home requirements. \n",
      "  - Public information campaigns. \n",
      "  - Restrictions on internal movements.  \n",
      "  - International travel controls.\n",
      "\n",
      "COVID-19 lock downs by country: Wikipedia - [link](https://en.wikipedia.org/wiki/COVID-19_lockdowns_by_country)\n",
      "\n",
      "**Understanding COVID-19 Isolation:**\n",
      "- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n",
      "- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https://www.webmd.com/covid/news/20240223/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n",
      "\n",
      "The Toll of Isolation:\n",
      "- Mental Health - Prolonged isolation can harm mental health, leading to:\n",
      "  - Anxiety and stress\n",
      "  - Depression\n",
      "  - Loneliness\n",
      "  - Difficulty sleeping\n",
      "- Strained Relationships:\n",
      "  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n",
      "  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n",
      "  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n",
      "\n",
      "Potential Impact of CDC Changes:\n",
      "- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n",
      "  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n",
      "  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n",
      "  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n",
      "  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n",
      "  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n",
      "\n",
      "Why the Change?:\n",
      "- The CDC likely grapples with these factors:\n",
      "  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n",
      "  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n",
      "  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n",
      "\n",
      "This change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n",
      "- It underscores the need for:\n",
      "  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n",
      "  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n",
      "  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n",
      "\n",
      "**Behavioural Manipulation of Viruses:**\n",
      "\n",
      "How some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n",
      "- Why Manipulate Behaviour?\n",
      "  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n",
      "- Examples of Viral Manipulation:\n",
      "  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n",
      "  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n",
      "  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n",
      "  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n",
      "\n",
      "How Manipulation Works.\n",
      " - The mechanisms are diverse and complex, but some common themes include:\n",
      "  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n",
      "  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal – which could be manipulated by the pathogen.\n",
      "  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n",
      "\n",
      "Important Notes:\n",
      "- Evolving Science: \n",
      "  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n",
      "- Not Every Virus: \n",
      "  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n",
      "- Ethical Considerations: \n",
      "  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n",
      "\n",
      "There's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n",
      "- Transmission Mechanisms: \n",
      "  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n",
      "  - It doesn't require specific behavioural changes in the host for successful transmission. \n",
      "  - Simply being in close proximity to others when infected puts them at risk.\n",
      "- Symptoms and Behaviour: \n",
      "  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n",
      "  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n",
      "- Focus of Research: \n",
      "  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n",
      "  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n",
      "- Long-Term Effects: \n",
      "  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n",
      "  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n",
      "- Social and Psychological Impacts: \n",
      "  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n",
      "\n",
      "While there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7175891/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n",
      "\n",
      "\n",
      "**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n",
      "- Close Confinement: \n",
      "  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n",
      "  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n",
      "- Reduced Genetic Diversity: \n",
      "  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n",
      "  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n",
      "- Weakened Immune Systems: \n",
      "  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n",
      "  - This makes the animals more susceptible to contracting and spreading diseases.\n",
      "- Antibiotic Overuse: \n",
      "  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n",
      "  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n",
      "- Increased Human-Animal Contact: \n",
      "  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n",
      "- Globalised Meat Trade: \n",
      "  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n",
      "\n",
      "The Spanish Flu - A Historical Example: [link](https://en.wikipedia.org/wiki/Spanish_flu)\n",
      "\n",
      "While the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n",
      "\n",
      "The Consequence:\n",
      "\n",
      "These factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans – sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n",
      "- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https://www.webmd.com/food-recipes/food-poisoning/news/20240510/cows-are-potential-spreaders-bird-flu-humans)\n",
      "- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https://www.nature.com/articles/d41586-024-01416-7)\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Coronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks – and the global pandemics they could ignite.\n",
      "\n",
      "\n",
      "Patrick Ford 🦇\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "*Immunity and the Connections of Mental Well Being.\n",
      "The Power of Food to Strengthen the Immune System, to Protect Us.* [link](https://www.kaggle.com/datasets/patricklford/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n",
      "\n",
      "*We did not weave the web of life !\n",
      "We are merely a strand in the web.* [link](https://www.kaggle.com/datasets/patricklford/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n",
      "- Global Demographic Shifts.\n",
      "- Inequality.\n",
      "- Climate change.\n",
      "- Resource depletion.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Theoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https://docs.google.com/document/d/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI/edit?usp=sharing)\n",
      "name 'detect' is not defined Language is not detected: loveall/email-campaign-management-for-sme\n",
      "name 'detect' is not defined Language is not detected: email-campaign-management-for-sme\n",
      "name 'detect' is not defined Language is not detected: loveall\n",
      "name 'detect' is not defined Language is not detected: Email Campaign Management for SME\n",
      "name 'detect' is not defined Language is not detected: Tracking Email Status  in a Multi Channel Marketing Platform\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Most of the small to medium business owners are making effective use of Gmail based Email marketing Strategies for offline targeting of converting their prospective customers into leads so that they stay with them in Business\n",
      "\n",
      "\n",
      "### Content\n",
      "we have different aspects of emails to characterize the mail and track the mail is ignored; read; acknowledged  by the reader\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "corefactors.in\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "amount of advertising dollars spent on a product determines the amount of its sales, we could use regression analysis to quantify the precise nature of the relationship between advertising and sales. here we want  everyone  to experiment with this fun data , what value we can derive from email as a tool for compaign marketing in a multi channel marketing strategy of a Small to Medium Businesses\n",
      "\n",
      "name 'detect' is not defined Language is not detected: email-campaign-management-for-sme\n",
      "name 'detect' is not defined Language is not detected: loveall\n",
      "name 'detect' is not defined Language is not detected: Email Campaign Management for SME\n",
      "name 'detect' is not defined Language is not detected: Tracking Email Status  in a Multi Channel Marketing Platform\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Most of the small to medium business owners are making effective use of Gmail based Email marketing Strategies for offline targeting of converting their prospective customers into leads so that they stay with them in Business\n",
      "\n",
      "\n",
      "### Content\n",
      "we have different aspects of emails to characterize the mail and track the mail is ignored; read; acknowledged  by the reader\n",
      "\n",
      "\n",
      "\n",
      "### Acknowledgements\n",
      "corefactors.in\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "amount of advertising dollars spent on a product determines the amount of its sales, we could use regression analysis to quantify the precise nature of the relationship between advertising and sales. here we want  everyone  to experiment with this fun data , what value we can derive from email as a tool for compaign marketing in a multi channel marketing strategy of a Small to Medium Businesses\n",
      "\n",
      "name 'detect' is not defined Language is not detected: loveall/appliances-energy-prediction\n",
      "name 'detect' is not defined Language is not detected: appliances-energy-prediction\n",
      "name 'detect' is not defined Language is not detected: loveall\n",
      "name 'detect' is not defined Language is not detected: Appliances Energy Prediction\n",
      "name 'detect' is not defined Language is not detected: Data driven prediction  of energy use of appliances\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      " Experimental data used to create regression models of appliances energy use in a low energy building.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters). \n",
      "\n",
      "### Acknowledgements\n",
      "Luis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS)\n",
      "\n",
      "Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link].\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Data used include measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station and recorded energy use of lighting fixtures. data filtering to remove non-predictive parameters and feature ranking plays an important role with this data. Different statistical models could be developed over this dataset. \n",
      "Highlights:\n",
      "The appliances energy consumption prediction in a low energy house is the dataset content\n",
      "Weather data from a nearby station was found to improve the prediction.\n",
      "\n",
      "Pressure, air temperature and wind speed are important parameters in the prediction.\n",
      "\n",
      "Data from a WSN that measures temperature and humidity increase the pred. accuracy.\n",
      "\n",
      "From the WSN, the kitchen, laundry and living room data ranked high in importance.\n",
      "name 'detect' is not defined Language is not detected: appliances-energy-prediction\n",
      "name 'detect' is not defined Language is not detected: loveall\n",
      "name 'detect' is not defined Language is not detected: Appliances Energy Prediction\n",
      "name 'detect' is not defined Language is not detected: Data driven prediction  of energy use of appliances\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      " Experimental data used to create regression models of appliances energy use in a low energy building.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters). \n",
      "\n",
      "### Acknowledgements\n",
      "Luis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS)\n",
      "\n",
      "Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link].\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Data used include measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station and recorded energy use of lighting fixtures. data filtering to remove non-predictive parameters and feature ranking plays an important role with this data. Different statistical models could be developed over this dataset. \n",
      "Highlights:\n",
      "The appliances energy consumption prediction in a low energy house is the dataset content\n",
      "Weather data from a nearby station was found to improve the prediction.\n",
      "\n",
      "Pressure, air temperature and wind speed are important parameters in the prediction.\n",
      "\n",
      "Data from a WSN that measures temperature and humidity increase the pred. accuracy.\n",
      "\n",
      "From the WSN, the kitchen, laundry and living room data ranked high in importance.\n",
      "name 'detect' is not defined Language is not detected: jitendra28rawat/bookcrossing\n",
      "name 'detect' is not defined Language is not detected: bookcrossing\n",
      "name 'detect' is not defined Language is not detected: jitendra28rawat\n",
      "name 'detect' is not defined Language is not detected: Book-Crossing\n",
      "name 'detect' is not defined Language is not detected: It book rating dataset compiled by Cai-Nicolas Ziegler.\n",
      "name 'detect' is not defined Language is not detected: Collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
      "\n",
      "[ ! ] Freely available for research use when acknowledged with the following reference (further details on the dataset are given in this publication):\n",
      "Improving Recommendation Lists Through Topic Diversification,\n",
      "Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen; Proceedings of the 14th International World Wide Web Conference (WWW '05), May 10-14, 2005, Chiba, Japan. To appear.\n",
      "name 'detect' is not defined Language is not detected: bookcrossing\n",
      "name 'detect' is not defined Language is not detected: jitendra28rawat\n",
      "name 'detect' is not defined Language is not detected: Book-Crossing\n",
      "name 'detect' is not defined Language is not detected: It book rating dataset compiled by Cai-Nicolas Ziegler.\n",
      "name 'detect' is not defined Language is not detected: Collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
      "\n",
      "[ ! ] Freely available for research use when acknowledged with the following reference (further details on the dataset are given in this publication):\n",
      "Improving Recommendation Lists Through Topic Diversification,\n",
      "Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen; Proceedings of the 14th International World Wide Web Conference (WWW '05), May 10-14, 2005, Chiba, Japan. To appear.\n",
      "name 'detect' is not defined Language is not detected: ashishvaya/recommendation-engine\n",
      "name 'detect' is not defined Language is not detected: recommendation-engine\n",
      "name 'detect' is not defined Language is not detected: ashishvaya\n",
      "name 'detect' is not defined Language is not detected: Recommendation Engine\n",
      "name 'detect' is not defined Language is not detected: Recommending the questions that a programmer should solve\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Recommending the questions that a programmer should solve given his/her current expertise is a big challenge for Online Judge Platforms but is an essential task to keep a programmer engaged on their platform.\n",
      "\n",
      "In this practice problem, you are given the data of programmers and questions that they have previously solved along with the time that they took to solve that particular question.\n",
      "\n",
      "As a data scientist, your task is to build a model that can predict the time taken to solve a problem given the user current status.\n",
      "\n",
      "This model will help online judges to decide the next level of questions to recommend to a user.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Data Files\n",
      "\n",
      " \n",
      "\n",
      "There are 3 training data files.\n",
      "\n",
      " \n",
      "\n",
      "train_submissions.csv - This contains 1,55,295 submissions which are selected randomly from 2,21,850 submissions. Contains 3 columns (‘user_id’, ‘problem_id’, ‘attempts_range’). The variable ‘attempts_range’ denoted the range no. in which attempts the user made to get the solution accepted lies.\n",
      " \n",
      "\n",
      "            We have used following criteria to define the attempts_range :-\n",
      "\n",
      "            attempts_range            No. of attempts lies inside\n",
      "\n",
      "            1                                         1-1\n",
      "\n",
      "            2                                         2-3\n",
      "\n",
      "            3                                         4-5\n",
      "\n",
      "            4                                         6-7\n",
      "\n",
      "            5                                         8-9\n",
      "\n",
      "            6                                         &gt;=10\n",
      "\n",
      "           \n",
      "\n",
      "user_data.csv - This is the file containing data of users. It contains the following features :-\n",
      "\n",
      "user_id - unique ID assigned to each user\n",
      "submission_count - total number of user submissions\n",
      "problem_solved - total number of accepted user submissions\n",
      "contribution - user contribution to the judge\n",
      "country - location of user\n",
      "follower_count - amount of users who have this user in followers\n",
      "last_online_time_seconds - time when user was last seen online\n",
      "max_rating - maximum rating of user\n",
      "rating - rating of user\n",
      "rank - can be one of ‘beginner’ ,’intermediate’ , ‘advanced’, ‘expert’\n",
      "registration_time_seconds - time when user was registered\n",
      " \n",
      "\n",
      "problem_data.csv - This is the file containing data of the problems. It contains the following features :-\n",
      "problem_id - unique ID assigned to each problem\n",
      "level_id - the difficulty level of the problem between ‘A’ to ‘N’\n",
      "points - amount of points for the problem\n",
      "tags - problem tag(s) like greedy, graphs, DFS etc.\n",
      " \n",
      "\n",
      "test_submissions.csv - This contains the remaining 66,555 submissions from total 2,21,850 submissions. Contains 1 column (ID). The ‘attempts_range’ column is to be predicted.\n",
      "name 'detect' is not defined Language is not detected: recommendation-engine\n",
      "name 'detect' is not defined Language is not detected: ashishvaya\n",
      "name 'detect' is not defined Language is not detected: Recommendation Engine\n",
      "name 'detect' is not defined Language is not detected: Recommending the questions that a programmer should solve\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "Recommending the questions that a programmer should solve given his/her current expertise is a big challenge for Online Judge Platforms but is an essential task to keep a programmer engaged on their platform.\n",
      "\n",
      "In this practice problem, you are given the data of programmers and questions that they have previously solved along with the time that they took to solve that particular question.\n",
      "\n",
      "As a data scientist, your task is to build a model that can predict the time taken to solve a problem given the user current status.\n",
      "\n",
      "This model will help online judges to decide the next level of questions to recommend to a user.\n",
      "\n",
      "\n",
      "### Content\n",
      "\n",
      "Data Files\n",
      "\n",
      " \n",
      "\n",
      "There are 3 training data files.\n",
      "\n",
      " \n",
      "\n",
      "train_submissions.csv - This contains 1,55,295 submissions which are selected randomly from 2,21,850 submissions. Contains 3 columns (‘user_id’, ‘problem_id’, ‘attempts_range’). The variable ‘attempts_range’ denoted the range no. in which attempts the user made to get the solution accepted lies.\n",
      " \n",
      "\n",
      "            We have used following criteria to define the attempts_range :-\n",
      "\n",
      "            attempts_range            No. of attempts lies inside\n",
      "\n",
      "            1                                         1-1\n",
      "\n",
      "            2                                         2-3\n",
      "\n",
      "            3                                         4-5\n",
      "\n",
      "            4                                         6-7\n",
      "\n",
      "            5                                         8-9\n",
      "\n",
      "            6                                         &gt;=10\n",
      "\n",
      "           \n",
      "\n",
      "user_data.csv - This is the file containing data of users. It contains the following features :-\n",
      "\n",
      "user_id - unique ID assigned to each user\n",
      "submission_count - total number of user submissions\n",
      "problem_solved - total number of accepted user submissions\n",
      "contribution - user contribution to the judge\n",
      "country - location of user\n",
      "follower_count - amount of users who have this user in followers\n",
      "last_online_time_seconds - time when user was last seen online\n",
      "max_rating - maximum rating of user\n",
      "rating - rating of user\n",
      "rank - can be one of ‘beginner’ ,’intermediate’ , ‘advanced’, ‘expert’\n",
      "registration_time_seconds - time when user was registered\n",
      " \n",
      "\n",
      "problem_data.csv - This is the file containing data of the problems. It contains the following features :-\n",
      "problem_id - unique ID assigned to each problem\n",
      "level_id - the difficulty level of the problem between ‘A’ to ‘N’\n",
      "points - amount of points for the problem\n",
      "tags - problem tag(s) like greedy, graphs, DFS etc.\n",
      " \n",
      "\n",
      "test_submissions.csv - This contains the remaining 66,555 submissions from total 2,21,850 submissions. Contains 1 column (ID). The ‘attempts_range’ column is to be predicted.\n",
      "name 'detect' is not defined Language is not detected: nagrajdesai/latest-10000-movies-dataset-from-tmdb\n",
      "name 'detect' is not defined Language is not detected: latest-10000-movies-dataset-from-tmdb\n",
      "name 'detect' is not defined Language is not detected: nagrajdesai\n",
      "name 'detect' is not defined Language is not detected: Latest 10000 Movies Dataset from TMDB\n",
      "name 'detect' is not defined Language is not detected: Newly released 10000 movies dataset from The Movie Database(TMDB)\n",
      "name 'detect' is not defined Language is not detected: This movies dataset can certainly be used for a variety of purposes, depending on goals and the insights you're looking to derive from the data. Here are some potential use cases for the dataset.\n",
      "\n",
      "**Movie Analysis**\n",
      "\n",
      "**Recommendation Systems**\n",
      "\n",
      "**Popularity Measurement**\n",
      "\n",
      "**Audience Engagement**\n",
      "\n",
      "**Comparative Analysis**\n",
      "\n",
      "\n",
      "*The dataset consists of various attributes related to movies. These attributes provide information about each entry in the dataset:*\n",
      "\n",
      "**1. Index:** \n",
      "- Index for each row\n",
      "\n",
      "**2. Title:** \n",
      "- The title attribute represents the name of the movie.\n",
      "\n",
      "**3. Original Language:** \n",
      "- This attribute signifies the language in which the movie was originally produced. It could offer insights into the target audience and geographical scope of the content.\n",
      "\n",
      "**4. Release Date:** \n",
      "- This attribute indicates when the movie was officially released for public viewing. The release date can impact factors like marketing strategies, competition with other releases, and audience anticipation.\n",
      "\n",
      "**5. Popularity:** \n",
      "- This attribute likely represents the measure of how well-known or talked-about a particular movie is within a given context. It could be based on factors such as online discussions, social media mentions, and viewer interest.\n",
      "\n",
      "**6. Vote Average:**\n",
      "- This attribute likely represents the average rating or score given to the movie by viewers who have voted. A higher average could imply that the content is generally well-received.\n",
      "\n",
      "**7. Vote Count:** \n",
      "- This attribute indicates the number of votes or ratings that the movie has received from viewers. A higher vote count might suggest a larger viewer base or a more engaging content.\n",
      "\n",
      "**8. Overview:** \n",
      "- This attribute provides a concise summary or description of the movie plot, themes, and overall content. It offers a glimpse into what the content is about.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: latest-10000-movies-dataset-from-tmdb\n",
      "name 'detect' is not defined Language is not detected: nagrajdesai\n",
      "name 'detect' is not defined Language is not detected: Latest 10000 Movies Dataset from TMDB\n",
      "name 'detect' is not defined Language is not detected: Newly released 10000 movies dataset from The Movie Database(TMDB)\n",
      "name 'detect' is not defined Language is not detected: This movies dataset can certainly be used for a variety of purposes, depending on goals and the insights you're looking to derive from the data. Here are some potential use cases for the dataset.\n",
      "\n",
      "**Movie Analysis**\n",
      "\n",
      "**Recommendation Systems**\n",
      "\n",
      "**Popularity Measurement**\n",
      "\n",
      "**Audience Engagement**\n",
      "\n",
      "**Comparative Analysis**\n",
      "\n",
      "\n",
      "*The dataset consists of various attributes related to movies. These attributes provide information about each entry in the dataset:*\n",
      "\n",
      "**1. Index:** \n",
      "- Index for each row\n",
      "\n",
      "**2. Title:** \n",
      "- The title attribute represents the name of the movie.\n",
      "\n",
      "**3. Original Language:** \n",
      "- This attribute signifies the language in which the movie was originally produced. It could offer insights into the target audience and geographical scope of the content.\n",
      "\n",
      "**4. Release Date:** \n",
      "- This attribute indicates when the movie was officially released for public viewing. The release date can impact factors like marketing strategies, competition with other releases, and audience anticipation.\n",
      "\n",
      "**5. Popularity:** \n",
      "- This attribute likely represents the measure of how well-known or talked-about a particular movie is within a given context. It could be based on factors such as online discussions, social media mentions, and viewer interest.\n",
      "\n",
      "**6. Vote Average:**\n",
      "- This attribute likely represents the average rating or score given to the movie by viewers who have voted. A higher average could imply that the content is generally well-received.\n",
      "\n",
      "**7. Vote Count:** \n",
      "- This attribute indicates the number of votes or ratings that the movie has received from viewers. A higher vote count might suggest a larger viewer base or a more engaging content.\n",
      "\n",
      "**8. Overview:** \n",
      "- This attribute provides a concise summary or description of the movie plot, themes, and overall content. It offers a glimpse into what the content is about.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: aswathrao/janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: aswathrao\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Machine Learning Hackethon\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "name 'detect' is not defined Language is not detected: janatahack-independence-day-2020-ml-hackathon\n",
      "name 'detect' is not defined Language is not detected: aswathrao\n",
      "name 'detect' is not defined Language is not detected: Janatahack: Independence Day 2020 ML Hackathon\n",
      "name 'detect' is not defined Language is not detected: Machine Learning Hackethon\n",
      "name 'detect' is not defined Language is not detected: Topic Modeling for Research Articles\n",
      "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n",
      "\n",
      "Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n",
      "\n",
      "Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n",
      "\n",
      "1. Computer Science\n",
      "\n",
      "2. Physics\n",
      "\n",
      "3. Mathematics\n",
      "\n",
      "4. Statistics\n",
      "\n",
      "5. Quantitative Biology\n",
      "\n",
      "6. Quantitative Finance\n",
      "name 'detect' is not defined Language is not detected: gsimonx37/vgchartz\n",
      "name 'detect' is not defined Language is not detected: vgchartz\n",
      "name 'detect' is not defined Language is not detected: gsimonx37\n",
      "name 'detect' is not defined Language is not detected: VGChartz\n",
      "name 'detect' is not defined Language is not detected: Video game sales in America, Europe, Japan and the rest of the world\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fb5be9743b224eed4a579ad0566c6cfa6%2Fheader.jpg?generation=1706017258113980&alt=media)\n",
      "\n",
      "Data obtained using a program from the site [vgchartz.com](https://www.vgchartz.com).\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fe7672b2b6da2ed0212f6023bc969097c%2Fdata_1.jpg?generation=1706017300688615&alt=media)\n",
      "\n",
      "\"Founded in 2005 by Brett Walton, VGChartz (Video Game Charts) is a business intelligence and research firm and publisher of the VGChartz.com websites. As an industry research firm, VGChartz publishes video game hardware estimates every week and hosts an ever-expanding game database with over 55,000 titles listed, featuring up-to-date shipment information and legacy sales data. The VGChartz.com website provides consumers with a range of content from news and sales features, to reviews and articles, to social networking and a community forum.\" - from the site [vgchartz.com](https://www.vgchartz.com). \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fa099c58fc8cb25b8e26989f05fe58488%2Fdata_2.jpg?generation=1706017370390411&alt=media)\n",
      "\n",
      "\"Since the end of 2018 VGChartz no longer produces estimates for software sales. This is because the high digital market share for software was making it both more difficult to produce reliable retail estimates and also making those estimates increasingly unrepresentative of the wider performance of the games in question. As a result, on the software front we now only record official shipment/sales data, where such data is made available by developers and publishers. The legacy data remains on the site for those who are interested in browsing through it.\" - from the site [vgchartz.com](https://www.vgchartz.com). \n",
      "\n",
      "# What can you do with the data set?\n",
      "\n",
      "If you are new to data analytics, try answering the following questions:\n",
      "- in what year did the active growth in the number of video games produced begin? What year was the most successful from this point of view? What can you conclude if you look at the number of video games released by country?\n",
      "- on what day and month were the largest number of video games released? What could be the reason for this pattern?\n",
      "- is there a dependence of the number of copies sold on the ratings of critics or users?\n",
      "- which gaming platforms, publishers and developers are the most common (the largest number of video games have been released over time)?\n",
      "- which gaming platforms, publishers and developers have the largest number of video game copies sold (over all time, the total number of copies sold was the largest)?\n",
      "\n",
      "If you have enough experience, try solving a regression problem. Train a model that can predict the number of copies sold of video games:\n",
      "- what signs can be used to prevent leakage of the target variable?\n",
      "- how do outliers affect the quality of the model?\n",
      "- which metric should be chosen to evaluate the model?\n",
      "- can adding new data improve the predictive ability of the model?\n",
      "- does the trained model have signs of heteroscedasticity of the residuals? How does this affect the predictive ability of the model? What can you do?\n",
      "\n",
      "# Field descriptions:\n",
      "\n",
      "The data contains the following fields:\n",
      "1. **name** – name of the video game.\n",
      "2. **date** - release date of the video game.\n",
      "3. **platform** - gaming platform (**All** – all gaming platforms, **Series** – all video game series).\n",
      "4. **publisher** – publisher.\n",
      "5. **developers** - developer.\n",
      "6. **shipped** - the number of copies sent (relevant for records with the values **All** and **Series** in the **platform** field).\n",
      "7. **total** - total number of copies sold (millions of copies).\n",
      "8. **america** - number of copies sold in America (millions of copies).\n",
      "9. **europe** - number of copies sold in Europe (millions of copies).\n",
      "10. **japan** - number of copies sold in Japan (millions of copies).\n",
      "11. **other** - other sales in the world.\n",
      "12. **vgc** - rating VGChartz.com.\n",
      "13. **critic** - critics' assessment.\n",
      "14. **user** - user rating.\n",
      "\n",
      "# Found an error or inaccuracy in the data?\n",
      "\n",
      "This dataset is the result of painstaking work. After collection and systematization, the data is checked for integrity and correctness. If you notice an error or inaccuracy in the data, or have a suggestion on how to improve the data set, please let me know.\n",
      "\n",
      "You can look at working with data in my [github repository](https://github.com/GSimonX37/VGChartz).\n",
      "name 'detect' is not defined Language is not detected: vgchartz\n",
      "name 'detect' is not defined Language is not detected: gsimonx37\n",
      "name 'detect' is not defined Language is not detected: VGChartz\n",
      "name 'detect' is not defined Language is not detected: Video game sales in America, Europe, Japan and the rest of the world\n",
      "name 'detect' is not defined Language is not detected: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fb5be9743b224eed4a579ad0566c6cfa6%2Fheader.jpg?generation=1706017258113980&alt=media)\n",
      "\n",
      "Data obtained using a program from the site [vgchartz.com](https://www.vgchartz.com).\n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fe7672b2b6da2ed0212f6023bc969097c%2Fdata_1.jpg?generation=1706017300688615&alt=media)\n",
      "\n",
      "\"Founded in 2005 by Brett Walton, VGChartz (Video Game Charts) is a business intelligence and research firm and publisher of the VGChartz.com websites. As an industry research firm, VGChartz publishes video game hardware estimates every week and hosts an ever-expanding game database with over 55,000 titles listed, featuring up-to-date shipment information and legacy sales data. The VGChartz.com website provides consumers with a range of content from news and sales features, to reviews and articles, to social networking and a community forum.\" - from the site [vgchartz.com](https://www.vgchartz.com). \n",
      "\n",
      "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15126770%2Fa099c58fc8cb25b8e26989f05fe58488%2Fdata_2.jpg?generation=1706017370390411&alt=media)\n",
      "\n",
      "\"Since the end of 2018 VGChartz no longer produces estimates for software sales. This is because the high digital market share for software was making it both more difficult to produce reliable retail estimates and also making those estimates increasingly unrepresentative of the wider performance of the games in question. As a result, on the software front we now only record official shipment/sales data, where such data is made available by developers and publishers. The legacy data remains on the site for those who are interested in browsing through it.\" - from the site [vgchartz.com](https://www.vgchartz.com). \n",
      "\n",
      "# What can you do with the data set?\n",
      "\n",
      "If you are new to data analytics, try answering the following questions:\n",
      "- in what year did the active growth in the number of video games produced begin? What year was the most successful from this point of view? What can you conclude if you look at the number of video games released by country?\n",
      "- on what day and month were the largest number of video games released? What could be the reason for this pattern?\n",
      "- is there a dependence of the number of copies sold on the ratings of critics or users?\n",
      "- which gaming platforms, publishers and developers are the most common (the largest number of video games have been released over time)?\n",
      "- which gaming platforms, publishers and developers have the largest number of video game copies sold (over all time, the total number of copies sold was the largest)?\n",
      "\n",
      "If you have enough experience, try solving a regression problem. Train a model that can predict the number of copies sold of video games:\n",
      "- what signs can be used to prevent leakage of the target variable?\n",
      "- how do outliers affect the quality of the model?\n",
      "- which metric should be chosen to evaluate the model?\n",
      "- can adding new data improve the predictive ability of the model?\n",
      "- does the trained model have signs of heteroscedasticity of the residuals? How does this affect the predictive ability of the model? What can you do?\n",
      "\n",
      "# Field descriptions:\n",
      "\n",
      "The data contains the following fields:\n",
      "1. **name** – name of the video game.\n",
      "2. **date** - release date of the video game.\n",
      "3. **platform** - gaming platform (**All** – all gaming platforms, **Series** – all video game series).\n",
      "4. **publisher** – publisher.\n",
      "5. **developers** - developer.\n",
      "6. **shipped** - the number of copies sent (relevant for records with the values **All** and **Series** in the **platform** field).\n",
      "7. **total** - total number of copies sold (millions of copies).\n",
      "8. **america** - number of copies sold in America (millions of copies).\n",
      "9. **europe** - number of copies sold in Europe (millions of copies).\n",
      "10. **japan** - number of copies sold in Japan (millions of copies).\n",
      "11. **other** - other sales in the world.\n",
      "12. **vgc** - rating VGChartz.com.\n",
      "13. **critic** - critics' assessment.\n",
      "14. **user** - user rating.\n",
      "\n",
      "# Found an error or inaccuracy in the data?\n",
      "\n",
      "This dataset is the result of painstaking work. After collection and systematization, the data is checked for integrity and correctness. If you notice an error or inaccuracy in the data, or have a suggestion on how to improve the data set, please let me know.\n",
      "\n",
      "You can look at working with data in my [github repository](https://github.com/GSimonX37/VGChartz).\n",
      "name 'detect' is not defined Language is not detected: mostafanofal/book-crossing-descriptions-from-google-api\n",
      "name 'detect' is not defined Language is not detected: book-crossing-descriptions-from-google-api\n",
      "name 'detect' is not defined Language is not detected: mostafanofal\n",
      "name 'detect' is not defined Language is not detected: Book Crossing Descriptions from Google API\n",
      "name 'detect' is not defined Language is not detected: # Dataset Source\n",
      "[Link](https://www.kaggle.com/datasets/ruchi798/bookcrossing-dataset)\n",
      "\n",
      "# Description\n",
      "Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit/implicit) about 271,379 books.\n",
      "\n",
      "# Google API\n",
      "- There is no information regarding book descriptions.\n",
      "- We use Google API to collect descriptions of books according to their ISTPN.\n",
      "- Create and save a new CSV file called Book_Descriptions which we used in Content-based Recommendation System with Cosine Similarity.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: book-crossing-descriptions-from-google-api\n",
      "name 'detect' is not defined Language is not detected: mostafanofal\n",
      "name 'detect' is not defined Language is not detected: Book Crossing Descriptions from Google API\n",
      "name 'detect' is not defined Language is not detected: # Dataset Source\n",
      "[Link](https://www.kaggle.com/datasets/ruchi798/bookcrossing-dataset)\n",
      "\n",
      "# Description\n",
      "Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit/implicit) about 271,379 books.\n",
      "\n",
      "# Google API\n",
      "- There is no information regarding book descriptions.\n",
      "- We use Google API to collect descriptions of books according to their ISTPN.\n",
      "- Create and save a new CSV file called Book_Descriptions which we used in Content-based Recommendation System with Cosine Similarity.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: mayank1897/condition-monitoring-of-hydraulic-systems\n",
      "name 'detect' is not defined Language is not detected: condition-monitoring-of-hydraulic-systems\n",
      "name 'detect' is not defined Language is not detected: mayank1897\n",
      "name 'detect' is not defined Language is not detected: Predictive Maintenance Of Hydraulics System\n",
      "name 'detect' is not defined Language is not detected: Condition monitoring of hydraulic systems\n",
      "name 'detect' is not defined Language is not detected: \n",
      "# Condition monitoring of hydraulic systems\n",
      "\n",
      "##Abstract: The data set addresses the condition assessment of a hydraulic test rig based on multi sensor data. Four fault types are superimposed with several severity grades impeding selective quantification.\n",
      "\n",
      "##Source:\n",
      "Creator: ZeMA gGmbH, Eschberger Weg 46, 66121 Saarbrücken\n",
      "Contact: t.schneider@zema.de, s.klein@zema.de, m.bastuck@lmt.uni-saarland.de, info@lmt.uni-saarland.de\n",
      "\n",
      "Data Type: Multivariate, Time-Series\n",
      "Task: Classification, Regression\n",
      "Attribute Type: Categorical, Real\n",
      "Area: CS/Engineering\n",
      "Format Type: Matrix\n",
      "Does your data set contain missing values? No\n",
      "\n",
      "Number of Instances: 2205\n",
      "\n",
      "Number of Attributes: 43680 (8x60 (1 Hz) + 2x600 (10 Hz) + 7x6000 (100 Hz))\n",
      "\n",
      "##Relevant Information:\n",
      "The data set was experimentally obtained with a hydraulic test rig. This test rig consists of a primary working and a secondary cooling-filtration circuit which are connected via the oil tank [1], [2]. The system cyclically repeats constant load cycles (duration 60 seconds) and measures process values such as pressures, volume flows and temperatures while the condition of four hydraulic components (cooler, valve, pump and accumulator) is quantitatively varied. \n",
      "\n",
      "##Attribute Information:\n",
      "The data set contains raw process sensor data (i.e. without feature extraction) which are structured as matrices (tab-delimited) with the rows representing the cycles and the columns the data points within a cycle.\n",
      "\n",
      "The target condition values are cycle-wise annotated in ‘profile.txt‘ (tab-delimited). As before, the row number represents the cycle number. The columns are\n",
      "\n",
      "###1: Cooler condition / %:\n",
      "\t3: close to total failure\n",
      "\t20: reduced efficiency\n",
      "\t100: full efficiency\n",
      "\n",
      "###2: Valve condition / %:\n",
      "\t100: optimal switching behavior\n",
      "\t90: small lag\n",
      "\t80: severe lag\n",
      "\t73: close to total failure\n",
      "\n",
      "###3: Internal pump leakage:\n",
      "\t0: no leakage\n",
      "\t1: weak leakage\n",
      "\t2: severe leakage\n",
      "\n",
      "###4: Hydraulic accumulator / bar:\n",
      "\t130: optimal pressure\n",
      "\t115: slightly reduced pressure\n",
      "\t100: severely reduced pressure\n",
      "\t90: close to total failure\n",
      "\n",
      "###5: stable flag:\n",
      "\t0: conditions were stable\n",
      "\t1: static conditions might not have been reached yet\n",
      "\n",
      "##Relevant Papers:\n",
      "[1] Nikolai Helwig, Eliseo Pignanelli, Andreas Schütze, ‘Condition Monitoring of a Complex Hydraulic System Using Multivariate Statistics’, in Proc. I2MTC-2015 - 2015 IEEE International Instrumentation and Measurement Technology Conference, paper PPS1-39, Pisa, Italy, May 11-14, 2015, doi: 10.1109/I2MTC.2015.7151267.\n",
      "[2] N. Helwig, A. Schütze, ‘Detecting and compensating sensor faults in a hydraulic condition monitoring system’, in Proc. SENSOR 2015 - 17th International Conference on Sensors and Measurement Technology, oral presentation D8.1, Nuremberg, Germany, May 19-21, 2015, doi: 10.5162/sensor2015/D8.1.\n",
      "[3] Tizian Schneider, Nikolai Helwig, Andreas Schütze, ‘Automatic feature extraction and selection for classification of cyclical time series data’, tm - Technisches Messen (2017), 84(3), 198–206, doi: 10.1515/teme-2016-0072.\n",
      "\n",
      "##Citation Requests:\n",
      "Nikolai Helwig, Eliseo Pignanelli, Andreas Schütze, ‘Condition Monitoring of a Complex Hydraulic System Using Multivariate Statistics’, in Proc. I2MTC-2015 - 2015 IEEE International Instrumentation and Measurement Technology Conference, paper PPS1-39, Pisa, Italy, May 11-14, 2015, doi: 10.1109/I2MTC.2015.7151267.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: condition-monitoring-of-hydraulic-systems\n",
      "name 'detect' is not defined Language is not detected: mayank1897\n",
      "name 'detect' is not defined Language is not detected: Predictive Maintenance Of Hydraulics System\n",
      "name 'detect' is not defined Language is not detected: Condition monitoring of hydraulic systems\n",
      "name 'detect' is not defined Language is not detected: \n",
      "# Condition monitoring of hydraulic systems\n",
      "\n",
      "##Abstract: The data set addresses the condition assessment of a hydraulic test rig based on multi sensor data. Four fault types are superimposed with several severity grades impeding selective quantification.\n",
      "\n",
      "##Source:\n",
      "Creator: ZeMA gGmbH, Eschberger Weg 46, 66121 Saarbrücken\n",
      "Contact: t.schneider@zema.de, s.klein@zema.de, m.bastuck@lmt.uni-saarland.de, info@lmt.uni-saarland.de\n",
      "\n",
      "Data Type: Multivariate, Time-Series\n",
      "Task: Classification, Regression\n",
      "Attribute Type: Categorical, Real\n",
      "Area: CS/Engineering\n",
      "Format Type: Matrix\n",
      "Does your data set contain missing values? No\n",
      "\n",
      "Number of Instances: 2205\n",
      "\n",
      "Number of Attributes: 43680 (8x60 (1 Hz) + 2x600 (10 Hz) + 7x6000 (100 Hz))\n",
      "\n",
      "##Relevant Information:\n",
      "The data set was experimentally obtained with a hydraulic test rig. This test rig consists of a primary working and a secondary cooling-filtration circuit which are connected via the oil tank [1], [2]. The system cyclically repeats constant load cycles (duration 60 seconds) and measures process values such as pressures, volume flows and temperatures while the condition of four hydraulic components (cooler, valve, pump and accumulator) is quantitatively varied. \n",
      "\n",
      "##Attribute Information:\n",
      "The data set contains raw process sensor data (i.e. without feature extraction) which are structured as matrices (tab-delimited) with the rows representing the cycles and the columns the data points within a cycle.\n",
      "\n",
      "The target condition values are cycle-wise annotated in ‘profile.txt‘ (tab-delimited). As before, the row number represents the cycle number. The columns are\n",
      "\n",
      "###1: Cooler condition / %:\n",
      "\t3: close to total failure\n",
      "\t20: reduced efficiency\n",
      "\t100: full efficiency\n",
      "\n",
      "###2: Valve condition / %:\n",
      "\t100: optimal switching behavior\n",
      "\t90: small lag\n",
      "\t80: severe lag\n",
      "\t73: close to total failure\n",
      "\n",
      "###3: Internal pump leakage:\n",
      "\t0: no leakage\n",
      "\t1: weak leakage\n",
      "\t2: severe leakage\n",
      "\n",
      "###4: Hydraulic accumulator / bar:\n",
      "\t130: optimal pressure\n",
      "\t115: slightly reduced pressure\n",
      "\t100: severely reduced pressure\n",
      "\t90: close to total failure\n",
      "\n",
      "###5: stable flag:\n",
      "\t0: conditions were stable\n",
      "\t1: static conditions might not have been reached yet\n",
      "\n",
      "##Relevant Papers:\n",
      "[1] Nikolai Helwig, Eliseo Pignanelli, Andreas Schütze, ‘Condition Monitoring of a Complex Hydraulic System Using Multivariate Statistics’, in Proc. I2MTC-2015 - 2015 IEEE International Instrumentation and Measurement Technology Conference, paper PPS1-39, Pisa, Italy, May 11-14, 2015, doi: 10.1109/I2MTC.2015.7151267.\n",
      "[2] N. Helwig, A. Schütze, ‘Detecting and compensating sensor faults in a hydraulic condition monitoring system’, in Proc. SENSOR 2015 - 17th International Conference on Sensors and Measurement Technology, oral presentation D8.1, Nuremberg, Germany, May 19-21, 2015, doi: 10.5162/sensor2015/D8.1.\n",
      "[3] Tizian Schneider, Nikolai Helwig, Andreas Schütze, ‘Automatic feature extraction and selection for classification of cyclical time series data’, tm - Technisches Messen (2017), 84(3), 198–206, doi: 10.1515/teme-2016-0072.\n",
      "\n",
      "##Citation Requests:\n",
      "Nikolai Helwig, Eliseo Pignanelli, Andreas Schütze, ‘Condition Monitoring of a Complex Hydraulic System Using Multivariate Statistics’, in Proc. I2MTC-2015 - 2015 IEEE International Instrumentation and Measurement Technology Conference, paper PPS1-39, Pisa, Italy, May 11-14, 2015, doi: 10.1109/I2MTC.2015.7151267.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: jainkanishk95/steam-reviews\n",
      "name 'detect' is not defined Language is not detected: steam-reviews\n",
      "name 'detect' is not defined Language is not detected: jainkanishk95\n",
      "name 'detect' is not defined Language is not detected: Steam Reviews\n",
      "name 'detect' is not defined Language is not detected: Steam User Reviews for different games, taken from AV JantaHack NLP Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is taken from AV JantaHack NLP Hackathon competition. It consists of user reviews of multiple Steam Games, along with a user recommendation.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The goal of this dataset is to tell a story about how far AI has come in handling NLP tasks. I've used this dataset in a series of posts to showcase the increemental improvement each new generation of algorithms and techniques have brought in towards Sentiment Classification task.\n",
      "name 'detect' is not defined Language is not detected: steam-reviews\n",
      "name 'detect' is not defined Language is not detected: jainkanishk95\n",
      "name 'detect' is not defined Language is not detected: Steam Reviews\n",
      "name 'detect' is not defined Language is not detected: Steam User Reviews for different games, taken from AV JantaHack NLP Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is taken from AV JantaHack NLP Hackathon competition. It consists of user reviews of multiple Steam Games, along with a user recommendation.\n",
      "\n",
      "\n",
      "### Inspiration\n",
      "\n",
      "The goal of this dataset is to tell a story about how far AI has come in handling NLP tasks. I've used this dataset in a series of posts to showcase the increemental improvement each new generation of algorithms and techniques have brought in towards Sentiment Classification task.\n",
      "name 'detect' is not defined Language is not detected: vineethakkinapalli/citibike-bike-sharingnewyork-cityjan-to-apr-2021\n",
      "name 'detect' is not defined Language is not detected: citibike-bike-sharingnewyork-cityjan-to-apr-2021\n",
      "name 'detect' is not defined Language is not detected: vineethakkinapalli\n",
      "name 'detect' is not defined Language is not detected: Citibike BikeSharing-NewYork-2020,2021(Jan to Apr)\n",
      "name 'detect' is not defined Language is not detected: Bike Sharing data in New York City from Citibike\n",
      "name 'detect' is not defined Language is not detected: **Citi Bike** is a privately owned public bicycle sharing system serving the New York City boroughs of the Bronx, Brooklyn, Manhattan, and Queens, as well as Jersey City, New Jersey. Named after lead sponsor Citigroup, it is operated by Motivate (formerly Alta Bicycle Share), with former Metropolitan Transportation Authority CEO Jay Walder as chief executive until September 30, 2018 when the company was acquired by **Lyft**. The system's bikes and stations use **BIXI-branded** **technology** from PBSC Urban Solutions. \n",
      "\n",
      "As of July 2019, there are 169,000 annual subscribers. Citi Bike riders took an average of 56,497 rides per day in 2019, and the system reached a total of 50 million rides in October 2017. source [wikipedia](https://en.wikipedia.org/wiki/Citi_Bike)\n",
      "\n",
      "This dataset contains ride details for the year 2021 from January to April. It contains 15 columns each:\n",
      "\n",
      "&gt; tripduration - Duration in Seconds\n",
      "&gt; starttime - Start Time and Date\n",
      "&gt; stoptime - Stop Time and Date\n",
      "&gt; start station id - ID of Start Station\n",
      "&gt; start station name - Name of Start Station\n",
      "&gt; start station latitude - Latitude of start station\n",
      "&gt; start station longitude - Longitude of start station\n",
      "&gt; end station id - ID of End Station\n",
      "&gt; end station name - Name of End Station\n",
      "&gt; end station latitude - Latitude of End station\n",
      "&gt; end station longitude - Longitude of End station\n",
      "&gt; Bike ID - Bike ID\n",
      "&gt; usertype -  Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member\n",
      "&gt; gender - Zero=unknown; 1=male; 2=female\n",
      "&gt; birthyear - Year of Birth\n",
      "name 'detect' is not defined Language is not detected: citibike-bike-sharingnewyork-cityjan-to-apr-2021\n",
      "name 'detect' is not defined Language is not detected: vineethakkinapalli\n",
      "name 'detect' is not defined Language is not detected: Citibike BikeSharing-NewYork-2020,2021(Jan to Apr)\n",
      "name 'detect' is not defined Language is not detected: Bike Sharing data in New York City from Citibike\n",
      "name 'detect' is not defined Language is not detected: **Citi Bike** is a privately owned public bicycle sharing system serving the New York City boroughs of the Bronx, Brooklyn, Manhattan, and Queens, as well as Jersey City, New Jersey. Named after lead sponsor Citigroup, it is operated by Motivate (formerly Alta Bicycle Share), with former Metropolitan Transportation Authority CEO Jay Walder as chief executive until September 30, 2018 when the company was acquired by **Lyft**. The system's bikes and stations use **BIXI-branded** **technology** from PBSC Urban Solutions. \n",
      "\n",
      "As of July 2019, there are 169,000 annual subscribers. Citi Bike riders took an average of 56,497 rides per day in 2019, and the system reached a total of 50 million rides in October 2017. source [wikipedia](https://en.wikipedia.org/wiki/Citi_Bike)\n",
      "\n",
      "This dataset contains ride details for the year 2021 from January to April. It contains 15 columns each:\n",
      "\n",
      "&gt; tripduration - Duration in Seconds\n",
      "&gt; starttime - Start Time and Date\n",
      "&gt; stoptime - Stop Time and Date\n",
      "&gt; start station id - ID of Start Station\n",
      "&gt; start station name - Name of Start Station\n",
      "&gt; start station latitude - Latitude of start station\n",
      "&gt; start station longitude - Longitude of start station\n",
      "&gt; end station id - ID of End Station\n",
      "&gt; end station name - Name of End Station\n",
      "&gt; end station latitude - Latitude of End station\n",
      "&gt; end station longitude - Longitude of End station\n",
      "&gt; Bike ID - Bike ID\n",
      "&gt; usertype -  Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member\n",
      "&gt; gender - Zero=unknown; 1=male; 2=female\n",
      "&gt; birthyear - Year of Birth\n",
      "name 'detect' is not defined Language is not detected: oscarm524/influence-of-coupons-on-order-patterns\n",
      "name 'detect' is not defined Language is not detected: influence-of-coupons-on-order-patterns\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Influence of Coupons on Order Patterns\n",
      "name 'detect' is not defined Language is not detected: Predict coupon usage and basket value\n",
      "name 'detect' is not defined Language is not detected: This dataset deals with the topic of coupon generation. What is interesting in this case is the impact of coupons on the shopping basket value as well as the redemption rate of the individual coupons. Coupons have been used as purchase incentives for some time now, not only by mail order companies. This presents interesting challenges, including the question: “Who responds to coupons?”, and then the question: “Who would have made the purchase even without the coupon?”. Only by giving equal consideration to both of these questions can we arrive at profitable couponing. Data mining processes, in particular scoring processes, can be used to answer these questions. Such processes make it possible to create forecasting models which provide answers to both questions.\n",
      "\n",
      "Using the historical order data from an online shop with accompanying coupon generation, a model should be created that comes up with a prediction for the redeemed coupons and for the shopping basket value for new orders within the shop. The historical data contains both a time stamp of the coupon generation and the orders as well as different products and customer attributes. The information ``coupon redeemed yes/no'' and the basket value of the order are also known in the historical data. \n",
      "\n",
      "# The Data\n",
      "\n",
      "Real anonymous shop data in the form of structured text files consisting of individual datasets are provided. Below are some points to note about the files:\n",
      "\n",
      "1. Each dataset is in a row of its own, ending with \"CR'\" (\"carriage return\", 0xD) or \"CR'' and \"LF'' (\"carriage return'' or \"line feed'', 0xD and 0xA).\n",
      "2. The first row has the same structure as the datasets, but contains the names of the respective columns (data fields).\n",
      "3. The top row and each dataset contains several fields separated from each other by the pipe symbol (\"|'').\n",
      "4. There is no escape character, quotes are not used.\n",
      "5. ASCII is the character set used.\n",
      "6. Missing values may occur.\n",
      "\n",
      "A brief description of the fields can be found in \"*features.pdf*\". The training data file is \"orders\\_train.txt\", and the test data file is \"orders\\_class.txt\". Notice that the test data file doesn't contain the target attributes \"coupon1Used\", \"coupon2Used\", \"coupon3Used\" and \"basketValue\".\n",
      "\n",
      "# The Task\n",
      "\n",
      "Historical data used in generating coupons over a period of several weeks is known for the task. One coupon applies to a single product. In addition, the orders are still given in response to the generation including the information as to which coupons have been redeemed and the total basket value of each order. Using this data, a model for predicting coupon redemption and the total basket value should be learned. The target attributes  \"coupon1Used\", \"coupon2Used\", \"coupon3Used\" are described with the value \"0\" for coupons not redeemed and \"1\" for coupons redeemed. The remaining target attribute \"basketValue\" contains the total basket value of the order in the form of a real number. An analysis as to whether the coupons will be redeemed as well as the total basket value should be made for a portion of the coupon generation. For more detail see the DMC\\_2015\\_task.pdf file.\n",
      "\n",
      "# Acknowledgements\n",
      "\n",
      "This dataset is publicly available in the [data-mining-cup-website](https://www.data-mining-cup.com/reviews/dmc-2015/). \n",
      "name 'detect' is not defined Language is not detected: influence-of-coupons-on-order-patterns\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Influence of Coupons on Order Patterns\n",
      "name 'detect' is not defined Language is not detected: Predict coupon usage and basket value\n",
      "name 'detect' is not defined Language is not detected: This dataset deals with the topic of coupon generation. What is interesting in this case is the impact of coupons on the shopping basket value as well as the redemption rate of the individual coupons. Coupons have been used as purchase incentives for some time now, not only by mail order companies. This presents interesting challenges, including the question: “Who responds to coupons?”, and then the question: “Who would have made the purchase even without the coupon?”. Only by giving equal consideration to both of these questions can we arrive at profitable couponing. Data mining processes, in particular scoring processes, can be used to answer these questions. Such processes make it possible to create forecasting models which provide answers to both questions.\n",
      "\n",
      "Using the historical order data from an online shop with accompanying coupon generation, a model should be created that comes up with a prediction for the redeemed coupons and for the shopping basket value for new orders within the shop. The historical data contains both a time stamp of the coupon generation and the orders as well as different products and customer attributes. The information ``coupon redeemed yes/no'' and the basket value of the order are also known in the historical data. \n",
      "\n",
      "# The Data\n",
      "\n",
      "Real anonymous shop data in the form of structured text files consisting of individual datasets are provided. Below are some points to note about the files:\n",
      "\n",
      "1. Each dataset is in a row of its own, ending with \"CR'\" (\"carriage return\", 0xD) or \"CR'' and \"LF'' (\"carriage return'' or \"line feed'', 0xD and 0xA).\n",
      "2. The first row has the same structure as the datasets, but contains the names of the respective columns (data fields).\n",
      "3. The top row and each dataset contains several fields separated from each other by the pipe symbol (\"|'').\n",
      "4. There is no escape character, quotes are not used.\n",
      "5. ASCII is the character set used.\n",
      "6. Missing values may occur.\n",
      "\n",
      "A brief description of the fields can be found in \"*features.pdf*\". The training data file is \"orders\\_train.txt\", and the test data file is \"orders\\_class.txt\". Notice that the test data file doesn't contain the target attributes \"coupon1Used\", \"coupon2Used\", \"coupon3Used\" and \"basketValue\".\n",
      "\n",
      "# The Task\n",
      "\n",
      "Historical data used in generating coupons over a period of several weeks is known for the task. One coupon applies to a single product. In addition, the orders are still given in response to the generation including the information as to which coupons have been redeemed and the total basket value of each order. Using this data, a model for predicting coupon redemption and the total basket value should be learned. The target attributes  \"coupon1Used\", \"coupon2Used\", \"coupon3Used\" are described with the value \"0\" for coupons not redeemed and \"1\" for coupons redeemed. The remaining target attribute \"basketValue\" contains the total basket value of the order in the form of a real number. An analysis as to whether the coupons will be redeemed as well as the total basket value should be made for a portion of the coupon generation. For more detail see the DMC\\_2015\\_task.pdf file.\n",
      "\n",
      "# Acknowledgements\n",
      "\n",
      "This dataset is publicly available in the [data-mining-cup-website](https://www.data-mining-cup.com/reviews/dmc-2015/). \n",
      "name 'detect' is not defined Language is not detected: oscarm524/book-recommendation\n",
      "name 'detect' is not defined Language is not detected: book-recommendation\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Book Recommendation\n",
      "name 'detect' is not defined Language is not detected: Build a recommendation system for an online bookstore in Germany\n",
      "name 'detect' is not defined Language is not detected: #Backgorund\n",
      "Before the pandemic, Johannes Gutenberg managed a flourishing little book shop in the historic city center of Mainz. He took great joy in building personal relationships with each of his clients, recommending books catered to their personal taste and assisting in widening their literary palette. In his city and beyond he developed a formidable reputation with a respectable base of loyal customers, who considered him more of a connoisseur than a traditional salesman.\n",
      "\n",
      "Unfortunately, this loyal base of customers is not enough to make his business profitable. And so, like many traditional retailers, Johannes also relies on walk-in customers.\n",
      "\n",
      "At the beginning of the pandemic, this source of revenue vanished. To keep his employees and cover ongoing costs, Johannes had to find an alternative form of revenue.\n",
      "\n",
      "With great initial reservation, he decided to expand his business by launching an online shop, which he believed would save his beloved business from imminent bankruptcy.\n",
      "\n",
      "At first, Johannes and his employees tried their best to provide suitable recommendations for every product manually. But as the number of products increased and associates worked to keep at least some personal contact to clients via phone and email, this manual process was just not feasible.\n",
      "\n",
      "Today, Johannes is looking for a reliable recommendation system to provide a targeted recommendation to every product page. This solution should meet his high personalization standards and only require a small amount of manual support to implement.\n",
      "\n",
      "In order to create a recommender model, the participants are provided with historical transaction and descriptive item data in the form of structured text files (.csv)\n",
      "\n",
      "#Data\n",
      "The data is provided in three individual files. One file containing the transactions (“transactions.csv”) one the descriptive item data (“items.csv”) and the final one (“evaluation.csv”) containing the template for the result submission. Here are some points to note about the files:\n",
      "\n",
      "1. The first line (top line) has the same structure as the data sets but contains the names of the respective columns (data fields).\n",
      "\n",
      "2. A list of all the column names, which occur in the appropriate order, can be found in the “features.pdf” file as well as brief descriptions and value ranges of the associated fields.\n",
      "\n",
      "3. The top row and each data set contain several fields, which are separated from each other by the “|” symbol.\n",
      "\n",
      "4. Floating point numbers are not rounded. “.” is used as the decimal separator.\n",
      "\n",
      "5. There is no escape character: quotes are not used.\n",
      "\n",
      "6. The encoding is “utf-8”.\n",
      "\n",
      "The “items.csv” file is a master data set that contains descriptive features. The features may be categorical or numerical. The list of features is explained in the “features.pdf” file. Each data line contains the description for one single item.\n",
      "\n",
      "The “transactions.csv” file contains information about clicks, baskets and orders over a period of three months. Each line displays one transaction for one single item. All the attributes are described in the “features.pdf” file.\n",
      "\n",
      "The “evalutation.csv” file contains a list of product IDs. This list is a subset of the products from the “items.csv” and the reference for the submission.\n",
      "\n",
      "#Task\n",
      "The goal for each participating team is to create a recommendation model based on historical transactions and item features. For any given product, the model should return its five best recommendations. In order to create a recommender model, the participants are provided with historical transaction and descriptive item data in the form of structured text files (.csv).\n",
      "\n",
      "#Acknowledgements\n",
      "This dataset is publicly available in [data-mining-cup-website](https://www.data-mining-cup.com/reviews/dmc-2021/). \n",
      "name 'detect' is not defined Language is not detected: book-recommendation\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Book Recommendation\n",
      "name 'detect' is not defined Language is not detected: Build a recommendation system for an online bookstore in Germany\n",
      "name 'detect' is not defined Language is not detected: #Backgorund\n",
      "Before the pandemic, Johannes Gutenberg managed a flourishing little book shop in the historic city center of Mainz. He took great joy in building personal relationships with each of his clients, recommending books catered to their personal taste and assisting in widening their literary palette. In his city and beyond he developed a formidable reputation with a respectable base of loyal customers, who considered him more of a connoisseur than a traditional salesman.\n",
      "\n",
      "Unfortunately, this loyal base of customers is not enough to make his business profitable. And so, like many traditional retailers, Johannes also relies on walk-in customers.\n",
      "\n",
      "At the beginning of the pandemic, this source of revenue vanished. To keep his employees and cover ongoing costs, Johannes had to find an alternative form of revenue.\n",
      "\n",
      "With great initial reservation, he decided to expand his business by launching an online shop, which he believed would save his beloved business from imminent bankruptcy.\n",
      "\n",
      "At first, Johannes and his employees tried their best to provide suitable recommendations for every product manually. But as the number of products increased and associates worked to keep at least some personal contact to clients via phone and email, this manual process was just not feasible.\n",
      "\n",
      "Today, Johannes is looking for a reliable recommendation system to provide a targeted recommendation to every product page. This solution should meet his high personalization standards and only require a small amount of manual support to implement.\n",
      "\n",
      "In order to create a recommender model, the participants are provided with historical transaction and descriptive item data in the form of structured text files (.csv)\n",
      "\n",
      "#Data\n",
      "The data is provided in three individual files. One file containing the transactions (“transactions.csv”) one the descriptive item data (“items.csv”) and the final one (“evaluation.csv”) containing the template for the result submission. Here are some points to note about the files:\n",
      "\n",
      "1. The first line (top line) has the same structure as the data sets but contains the names of the respective columns (data fields).\n",
      "\n",
      "2. A list of all the column names, which occur in the appropriate order, can be found in the “features.pdf” file as well as brief descriptions and value ranges of the associated fields.\n",
      "\n",
      "3. The top row and each data set contain several fields, which are separated from each other by the “|” symbol.\n",
      "\n",
      "4. Floating point numbers are not rounded. “.” is used as the decimal separator.\n",
      "\n",
      "5. There is no escape character: quotes are not used.\n",
      "\n",
      "6. The encoding is “utf-8”.\n",
      "\n",
      "The “items.csv” file is a master data set that contains descriptive features. The features may be categorical or numerical. The list of features is explained in the “features.pdf” file. Each data line contains the description for one single item.\n",
      "\n",
      "The “transactions.csv” file contains information about clicks, baskets and orders over a period of three months. Each line displays one transaction for one single item. All the attributes are described in the “features.pdf” file.\n",
      "\n",
      "The “evalutation.csv” file contains a list of product IDs. This list is a subset of the products from the “items.csv” and the reference for the submission.\n",
      "\n",
      "#Task\n",
      "The goal for each participating team is to create a recommendation model based on historical transactions and item features. For any given product, the model should return its five best recommendations. In order to create a recommender model, the participants are provided with historical transaction and descriptive item data in the form of structured text files (.csv).\n",
      "\n",
      "#Acknowledgements\n",
      "This dataset is publicly available in [data-mining-cup-website](https://www.data-mining-cup.com/reviews/dmc-2021/). \n",
      "name 'detect' is not defined Language is not detected: oscarm524/forecast-cyclically-demanded-products\n",
      "name 'detect' is not defined Language is not detected: forecast-cyclically-demanded-products\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Forecast Cyclically Demanded Products\n",
      "name 'detect' is not defined Language is not detected: Develop a forecasting model for cyclically demanded products\n",
      "name 'detect' is not defined Language is not detected: This data is about Pia and Philip business, a married couple. They started their new e-commerce business during the pandemic in 2020 by offering convenience goods online. They began by selling an assortment of masks and disinfectants, but quickly expanded to a wider range of various everyday commodities.\n",
      "\n",
      "Having both a background in traditional and online retail, they are aware of how distant and impersonal online shopping can feel and, at the same time, how important customer guidance and recommendations are for long-term customer loyalty.\n",
      "\n",
      "To differentiate themselves from the many other commodity shops, they decided to put an even more significant emphasis on personalized recommendations and offers.\n",
      "\n",
      "One key element of this strategy is a customized weekly newsletter that personally addresses each of their clients. The newsletter includes user favorites, products similar customers liked, new additions, and special offers.\n",
      "However, they quickly noticed a problem: repeated recommendations of recently purchased products. One quick workaround for this issue was implementing a filter that would exclude products from the recommendation for a fixed number of days. This, however, did not meet the high standards of Pia and Philip.\n",
      "\n",
      "They are instead looking for a model that can reliably predict the week that a returning customer might repurchase one of their frequently purchased items.\n",
      "\n",
      "By knowing the estimated week of replenishment, products can be added to the newsletter as a reminder, thus increasing basket sizes and profits.\n",
      "\n",
      "Since the owners are only interested in the best possible solution, they organized a contest to benchmark competing prediction approaches.\n",
      "\n",
      "# The Data\n",
      "\n",
      "To create a prediction model, the participants will receive historical transaction and descriptive item data in the form of structured text files (.csv).\n",
      "\n",
      "The data is provided in four individual files. One file containing the orders (\"orders.csv\"), one containing descriptive item data (\"items.csv\"), one containing item and category hierarchy information (category_hierarchy.csv), and a template for the result submission (\"submission.csv\"). Here are some points to note about the files:\n",
      "\n",
      "1. The first line (top line) has the same structure as the data sets but contains the names of the respective columns (data fields).\n",
      "2. A list of all the column names, which occur in the appropriate order, can be found in the \"features.pdf\" file along with brief descriptions and value ranges for the associated fields.\n",
      "3. The top row and each data set contain several fields separated from each other by the \"|\" (pipe) symbol.\n",
      "4. There is no escape character: quotes are not used.\n",
      "5. The encoding is \"utf-8\".\n",
      "\n",
      "The \"items.csv\" file is a master data set that contains descriptive features. All features are categorical but numerically encoded. The list of features is explained in the \"features.pdf\" file. Each data line contains the description for one single item.\n",
      "\n",
      "The \"orders.csv\" file contains information about user-specific orders across eight months. Each line displays one transaction for one single item. All attributes are described in the \"features.pdf\" file.\n",
      "\n",
      "The \"category\\_hierarchy.csv\" file contains two columns of encoded categories, that maps each category to its parent category.\n",
      "\n",
      "The \"submission.csv\" file is the reference for submission and contains a predefined subset of userID and itemID combinations as well as an empty \"prediction\" column for the participants to fill in.\n",
      "\n",
      "# The Task\n",
      "\n",
      "The goal is to predict the user-based replenishment of a product based on historical orders and item features. Individual items and user specific orders are given for the period between 01.06.2020 and 31.01.2021. The prediction period is between 01.02.2021 and 28.02.2021, which is exactly four weeks long.\n",
      "\n",
      "For a predefined subset of user and product combinations, the participants shall predict if and when a product will be purchased during the prediction period.\n",
      "\n",
      "The prediction column in the \"submission.csv\" file must be filled accordingly.\n",
      "\n",
      "- 0: no replenishment during that period\n",
      "- 1: replenishment in the first week\n",
      "- 2: replenishment in the second week\n",
      "- 3: replenishment in the third week\n",
      "- 4: replenishment in the fourth week\n",
      "name 'detect' is not defined Language is not detected: forecast-cyclically-demanded-products\n",
      "name 'detect' is not defined Language is not detected: oscarm524\n",
      "name 'detect' is not defined Language is not detected: Forecast Cyclically Demanded Products\n",
      "name 'detect' is not defined Language is not detected: Develop a forecasting model for cyclically demanded products\n",
      "name 'detect' is not defined Language is not detected: This data is about Pia and Philip business, a married couple. They started their new e-commerce business during the pandemic in 2020 by offering convenience goods online. They began by selling an assortment of masks and disinfectants, but quickly expanded to a wider range of various everyday commodities.\n",
      "\n",
      "Having both a background in traditional and online retail, they are aware of how distant and impersonal online shopping can feel and, at the same time, how important customer guidance and recommendations are for long-term customer loyalty.\n",
      "\n",
      "To differentiate themselves from the many other commodity shops, they decided to put an even more significant emphasis on personalized recommendations and offers.\n",
      "\n",
      "One key element of this strategy is a customized weekly newsletter that personally addresses each of their clients. The newsletter includes user favorites, products similar customers liked, new additions, and special offers.\n",
      "However, they quickly noticed a problem: repeated recommendations of recently purchased products. One quick workaround for this issue was implementing a filter that would exclude products from the recommendation for a fixed number of days. This, however, did not meet the high standards of Pia and Philip.\n",
      "\n",
      "They are instead looking for a model that can reliably predict the week that a returning customer might repurchase one of their frequently purchased items.\n",
      "\n",
      "By knowing the estimated week of replenishment, products can be added to the newsletter as a reminder, thus increasing basket sizes and profits.\n",
      "\n",
      "Since the owners are only interested in the best possible solution, they organized a contest to benchmark competing prediction approaches.\n",
      "\n",
      "# The Data\n",
      "\n",
      "To create a prediction model, the participants will receive historical transaction and descriptive item data in the form of structured text files (.csv).\n",
      "\n",
      "The data is provided in four individual files. One file containing the orders (\"orders.csv\"), one containing descriptive item data (\"items.csv\"), one containing item and category hierarchy information (category_hierarchy.csv), and a template for the result submission (\"submission.csv\"). Here are some points to note about the files:\n",
      "\n",
      "1. The first line (top line) has the same structure as the data sets but contains the names of the respective columns (data fields).\n",
      "2. A list of all the column names, which occur in the appropriate order, can be found in the \"features.pdf\" file along with brief descriptions and value ranges for the associated fields.\n",
      "3. The top row and each data set contain several fields separated from each other by the \"|\" (pipe) symbol.\n",
      "4. There is no escape character: quotes are not used.\n",
      "5. The encoding is \"utf-8\".\n",
      "\n",
      "The \"items.csv\" file is a master data set that contains descriptive features. All features are categorical but numerically encoded. The list of features is explained in the \"features.pdf\" file. Each data line contains the description for one single item.\n",
      "\n",
      "The \"orders.csv\" file contains information about user-specific orders across eight months. Each line displays one transaction for one single item. All attributes are described in the \"features.pdf\" file.\n",
      "\n",
      "The \"category\\_hierarchy.csv\" file contains two columns of encoded categories, that maps each category to its parent category.\n",
      "\n",
      "The \"submission.csv\" file is the reference for submission and contains a predefined subset of userID and itemID combinations as well as an empty \"prediction\" column for the participants to fill in.\n",
      "\n",
      "# The Task\n",
      "\n",
      "The goal is to predict the user-based replenishment of a product based on historical orders and item features. Individual items and user specific orders are given for the period between 01.06.2020 and 31.01.2021. The prediction period is between 01.02.2021 and 28.02.2021, which is exactly four weeks long.\n",
      "\n",
      "For a predefined subset of user and product combinations, the participants shall predict if and when a product will be purchased during the prediction period.\n",
      "\n",
      "The prediction column in the \"submission.csv\" file must be filled accordingly.\n",
      "\n",
      "- 0: no replenishment during that period\n",
      "- 1: replenishment in the first week\n",
      "- 2: replenishment in the second week\n",
      "- 3: replenishment in the third week\n",
      "- 4: replenishment in the fourth week\n",
      "name 'detect' is not defined Language is not detected: jummyegg/rawg-game-dataset\n",
      "name 'detect' is not defined Language is not detected: rawg-game-dataset\n",
      "name 'detect' is not defined Language is not detected: jummyegg\n",
      "name 'detect' is not defined Language is not detected: Video Game Dataset\n",
      "name 'detect' is not defined Language is not detected: 474417 Game with Metacritic Score, Ratings, Genres, Publishers, Platforms, ...\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains 474417 video games on over 50 platforms including mobiles. All game information was obtained using Python with [RAWG API](https://rawg.io/apidocs). This dataset was last updated on **Dec 22nd 2020**. If you are interested in obtaining more recent games, visit the [GitHub](https://github.com/trung-hn/game-encyclopedia) page for more information. I plan to update this dataset annually.\n",
      "\n",
      "### Content\n",
      "Each row contains information about one game. There are several columns that have multiple values like platforms, genres, ... In those cases, values are separated by double pipes `||`.\n",
      "\n",
      "### Column definitions:\n",
      "- `id`: An unique ID identifying this Game in RAWG Database\n",
      "- `slug`: An unique slug identifying this Game in RAWG Database\n",
      "- `name`: Name of the game\n",
      "- `metacritic`: Rating of the game on [Metacritic](https://www.metacritic.com/game)\n",
      "- `released`: The date the game was released\n",
      "- `tba`: To be announced state\n",
      "- `updated`: The date the game was last updated\n",
      "- `website`: Game Website\n",
      "- `rating`: Rating rated by RAWG user\n",
      "- `rating_top`: Maximum rating\n",
      "- `playtime`: Hours needed to complete the game\n",
      "- `achievements_count`: Number of achievements in game\n",
      "- `ratings_count`: Number of RAWG users who rated the game\n",
      "- `suggestions_count`: Number of RAWG users who suggested the game\n",
      "- `game_series_count`: Number of games in the series\n",
      "- `reviews_count`: Number of RAWG users who reviewed the game\n",
      "- `platforms`: Platforms game was released on. **Separated** by `||`\n",
      "- `developers`: Game developers. **Separated** by `||`\n",
      "- `genres`: Game genres. **Separated** by `||`\n",
      "- `publishers`: Game publishers. **Separated** by `||`\n",
      "- `esrb_rating`: ESRB ratings\n",
      "- `added_status_yet`: Number of RAWG users had the game as \"Not played\"\n",
      "- `added_status_owned`: Number of RAWG users had the game as \"Owned\"\n",
      "- `added_status_beaten`: Number of RAWG users had the game as \"Completed\"\n",
      "- `added_status_toplay`: Number of RAWG users had the game as \"To play\"\n",
      "- `added_status_dropped`: Number of RAWG users had the game as \"Played but not beaten\"\n",
      "- `added_status_playing`: Number of RAWG users had the game as \"Playing\"\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to [RAWG](https://rawg.io/) for providing easy to use and fast [API](https://rawg.io/apidocs) \\\n",
      "Icon made by <a href=\"https://www.flaticon.com/authors/good-ware\">Good Ware</a> from <a href=\"https://www.flaticon.com/\">www.flaticon.com</a>\n",
      "\n",
      "### Inspiration\n",
      "With this data, one can create a game recommendation platform as well as drawing insights about the gaming industry and gaming trends.\n",
      "name 'detect' is not defined Language is not detected: rawg-game-dataset\n",
      "name 'detect' is not defined Language is not detected: jummyegg\n",
      "name 'detect' is not defined Language is not detected: Video Game Dataset\n",
      "name 'detect' is not defined Language is not detected: 474417 Game with Metacritic Score, Ratings, Genres, Publishers, Platforms, ...\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "This dataset contains 474417 video games on over 50 platforms including mobiles. All game information was obtained using Python with [RAWG API](https://rawg.io/apidocs). This dataset was last updated on **Dec 22nd 2020**. If you are interested in obtaining more recent games, visit the [GitHub](https://github.com/trung-hn/game-encyclopedia) page for more information. I plan to update this dataset annually.\n",
      "\n",
      "### Content\n",
      "Each row contains information about one game. There are several columns that have multiple values like platforms, genres, ... In those cases, values are separated by double pipes `||`.\n",
      "\n",
      "### Column definitions:\n",
      "- `id`: An unique ID identifying this Game in RAWG Database\n",
      "- `slug`: An unique slug identifying this Game in RAWG Database\n",
      "- `name`: Name of the game\n",
      "- `metacritic`: Rating of the game on [Metacritic](https://www.metacritic.com/game)\n",
      "- `released`: The date the game was released\n",
      "- `tba`: To be announced state\n",
      "- `updated`: The date the game was last updated\n",
      "- `website`: Game Website\n",
      "- `rating`: Rating rated by RAWG user\n",
      "- `rating_top`: Maximum rating\n",
      "- `playtime`: Hours needed to complete the game\n",
      "- `achievements_count`: Number of achievements in game\n",
      "- `ratings_count`: Number of RAWG users who rated the game\n",
      "- `suggestions_count`: Number of RAWG users who suggested the game\n",
      "- `game_series_count`: Number of games in the series\n",
      "- `reviews_count`: Number of RAWG users who reviewed the game\n",
      "- `platforms`: Platforms game was released on. **Separated** by `||`\n",
      "- `developers`: Game developers. **Separated** by `||`\n",
      "- `genres`: Game genres. **Separated** by `||`\n",
      "- `publishers`: Game publishers. **Separated** by `||`\n",
      "- `esrb_rating`: ESRB ratings\n",
      "- `added_status_yet`: Number of RAWG users had the game as \"Not played\"\n",
      "- `added_status_owned`: Number of RAWG users had the game as \"Owned\"\n",
      "- `added_status_beaten`: Number of RAWG users had the game as \"Completed\"\n",
      "- `added_status_toplay`: Number of RAWG users had the game as \"To play\"\n",
      "- `added_status_dropped`: Number of RAWG users had the game as \"Played but not beaten\"\n",
      "- `added_status_playing`: Number of RAWG users had the game as \"Playing\"\n",
      "\n",
      "### Acknowledgements\n",
      "Thanks to [RAWG](https://rawg.io/) for providing easy to use and fast [API](https://rawg.io/apidocs) \\\n",
      "Icon made by <a href=\"https://www.flaticon.com/authors/good-ware\">Good Ware</a> from <a href=\"https://www.flaticon.com/\">www.flaticon.com</a>\n",
      "\n",
      "### Inspiration\n",
      "With this data, one can create a game recommendation platform as well as drawing insights about the gaming industry and gaming trends.\n",
      "name 'detect' is not defined Language is not detected: jordan75/top-5000-mydramalist\n",
      "name 'detect' is not defined Language is not detected: top-5000-mydramalist\n",
      "name 'detect' is not defined Language is not detected: jordan75\n",
      "name 'detect' is not defined Language is not detected: Top 5000 MyDramaList\n",
      "name 'detect' is not defined Language is not detected: Data about top 5000 drama listed on MyDramaList [2021-11-11]\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "The data has been retrieved by crawling each page from https://mydramalist.com/shows/top\n",
      "As we retrieve every show listed on this page regardless of any condition, our data also includes dramas produced from other countries such as **Japan** or **China** for example.\n",
      "\n",
      "### Datapoints\n",
      "\n",
      "| Key                 \t| Type       \t| Description                                                \t|\n",
      "|---------------------\t|------------\t|------------------------------------------------------------\t|\n",
      "| name                \t| String     \t| Name of the drama                                          \t|\n",
      "| synopsis            \t| String     \t| Synopsis of the drama                                      \t|\n",
      "| duration_in_minutes \t| Integer    \t| Duration of each episodes (in minutes)                     \t|\n",
      "| nb_episodes         \t| Integer    \t| Number of episodes                                         \t|\n",
      "| country_origin      \t| String     \t| Drama's country of origin                                  \t|\n",
      "| ratings             \t| Float      \t| Ratings from MyDramaList's users                           \t|\n",
      "| ranking             \t| Integer    \t| Ranking based on the ratings                               \t|\n",
      "| popularity_rank     \t| Integer    \t| Ranking based on the drama's popularity                    \t|\n",
      "| nb_watchers         \t| Integer    \t| Number of MyDramaList's users currently watching the drama \t|\n",
      "| nb_ratings          \t| Integer    \t| Number of users that have rated the drama                  \t|\n",
      "| nb_reviews          \t| Integer    \t| Number of reviews written by MyDramaList's users           \t|\n",
      "| streamed_on         \t| List       \t| List of platforms on which the drama is broadcaster        \t|\n",
      "| genres              \t| List       \t| List of genres associated to the drama                     \t|\n",
      "| tags                \t| List       \t| List of tags associated to the drama                       \t|\n",
      "| mydramalist_url     \t| String     \t| URL of the drama on MyDramaList                            \t|\n",
      "| director             \t| List \t        | List of the drama's director(s)      \t                        |\n",
      "| screenwriter          | List \t        | List of the drama's screenwriter(s)                           |\n",
      "| main_roles            | List \t        | List of the actors having a main role in the drama          \t|\n",
      "| support_roles         | List \t        | List of the actors having a support role in the drama         |\n",
      "| guest_roles           | List \t        | List of the actors having a guest role in the drama        \t|\n",
      "\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Updating the data\n",
      "\n",
      "I'm not necessarily planning on updating this dataset (even though I may end up doing sometimes), however, the source code used to retrieve all this data can be found on my [GitHub repo](https://github.com/Jordan9675/MyDramaScraper)\n",
      "\n",
      "Additionally, on this repository, you'll find one spider (crawler) to retrieve the score a given user has attributed to a given show which can allow to perform some recommendation system.\n",
      "\n",
      "The whole project is not really maintained anymore but I may work on it again if enough people are considering the project as relevant and useful.\n",
      "name 'detect' is not defined Language is not detected: top-5000-mydramalist\n",
      "name 'detect' is not defined Language is not detected: jordan75\n",
      "name 'detect' is not defined Language is not detected: Top 5000 MyDramaList\n",
      "name 'detect' is not defined Language is not detected: Data about top 5000 drama listed on MyDramaList [2021-11-11]\n",
      "name 'detect' is not defined Language is not detected: ### Content\n",
      "\n",
      "The data has been retrieved by crawling each page from https://mydramalist.com/shows/top\n",
      "As we retrieve every show listed on this page regardless of any condition, our data also includes dramas produced from other countries such as **Japan** or **China** for example.\n",
      "\n",
      "### Datapoints\n",
      "\n",
      "| Key                 \t| Type       \t| Description                                                \t|\n",
      "|---------------------\t|------------\t|------------------------------------------------------------\t|\n",
      "| name                \t| String     \t| Name of the drama                                          \t|\n",
      "| synopsis            \t| String     \t| Synopsis of the drama                                      \t|\n",
      "| duration_in_minutes \t| Integer    \t| Duration of each episodes (in minutes)                     \t|\n",
      "| nb_episodes         \t| Integer    \t| Number of episodes                                         \t|\n",
      "| country_origin      \t| String     \t| Drama's country of origin                                  \t|\n",
      "| ratings             \t| Float      \t| Ratings from MyDramaList's users                           \t|\n",
      "| ranking             \t| Integer    \t| Ranking based on the ratings                               \t|\n",
      "| popularity_rank     \t| Integer    \t| Ranking based on the drama's popularity                    \t|\n",
      "| nb_watchers         \t| Integer    \t| Number of MyDramaList's users currently watching the drama \t|\n",
      "| nb_ratings          \t| Integer    \t| Number of users that have rated the drama                  \t|\n",
      "| nb_reviews          \t| Integer    \t| Number of reviews written by MyDramaList's users           \t|\n",
      "| streamed_on         \t| List       \t| List of platforms on which the drama is broadcaster        \t|\n",
      "| genres              \t| List       \t| List of genres associated to the drama                     \t|\n",
      "| tags                \t| List       \t| List of tags associated to the drama                       \t|\n",
      "| mydramalist_url     \t| String     \t| URL of the drama on MyDramaList                            \t|\n",
      "| director             \t| List \t        | List of the drama's director(s)      \t                        |\n",
      "| screenwriter          | List \t        | List of the drama's screenwriter(s)                           |\n",
      "| main_roles            | List \t        | List of the actors having a main role in the drama          \t|\n",
      "| support_roles         | List \t        | List of the actors having a support role in the drama         |\n",
      "| guest_roles           | List \t        | List of the actors having a guest role in the drama        \t|\n",
      "\n",
      "\n",
      "What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "\n",
      "\n",
      "### Updating the data\n",
      "\n",
      "I'm not necessarily planning on updating this dataset (even though I may end up doing sometimes), however, the source code used to retrieve all this data can be found on my [GitHub repo](https://github.com/Jordan9675/MyDramaScraper)\n",
      "\n",
      "Additionally, on this repository, you'll find one spider (crawler) to retrieve the score a given user has attributed to a given show which can allow to perform some recommendation system.\n",
      "\n",
      "The whole project is not really maintained anymore but I may work on it again if enough people are considering the project as relevant and useful.\n",
      "name 'detect' is not defined Language is not detected: vikasukani/news-data-set-fake-news-with-python\n",
      "name 'detect' is not defined Language is not detected: news-data-set-fake-news-with-python\n",
      "name 'detect' is not defined Language is not detected: vikasukani\n",
      "name 'detect' is not defined Language is not detected: News Data Set - Fake OR Real \n",
      "name 'detect' is not defined Language is not detected: This is the News Dataset File for Predict Whether News is Real or Fake.\n",
      "name 'detect' is not defined Language is not detected: news-data-set-fake-news-with-python\n",
      "name 'detect' is not defined Language is not detected: vikasukani\n",
      "name 'detect' is not defined Language is not detected: News Data Set - Fake OR Real \n",
      "name 'detect' is not defined Language is not detected: This is the News Dataset File for Predict Whether News is Real or Fake.\n",
      "name 'detect' is not defined Language is not detected: roopeshbharatwajkr/ecommerce-dataset\n",
      "name 'detect' is not defined Language is not detected: ecommerce-dataset\n",
      "name 'detect' is not defined Language is not detected: roopeshbharatwajkr\n",
      "name 'detect' is not defined Language is not detected: E-Commerce DataSet\n",
      "name 'detect' is not defined Language is not detected: ### About Data \n",
      "\n",
      "This Data set is about the Sales Data of the Leading United Stated Online E-commerce organization, This is the real- time Data with Modification of Critical Geographical Content and Critical Id  \n",
      "\n",
      "\n",
      "### Data Set Description\n",
      "\n",
      "1. Transaction Id:\n",
      "2. Customer_ Id:     \n",
      "3. Date:                        \n",
      "4. Product:             \n",
      "5. Gender:\n",
      "6. Device_Type:\n",
      "7. Country:\n",
      "8. State:\n",
      "9. City:\n",
      "10. Category:\n",
      "11. Customer_Login_Type:\n",
      "12. Delivery_Type\t Quantity:\n",
      "13. Transaction Start:\n",
      "14. Transaction_Result:\n",
      "15. Amount US$:\n",
      "16. Individual_Price_US$:\n",
      "17. Year_Month:\n",
      "18. Time:\n",
      "\n",
      "### Task\n",
      "\n",
      " With this data set, Time Series Analysis Prediction of Sales, and Forecasting can be done, and other Machine Learning Algorithms can be used (Regression, Classification)\n",
      "name 'detect' is not defined Language is not detected: ecommerce-dataset\n",
      "name 'detect' is not defined Language is not detected: roopeshbharatwajkr\n",
      "name 'detect' is not defined Language is not detected: E-Commerce DataSet\n",
      "name 'detect' is not defined Language is not detected: ### About Data \n",
      "\n",
      "This Data set is about the Sales Data of the Leading United Stated Online E-commerce organization, This is the real- time Data with Modification of Critical Geographical Content and Critical Id  \n",
      "\n",
      "\n",
      "### Data Set Description\n",
      "\n",
      "1. Transaction Id:\n",
      "2. Customer_ Id:     \n",
      "3. Date:                        \n",
      "4. Product:             \n",
      "5. Gender:\n",
      "6. Device_Type:\n",
      "7. Country:\n",
      "8. State:\n",
      "9. City:\n",
      "10. Category:\n",
      "11. Customer_Login_Type:\n",
      "12. Delivery_Type\t Quantity:\n",
      "13. Transaction Start:\n",
      "14. Transaction_Result:\n",
      "15. Amount US$:\n",
      "16. Individual_Price_US$:\n",
      "17. Year_Month:\n",
      "18. Time:\n",
      "\n",
      "### Task\n",
      "\n",
      " With this data set, Time Series Analysis Prediction of Sales, and Forecasting can be done, and other Machine Learning Algorithms can be used (Regression, Classification)\n",
      "name 'detect' is not defined Language is not detected: rrp170330/variability-in-the-poverty-rate-in-the-us-counties\n",
      "name 'detect' is not defined Language is not detected: variability-in-the-poverty-rate-in-the-us-counties\n",
      "name 'detect' is not defined Language is not detected: rrp170330\n",
      "name 'detect' is not defined Language is not detected: variability in the poverty rate in the US counties\n",
      "name 'detect' is not defined Language is not detected: variability in the poverty rate in the US counties\n",
      "name 'detect' is not defined Language is not detected: Goal and Objective :\n",
      "Primary objective is to study variability in the poverty rate in the US counties by means of one or more of independent or control variable and provide best suitable model to quantify relationships in determining target value\n",
      "Our goal is to design various models to take into consideration the effect of various factors like employment, population and education to predict the poverty rate in all US Counties\n",
      "We further wish to analyze the status of a county based on whether it is metropolitan or not\n",
      "\n",
      "List of datasets:\n",
      "\n",
      "Socioeconomic indicators like poverty rates, population change, unemployment rates, and education levels vary geographically across U.S. States and counties\n",
      "1. Unemployment\n",
      "2. PovertyEstimates\n",
      "3.Population Estimates\n",
      "4. Education\n",
      "\n",
      "All the four individual datasets have common unique id FIPS Code defined as State-County FIPS Code. It is unique for each county falling under the states. In our dataset, we are covering all 52 USA states including federal district DC and Puerto Rico.\n",
      "\n",
      "Data Modelling :\n",
      "\n",
      "Target Variable: Metro_2015 – This binary variable shows status of County as Metro or Non-Metro\n",
      "A decision tree model designed using Metro_2015 as target variable will efficiently determine the classification of the population into Metro and Non-metro counties. \n",
      "Dataset will be partitioned into training and validation datasets before implementing decision tree rules. \n",
      "The attributes that will be considered in selecting best model will be fit statistics, misclassification rate, and average square error.\n",
      "\n",
      "Clustering can be performed to create the collection of objects similar to each other which will give insight into data distribution. \n",
      " Variables will be standardized before performing clustering to avoid noisy data and outliers. Euclidean distance will be the measure to determine stability and separation.\n",
      "\n",
      "Recommendation :\n",
      "\n",
      "The regression equation determines % Poverty rate in a particular county based on significant factors. This model can be \n",
      "This model can be used by education boards to increase or decrease the funds spent on the education system in different counties in order to lower the poverty rate.\n",
      "Census board can use this model in identifying poverty line index based on a population estimate an average household income.\n",
      "By estimating the poverty rate and considering factors like unemployment and education, an analysis can be done to set up employment opportunities in targeted counties.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: variability-in-the-poverty-rate-in-the-us-counties\n",
      "name 'detect' is not defined Language is not detected: rrp170330\n",
      "name 'detect' is not defined Language is not detected: variability in the poverty rate in the US counties\n",
      "name 'detect' is not defined Language is not detected: variability in the poverty rate in the US counties\n",
      "name 'detect' is not defined Language is not detected: Goal and Objective :\n",
      "Primary objective is to study variability in the poverty rate in the US counties by means of one or more of independent or control variable and provide best suitable model to quantify relationships in determining target value\n",
      "Our goal is to design various models to take into consideration the effect of various factors like employment, population and education to predict the poverty rate in all US Counties\n",
      "We further wish to analyze the status of a county based on whether it is metropolitan or not\n",
      "\n",
      "List of datasets:\n",
      "\n",
      "Socioeconomic indicators like poverty rates, population change, unemployment rates, and education levels vary geographically across U.S. States and counties\n",
      "1. Unemployment\n",
      "2. PovertyEstimates\n",
      "3.Population Estimates\n",
      "4. Education\n",
      "\n",
      "All the four individual datasets have common unique id FIPS Code defined as State-County FIPS Code. It is unique for each county falling under the states. In our dataset, we are covering all 52 USA states including federal district DC and Puerto Rico.\n",
      "\n",
      "Data Modelling :\n",
      "\n",
      "Target Variable: Metro_2015 – This binary variable shows status of County as Metro or Non-Metro\n",
      "A decision tree model designed using Metro_2015 as target variable will efficiently determine the classification of the population into Metro and Non-metro counties. \n",
      "Dataset will be partitioned into training and validation datasets before implementing decision tree rules. \n",
      "The attributes that will be considered in selecting best model will be fit statistics, misclassification rate, and average square error.\n",
      "\n",
      "Clustering can be performed to create the collection of objects similar to each other which will give insight into data distribution. \n",
      " Variables will be standardized before performing clustering to avoid noisy data and outliers. Euclidean distance will be the measure to determine stability and separation.\n",
      "\n",
      "Recommendation :\n",
      "\n",
      "The regression equation determines % Poverty rate in a particular county based on significant factors. This model can be \n",
      "This model can be used by education boards to increase or decrease the funds spent on the education system in different counties in order to lower the poverty rate.\n",
      "Census board can use this model in identifying poverty line index based on a population estimate an average household income.\n",
      "By estimating the poverty rate and considering factors like unemployment and education, an analysis can be done to set up employment opportunities in targeted counties.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: syedmubarak/netflix-dataset-latest-2021\n",
      "name 'detect' is not defined Language is not detected: netflix-dataset-latest-2021\n",
      "name 'detect' is not defined Language is not detected: syedmubarak\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset Latest 2021\n",
      "name 'detect' is not defined Language is not detected: User Based Movie Recommendation System based on Collaborative Filter\n",
      "name 'detect' is not defined Language is not detected: ![](data:image/png;base64,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)\n",
      "\n",
      "\n",
      "1. Developed user-based movie recommendation system by implementing user-user collaborative filtering.\n",
      "2. Used Netflix movie dataset containing 100,000 user records for developing recommendation engine.\n",
      "3. Reduced run time and space complexity significantly.\n",
      "4. Implementation in both C++ and Python separately.\n",
      "name 'detect' is not defined Language is not detected: netflix-dataset-latest-2021\n",
      "name 'detect' is not defined Language is not detected: syedmubarak\n",
      "name 'detect' is not defined Language is not detected: Netflix Dataset Latest 2021\n",
      "name 'detect' is not defined Language is not detected: User Based Movie Recommendation System based on Collaborative Filter\n",
      "name 'detect' is not defined Language is not detected: ![](data:image/png;base64,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)\n",
      "\n",
      "\n",
      "1. Developed user-based movie recommendation system by implementing user-user collaborative filtering.\n",
      "2. Used Netflix movie dataset containing 100,000 user records for developing recommendation engine.\n",
      "3. Reduced run time and space complexity significantly.\n",
      "4. Implementation in both C++ and Python separately.\n",
      "name 'detect' is not defined Language is not detected: rajattomar132/payment-date-dataset\n",
      "name 'detect' is not defined Language is not detected: payment-date-dataset\n",
      "name 'detect' is not defined Language is not detected: rajattomar132\n",
      "name 'detect' is not defined Language is not detected: Payment Date Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset about Payment Date perfect for payment date prediction\n",
      "name 'detect' is not defined Language is not detected: The invoices dataset contains the past payment information and behaviour of various buyers. Based on the previous payment patterns, the ML model will predict what will be the date a payment is made by the customer for an invoice.\n",
      "The model will also predict which aging bucket the invoice falls into based on the predicted payment date.\n",
      "name 'detect' is not defined Language is not detected: payment-date-dataset\n",
      "name 'detect' is not defined Language is not detected: rajattomar132\n",
      "name 'detect' is not defined Language is not detected: Payment Date Dataset\n",
      "name 'detect' is not defined Language is not detected: Dataset about Payment Date perfect for payment date prediction\n",
      "name 'detect' is not defined Language is not detected: The invoices dataset contains the past payment information and behaviour of various buyers. Based on the previous payment patterns, the ML model will predict what will be the date a payment is made by the customer for an invoice.\n",
      "The model will also predict which aging bucket the invoice falls into based on the predicted payment date.\n",
      "name 'detect' is not defined Language is not detected: prashant111/us-presidential-election-data\n",
      "name 'detect' is not defined Language is not detected: us-presidential-election-data\n",
      "name 'detect' is not defined Language is not detected: prashant111\n",
      "name 'detect' is not defined Language is not detected: US Presidential Election Data\n",
      "name 'detect' is not defined Language is not detected: # DESCRIPTION\n",
      "Election results for both the 2012 and 2016 US Presidential Elections.\n",
      "\n",
      "## SUMMARY\n",
      "These data files contain election results for both the 2012 and 2016 US Presidential Elections, include proportions of votes cast for Romney, Obama (2012) and Trump, Clinton (2016).\n",
      "name 'detect' is not defined Language is not detected: us-presidential-election-data\n",
      "name 'detect' is not defined Language is not detected: prashant111\n",
      "name 'detect' is not defined Language is not detected: US Presidential Election Data\n",
      "name 'detect' is not defined Language is not detected: # DESCRIPTION\n",
      "Election results for both the 2012 and 2016 US Presidential Elections.\n",
      "\n",
      "## SUMMARY\n",
      "These data files contain election results for both the 2012 and 2016 US Presidential Elections, include proportions of votes cast for Romney, Obama (2012) and Trump, Clinton (2016).\n",
      "name 'detect' is not defined Language is not detected: abdelrhamanfakhry/movies-data-for-ml-dl-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: movies-data-for-ml-dl-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: abdelrhamanfakhry\n",
      "name 'detect' is not defined Language is not detected: Movies Data  For ML & DL Recommendation System \n",
      "name 'detect' is not defined Language is not detected: Recommendation Data using Cosine Similarity Model or Any ML & DL  Model\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "Movie Recommendation ,Consists of 5000 Movies Released from 1987 to 2015\n",
      "\n",
      "Content\n",
      "The Data are Contained in Two Files tmdb_5000_credits.csv and  tmdb_5000_movies.csv , More Details about the contents and use of all these files follows.\n",
      "name 'detect' is not defined Language is not detected: movies-data-for-ml-dl-recommendation-system\n",
      "name 'detect' is not defined Language is not detected: abdelrhamanfakhry\n",
      "name 'detect' is not defined Language is not detected: Movies Data  For ML & DL Recommendation System \n",
      "name 'detect' is not defined Language is not detected: Recommendation Data using Cosine Similarity Model or Any ML & DL  Model\n",
      "name 'detect' is not defined Language is not detected: Context\n",
      "Movie Recommendation ,Consists of 5000 Movies Released from 1987 to 2015\n",
      "\n",
      "Content\n",
      "The Data are Contained in Two Files tmdb_5000_credits.csv and  tmdb_5000_movies.csv , More Details about the contents and use of all these files follows.\n",
      "name 'detect' is not defined Language is not detected: aryansakhala/coffee-recommendation\n",
      "name 'detect' is not defined Language is not detected: coffee-recommendation\n",
      "name 'detect' is not defined Language is not detected: aryansakhala\n",
      "name 'detect' is not defined Language is not detected: Coffee Recommendation\n",
      "name 'detect' is not defined Language is not detected: This is web scrapped data\n",
      "name 'detect' is not defined Language is not detected: coffee-recommendation\n",
      "name 'detect' is not defined Language is not detected: aryansakhala\n",
      "name 'detect' is not defined Language is not detected: Coffee Recommendation\n",
      "name 'detect' is not defined Language is not detected: This is web scrapped data\n",
      "name 'detect' is not defined Language is not detected: kanishkbansalajm/recommendationsystemspotify\n",
      "name 'detect' is not defined Language is not detected: recommendationsystemspotify\n",
      "name 'detect' is not defined Language is not detected: kanishkbansalajm\n",
      "name 'detect' is not defined Language is not detected:  recommendation-system-spotify\n",
      "name 'detect' is not defined Language is not detected: This dataset is for the Spotify recommendation system, https://github.com/Kanishk-Bansal/recommendation-system-spotify\n",
      "name 'detect' is not defined Language is not detected: recommendationsystemspotify\n",
      "name 'detect' is not defined Language is not detected: kanishkbansalajm\n",
      "name 'detect' is not defined Language is not detected:  recommendation-system-spotify\n",
      "name 'detect' is not defined Language is not detected: This dataset is for the Spotify recommendation system, https://github.com/Kanishk-Bansal/recommendation-system-spotify\n",
      "name 'detect' is not defined Language is not detected: cbhavik/music-taste-recommendation\n",
      "name 'detect' is not defined Language is not detected: music-taste-recommendation\n",
      "name 'detect' is not defined Language is not detected: cbhavik\n",
      "name 'detect' is not defined Language is not detected: Spotify Popularity 2019-22\n",
      "name 'detect' is not defined Language is not detected: 1M+ ratings from 7K anonymized users for 200k songs!!!\n",
      "name 'detect' is not defined Language is not detected: We present the Spotify Music dataset with the goal of enabling researchers and practitioners from research to mitigate the noisy feedback and self-selection biases inherent in the data collected by existing music platforms. These biases are likely to have significant impact on the fairness, transparency and quality of recommendation systems. Much has already been written about recommendation algorithms and evaluation metrics and we hope this dataset helps the community to focus on the impact of the data collection mechanisms.\n",
      "\n",
      "Noisy feedback biases arise in implicit data sets collected by streaming apps. Such apps collect user actions without recording the context of the user and without the knowledge that they are being surveyed. Consequently, a song stream from a recommended playlist may be falsely interpreted as an indication that the song was enjoyed, when in fact it was played in the background. A skipped song may be falsely interpreted as an indication that the song was disliked, when in fact the user may not be in the mood for the song in their present context. Self-selection biases arise in explicit data sets collected by apps that ask users to give ratings. Since rating is optional, the users most incentivized to rate are users who are very happy or very unhappy about their experience with the rated item.\n",
      "\n",
      "The Spotify Music dataset currently consists of 6696 anonymized users, 181,663 anonymized songs and 1,017,947 binary ratings and the data collection is still on-going. \n",
      "name 'detect' is not defined Language is not detected: music-taste-recommendation\n",
      "name 'detect' is not defined Language is not detected: cbhavik\n",
      "name 'detect' is not defined Language is not detected: Spotify Popularity 2019-22\n",
      "name 'detect' is not defined Language is not detected: 1M+ ratings from 7K anonymized users for 200k songs!!!\n",
      "name 'detect' is not defined Language is not detected: We present the Spotify Music dataset with the goal of enabling researchers and practitioners from research to mitigate the noisy feedback and self-selection biases inherent in the data collected by existing music platforms. These biases are likely to have significant impact on the fairness, transparency and quality of recommendation systems. Much has already been written about recommendation algorithms and evaluation metrics and we hope this dataset helps the community to focus on the impact of the data collection mechanisms.\n",
      "\n",
      "Noisy feedback biases arise in implicit data sets collected by streaming apps. Such apps collect user actions without recording the context of the user and without the knowledge that they are being surveyed. Consequently, a song stream from a recommended playlist may be falsely interpreted as an indication that the song was enjoyed, when in fact it was played in the background. A skipped song may be falsely interpreted as an indication that the song was disliked, when in fact the user may not be in the mood for the song in their present context. Self-selection biases arise in explicit data sets collected by apps that ask users to give ratings. Since rating is optional, the users most incentivized to rate are users who are very happy or very unhappy about their experience with the rated item.\n",
      "\n",
      "The Spotify Music dataset currently consists of 6696 anonymized users, 181,663 anonymized songs and 1,017,947 binary ratings and the data collection is still on-going. \n",
      "name 'detect' is not defined Language is not detected: shreyjain1107/hackrx-20-bajaj-fin-serv\n",
      "name 'detect' is not defined Language is not detected: hackrx-20-bajaj-fin-serv\n",
      "name 'detect' is not defined Language is not detected: shreyjain1107\n",
      "name 'detect' is not defined Language is not detected: HackRx 2.0 Bajaj Fin Serv\n",
      "name 'detect' is not defined Language is not detected: (Social) Content: Re-defining search and recommendation engine for a website\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is scraped from Bajaj Finserv website which contains lots of articles and blogs and product information regarding financial products like various kinds of insurance, loans, EMIs etc. \n",
      "\n",
      "### Content\n",
      "\n",
      "The data that we have scraped and presented is as follows -\n",
      "All the paragraph and text that has been scraped are added in the file paras-and-lines-website-scraped.csv.\n",
      "All the tweets that were scraped are added in another CSV file. \n",
      "All the URLs of the blog webpages present in the website are present in another CSV file. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank Bajaj Finserv for providing the data for our research.  \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Using this large corpus of text on a specific domain i.e. financial products, we are trying to create a recommendation system that recommends the web pages that are most closely related to a keyword search.  \n",
      "name 'detect' is not defined Language is not detected: hackrx-20-bajaj-fin-serv\n",
      "name 'detect' is not defined Language is not detected: shreyjain1107\n",
      "name 'detect' is not defined Language is not detected: HackRx 2.0 Bajaj Fin Serv\n",
      "name 'detect' is not defined Language is not detected: (Social) Content: Re-defining search and recommendation engine for a website\n",
      "name 'detect' is not defined Language is not detected: ### Context\n",
      "\n",
      "This data is scraped from Bajaj Finserv website which contains lots of articles and blogs and product information regarding financial products like various kinds of insurance, loans, EMIs etc. \n",
      "\n",
      "### Content\n",
      "\n",
      "The data that we have scraped and presented is as follows -\n",
      "All the paragraph and text that has been scraped are added in the file paras-and-lines-website-scraped.csv.\n",
      "All the tweets that were scraped are added in another CSV file. \n",
      "All the URLs of the blog webpages present in the website are present in another CSV file. \n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "We would like to thank Bajaj Finserv for providing the data for our research.  \n",
      "\n",
      "### Inspiration\n",
      "\n",
      "Using this large corpus of text on a specific domain i.e. financial products, we are trying to create a recommendation system that recommends the web pages that are most closely related to a keyword search.  \n",
      "name 'detect' is not defined Language is not detected: jmmvutu/lottery-features-for-machine-learning-ai\n",
      "name 'detect' is not defined Language is not detected: lottery-features-for-machine-learning-ai\n",
      "name 'detect' is not defined Language is not detected: jmmvutu\n",
      "name 'detect' is not defined Language is not detected: Lottery features for time series Machine Learning\n",
      "name 'detect' is not defined Language is not detected: Signal processing and Feature-engineering based on lottery draw results\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "\n",
      "Winning the lottery has always been a dream for a lot of people. Because of that, a lot of work have been done in the past to try and tackle the challenge.\n",
      "We saw the rise of different approaches, from computer softwares that optimize your lottery picks to numerical and statistical analysis, as well as esoteric approaches.\n",
      "\n",
      "More recently, with the rise of *machine learning* (_hereafter: **ML**_), people have tried to tackle the problem by trying to have a ML model predict the next combination.\n",
      "However, most of those who undertook to try this approach had poor understanding of ML-related basics such as data preprocessing and signal processing.\n",
      "&gt; As an example of bad things that were done, there were projects that built a ML model that took raw draw results and outputted N numbers, hoping that those N numbers would be the correct combination of the next draw.\n",
      "&gt; To give you an analogy with a  real world example, it would be like taking in the raw data (bytes) of an audio file as the (sole) input of a model and hope that it outputs a concept like *bpm*, *music genre*, ...\n",
      "&gt; Of course, this resulted in extremely bad results and **ML models that didn't learn a single thing**.\n",
      "&gt; Similarly, it would be the same as trying to predict stock prices using only the price as the sole input variable. It would be bound to fail without creating higher-level features. Models that do not do that would never succeed.\n",
      "\n",
      "\n",
      "Because Machine-Learning-based approaches *were bound* to fail before even beginning *unless something was done* regarding data and signal processing, I decided to make my contribution by crafting higher-level features (/ abstract concepts) from historic lottery data.\n",
      "\n",
      "I leave under mouth discussions about the mathematical theory of probability (which I explained more in [the repository of Lofea](https://github.com/JeffMv/Lofea), a project I created to generate this dataset) or why mathematicians say it would be theoretically impossible to predict. Those specifics and other questions can be discussed in the comments section.\n",
      "To let people still dream enough to try and tackle the problem, I'd like to point out that stock market prices (which are also numerical time series data) are said to be unpredictable due to the [Efficient Market Hypothesis](https://en.wikipedia.org/wiki/Efficient-market_hypothesis). Regardless, firms and individuals have been trying their best to try and predict the evolution of stock prices. Although theories tell us something is impossible in theory, there might be a practical implementation flaw that might get exploited if studied carefully enough. Who knows unless they try ?\n",
      "\n",
      "\n",
      "## Content\n",
      "Preprocessed historical results.\n",
      "\n",
      "**Tackling a big and complex task** often requires problem solving methodologies, such as *divide and conquer*. This is why instead of tackling a regular pick 6 among 49 or so lottery, this dataset focuses on simple 1/10 lottery data (i.e. pick 1 among 10). But it also includes a version for the Euromillions (a 5/50 lottery).\n",
      "\n",
      "You will find in the archive `features.04-2021` files containing computed features, as well as the whole draw histories used to compute them.\n",
      "One lottery is the [Euromillions](https://www.fdj.fr/jeux-de-tirage/euromillions-my-million/resultats), and the other is  [TrioMagic](https://jeux.loro.ch/games/magic3), though similar datasets can be crafted for lotteries that share their respective formats.\n",
      "Regarding *1/10* lottery, since most of *1/10* lotteries have several *pools* (/columns) from which one has to pick, this kind of dataset with higher-level features can be created for each column individually and compared among different lotteries.\n",
      "\n",
      "### Preprocessing\n",
      "\n",
      "The big idea here is to preprocess historic draws as if it was a *signal* or a *time serie* and create higher-level features based on it.\n",
      "Inspired by the approach of working with numerical time series signals such as stock market prices.\n",
      "\n",
      "### Labels (referred to as \"_Targets_\" in the dataset)\n",
      "\n",
      "There are several high-level concepts we may want to predict, such as the parity of the next draw. This would be a *classification problem*. You may also choose to tackle a *regression* problem, such as trying to predict the repartition of rate of even numbers in the next N draws (for instance N=2, 3 or 5).\n",
      "There are also other possible targets besides the parity, such as the *Universe length*, which will be described below.\n",
      "\n",
      "The target you choose to predict may influence what kind of features you will try to include or craft.\n",
      "\n",
      "### Features\n",
      "\n",
      "Several of the features included in this dataset are based on a concept I came up with I called « _**Universe Length**_ ».\n",
      "Basically, *Universe Length* (referred to as `ULen` in the dataset) is the number of different numbers in a given time frame.\n",
      "For instance in a 1/10 lottery, *Universe Length* over a _**time frame** of 10 draws_ with the following draw history `[3,4,1,4,9,5,5,9,8,1]` would be `6` since there are 6 different numbers drawn in this running window frame.\n",
      "\n",
      "Similarly, there are other features that are based on a **running window `frame`**.\n",
      "For the lotteries 5/50 and 1/10, setting the running window to 10 was a reasonable choice. (For the stars of Euromillions, which is a 2/12, the runnig window `frame` was set to 6). \n",
      "\n",
      "#### **Approach - Features of draws**\n",
      "\n",
      "There are different kind of approaches. Among them, one approach is to make statistics and features related to each ball. And another approach is to study the characteristics of draws (instead of individual balls).\n",
      "In this dataset (or at least the early version of it) the chosen approach was the latter: studying draws.\n",
      "\n",
      "The names of features and targets of this dataset have been chosen for the clarity and unicity (in spite of shortness). Thus they are quite verbose, so feel free to rename them when you get used to them.\n",
      "Their verbosity allows distinguishing between variations around a same concept, or variations of running window or such...\n",
      "\n",
      "\n",
      "**Explanation of features:**\n",
      "\n",
      "- `universe-length` : Number of *different* numbers in the current running window. (Always see the file's description to know the applied running window `frame`).\n",
      "- `universe-length-offset-from-center` : based on `universe-length`. It just shows the distance to the center of all possible universe length values. Note that just because a universe length is possible does not mean it will ever occur. For instance, the lowest possible universe length would mean that only the same ball/set of drawn number is drawn over and over again within the running window. Although this could theoretically happen, its low probability makes it impossible.\n",
      "- `parity` : Number of even numbers in the *current* draw\n",
      "- `parity-over-frame` : Number of even numbers in the draws of the *running window*\n",
      "- `last-moving-direction-of-universe-length` : Las moving direction of the `universe-length` feature. -1 means decreasing, +1 means increasing.\n",
      "\n",
      "- `move-balance-of-universe-length_latest-minus-mean` : formula: The mean of universe lengths over the running window, minus the current value of the `universe length`\n",
      "- `move-balance-of-universe-length_mean-minus-earliest` : formula: Current value of the `universe length` minus the mean of universe lengths over the running window\n",
      "- `move-balance-of-universe-length_latest-minus-mean_runningWindowX2`: same as the other one, but the mean is applied on a frame twice bigger.\n",
      "- `move-balance-of-universe-length_mean-minus-earliest_runningWindowX2`: same concept as the above feature\n",
      "\n",
      "- `universe-length-drop` : How much the universe length can at most drop in the next draw's result\n",
      "- `universe-length-increase` : How much the universe length can at most increase in the next draw's result\n",
      "- `universe-length-repetition-same` : the number of times the current `universe length` has been repeated successively\n",
      "- `greater-universe-length-than-repetition` : the number successive times we find a higher `universe length` than the current one over the running window\n",
      "- `universe-length-didfollowincrease` : how much did the universe length increase *from the previous* draw. 0 means no increase or decrease.\n",
      "\n",
      "- `mean-frequency-of-drawn-numbers` : the mean frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n",
      "- `median-frequency-of-drawn-numbers` : the median frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n",
      "- `mean-frequency-of-drawn-numbers-over-X-draws` : same as `mean-frequency-of-drawn-numbers` but over a running window of `X` draws.\n",
      "- `median-frequency-of-drawn-numbers-over-X-draws` : same as `median-frequency-of-drawn-numbers` but over a running window of `X` draws.\n",
      "\n",
      "- `mean-gap-of-drawn-numbers` : the mean appearance gap of each the drawn balls. Takes the individual gaps of each of the balls in the current result draw (i.e. the number of draws between the last time a given ball was drawn), sums them up, and then divides by the number of balls in the draw.\n",
      "- `median-gap-of-drawn-numbers` : like `mean-gap-of-drawn-numbers` but using the median instead of the mean. Note that in 1/N lottery pools (one ball pulled out of N), the two features are always equal, which is logic.\n",
      "- `mean-gap-of-drawn-numbers-bounded-at-X-draws` : where `X` is a number. Same as its counterpart `mean-gap-of-drawn-numbers` but over a running window of `X` draws.\n",
      "- `median-gap-of-drawn-numbers-bounded-at-X-draws` : similar to the previous one, but using the median.\n",
      "\n",
      "- `mean-of-4-gaps-of-each-drawn-numbers` : formula : systematically take the last 4 gaps of each of the balls in the current draw. Sum all these gaps together and take the overall mean. (i.e. average of all those gaps)\n",
      "- `median-of-4-gaps-of-each-drawn-numbers` : same formula as its paronym, but takes the median of all. For 1/N lotteries, both are equal.\n",
      "- `median-of-means-of-X-gaps-of-each-drawn-numbers` : where `X` is a number. Formula : systematically take the last X gaps of each of the balls of the current draw. Take the mean for each ball. Then sum up all the means together and take the median of them. In short, it is a median( of average( of [X-latest-gaps-of-symbol-Y] )). A median of averages may not make much sense statistically speaking. This feature is provided as is.\n",
      "\n",
      "\n",
      "\n",
      "**TARGET columns** (i.e. supervised learning)\n",
      "(i.e. ideas of what you might want to predict)\n",
      "\n",
      "- `target_universe-length-willFollowIncrease` : TARGET feature (supervised learning). Same as the `universe-length-didfollowincrease` feature but for the draw that comes in the future. (DO NOT mistake them). One goal can be to predict this value given only the other features. Was named `universe-length-willfollowincrease` in an earlier version of the dataset.\n",
      "- `target_coming-universe-length-change-in-next-draw` : same as `target_universe-length-willFollowIncrease` but indicates decreases as well as increases and stagnations. Might be better suited for a regression, but you do as you see fit.\n",
      "- `target_coming-mean-universe-length-change-in-next-2-draws` : takes the mean change of universe length in the next 2 draws instead of only one. See `target_coming-universe-length-change-in-next-draw`.\n",
      "- `target_future-1rst-value-of-universe-length-center-offset-from-center` : the next value of the feature `universe-length-offset-from-center`. You may want to try to predict that.\n",
      "- `target_coming-universe-length-center-offset-change-in-next-draw` : the relative change of the feature `universe-length-offset-from-center` that will occur in the next draw.\n",
      "- `target_coming-mean-universe-length-center-offset-change-in-next-2-draws` : same as the previous feature, but we take the mean change in the next 2 draws.\n",
      "\n",
      "\n",
      "**Special columns:**\n",
      "- `date` : date of the lottery draw\n",
      "- `draw` / `draw-result` : most recent draw of the specified date. displayed for convenience\n",
      "- `running-window-frame-length` : convenience a CONSTANT column. it is there to remind you of the base *running window* size (also called *frame length*) used to compute most features. When some features use a given multiple of the *frame length*, this is the value that gets multiplied.\n",
      "- `draw-id` : the draw identifier. It can be the date or in another form (such as reversed date `yyyymmdd`). Only displayed for convenience\n",
      "\n",
      "\n",
      "## Inspiration / Ideas of approaches\n",
      "\n",
      "- Transforming the problem into another that outputs binary values.\n",
      "   This would allow you/us to tap into the enormity of works and theorems done for _**binomial problems**_ both in statistics and probability.\n",
      "   It would allow you to compare the distribution of the randomness to much more scholarly examples (such as heads or tails) and deduce the law of probability / probability distribution / random variable behind a particular lottery.\n",
      "   &gt; If I remember correctly a maths course, there should be a **theorem / lemme** in probability that say how likely a binomial distribution is to take exaggerated values (big outliers). Regardless, there are a lot of results already, such as **Bernoulli trial** to study and compare theoretical and practical results.\n",
      "\n",
      "\n",
      "- Treating the problem as a numerical time series problem\n",
      "- Treating the problem as a signal processing problem\n",
      "- Asking different high-level questions and formulating the problem differently. For instance\n",
      "  - Trying to predict the repartition of a feature (like parity) in the next 5 draws (instead of only predicting the parity of the next draw)\n",
      "\n",
      "- Trying to identify critical points in time where several features converge towards predicting the same thing.\n",
      "   For instance, there could be a time where the parity of the last 10 drawn numbers was even. If on top of that several even numbers have had a particularly high appearance rate in a preceding big frame of time, then these two features would lean towards thinking that there is a higher chance of seeing an odd number drawn next. (Though probability independence would say that such convergence are meaningless, here we are to suppose the opposite until we face the hard truth, as in a proof by contradiction).\n",
      "   See the note about **Bernoulli trials**-related *theorem / lemme* as mentioned above.\n",
      "\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "\n",
      "## License\n",
      "\n",
      "Dataset licensed under [CC-BY](https://creativecommons.org/licenses/by/4.0/)\n",
      "\n",
      "Personal project for generating the dataset: [Lofea](https://github.com/JeffMv/Lofea) under [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
      "name 'detect' is not defined Language is not detected: lottery-features-for-machine-learning-ai\n",
      "name 'detect' is not defined Language is not detected: jmmvutu\n",
      "name 'detect' is not defined Language is not detected: Lottery features for time series Machine Learning\n",
      "name 'detect' is not defined Language is not detected: Signal processing and Feature-engineering based on lottery draw results\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "\n",
      "Winning the lottery has always been a dream for a lot of people. Because of that, a lot of work have been done in the past to try and tackle the challenge.\n",
      "We saw the rise of different approaches, from computer softwares that optimize your lottery picks to numerical and statistical analysis, as well as esoteric approaches.\n",
      "\n",
      "More recently, with the rise of *machine learning* (_hereafter: **ML**_), people have tried to tackle the problem by trying to have a ML model predict the next combination.\n",
      "However, most of those who undertook to try this approach had poor understanding of ML-related basics such as data preprocessing and signal processing.\n",
      "&gt; As an example of bad things that were done, there were projects that built a ML model that took raw draw results and outputted N numbers, hoping that those N numbers would be the correct combination of the next draw.\n",
      "&gt; To give you an analogy with a  real world example, it would be like taking in the raw data (bytes) of an audio file as the (sole) input of a model and hope that it outputs a concept like *bpm*, *music genre*, ...\n",
      "&gt; Of course, this resulted in extremely bad results and **ML models that didn't learn a single thing**.\n",
      "&gt; Similarly, it would be the same as trying to predict stock prices using only the price as the sole input variable. It would be bound to fail without creating higher-level features. Models that do not do that would never succeed.\n",
      "\n",
      "\n",
      "Because Machine-Learning-based approaches *were bound* to fail before even beginning *unless something was done* regarding data and signal processing, I decided to make my contribution by crafting higher-level features (/ abstract concepts) from historic lottery data.\n",
      "\n",
      "I leave under mouth discussions about the mathematical theory of probability (which I explained more in [the repository of Lofea](https://github.com/JeffMv/Lofea), a project I created to generate this dataset) or why mathematicians say it would be theoretically impossible to predict. Those specifics and other questions can be discussed in the comments section.\n",
      "To let people still dream enough to try and tackle the problem, I'd like to point out that stock market prices (which are also numerical time series data) are said to be unpredictable due to the [Efficient Market Hypothesis](https://en.wikipedia.org/wiki/Efficient-market_hypothesis). Regardless, firms and individuals have been trying their best to try and predict the evolution of stock prices. Although theories tell us something is impossible in theory, there might be a practical implementation flaw that might get exploited if studied carefully enough. Who knows unless they try ?\n",
      "\n",
      "\n",
      "## Content\n",
      "Preprocessed historical results.\n",
      "\n",
      "**Tackling a big and complex task** often requires problem solving methodologies, such as *divide and conquer*. This is why instead of tackling a regular pick 6 among 49 or so lottery, this dataset focuses on simple 1/10 lottery data (i.e. pick 1 among 10). But it also includes a version for the Euromillions (a 5/50 lottery).\n",
      "\n",
      "You will find in the archive `features.04-2021` files containing computed features, as well as the whole draw histories used to compute them.\n",
      "One lottery is the [Euromillions](https://www.fdj.fr/jeux-de-tirage/euromillions-my-million/resultats), and the other is  [TrioMagic](https://jeux.loro.ch/games/magic3), though similar datasets can be crafted for lotteries that share their respective formats.\n",
      "Regarding *1/10* lottery, since most of *1/10* lotteries have several *pools* (/columns) from which one has to pick, this kind of dataset with higher-level features can be created for each column individually and compared among different lotteries.\n",
      "\n",
      "### Preprocessing\n",
      "\n",
      "The big idea here is to preprocess historic draws as if it was a *signal* or a *time serie* and create higher-level features based on it.\n",
      "Inspired by the approach of working with numerical time series signals such as stock market prices.\n",
      "\n",
      "### Labels (referred to as \"_Targets_\" in the dataset)\n",
      "\n",
      "There are several high-level concepts we may want to predict, such as the parity of the next draw. This would be a *classification problem*. You may also choose to tackle a *regression* problem, such as trying to predict the repartition of rate of even numbers in the next N draws (for instance N=2, 3 or 5).\n",
      "There are also other possible targets besides the parity, such as the *Universe length*, which will be described below.\n",
      "\n",
      "The target you choose to predict may influence what kind of features you will try to include or craft.\n",
      "\n",
      "### Features\n",
      "\n",
      "Several of the features included in this dataset are based on a concept I came up with I called « _**Universe Length**_ ».\n",
      "Basically, *Universe Length* (referred to as `ULen` in the dataset) is the number of different numbers in a given time frame.\n",
      "For instance in a 1/10 lottery, *Universe Length* over a _**time frame** of 10 draws_ with the following draw history `[3,4,1,4,9,5,5,9,8,1]` would be `6` since there are 6 different numbers drawn in this running window frame.\n",
      "\n",
      "Similarly, there are other features that are based on a **running window `frame`**.\n",
      "For the lotteries 5/50 and 1/10, setting the running window to 10 was a reasonable choice. (For the stars of Euromillions, which is a 2/12, the runnig window `frame` was set to 6). \n",
      "\n",
      "#### **Approach - Features of draws**\n",
      "\n",
      "There are different kind of approaches. Among them, one approach is to make statistics and features related to each ball. And another approach is to study the characteristics of draws (instead of individual balls).\n",
      "In this dataset (or at least the early version of it) the chosen approach was the latter: studying draws.\n",
      "\n",
      "The names of features and targets of this dataset have been chosen for the clarity and unicity (in spite of shortness). Thus they are quite verbose, so feel free to rename them when you get used to them.\n",
      "Their verbosity allows distinguishing between variations around a same concept, or variations of running window or such...\n",
      "\n",
      "\n",
      "**Explanation of features:**\n",
      "\n",
      "- `universe-length` : Number of *different* numbers in the current running window. (Always see the file's description to know the applied running window `frame`).\n",
      "- `universe-length-offset-from-center` : based on `universe-length`. It just shows the distance to the center of all possible universe length values. Note that just because a universe length is possible does not mean it will ever occur. For instance, the lowest possible universe length would mean that only the same ball/set of drawn number is drawn over and over again within the running window. Although this could theoretically happen, its low probability makes it impossible.\n",
      "- `parity` : Number of even numbers in the *current* draw\n",
      "- `parity-over-frame` : Number of even numbers in the draws of the *running window*\n",
      "- `last-moving-direction-of-universe-length` : Las moving direction of the `universe-length` feature. -1 means decreasing, +1 means increasing.\n",
      "\n",
      "- `move-balance-of-universe-length_latest-minus-mean` : formula: The mean of universe lengths over the running window, minus the current value of the `universe length`\n",
      "- `move-balance-of-universe-length_mean-minus-earliest` : formula: Current value of the `universe length` minus the mean of universe lengths over the running window\n",
      "- `move-balance-of-universe-length_latest-minus-mean_runningWindowX2`: same as the other one, but the mean is applied on a frame twice bigger.\n",
      "- `move-balance-of-universe-length_mean-minus-earliest_runningWindowX2`: same concept as the above feature\n",
      "\n",
      "- `universe-length-drop` : How much the universe length can at most drop in the next draw's result\n",
      "- `universe-length-increase` : How much the universe length can at most increase in the next draw's result\n",
      "- `universe-length-repetition-same` : the number of times the current `universe length` has been repeated successively\n",
      "- `greater-universe-length-than-repetition` : the number successive times we find a higher `universe length` than the current one over the running window\n",
      "- `universe-length-didfollowincrease` : how much did the universe length increase *from the previous* draw. 0 means no increase or decrease.\n",
      "\n",
      "- `mean-frequency-of-drawn-numbers` : the mean frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n",
      "- `median-frequency-of-drawn-numbers` : the median frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n",
      "- `mean-frequency-of-drawn-numbers-over-X-draws` : same as `mean-frequency-of-drawn-numbers` but over a running window of `X` draws.\n",
      "- `median-frequency-of-drawn-numbers-over-X-draws` : same as `median-frequency-of-drawn-numbers` but over a running window of `X` draws.\n",
      "\n",
      "- `mean-gap-of-drawn-numbers` : the mean appearance gap of each the drawn balls. Takes the individual gaps of each of the balls in the current result draw (i.e. the number of draws between the last time a given ball was drawn), sums them up, and then divides by the number of balls in the draw.\n",
      "- `median-gap-of-drawn-numbers` : like `mean-gap-of-drawn-numbers` but using the median instead of the mean. Note that in 1/N lottery pools (one ball pulled out of N), the two features are always equal, which is logic.\n",
      "- `mean-gap-of-drawn-numbers-bounded-at-X-draws` : where `X` is a number. Same as its counterpart `mean-gap-of-drawn-numbers` but over a running window of `X` draws.\n",
      "- `median-gap-of-drawn-numbers-bounded-at-X-draws` : similar to the previous one, but using the median.\n",
      "\n",
      "- `mean-of-4-gaps-of-each-drawn-numbers` : formula : systematically take the last 4 gaps of each of the balls in the current draw. Sum all these gaps together and take the overall mean. (i.e. average of all those gaps)\n",
      "- `median-of-4-gaps-of-each-drawn-numbers` : same formula as its paronym, but takes the median of all. For 1/N lotteries, both are equal.\n",
      "- `median-of-means-of-X-gaps-of-each-drawn-numbers` : where `X` is a number. Formula : systematically take the last X gaps of each of the balls of the current draw. Take the mean for each ball. Then sum up all the means together and take the median of them. In short, it is a median( of average( of [X-latest-gaps-of-symbol-Y] )). A median of averages may not make much sense statistically speaking. This feature is provided as is.\n",
      "\n",
      "\n",
      "\n",
      "**TARGET columns** (i.e. supervised learning)\n",
      "(i.e. ideas of what you might want to predict)\n",
      "\n",
      "- `target_universe-length-willFollowIncrease` : TARGET feature (supervised learning). Same as the `universe-length-didfollowincrease` feature but for the draw that comes in the future. (DO NOT mistake them). One goal can be to predict this value given only the other features. Was named `universe-length-willfollowincrease` in an earlier version of the dataset.\n",
      "- `target_coming-universe-length-change-in-next-draw` : same as `target_universe-length-willFollowIncrease` but indicates decreases as well as increases and stagnations. Might be better suited for a regression, but you do as you see fit.\n",
      "- `target_coming-mean-universe-length-change-in-next-2-draws` : takes the mean change of universe length in the next 2 draws instead of only one. See `target_coming-universe-length-change-in-next-draw`.\n",
      "- `target_future-1rst-value-of-universe-length-center-offset-from-center` : the next value of the feature `universe-length-offset-from-center`. You may want to try to predict that.\n",
      "- `target_coming-universe-length-center-offset-change-in-next-draw` : the relative change of the feature `universe-length-offset-from-center` that will occur in the next draw.\n",
      "- `target_coming-mean-universe-length-center-offset-change-in-next-2-draws` : same as the previous feature, but we take the mean change in the next 2 draws.\n",
      "\n",
      "\n",
      "**Special columns:**\n",
      "- `date` : date of the lottery draw\n",
      "- `draw` / `draw-result` : most recent draw of the specified date. displayed for convenience\n",
      "- `running-window-frame-length` : convenience a CONSTANT column. it is there to remind you of the base *running window* size (also called *frame length*) used to compute most features. When some features use a given multiple of the *frame length*, this is the value that gets multiplied.\n",
      "- `draw-id` : the draw identifier. It can be the date or in another form (such as reversed date `yyyymmdd`). Only displayed for convenience\n",
      "\n",
      "\n",
      "## Inspiration / Ideas of approaches\n",
      "\n",
      "- Transforming the problem into another that outputs binary values.\n",
      "   This would allow you/us to tap into the enormity of works and theorems done for _**binomial problems**_ both in statistics and probability.\n",
      "   It would allow you to compare the distribution of the randomness to much more scholarly examples (such as heads or tails) and deduce the law of probability / probability distribution / random variable behind a particular lottery.\n",
      "   &gt; If I remember correctly a maths course, there should be a **theorem / lemme** in probability that say how likely a binomial distribution is to take exaggerated values (big outliers). Regardless, there are a lot of results already, such as **Bernoulli trial** to study and compare theoretical and practical results.\n",
      "\n",
      "\n",
      "- Treating the problem as a numerical time series problem\n",
      "- Treating the problem as a signal processing problem\n",
      "- Asking different high-level questions and formulating the problem differently. For instance\n",
      "  - Trying to predict the repartition of a feature (like parity) in the next 5 draws (instead of only predicting the parity of the next draw)\n",
      "\n",
      "- Trying to identify critical points in time where several features converge towards predicting the same thing.\n",
      "   For instance, there could be a time where the parity of the last 10 drawn numbers was even. If on top of that several even numbers have had a particularly high appearance rate in a preceding big frame of time, then these two features would lean towards thinking that there is a higher chance of seeing an odd number drawn next. (Though probability independence would say that such convergence are meaningless, here we are to suppose the opposite until we face the hard truth, as in a proof by contradiction).\n",
      "   See the note about **Bernoulli trials**-related *theorem / lemme* as mentioned above.\n",
      "\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "\n",
      "## License\n",
      "\n",
      "Dataset licensed under [CC-BY](https://creativecommons.org/licenses/by/4.0/)\n",
      "\n",
      "Personal project for generating the dataset: [Lofea](https://github.com/JeffMv/Lofea) under [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
      "name 'detect' is not defined Language is not detected: jmmvutu/dating-app-lovoo-user-profiles\n",
      "name 'detect' is not defined Language is not detected: dating-app-lovoo-user-profiles\n",
      "name 'detect' is not defined Language is not detected: jmmvutu\n",
      "name 'detect' is not defined Language is not detected: Dating App User Profiles' stats - Lovoo v3\n",
      "name 'detect' is not defined Language is not detected: User fame and behaviour on a dating app\n",
      "name 'detect' is not defined Language is not detected: ### Foreword\n",
      "\n",
      "This dataset is a preview of a bigger dataset.\n",
      "My [Telegram bot](https://t.me/JfreexDatasets_bot) will answer your queries for more data and also allow you to contact me.\n",
      "\n",
      "# Context\n",
      "\n",
      "When Dating apps like Tinder were becoming viral, people wanted to have the best profile in order to get more matches and more potential encounters.\n",
      "Unlike other previous dating platforms, those new ones emphasized on the mutuality of attraction before allowing any two people to get in touch and chat. This made it all the more important to create the best profile in order to get *the best first impression*.\n",
      "\n",
      "Parallel to that, we Humans have always been in awe before charismatic and inspiring people. The more charismatic people tend to be followed and listened to by more people.\n",
      "Through their metrics such as the number of friends/followers, social networks give some ways of \"measuring\" the potential charisma of some people.\n",
      "\n",
      "In regard to all that, one can then think:\n",
      "- what makes a great user profile ?\n",
      "- how to make the best first impression in order to get more matches (and ultimately find love, or new friendships) ?\n",
      "- what makes a person charismatic ?\n",
      "- how do charismatic people present themselves ?\n",
      "\n",
      "In order to try and understand those different social questions, I decided to create a dataset of user profile informations using the social network *Lovoo* when it came out. By using different methodologies, I was able to gather user profile data, as well as some usually unavailable metrics (such as the number of profile visits).\n",
      "\n",
      "\n",
      "# Content\n",
      "\n",
      "\n",
      "The dataset contains user profile infos of users of the website [Lovoo](https://lovoo.com).\n",
      "\n",
      "The dataset was gathered during spring 2015 (april, may). At that time, Lovoo was expanding in european countries (among others), while Tinder was trending both in America and in Europe.\n",
      "At that time the iOS version of the Lovoo app was in version 3.\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "### Accessory image data\n",
      "\n",
      "The dataset references pictures (field `pictureId`) of user profiles. These pictures are also available for a fraction of users but have not been uploaded and should be asked separately.\n",
      "\n",
      "The idea when gathering the profile pictures was to determine whether some correlations could be identified between a profile picture and the reputation or success of a given profile. Since first impression matters, a sound hypothesis to make is that the profile picture might have a great influence on the number of profile visits, matches and so on. Do not forget that only a fraction of a user's profile is seen when browsing through a list of users.\n",
      "\n",
      "![App preview of browsing profiles](https://s1.dmcdn.net/v/BnWkG1M7WuJDq2PKP/x480)\n",
      "\n",
      "### Details about collection methodology\n",
      "\n",
      "\n",
      "In order to gather the data, I developed a set of tools that would save the data while browsing through profiles and doing searches. Because of this approach (and the constraints that forced me to develop this approach) I could only gather user profiles that were **recommended** by *Lovoo*'s algorithm for 2 profiles I created for this purpose occasion (male, open to friends & chats & dates). That is why there are only female users in the dataset.\n",
      "Another work could be done to fetch similar data for both genders or other age ranges.\n",
      "\n",
      "*Regarding the number of user profiles*\n",
      "It turned out that the recommendation algorithm always seemed to output the same set of user profiles. This meant *Lovoo*'s algorithm was probably heavily relying on settings like location (to recommend more people nearby than people in different places or countries) and maybe cookies. This diminished the number of different user profiles that would be presented and included in the dataset.\n",
      "\n",
      "\n",
      "\n",
      "# Inspiration\n",
      "\n",
      "As mentioned in the introduction, there are a lot of questions we can answer using a dataset such as this one. Some questions are related to \n",
      "- popularity, charisma\n",
      "- census and demographic studies.\n",
      "  - Statistics about the interest of people joining dating apps (making friends, finding someone to date, finding true love, ...).\n",
      "- Detecting influencers / potential influencers and studying them\n",
      "\n",
      "Previously mentioned:\n",
      "- what makes a great user profile ?\n",
      "- how to make the best first impression in order to get more matches (and ultimately find love, or new friendships) ?\n",
      "- what makes a person charismatic ?\n",
      "- how do charismatic people present themselves ?\n",
      "\n",
      "Other works:\n",
      "- A **[starter analysis](https://data.world/jfreex/dating-app-user-profiles-stats-lovoo-v3/workspace/query?queryid=337bc665-b38b-474c-a69c-f47728df31bd)** is available on **my data.world** account, made using a SQL query. Another file has been created through that mean on the [dataset page](https://data.world/jfreex/dating-app-user-profiles-stats-lovoo-v3).\n",
      "- The [kaggle version of the dataset](https://www.kaggle.com/jmmvutu/dating-app-lovoo-user-profiles) might contain a starter kernel.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: dating-app-lovoo-user-profiles\n",
      "name 'detect' is not defined Language is not detected: jmmvutu\n",
      "name 'detect' is not defined Language is not detected: Dating App User Profiles' stats - Lovoo v3\n",
      "name 'detect' is not defined Language is not detected: User fame and behaviour on a dating app\n",
      "name 'detect' is not defined Language is not detected: ### Foreword\n",
      "\n",
      "This dataset is a preview of a bigger dataset.\n",
      "My [Telegram bot](https://t.me/JfreexDatasets_bot) will answer your queries for more data and also allow you to contact me.\n",
      "\n",
      "# Context\n",
      "\n",
      "When Dating apps like Tinder were becoming viral, people wanted to have the best profile in order to get more matches and more potential encounters.\n",
      "Unlike other previous dating platforms, those new ones emphasized on the mutuality of attraction before allowing any two people to get in touch and chat. This made it all the more important to create the best profile in order to get *the best first impression*.\n",
      "\n",
      "Parallel to that, we Humans have always been in awe before charismatic and inspiring people. The more charismatic people tend to be followed and listened to by more people.\n",
      "Through their metrics such as the number of friends/followers, social networks give some ways of \"measuring\" the potential charisma of some people.\n",
      "\n",
      "In regard to all that, one can then think:\n",
      "- what makes a great user profile ?\n",
      "- how to make the best first impression in order to get more matches (and ultimately find love, or new friendships) ?\n",
      "- what makes a person charismatic ?\n",
      "- how do charismatic people present themselves ?\n",
      "\n",
      "In order to try and understand those different social questions, I decided to create a dataset of user profile informations using the social network *Lovoo* when it came out. By using different methodologies, I was able to gather user profile data, as well as some usually unavailable metrics (such as the number of profile visits).\n",
      "\n",
      "\n",
      "# Content\n",
      "\n",
      "\n",
      "The dataset contains user profile infos of users of the website [Lovoo](https://lovoo.com).\n",
      "\n",
      "The dataset was gathered during spring 2015 (april, may). At that time, Lovoo was expanding in european countries (among others), while Tinder was trending both in America and in Europe.\n",
      "At that time the iOS version of the Lovoo app was in version 3.\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "### Accessory image data\n",
      "\n",
      "The dataset references pictures (field `pictureId`) of user profiles. These pictures are also available for a fraction of users but have not been uploaded and should be asked separately.\n",
      "\n",
      "The idea when gathering the profile pictures was to determine whether some correlations could be identified between a profile picture and the reputation or success of a given profile. Since first impression matters, a sound hypothesis to make is that the profile picture might have a great influence on the number of profile visits, matches and so on. Do not forget that only a fraction of a user's profile is seen when browsing through a list of users.\n",
      "\n",
      "![App preview of browsing profiles](https://s1.dmcdn.net/v/BnWkG1M7WuJDq2PKP/x480)\n",
      "\n",
      "### Details about collection methodology\n",
      "\n",
      "\n",
      "In order to gather the data, I developed a set of tools that would save the data while browsing through profiles and doing searches. Because of this approach (and the constraints that forced me to develop this approach) I could only gather user profiles that were **recommended** by *Lovoo*'s algorithm for 2 profiles I created for this purpose occasion (male, open to friends & chats & dates). That is why there are only female users in the dataset.\n",
      "Another work could be done to fetch similar data for both genders or other age ranges.\n",
      "\n",
      "*Regarding the number of user profiles*\n",
      "It turned out that the recommendation algorithm always seemed to output the same set of user profiles. This meant *Lovoo*'s algorithm was probably heavily relying on settings like location (to recommend more people nearby than people in different places or countries) and maybe cookies. This diminished the number of different user profiles that would be presented and included in the dataset.\n",
      "\n",
      "\n",
      "\n",
      "# Inspiration\n",
      "\n",
      "As mentioned in the introduction, there are a lot of questions we can answer using a dataset such as this one. Some questions are related to \n",
      "- popularity, charisma\n",
      "- census and demographic studies.\n",
      "  - Statistics about the interest of people joining dating apps (making friends, finding someone to date, finding true love, ...).\n",
      "- Detecting influencers / potential influencers and studying them\n",
      "\n",
      "Previously mentioned:\n",
      "- what makes a great user profile ?\n",
      "- how to make the best first impression in order to get more matches (and ultimately find love, or new friendships) ?\n",
      "- what makes a person charismatic ?\n",
      "- how do charismatic people present themselves ?\n",
      "\n",
      "Other works:\n",
      "- A **[starter analysis](https://data.world/jfreex/dating-app-user-profiles-stats-lovoo-v3/workspace/query?queryid=337bc665-b38b-474c-a69c-f47728df31bd)** is available on **my data.world** account, made using a SQL query. Another file has been created through that mean on the [dataset page](https://data.world/jfreex/dating-app-user-profiles-stats-lovoo-v3).\n",
      "- The [kaggle version of the dataset](https://www.kaggle.com/jmmvutu/dating-app-lovoo-user-profiles) might contain a starter kernel.\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sergiosaharovskiy/s3e14-oofs\n",
      "name 'detect' is not defined Language is not detected: s3e14-oofs\n",
      "name 'detect' is not defined Language is not detected: sergiosaharovskiy\n",
      "name 'detect' is not defined Language is not detected: S3E14 OOFS\n",
      "name 'detect' is not defined Language is not detected: The competition winning solutiion oofs\n",
      "name 'detect' is not defined Language is not detected: The dataset is consist of 10 submission files and 10 respective out-fold-predictions + 1 file called ffs.\n",
      "\n",
      "Some of the oofs were not used during the final ensemble but added as acknowledgment of the authors’ work. \n",
      "\n",
      "\n",
      "**ffs.csv** file was obtrained by using the following code:\n",
      "```python\n",
      "ffs = ['fruitset', 'fruitmass', 'id']\n",
      "\n",
      "# merges train and test and finds final common samples.\n",
      "ids = train[ffs].merge(test[ffs], on=ffs[:-1], how='inner')['id_y'].unique()\n",
      "final_samples = test[test.id.isin(ids)].drop_duplicates(subset=ffs[:-1])[ffs[:-1]]\n",
      "train['pred'] = cop # already blended and corrected oofs up to mae 335.9858.\n",
      "d = dict()\n",
      "\n",
      "VERBOSE = False\n",
      "\n",
      "# runs for loop to check what re-assigned value gives the bigger improvement \n",
      "# for out-of-fold predictions, in the end of the loop it sets values to default.\n",
      "for i in tqdm(final_samples.values):\n",
      "    best_mae = 335.9858\n",
      "    for yl in sorted(train['yield'].unique()):\n",
      "        train.loc[train['fruitset'].eq(i[0]) & train['fruitmass'].eq(i[1]), 'pred'] = yl\n",
      "        sc = mae(train[target_name[0]], train['pred'])\n",
      "        if sc &lt; best_mae:\n",
      "            best_mae = sc\n",
      "            d['_'.join(i.astype(str))] = [sc, yl]\n",
      "            \n",
      "            if VERBOSE and sc &lt; 335.95:\n",
      "                print(sc, yl)\n",
      "        \n",
      "        train['pred'] = cop\n",
      "```\n",
      "name 'detect' is not defined Language is not detected: s3e14-oofs\n",
      "name 'detect' is not defined Language is not detected: sergiosaharovskiy\n",
      "name 'detect' is not defined Language is not detected: S3E14 OOFS\n",
      "name 'detect' is not defined Language is not detected: The competition winning solutiion oofs\n",
      "name 'detect' is not defined Language is not detected: The dataset is consist of 10 submission files and 10 respective out-fold-predictions + 1 file called ffs.\n",
      "\n",
      "Some of the oofs were not used during the final ensemble but added as acknowledgment of the authors’ work. \n",
      "\n",
      "\n",
      "**ffs.csv** file was obtrained by using the following code:\n",
      "```python\n",
      "ffs = ['fruitset', 'fruitmass', 'id']\n",
      "\n",
      "# merges train and test and finds final common samples.\n",
      "ids = train[ffs].merge(test[ffs], on=ffs[:-1], how='inner')['id_y'].unique()\n",
      "final_samples = test[test.id.isin(ids)].drop_duplicates(subset=ffs[:-1])[ffs[:-1]]\n",
      "train['pred'] = cop # already blended and corrected oofs up to mae 335.9858.\n",
      "d = dict()\n",
      "\n",
      "VERBOSE = False\n",
      "\n",
      "# runs for loop to check what re-assigned value gives the bigger improvement \n",
      "# for out-of-fold predictions, in the end of the loop it sets values to default.\n",
      "for i in tqdm(final_samples.values):\n",
      "    best_mae = 335.9858\n",
      "    for yl in sorted(train['yield'].unique()):\n",
      "        train.loc[train['fruitset'].eq(i[0]) & train['fruitmass'].eq(i[1]), 'pred'] = yl\n",
      "        sc = mae(train[target_name[0]], train['pred'])\n",
      "        if sc &lt; best_mae:\n",
      "            best_mae = sc\n",
      "            d['_'.join(i.astype(str))] = [sc, yl]\n",
      "            \n",
      "            if VERBOSE and sc &lt; 335.95:\n",
      "                print(sc, yl)\n",
      "        \n",
      "        train['pred'] = cop\n",
      "```\n",
      "name 'detect' is not defined Language is not detected: mohamedragabmahmued/www-kaggle-comdatasetsmovie-recommendation\n",
      "name 'detect' is not defined Language is not detected: www-kaggle-comdatasetsmovie-recommendation\n",
      "name 'detect' is not defined Language is not detected: mohamedragabmahmued\n",
      "name 'detect' is not defined Language is not detected: www.kaggle.com/datasets/Movie Recommendation\n",
      "name 'detect' is not defined Language is not detected: www-kaggle-comdatasetsmovie-recommendation\n",
      "name 'detect' is not defined Language is not detected: mohamedragabmahmued\n",
      "name 'detect' is not defined Language is not detected: www.kaggle.com/datasets/Movie Recommendation\n",
      "name 'detect' is not defined Language is not detected: rayankazi/hackerearth-holiday-season-deep-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: hackerearth-holiday-season-deep-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: rayankazi\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Holiday Season Deep Learning Challenge\n",
      "name 'detect' is not defined Language is not detected: Is it still a Holiday Season- Image Classification\n",
      "name 'detect' is not defined Language is not detected: ## Content\n",
      "This dataset consists of over 6000 images and was provided by HackerEarth during the contest: [HackerEarth Deep Learning Challenge: 'Tis STILL the season to be jolly](https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/). It was uploaded to Kaggle for easy use and archiving different models.\n",
      "**Note:** The author has neither collected nor created the dataset. It has just been uploaded.\n",
      "\n",
      "## Problem statement\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays. \n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a **snowman**, a **Christmas tree**, **flights**, and the like. \n",
      "<p><strong>Task</strong></p>\n",
      "\n",
      "<p>You are given the following six categories. You are required to classify the images in the dataset based on these categories.</p>\n",
      "\n",
      "<ul>\n",
      "\t<li>Miscellaneous</li>\n",
      "\t<li>Christmas_Tree</li>\n",
      "\t<li>Jacket</li>\n",
      "\t<li>Candle</li>\n",
      "\t<li>Airplane</li>\n",
      "\t<li>Snowman</li>\n",
      "</ul>\n",
      "\n",
      "<h2>Data description</h2>\n",
      "\n",
      "<p>This data set consists of the following two columns:</p>\n",
      "\n",
      "<table>\n",
      "\t<tbody>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\"><strong>Column Name</strong></td>\n",
      "\t\t\t<td style=\"text-align: center\"><strong>Description</strong></td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\">Image</td>\n",
      "\t\t\t<td>Name of image</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\">Class</td>\n",
      "\t\t\t<td>Category of image</td>\n",
      "\t\t</tr>\n",
      "\t</tbody>\n",
      "</table>\n",
      "\n",
      "<p><br>\n",
      "The data folder consists of two folders and one .csv file. The details are as follows:</p>\n",
      "\n",
      "<ul>\n",
      "\t<li><strong>train</strong>: Contains 6469 images for 6&nbsp;classes</li>\n",
      "\t<li><strong>test</strong>: Contains 3489 images</li>\n",
      "\t<li><strong>train.csv</strong>: 6469 x 2</li>\n",
      "</ul>\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "**Link to Contest:** https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/\n",
      "\n",
      "name 'detect' is not defined Language is not detected: hackerearth-holiday-season-deep-learning-challenge\n",
      "name 'detect' is not defined Language is not detected: rayankazi\n",
      "name 'detect' is not defined Language is not detected: HackerEarth Holiday Season Deep Learning Challenge\n",
      "name 'detect' is not defined Language is not detected: Is it still a Holiday Season- Image Classification\n",
      "name 'detect' is not defined Language is not detected: ## Content\n",
      "This dataset consists of over 6000 images and was provided by HackerEarth during the contest: [HackerEarth Deep Learning Challenge: 'Tis STILL the season to be jolly](https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/). It was uploaded to Kaggle for easy use and archiving different models.\n",
      "**Note:** The author has neither collected nor created the dataset. It has just been uploaded.\n",
      "\n",
      "## Problem statement\n",
      "An e-commerce platform is getting all geared up for a season clearance sale and plans to leverage social media as the primary channel to reach their audiences. The campaign’s target group are individuals/families that have recently posted a picture of their indoor Christmas decor or are traveling during the holidays. \n",
      "\n",
      "You work at a leading social media platform and your team has been tasked to build a product recommendation engine for consumers. As part of the development team, your role is to use Deep Learning to develop a model that classifies images based on elements within the picture. These elements should be related to decor or holiday season vacations, such as a **snowman**, a **Christmas tree**, **flights**, and the like. \n",
      "<p><strong>Task</strong></p>\n",
      "\n",
      "<p>You are given the following six categories. You are required to classify the images in the dataset based on these categories.</p>\n",
      "\n",
      "<ul>\n",
      "\t<li>Miscellaneous</li>\n",
      "\t<li>Christmas_Tree</li>\n",
      "\t<li>Jacket</li>\n",
      "\t<li>Candle</li>\n",
      "\t<li>Airplane</li>\n",
      "\t<li>Snowman</li>\n",
      "</ul>\n",
      "\n",
      "<h2>Data description</h2>\n",
      "\n",
      "<p>This data set consists of the following two columns:</p>\n",
      "\n",
      "<table>\n",
      "\t<tbody>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\"><strong>Column Name</strong></td>\n",
      "\t\t\t<td style=\"text-align: center\"><strong>Description</strong></td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\">Image</td>\n",
      "\t\t\t<td>Name of image</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td style=\"text-align: center\">Class</td>\n",
      "\t\t\t<td>Category of image</td>\n",
      "\t\t</tr>\n",
      "\t</tbody>\n",
      "</table>\n",
      "\n",
      "<p><br>\n",
      "The data folder consists of two folders and one .csv file. The details are as follows:</p>\n",
      "\n",
      "<ul>\n",
      "\t<li><strong>train</strong>: Contains 6469 images for 6&nbsp;classes</li>\n",
      "\t<li><strong>test</strong>: Contains 3489 images</li>\n",
      "\t<li><strong>train.csv</strong>: 6469 x 2</li>\n",
      "</ul>\n",
      "\n",
      "### Acknowledgements\n",
      "\n",
      "**Link to Contest:** https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/\n",
      "\n",
      "name 'detect' is not defined Language is not detected: abdelrahman16/car-data\n",
      "name 'detect' is not defined Language is not detected: car-data\n",
      "name 'detect' is not defined Language is not detected: abdelrahman16\n",
      "name 'detect' is not defined Language is not detected: car_data\n",
      "name 'detect' is not defined Language is not detected: 🔍 Dataset Overview:\n",
      "\n",
      "🔢 Unnamed: 0: Unique identifier for each entry.\n",
      "\n",
      "🚗 Brand: The brand of the car (e.g., Ford, Hyundai, Audi).\n",
      "\n",
      "🚗 Model: The specific model of the car.\n",
      "\n",
      "🎨 Color: The color of the car.\n",
      "\n",
      "📅 Registration Date: The date when the car was registered.\n",
      "\n",
      "📅 Year: The manufacturing year of the car.\n",
      "\n",
      "💶 Price in Euro: The price of the car in euros.\n",
      "\n",
      "⚡ Power (kW): The power of the car's engine in kilowatts.\n",
      "\n",
      "⚡ Power (PS): The power of the car's engine in horsepower.\n",
      "\n",
      "🔄 Transmission Type: The type of transmission (e.g., Automatic, Manual).\n",
      "\n",
      "⛽ Fuel Type: The type of fuel used by the car (e.g., Petrol, Diesel, Electric).\n",
      "\n",
      "⛽ Fuel Consumption (L/100km): The fuel consumption in liters per 100 kilometers.\n",
      "\n",
      "⛽ Fuel Consumption (g/km): The fuel consumption in grams per kilometer.\n",
      "\n",
      "🛣️ Mileage (km): The total distance the car has traveled, in kilometers.\n",
      "\n",
      "📜 Offer Description: Additional description of the car's features and conditions.\n",
      "\n",
      "\n",
      "# Steps to Build the Prediction Model:\n",
      "\n",
      "## Data Preprocessing:\n",
      "\n",
      "1-Handle Missing Values:\n",
      "- Check for and handle missing values appropriately (e.g., imputation, removal).\n",
      "\n",
      "2-Convert Data Types:\n",
      "- Ensure all numerical values are in the correct format (e.g., price_in_euro, power_kw, mileage_in_km).\n",
      "\n",
      "3-Handle Categorical Variables:\n",
      "- Convert categorical variables into numerical values using techniques like one-hot encoding or label encoding (e.g., brand, model, color).\n",
      "\n",
      "## Feature Selection:\n",
      "\n",
      "1-Correlation Analysis:\n",
      "- Use correlation analysis to identify features that have a strong relationship with the target variable (price_in_euro).\n",
      "\n",
      "2-Feature Importance:\n",
      "- Use feature importance from tree-based models (e.g., Random Forest) to select the most relevant features.\n",
      "\n",
      "## Model Selection:\n",
      "\n",
      "1-Algorithm Choice:\n",
      "- Choose appropriate machine learning algorithms (e.g., Linear Regression, Decision Tree, Random Forest, Gradient Boosting).\n",
      "\n",
      "2-Model Comparison:\n",
      "- Train multiple models and compare their performance using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
      "\n",
      "## Model Training and Evaluation:\n",
      "\n",
      "1-Train the Model:\n",
      "- Split the data into training and testing sets.\n",
      "- Train the selected model(s) on the training set.\n",
      "\n",
      "2-Evaluate the Model:\n",
      "- Evaluate model performance on the testing set.\n",
      "- Fine-tune the model based on evaluation metrics.\n",
      "\n",
      "This comprehensive dataset and prediction model are valuable for anyone interested in automotive market analysis, price prediction, and understanding vehicle valuation. Happy modeling! 🚀\n",
      "\n",
      "Please upvote if you find this helpful! 👍\n",
      "\n",
      "https://www.kaggle.com/code/abdelrahman16/car-data-cleaning-visualization\n",
      "name 'detect' is not defined Language is not detected: car-data\n",
      "name 'detect' is not defined Language is not detected: abdelrahman16\n",
      "name 'detect' is not defined Language is not detected: car_data\n",
      "name 'detect' is not defined Language is not detected: 🔍 Dataset Overview:\n",
      "\n",
      "🔢 Unnamed: 0: Unique identifier for each entry.\n",
      "\n",
      "🚗 Brand: The brand of the car (e.g., Ford, Hyundai, Audi).\n",
      "\n",
      "🚗 Model: The specific model of the car.\n",
      "\n",
      "🎨 Color: The color of the car.\n",
      "\n",
      "📅 Registration Date: The date when the car was registered.\n",
      "\n",
      "📅 Year: The manufacturing year of the car.\n",
      "\n",
      "💶 Price in Euro: The price of the car in euros.\n",
      "\n",
      "⚡ Power (kW): The power of the car's engine in kilowatts.\n",
      "\n",
      "⚡ Power (PS): The power of the car's engine in horsepower.\n",
      "\n",
      "🔄 Transmission Type: The type of transmission (e.g., Automatic, Manual).\n",
      "\n",
      "⛽ Fuel Type: The type of fuel used by the car (e.g., Petrol, Diesel, Electric).\n",
      "\n",
      "⛽ Fuel Consumption (L/100km): The fuel consumption in liters per 100 kilometers.\n",
      "\n",
      "⛽ Fuel Consumption (g/km): The fuel consumption in grams per kilometer.\n",
      "\n",
      "🛣️ Mileage (km): The total distance the car has traveled, in kilometers.\n",
      "\n",
      "📜 Offer Description: Additional description of the car's features and conditions.\n",
      "\n",
      "\n",
      "# Steps to Build the Prediction Model:\n",
      "\n",
      "## Data Preprocessing:\n",
      "\n",
      "1-Handle Missing Values:\n",
      "- Check for and handle missing values appropriately (e.g., imputation, removal).\n",
      "\n",
      "2-Convert Data Types:\n",
      "- Ensure all numerical values are in the correct format (e.g., price_in_euro, power_kw, mileage_in_km).\n",
      "\n",
      "3-Handle Categorical Variables:\n",
      "- Convert categorical variables into numerical values using techniques like one-hot encoding or label encoding (e.g., brand, model, color).\n",
      "\n",
      "## Feature Selection:\n",
      "\n",
      "1-Correlation Analysis:\n",
      "- Use correlation analysis to identify features that have a strong relationship with the target variable (price_in_euro).\n",
      "\n",
      "2-Feature Importance:\n",
      "- Use feature importance from tree-based models (e.g., Random Forest) to select the most relevant features.\n",
      "\n",
      "## Model Selection:\n",
      "\n",
      "1-Algorithm Choice:\n",
      "- Choose appropriate machine learning algorithms (e.g., Linear Regression, Decision Tree, Random Forest, Gradient Boosting).\n",
      "\n",
      "2-Model Comparison:\n",
      "- Train multiple models and compare their performance using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
      "\n",
      "## Model Training and Evaluation:\n",
      "\n",
      "1-Train the Model:\n",
      "- Split the data into training and testing sets.\n",
      "- Train the selected model(s) on the training set.\n",
      "\n",
      "2-Evaluate the Model:\n",
      "- Evaluate model performance on the testing set.\n",
      "- Fine-tune the model based on evaluation metrics.\n",
      "\n",
      "This comprehensive dataset and prediction model are valuable for anyone interested in automotive market analysis, price prediction, and understanding vehicle valuation. Happy modeling! 🚀\n",
      "\n",
      "Please upvote if you find this helpful! 👍\n",
      "\n",
      "https://www.kaggle.com/code/abdelrahman16/car-data-cleaning-visualization\n",
      "name 'detect' is not defined Language is not detected: thekenjin/amazonapparelsdata\n",
      "name 'detect' is not defined Language is not detected: amazonapparelsdata\n",
      "name 'detect' is not defined Language is not detected: thekenjin\n",
      "name 'detect' is not defined Language is not detected: Amazon apparels data review\n",
      "name 'detect' is not defined Language is not detected:  amazon women's clothing products and clothing product reviews\n",
      "name 'detect' is not defined Language is not detected: Online E-commerce websites like Amazon, Filpkart uses different recommendation models to provide different suggestions to different users. Amazon currently uses item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in real time. This type of filtering matches each of the user's purchased and rated items to similar items, then combines those similar items into a recommendation list for the user. In this project we are going to build recommendation model for the electronics products of Amazon.\n",
      "\n",
      "The dataset here is taken from the below website.\n",
      "\n",
      "Source - Amazon Reviews data (http://jmcauley.ucsd.edu/data/amazon/) The repository has several datasets. For this case study, we are using the Electronics dataset.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: amazonapparelsdata\n",
      "name 'detect' is not defined Language is not detected: thekenjin\n",
      "name 'detect' is not defined Language is not detected: Amazon apparels data review\n",
      "name 'detect' is not defined Language is not detected:  amazon women's clothing products and clothing product reviews\n",
      "name 'detect' is not defined Language is not detected: Online E-commerce websites like Amazon, Filpkart uses different recommendation models to provide different suggestions to different users. Amazon currently uses item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in real time. This type of filtering matches each of the user's purchased and rated items to similar items, then combines those similar items into a recommendation list for the user. In this project we are going to build recommendation model for the electronics products of Amazon.\n",
      "\n",
      "The dataset here is taken from the below website.\n",
      "\n",
      "Source - Amazon Reviews data (http://jmcauley.ucsd.edu/data/amazon/) The repository has several datasets. For this case study, we are using the Electronics dataset.\n",
      "\n",
      "\n",
      "name 'detect' is not defined Language is not detected: sanjaysuthraye96/uber-location-price-data\n",
      "name 'detect' is not defined Language is not detected: uber-location-price-data\n",
      "name 'detect' is not defined Language is not detected: sanjaysuthraye96\n",
      "name 'detect' is not defined Language is not detected: Uber location price data\n",
      "name 'detect' is not defined Language is not detected: Price prediction based on Geolocation\n",
      "name 'detect' is not defined Language is not detected: uber-location-price-data\n",
      "name 'detect' is not defined Language is not detected: sanjaysuthraye96\n",
      "name 'detect' is not defined Language is not detected: Uber location price data\n",
      "name 'detect' is not defined Language is not detected: Price prediction based on Geolocation\n",
      "name 'detect' is not defined Language is not detected: kriegsmaschine/soccer-players-values-and-their-statistics\n",
      "name 'detect' is not defined Language is not detected: soccer-players-values-and-their-statistics\n",
      "name 'detect' is not defined Language is not detected: kriegsmaschine\n",
      "name 'detect' is not defined Language is not detected: Soccer players values and their statistics\n",
      "name 'detect' is not defined Language is not detected: transfermarkt.de and fbref.com merged data for 2017/18-2019/20 seasons\n",
      "name 'detect' is not defined Language is not detected: Merged data from transfermarkt.de and fbref.com regarding soccer players from top 5 European leagues, used for my bachelor's thesis \"Modelling Football Players Values and Their Determinants on a Transfer Market using Robust Regression Models'. More details on:\n",
      " https://github.com/RSKriegs/Modelling-Football-Players-Values-on-Transfer-Market-and-Their-Determinants-using-Robust-Regression\n",
      "\n",
      "name 'detect' is not defined Language is not detected: soccer-players-values-and-their-statistics\n",
      "name 'detect' is not defined Language is not detected: kriegsmaschine\n",
      "name 'detect' is not defined Language is not detected: Soccer players values and their statistics\n",
      "name 'detect' is not defined Language is not detected: transfermarkt.de and fbref.com merged data for 2017/18-2019/20 seasons\n",
      "name 'detect' is not defined Language is not detected: Merged data from transfermarkt.de and fbref.com regarding soccer players from top 5 European leagues, used for my bachelor's thesis \"Modelling Football Players Values and Their Determinants on a Transfer Market using Robust Regression Models'. More details on:\n",
      " https://github.com/RSKriegs/Modelling-Football-Players-Values-on-Transfer-Market-and-Their-Determinants-using-Robust-Regression\n",
      "\n",
      "name 'detect' is not defined Language is not detected: ahmedshahriarsakib/usa-real-estate-dataset\n",
      "name 'detect' is not defined Language is not detected: usa-real-estate-dataset\n",
      "name 'detect' is not defined Language is not detected: ahmedshahriarsakib\n",
      "name 'detect' is not defined Language is not detected: USA Real Estate Dataset\n",
      "name 'detect' is not defined Language is not detected: Real Estate listings (2.2M+) in the US broken by State and zip code\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "\n",
      "This dataset contains Real Estate listings in the US broken by State and zip code.\n",
      "\n",
      "## Download\n",
      "\n",
      "kaggle API Command\n",
      "```\n",
      "!kaggle datasets download -d ahmedshahriarsakib/usa-real-estate-dataset\n",
      "```\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset has 1 CSV file with 10 columns -\n",
      "\n",
      "1. realtor-data.csv (2,226,382 entries)\n",
      "  - brokered by (categorically encoded agency/broker)\n",
      "  - status (Housing status - a. ready for sale or b. ready to build) \n",
      "  - price (Housing price, it is either the current listing price or recently sold price if the house is sold recently)\n",
      "  - bed (# of beds)\n",
      "  - bath (# of bathrooms)\n",
      "  - acre_lot (Property / Land size in acres)\n",
      "  - street (categorically encoded street address)\n",
      "  - city (city name)\n",
      "  - state (state name)\n",
      "  - zip_code (postal code of the area)\n",
      "  - house_size (house area/size/living space  in square feet)\n",
      "  - prev_sold_date (Previously sold date)\n",
      "\n",
      "\n",
      "NB: \n",
      "1. `brokered by` and `street` addresses were categorically encoded due to privacy policy \n",
      "2. `acre_lot` means the total land area, and ` house_size` denotes the living space/building area\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "Data was collected from -\n",
      "- [https://www.realtor.com/](https://www.realtor.com/) - A real estate listing website operated by the News Corp subsidiary Move, Inc. and based in Santa Clara, California. It is the second most visited real estate listing website in the United States as of 2024, with over 100 million monthly active users. \n",
      "\n",
      "\n",
      "## Cover Image\n",
      "\n",
      "Image by <a href=\"https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3261160\">Mohamed Hassan</a> from <a href=\"https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3261160\">Pixabay</a>\n",
      "\n",
      "## Disclaimer\n",
      "\n",
      "The data and information in the data set provided here are intended to use for educational purposes only. I do not own any data, and all rights are reserved to the respective owners.\n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Can we predict housing prices based on the features?\n",
      "- How are housing price and location attributes correlated?\n",
      "- What is the overall picture of the USA housing prices w.r.t. locations?\n",
      "- Do house attributes (bedroom, bathroom count) strongly correlate with the price? Are there any hidden patterns?\n",
      "name 'detect' is not defined Language is not detected: usa-real-estate-dataset\n",
      "name 'detect' is not defined Language is not detected: ahmedshahriarsakib\n",
      "name 'detect' is not defined Language is not detected: USA Real Estate Dataset\n",
      "name 'detect' is not defined Language is not detected: Real Estate listings (2.2M+) in the US broken by State and zip code\n",
      "name 'detect' is not defined Language is not detected: ## Context\n",
      "\n",
      "This dataset contains Real Estate listings in the US broken by State and zip code.\n",
      "\n",
      "## Download\n",
      "\n",
      "kaggle API Command\n",
      "```\n",
      "!kaggle datasets download -d ahmedshahriarsakib/usa-real-estate-dataset\n",
      "```\n",
      "\n",
      "### Content\n",
      "\n",
      "The dataset has 1 CSV file with 10 columns -\n",
      "\n",
      "1. realtor-data.csv (2,226,382 entries)\n",
      "  - brokered by (categorically encoded agency/broker)\n",
      "  - status (Housing status - a. ready for sale or b. ready to build) \n",
      "  - price (Housing price, it is either the current listing price or recently sold price if the house is sold recently)\n",
      "  - bed (# of beds)\n",
      "  - bath (# of bathrooms)\n",
      "  - acre_lot (Property / Land size in acres)\n",
      "  - street (categorically encoded street address)\n",
      "  - city (city name)\n",
      "  - state (state name)\n",
      "  - zip_code (postal code of the area)\n",
      "  - house_size (house area/size/living space  in square feet)\n",
      "  - prev_sold_date (Previously sold date)\n",
      "\n",
      "\n",
      "NB: \n",
      "1. `brokered by` and `street` addresses were categorically encoded due to privacy policy \n",
      "2. `acre_lot` means the total land area, and ` house_size` denotes the living space/building area\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "Data was collected from -\n",
      "- [https://www.realtor.com/](https://www.realtor.com/) - A real estate listing website operated by the News Corp subsidiary Move, Inc. and based in Santa Clara, California. It is the second most visited real estate listing website in the United States as of 2024, with over 100 million monthly active users. \n",
      "\n",
      "\n",
      "## Cover Image\n",
      "\n",
      "Image by <a href=\"https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3261160\">Mohamed Hassan</a> from <a href=\"https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3261160\">Pixabay</a>\n",
      "\n",
      "## Disclaimer\n",
      "\n",
      "The data and information in the data set provided here are intended to use for educational purposes only. I do not own any data, and all rights are reserved to the respective owners.\n",
      "\n",
      "\n",
      "## Inspiration\n",
      "\n",
      "- Can we predict housing prices based on the features?\n",
      "- How are housing price and location attributes correlated?\n",
      "- What is the overall picture of the USA housing prices w.r.t. locations?\n",
      "- Do house attributes (bedroom, bathroom count) strongly correlate with the price? Are there any hidden patterns?\n",
      "name 'detect' is not defined Language is not detected: justinnguyen0x0x/best-books-of-the-21st-century-dataset\n",
      "name 'detect' is not defined Language is not detected: best-books-of-the-21st-century-dataset\n",
      "name 'detect' is not defined Language is not detected: justinnguyen0x0x\n",
      "name 'detect' is not defined Language is not detected: Best Books of The 21st Century Dataset\n",
      "name 'detect' is not defined Language is not detected: Over 9000 books of the 21st century on goodreads\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "\n",
      "Goodreads is the world’s largest site for readers and book recommendations. Using it, you can track the books you are reading, have read, and want to read.\n",
      "It also has a recommendation system that analyzes 20 billion data points to give suggestions tailored to your literary tastes.\n",
      "This dataset contains over 9000 different books with various genres, taken from the Best Books of 21st century list on goodreads\n",
      "\n",
      "\n",
      "# Content\n",
      "###This file contains 14 columns\n",
      "id: id of the book\n",
      "title: book's title\n",
      "series: book's series. If the book doesn't belong to any series the value will be null\n",
      "author: author of the book\n",
      "book\\_link: book's URL on GoodReads\n",
      "genre: genres of the book (ordered by number of genre votes)\n",
      "date\\_published: published date\n",
      "publisher: publisher of the book\n",
      "num\\_of\\_page: number of pages\n",
      "lang: language of the book\n",
      "review\\_count: number of reviews\n",
      "rating\\_count: number of ratings\n",
      "rate: rating\n",
      "award: awards of the book\n",
      "\n",
      "# Preprocessing suggestions\n",
      "series: if doesn't belong to a series -&gt; 'Single'\n",
      "genre: take the first genre (the most voted genre by user) or take the top 3 genres\n",
      "date\\_published: some of them contains only month or year -&gt; convert all to year\n",
      "award: you can count the number of awards, split by comma\n",
      "name 'detect' is not defined Language is not detected: best-books-of-the-21st-century-dataset\n",
      "name 'detect' is not defined Language is not detected: justinnguyen0x0x\n",
      "name 'detect' is not defined Language is not detected: Best Books of The 21st Century Dataset\n",
      "name 'detect' is not defined Language is not detected: Over 9000 books of the 21st century on goodreads\n",
      "name 'detect' is not defined Language is not detected: # Context\n",
      "\n",
      "Goodreads is the world’s largest site for readers and book recommendations. Using it, you can track the books you are reading, have read, and want to read.\n",
      "It also has a recommendation system that analyzes 20 billion data points to give suggestions tailored to your literary tastes.\n",
      "This dataset contains over 9000 different books with various genres, taken from the Best Books of 21st century list on goodreads\n",
      "\n",
      "\n",
      "# Content\n",
      "###This file contains 14 columns\n",
      "id: id of the book\n",
      "title: book's title\n",
      "series: book's series. If the book doesn't belong to any series the value will be null\n",
      "author: author of the book\n",
      "book\\_link: book's URL on GoodReads\n",
      "genre: genres of the book (ordered by number of genre votes)\n",
      "date\\_published: published date\n",
      "publisher: publisher of the book\n",
      "num\\_of\\_page: number of pages\n",
      "lang: language of the book\n",
      "review\\_count: number of reviews\n",
      "rating\\_count: number of ratings\n",
      "rate: rating\n",
      "award: awards of the book\n",
      "\n",
      "# Preprocessing suggestions\n",
      "series: if doesn't belong to a series -&gt; 'Single'\n",
      "genre: take the first genre (the most voted genre by user) or take the top 3 genres\n",
      "date\\_published: some of them contains only month or year -&gt; convert all to year\n",
      "award: you can count the number of awards, split by comma\n",
      "name 'detect' is not defined Language is not detected: taniadh/skin-care\n",
      "name 'detect' is not defined Language is not detected: skin-care\n",
      "name 'detect' is not defined Language is not detected: taniadh\n",
      "name 'detect' is not defined Language is not detected: Skin Care\n",
      "name 'detect' is not defined Language is not detected: Korean Skincare product lists and reviews\n",
      "name 'detect' is not defined Language is not detected: **Motive**\n",
      "This data was created to build a knowledge-based recommendation system. Where user will give their preferences and they get recommended products based on their similarity to top rated products reviews. I attempted to solve the problem of cold-start when there is no new customer data available. This data can also be used for content-based recommendation system, sentiment analysis etc.,\n",
      "\n",
      "**Data**\n",
      "The data is relational data with two files: \n",
      "1. **productlist.csv** with following attributes: product ID, product name, product brand, price, description and product type\n",
      "2.  **productReviews.csv** with following attributes: product ID and review\n",
      "\n",
      "**Source**\n",
      "The data was web scraped from https://sokoglam.com/\n",
      "name 'detect' is not defined Language is not detected: skin-care\n",
      "name 'detect' is not defined Language is not detected: taniadh\n",
      "name 'detect' is not defined Language is not detected: Skin Care\n",
      "name 'detect' is not defined Language is not detected: Korean Skincare product lists and reviews\n",
      "name 'detect' is not defined Language is not detected: **Motive**\n",
      "This data was created to build a knowledge-based recommendation system. Where user will give their preferences and they get recommended products based on their similarity to top rated products reviews. I attempted to solve the problem of cold-start when there is no new customer data available. This data can also be used for content-based recommendation system, sentiment analysis etc.,\n",
      "\n",
      "**Data**\n",
      "The data is relational data with two files: \n",
      "1. **productlist.csv** with following attributes: product ID, product name, product brand, price, description and product type\n",
      "2.  **productReviews.csv** with following attributes: product ID and review\n",
      "\n",
      "**Source**\n",
      "The data was web scraped from https://sokoglam.com/\n",
      "name 'detect' is not defined Language is not detected: yasserh/uber-fares-dataset\n",
      "name 'detect' is not defined Language is not detected: uber-fares-dataset\n",
      "name 'detect' is not defined Language is not detected: yasserh\n",
      "name 'detect' is not defined Language is not detected: Uber Fares Dataset\n",
      "name 'detect' is not defined Language is not detected: Can you predict the fare for Uber Rides - Regression Problem\n",
      "name 'detect' is not defined Language is not detected: ![](https://raw.githubusercontent.com/Masterx-AI/Project_Uber_Fare_Prediction/main/Uber1.jpg)\n",
      "\n",
      "### Description:\n",
      "\n",
      "The project is about on world's largest taxi company Uber inc. In this project, we're looking to predict the fare for their future transactional cases. Uber delivers service to lakhs of customers daily. Now it becomes really important to manage their data properly to come up with new business ideas to get best results. Eventually, it becomes really important to estimate the fare prices accurately.\n",
      "\n",
      "### The datset contains the following fields:\n",
      "* key - a unique identifier for each trip\n",
      "* fare_amount - the cost of each trip in usd\n",
      "* pickup_datetime - date and time when the meter was engaged\n",
      "* passenger_count - the number of passengers in the vehicle (driver entered value)\n",
      "* pickup_longitude - the longitude where the meter was engaged\n",
      "* pickup_latitude - the latitude where the meter was engaged\n",
      "* dropoff_longitude - the longitude where the meter was disengaged\n",
      "* dropoff_latitude - the latitude where the meter was disengaged\n",
      "\n",
      "### Acknowledgement: \n",
      "The dataset is referred from Kaggle.\n",
      "\n",
      "### Objective:\n",
      "- Understand the Dataset & cleanup (if required).\n",
      "- Build Regression models to predict the fare price of uber ride.\n",
      "- Also evaluate the models & compare thier respective scores like R2, RMSE, etc.\n",
      "name 'detect' is not defined Language is not detected: uber-fares-dataset\n",
      "name 'detect' is not defined Language is not detected: yasserh\n",
      "name 'detect' is not defined Language is not detected: Uber Fares Dataset\n",
      "name 'detect' is not defined Language is not detected: Can you predict the fare for Uber Rides - Regression Problem\n",
      "name 'detect' is not defined Language is not detected: ![](https://raw.githubusercontent.com/Masterx-AI/Project_Uber_Fare_Prediction/main/Uber1.jpg)\n",
      "\n",
      "### Description:\n",
      "\n",
      "The project is about on world's largest taxi company Uber inc. In this project, we're looking to predict the fare for their future transactional cases. Uber delivers service to lakhs of customers daily. Now it becomes really important to manage their data properly to come up with new business ideas to get best results. Eventually, it becomes really important to estimate the fare prices accurately.\n",
      "\n",
      "### The datset contains the following fields:\n",
      "* key - a unique identifier for each trip\n",
      "* fare_amount - the cost of each trip in usd\n",
      "* pickup_datetime - date and time when the meter was engaged\n",
      "* passenger_count - the number of passengers in the vehicle (driver entered value)\n",
      "* pickup_longitude - the longitude where the meter was engaged\n",
      "* pickup_latitude - the latitude where the meter was engaged\n",
      "* dropoff_longitude - the longitude where the meter was disengaged\n",
      "* dropoff_latitude - the latitude where the meter was disengaged\n",
      "\n",
      "### Acknowledgement: \n",
      "The dataset is referred from Kaggle.\n",
      "\n",
      "### Objective:\n",
      "- Understand the Dataset & cleanup (if required).\n",
      "- Build Regression models to predict the fare price of uber ride.\n",
      "- Also evaluate the models & compare thier respective scores like R2, RMSE, etc.\n",
      "name 'detect' is not defined Language is not detected: yasserh/nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: yasserh\n",
      "name 'detect' is not defined Language is not detected: NYC Taxi Trip Duration\n",
      "name 'detect' is not defined Language is not detected: Predict the NYC Taxi Trip Duration - Intermediate ML Project\n",
      "name 'detect' is not defined Language is not detected: ![](https://raw.githubusercontent.com/Masterx-AI/Project_NYC_Taxi_Trip_Duration_Prediction/main/taxi.jpg)\n",
      "\n",
      "### Description:\n",
      "\n",
      "The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set.\n",
      "\n",
      "The datset contains the following fields:\n",
      "id - a unique identifier for each trip\n",
      "vendor_id - a code indicating the provider associated with the trip record\n",
      "pickup_datetime - date and time when the meter was engaged\n",
      "dropoff_datetime - date and time when the meter was disengaged\n",
      "passenger_count - the number of passengers in the vehicle (driver entered value)\n",
      "pickup_longitude - the longitude where the meter was engaged\n",
      "pickup_latitude - the latitude where the meter was engaged\n",
      "dropoff_longitude - the longitude where the meter was disengaged\n",
      "dropoff_latitude - the latitude where the meter was disengaged\n",
      "store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n",
      "trip_duration - duration of the trip in seconds\n",
      "\n",
      "### Acknowledgement: \n",
      "The dataset is taken from Kaggle:\\\n",
      "https://www.kaggle.com/c/nyc-taxi-trip-duration/data\n",
      "\n",
      "### Objective:\n",
      "- Understand the Dataset & cleanup (if required).\n",
      "- Build Regression models to predict the duration of taxi trip.\n",
      "- Also evaluate the models & compare thier respective scores like R2, RMSE, etc.!\n",
      "name 'detect' is not defined Language is not detected: nyc-taxi-trip-duration\n",
      "name 'detect' is not defined Language is not detected: yasserh\n",
      "name 'detect' is not defined Language is not detected: NYC Taxi Trip Duration\n",
      "name 'detect' is not defined Language is not detected: Predict the NYC Taxi Trip Duration - Intermediate ML Project\n",
      "name 'detect' is not defined Language is not detected: ![](https://raw.githubusercontent.com/Masterx-AI/Project_NYC_Taxi_Trip_Duration_Prediction/main/taxi.jpg)\n",
      "\n",
      "### Description:\n",
      "\n",
      "The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set.\n",
      "\n",
      "The datset contains the following fields:\n",
      "id - a unique identifier for each trip\n",
      "vendor_id - a code indicating the provider associated with the trip record\n",
      "pickup_datetime - date and time when the meter was engaged\n",
      "dropoff_datetime - date and time when the meter was disengaged\n",
      "passenger_count - the number of passengers in the vehicle (driver entered value)\n",
      "pickup_longitude - the longitude where the meter was engaged\n",
      "pickup_latitude - the latitude where the meter was engaged\n",
      "dropoff_longitude - the longitude where the meter was disengaged\n",
      "dropoff_latitude - the latitude where the meter was disengaged\n",
      "store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n",
      "trip_duration - duration of the trip in seconds\n",
      "\n",
      "### Acknowledgement: \n",
      "The dataset is taken from Kaggle:\\\n",
      "https://www.kaggle.com/c/nyc-taxi-trip-duration/data\n",
      "\n",
      "### Objective:\n",
      "- Understand the Dataset & cleanup (if required).\n",
      "- Build Regression models to predict the duration of taxi trip.\n",
      "- Also evaluate the models & compare thier respective scores like R2, RMSE, etc.!\n",
      "name 'detect' is not defined Language is not detected: wsdbqwdbqwd/virat-kholi-and-dhoni-image-dataset\n",
      "name 'detect' is not defined Language is not detected: virat-kholi-and-dhoni-image-dataset\n",
      "name 'detect' is not defined Language is not detected: wsdbqwdbqwd\n",
      "name 'detect' is not defined Language is not detected: viratkholi and dhoni image dataset\n",
      "name 'detect' is not defined Language is not detected: Image Classification: The dataset can be used to build an image classification model that can identify and distinguish between the two cricket celebrities. This could be valuable for fan engagement, media analysis, or even for fun interactive applications.\n",
      "\n",
      "Facial Recognition: Using the dataset, you can develop a facial recognition system that can identify the cricket celebrities' faces in different images or videos. This could be applied in security systems, event management, or social media monitoring.\n",
      "\n",
      "Sentiment Analysis: By collecting images of the cricket celebrities from various sources, you can create a dataset for sentiment analysis. Analyzing people's reactions based on their facial expressions during matches, interviews, or events can provide insights into their popularity and public perception.\n",
      "\n",
      "Image Generation: You can use the dataset to build a generative model that can create new images of the cricket celebrities. This could be used in the entertainment industry, creating custom merchandise, or for generating personalized content for fans.\n",
      "\n",
      "Data Augmentation: The dataset can be utilized to augment other image datasets that might be limited in size. This technique can improve the performance of machine learning models by increasing the diversity of the training data.\n",
      "\n",
      "Social Media Analytics: Analyzing images shared on social media platforms featuring the cricket celebrities can provide valuable information about their popularity, fan engagement, and reach. It can also help in detecting and tracking trends related to these celebrities.\n",
      "\n",
      "Celebrity Endorsement Analysis: Using the dataset, you can study the impact of celebrity endorsements on various products and brands. By analyzing the response and engagement levels of consumers, companies can make more informed decisions about celebrity endorsements.\n",
      "\n",
      "Image Retrieval: Building an image retrieval system using the dataset can allow users to search for specific images of the cricket celebrities based on similarity or context. This could be helpful for media professionals or fans looking for specific moments or poses of the stars.\n",
      "\n",
      "Transfer Learning: The dataset can serve as a pre-trained model for transfer learning in other computer vision tasks. You can use the knowledge gained from this dataset to boost the performance of models on related tasks, such as sports image analysis or athlete recognition.\n",
      "\n",
      "Content Recommendation: Incorporating the dataset into a content recommendation system can help personalize content suggestions for users based on their interest in the cricket celebrities. This can be useful for media platforms or sports-related apps.\n",
      "name 'detect' is not defined Language is not detected: virat-kholi-and-dhoni-image-dataset\n",
      "name 'detect' is not defined Language is not detected: wsdbqwdbqwd\n",
      "name 'detect' is not defined Language is not detected: viratkholi and dhoni image dataset\n",
      "name 'detect' is not defined Language is not detected: Image Classification: The dataset can be used to build an image classification model that can identify and distinguish between the two cricket celebrities. This could be valuable for fan engagement, media analysis, or even for fun interactive applications.\n",
      "\n",
      "Facial Recognition: Using the dataset, you can develop a facial recognition system that can identify the cricket celebrities' faces in different images or videos. This could be applied in security systems, event management, or social media monitoring.\n",
      "\n",
      "Sentiment Analysis: By collecting images of the cricket celebrities from various sources, you can create a dataset for sentiment analysis. Analyzing people's reactions based on their facial expressions during matches, interviews, or events can provide insights into their popularity and public perception.\n",
      "\n",
      "Image Generation: You can use the dataset to build a generative model that can create new images of the cricket celebrities. This could be used in the entertainment industry, creating custom merchandise, or for generating personalized content for fans.\n",
      "\n",
      "Data Augmentation: The dataset can be utilized to augment other image datasets that might be limited in size. This technique can improve the performance of machine learning models by increasing the diversity of the training data.\n",
      "\n",
      "Social Media Analytics: Analyzing images shared on social media platforms featuring the cricket celebrities can provide valuable information about their popularity, fan engagement, and reach. It can also help in detecting and tracking trends related to these celebrities.\n",
      "\n",
      "Celebrity Endorsement Analysis: Using the dataset, you can study the impact of celebrity endorsements on various products and brands. By analyzing the response and engagement levels of consumers, companies can make more informed decisions about celebrity endorsements.\n",
      "\n",
      "Image Retrieval: Building an image retrieval system using the dataset can allow users to search for specific images of the cricket celebrities based on similarity or context. This could be helpful for media professionals or fans looking for specific moments or poses of the stars.\n",
      "\n",
      "Transfer Learning: The dataset can serve as a pre-trained model for transfer learning in other computer vision tasks. You can use the knowledge gained from this dataset to boost the performance of models on related tasks, such as sports image analysis or athlete recognition.\n",
      "\n",
      "Content Recommendation: Incorporating the dataset into a content recommendation system can help personalize content suggestions for users based on their interest in the cricket celebrities. This can be useful for media platforms or sports-related apps.\n",
      "name 'detect' is not defined Language is not detected: arbazkhan971/the-great-indian-hiring-hackathon\n",
      "name 'detect' is not defined Language is not detected: the-great-indian-hiring-hackathon\n",
      "name 'detect' is not defined Language is not detected: arbazkhan971\n",
      "name 'detect' is not defined Language is not detected: The Great Indian Hiring Hackathon\n",
      "name 'detect' is not defined Language is not detected: The Great Indian Hiring Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Overview\n",
      "The current pandemic has dwindled the data science job market likewise recruiters are also facing difficulties filtering the right talent. To bridge this gap we bring a chance for the MachineHack community to compete for jobs with some of the key analytics players for a rewarding career in Data Science. In this competition, we are challenging the MachineHack community to come up with an algorithm to predict the price of retail items belonging to different categories. Foretelling the Retail price can be a daunting task due to the huge datasets with a variety of attributes ranging from Text, Numbers(floats, integers), and DateTime. Also, outliers can be a big problem when dealing with unit prices.\n",
      "\n",
      "With a key focus on the Data Scientist role in an esteemed organization, this hackathon can help freshers and experienced folks prove their mettle and land up in a rewarding career.\n",
      "\n",
      "By participating in this hackathon, every participant will be eligible for the Data Scientist job role by making sure their MachineHack Information with Resume is up to date.\n",
      "\n",
      " \n",
      "\n",
      "### Dataset Description:\n",
      "Train.csv - 284780 rows x 8 columns (Inlcudes UnitPrice Columns as Target)\n",
      "Test.csv - 122049 rows x 7 columns\n",
      "Sample Submission.csv - Please check the Evaluation section for more details on how to generate a valid submission\n",
      " \n",
      "\n",
      "### Attribute Description:\n",
      "Invoice No - Invoice ID, encoded as Label\n",
      "StockCode - Unique code per stock, encoded as Label\n",
      "Description - The Description, encoded as Label\n",
      "Quantity - Quantity purchased\n",
      "InvoiceDate - Date of purchase\n",
      "UnitPrice - The target value, price of every product\n",
      "CustomerID - Unique Identifier for every country\n",
      "Country - Country of sales, encoded as Label\n",
      " \n",
      "\n",
      "### Skills:\n",
      "Multivariate Regression\n",
      "Big dataset, underfitting vs overfitting\n",
      "Optimizing RMSE to generalize well on unseen data\n",
      "### Acknowledgements\n",
      "This dataset is taken from Machine Hack challenge https://www.machinehack.com/hackathons/retail_price_prediction_mega_hiring_hackathon/overview\n",
      "\n",
      "name 'detect' is not defined Language is not detected: the-great-indian-hiring-hackathon\n",
      "name 'detect' is not defined Language is not detected: arbazkhan971\n",
      "name 'detect' is not defined Language is not detected: The Great Indian Hiring Hackathon\n",
      "name 'detect' is not defined Language is not detected: The Great Indian Hiring Hackathon\n",
      "name 'detect' is not defined Language is not detected: ### Overview\n",
      "The current pandemic has dwindled the data science job market likewise recruiters are also facing difficulties filtering the right talent. To bridge this gap we bring a chance for the MachineHack community to compete for jobs with some of the key analytics players for a rewarding career in Data Science. In this competition, we are challenging the MachineHack community to come up with an algorithm to predict the price of retail items belonging to different categories. Foretelling the Retail price can be a daunting task due to the huge datasets with a variety of attributes ranging from Text, Numbers(floats, integers), and DateTime. Also, outliers can be a big problem when dealing with unit prices.\n",
      "\n",
      "With a key focus on the Data Scientist role in an esteemed organization, this hackathon can help freshers and experienced folks prove their mettle and land up in a rewarding career.\n",
      "\n",
      "By participating in this hackathon, every participant will be eligible for the Data Scientist job role by making sure their MachineHack Information with Resume is up to date.\n",
      "\n",
      " \n",
      "\n",
      "### Dataset Description:\n",
      "Train.csv - 284780 rows x 8 columns (Inlcudes UnitPrice Columns as Target)\n",
      "Test.csv - 122049 rows x 7 columns\n",
      "Sample Submission.csv - Please check the Evaluation section for more details on how to generate a valid submission\n",
      " \n",
      "\n",
      "### Attribute Description:\n",
      "Invoice No - Invoice ID, encoded as Label\n",
      "StockCode - Unique code per stock, encoded as Label\n",
      "Description - The Description, encoded as Label\n",
      "Quantity - Quantity purchased\n",
      "InvoiceDate - Date of purchase\n",
      "UnitPrice - The target value, price of every product\n",
      "CustomerID - Unique Identifier for every country\n",
      "Country - Country of sales, encoded as Label\n",
      " \n",
      "\n",
      "### Skills:\n",
      "Multivariate Regression\n",
      "Big dataset, underfitting vs overfitting\n",
      "Optimizing RMSE to generalize well on unseen data\n",
      "### Acknowledgements\n",
      "This dataset is taken from Machine Hack challenge https://www.machinehack.com/hackathons/retail_price_prediction_mega_hiring_hackathon/overview\n",
      "\n",
      "name 'detect' is not defined Language is not detected: shalfey/extended-crab-age-prediction\n",
      "name 'detect' is not defined Language is not detected: extended-crab-age-prediction\n",
      "name 'detect' is not defined Language is not detected: shalfey\n",
      "name 'detect' is not defined Language is not detected: Extended Crab Age Prediction\n",
      "name 'detect' is not defined Language is not detected: Additional observations for Crab Age Prediction dataset (Playgrounds S3E16)\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "This dataset is synthetically generated for [Playground Season 3 Episode 16](https://www.kaggle.com/competitions/playground-series-s3e16/overview) competition. Original dataset is available on [Kaggle](https://www.kaggle.com/datasets/sidhus/crab-age-prediction?sort=published). 200000 of additional rows are available for use. It's recommended to concatenate it to competition train dataset, but avoid using it for validation, as it will yield overly optimistic cross-validation score.\n",
      "\n",
      "This data is shared for competition purposes only and is synthetically generated using [this code](https://www.kaggle.com/code/inversion/make-synthetic-crab-age-data). May contain noise (like numerical values in 'Sex' column or `height` of 0) due to its synthetic nature.\n",
      "name 'detect' is not defined Language is not detected: extended-crab-age-prediction\n",
      "name 'detect' is not defined Language is not detected: shalfey\n",
      "name 'detect' is not defined Language is not detected: Extended Crab Age Prediction\n",
      "name 'detect' is not defined Language is not detected: Additional observations for Crab Age Prediction dataset (Playgrounds S3E16)\n",
      "name 'detect' is not defined Language is not detected: **Context**\n",
      "\n",
      "This dataset is synthetically generated for [Playground Season 3 Episode 16](https://www.kaggle.com/competitions/playground-series-s3e16/overview) competition. Original dataset is available on [Kaggle](https://www.kaggle.com/datasets/sidhus/crab-age-prediction?sort=published). 200000 of additional rows are available for use. It's recommended to concatenate it to competition train dataset, but avoid using it for validation, as it will yield overly optimistic cross-validation score.\n",
      "\n",
      "This data is shared for competition purposes only and is synthetically generated using [this code](https://www.kaggle.com/code/inversion/make-synthetic-crab-age-data). May contain noise (like numerical values in 'Sex' column or `height` of 0) due to its synthetic nature.\n",
      "name 'detect' is not defined Language is not detected: lucifierx/movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: lucifierx\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation / Recommender system / Movies and Series Data\n",
      "name 'detect' is not defined Language is not detected: movie-recommendation\n",
      "name 'detect' is not defined Language is not detected: lucifierx\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation\n",
      "name 'detect' is not defined Language is not detected: Movie Recommendation / Recommender system / Movies and Series Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from gensim.corpora import Dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "# collect values: {\"key_name\":[\"number of reccords with key\",\"number of empty recpords\",\n",
    "                                # \"max_size\",\"min_size\",\"avg_size\"]}\n",
    "used_keys = defaultdict(lambda:[0,0,\n",
    "                               1e10,-1,0])\n",
    "\n",
    "tags = set()\n",
    "languages = defaultdict(str)\n",
    "# vocabulary = Dictionary()\n",
    "\n",
    "for file_path in data_index.path_metadata:\n",
    "    file_dict = load_metadata(file_path)\n",
    "    for k in file_dict.keys():\n",
    "        # number of records and number of missing \n",
    "        used_keys[k][0] += 1\n",
    "        if len(str(file_dict[k]))==0:\n",
    "            used_keys[k][1] += 1\n",
    "        \n",
    "        # number of words\n",
    "        if len(str(file_dict[k]))!=0:\n",
    "            current_seq_len = max( len(str(file_dict[k]).split(\" \")),\n",
    "                                   len(str(file_dict[k]).split(\"-\")),\n",
    "                                   len(str(file_dict[k]).split(\",\"))\n",
    "                                 )\n",
    "            # current_seq_len = len(str(file_dict[k]).split(\" \"))\n",
    "            \n",
    "            used_keys[k][2] = min(used_keys[k][2],current_seq_len)\n",
    "            used_keys[k][3] = max(used_keys[k][3],current_seq_len)\n",
    "            used_keys[k][4] += current_seq_len\n",
    "        \n",
    "        # language detection\n",
    "        if type(file_dict[k]) is str and len(file_dict[k])>0:\n",
    "            try:\n",
    "                detected_lang = detect(file_dict[k])\n",
    "                languages[detected_lang] = [file_dict[k],file_path]\n",
    "            except Exception  as e:\n",
    "                print(e,\"Language is not detected:\",file_dict[k])\n",
    "        \n",
    "        for word in file_dict['keywords']:\n",
    "            tags.add(word)\n",
    "        \n",
    "    \n",
    "    # vocabulary.add_documents()\n",
    "\n",
    "used_keys = dict(used_keys)\n",
    "used_keys = pd.DataFrame(used_keys)\n",
    "used_keys = used_keys.T.reset_index()\n",
    "used_keys.columns = [\"key_name\",\"number of reccords with key\",\"number of empty recpords\",\"min_size\",\"max_size\",\"avg_size\"]\n",
    "used_keys[\"avg_size\"] = used_keys[\"avg_size\"]/(659 - used_keys[\"number of empty recpords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b333c7-e99c-4a3a-9b3f-5be65627be65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c613d8b6-77d9-4a0a-885d-0c1cbec4537d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'en': [\"I have gathered this dataset  by scraping the rolling page of the steam search site: https://store.steampowered.com/search/?category1=998&ndl=1&ignore_preferences=1\\nThe data has been scraped in early September.\\nThe data is unorganized and needs cleaning.\\n\\nIf you want to see how I created recommendation system below you can read the description and ordering of notebooks. (check notebooks by owner)\\n\\nNames of notebooks:\\n- Preprocessing\\n- EDA of preprocessed data\\n- ML_Analysis_Main\\n- ML_Analysis _improving\\n- Recommendation System - Hybrid (the main target)\\n- Recommendations system Item-based (just for example)\\n\\nA small description\\n\\nWe started by scraping a video game platform steam. We used a rolling page in which the games were loaded endlessly while scrolling until games ran out. The order is less important. In the first scraping phase, we obtained Game data, namely name, price, discounted price, release date, and Link. Later, using the link, we expanded the data and extracted more information for each game. We merged the two datasets into a single file (which is uploaded here) and moved to the next step.\\n\\nIn the second stage, we did preprocessing before we started EDA directly. This could have happened during the EDA, but it was possible to carry out certain manipulations from the beginning. We fixed what we could easily see before EDA and then moved to EDA. This phase was very important because several features were removed and added, new ones were created etc.\\n\\nEDA describes a lot of things about data that happened\\nCleaned, sorted, and visualized. Finally, data was prepared for analysis.\\n\\nFirst in the analysis phase\\nThe Jupiter notebook I worked in is Analysis_Main, where I made the main \\nmachine-learning manipulations and finally got the game_recommendations CSV file.\\nOne cell in this file returns an error, which I intentionally left as is and indicated why.\\n\\nThe next file is Analysis – improving, where the game_recommendations created in the previous notebook\\nwas used and improved into the file game_recommendations2. (improved the model)\\n\\nAfter that, the games were already grouped and we got the data\\nwhich showed 10 recommendations per game. With this, the system can be considered somewhat complete, but I decided to use the user's data for testing purposes. I took the data from Kaggle, just for testing purposes (scraping steam by a customer was completely limited). This file is called steam-200k where there is data on users. Recommendation System - Hybrid is the file in which the main test is used.\\nThe file is hybrid, which means that a collaborative and content-based recommendation approach is used. \\n\\nIn this notebook, one cell saves only content-based file - user_recommendations_ContentBased.\\n\\nAfter that I additionally made a Test_Collaborative_ItemBased notebook where only the collaborative approach is implemented, just for testing.\\n\\nIf the Data is used give credit to the owner.\",\n",
       "              '../datasets/raw_datasets/kaggle/nikatomashvili/steam-games-dataset/dataset-metadata.json'],\n",
       "             'nl': ['deepak007chaubey',\n",
       "              '../datasets/raw_datasets/kaggle/deepak007chaubey/housevalueestimation/dataset-metadata.json'],\n",
       "             'it': ['american-university-data-ipeds-dataset',\n",
       "              '../datasets/raw_datasets/kaggle/sumithbhongale/american-university-data-ipeds-dataset/dataset-metadata.json'],\n",
       "             'so': ['adithyaawati',\n",
       "              '../datasets/raw_datasets/kaggle/adithyaawati/apartments-for-rent-classified/dataset-metadata.json'],\n",
       "             'id': ['steam-games-dataset',\n",
       "              '../datasets/raw_datasets/kaggle/nikatomashvili/steam-games-dataset/dataset-metadata.json'],\n",
       "             'ro': ['Pakistan car prices',\n",
       "              '../datasets/raw_datasets/kaggle/zaynshahbaz/pakistan-car-prices/dataset-metadata.json'],\n",
       "             'sw': ['nikatomashvili',\n",
       "              '../datasets/raw_datasets/kaggle/nikatomashvili/steam-games-dataset/dataset-metadata.json'],\n",
       "             'tl': ['samanemami',\n",
       "              '../datasets/raw_datasets/kaggle/samanemami/river-flowrf2/dataset-metadata.json'],\n",
       "             'cy': ['ml-challenge',\n",
       "              '../datasets/raw_datasets/kaggle/teesoong/ml-challenge/dataset-metadata.json'],\n",
       "             'fr': ['Apartment rental advertisement descriptions in US.',\n",
       "              '../datasets/raw_datasets/kaggle/adithyaawati/apartments-for-rent-classified/dataset-metadata.json'],\n",
       "             'da': ['samanemami/river-flowrf2',\n",
       "              '../datasets/raw_datasets/kaggle/samanemami/river-flowrf2/dataset-metadata.json'],\n",
       "             'et': ['nikatomashvili/steam-games-dataset',\n",
       "              '../datasets/raw_datasets/kaggle/nikatomashvili/steam-games-dataset/dataset-metadata.json'],\n",
       "             'ca': ['aiadventures-dataset-1',\n",
       "              '../datasets/raw_datasets/kaggle/pranavuikey/zomato-eda/dataset-metadata.json'],\n",
       "             'af': ['Movies Dataset TMDB',\n",
       "              '../datasets/raw_datasets/kaggle/vishnurajyadav12/movies-dataset-tmdb/dataset-metadata.json'],\n",
       "             'sq': ['threnjen',\n",
       "              '../datasets/raw_datasets/kaggle/threnjen/40-years-of-air-quality-index-from-the-epa-daily/dataset-metadata.json'],\n",
       "             'de': ['lorentzyeung',\n",
       "              '../datasets/raw_datasets/kaggle/lorentzyeung/all-japanese-anime-titles-in-imdb/dataset-metadata.json'],\n",
       "             'no': ['jaimevalero/developers-and-programming-languages',\n",
       "              '../datasets/raw_datasets/kaggle/jaimevalero/developers-and-programming-languages/dataset-metadata.json'],\n",
       "             'sl': ['karkavelrajaj',\n",
       "              '../datasets/raw_datasets/kaggle/karkavelrajaj/imdb-top-250-shows/dataset-metadata.json'],\n",
       "             'es': ['denizmac/content-based-rec',\n",
       "              '../datasets/raw_datasets/kaggle/denizmac/content-based-rec/dataset-metadata.json'],\n",
       "             'sv': ['jillanisofttech',\n",
       "              '../datasets/raw_datasets/kaggle/jillanisofttech/pakistan-house-price-dataset/dataset-metadata.json'],\n",
       "             'fi': ['HouseValueEstomation',\n",
       "              '../datasets/raw_datasets/kaggle/deepak007chaubey/housevalueestimation/dataset-metadata.json'],\n",
       "             'pl': ['rymnikski',\n",
       "              '../datasets/raw_datasets/kaggle/rymnikski/dataset-for-collaborative-filters/dataset-metadata.json'],\n",
       "             'pt': ['2022 Airbnb listings for Rio de Janeiro, Brazil. ',\n",
       "              '../datasets/raw_datasets/kaggle/liamarguedas/rio-de-janeiro-airbnb-listings-2022/dataset-metadata.json'],\n",
       "             'tr': ['denizmac',\n",
       "              '../datasets/raw_datasets/kaggle/denizmac/content-based-rec/dataset-metadata.json'],\n",
       "             'lv': ['vishnurajyadav12/movies-dataset-tmdb',\n",
       "              '../datasets/raw_datasets/kaggle/vishnurajyadav12/movies-dataset-tmdb/dataset-metadata.json'],\n",
       "             'lt': ['bninopaul',\n",
       "              '../datasets/raw_datasets/kaggle/bninopaul/microsoft-news-recommendation-dataset-train/dataset-metadata.json'],\n",
       "             'hr': ['zaynshahbaz/pakistan-car-prices',\n",
       "              '../datasets/raw_datasets/kaggle/zaynshahbaz/pakistan-car-prices/dataset-metadata.json'],\n",
       "             'sk': ['nfedorov',\n",
       "              '../datasets/raw_datasets/kaggle/nfedorov/top-2000-board-games-ratings/dataset-metadata.json'],\n",
       "             'cs': ['jacklacey',\n",
       "              '../datasets/raw_datasets/kaggle/jacklacey/left-4-dead-2-20000-player-stats/dataset-metadata.json'],\n",
       "             'hu': ['Amazon Sales Dataset',\n",
       "              '../datasets/raw_datasets/kaggle/karkavelrajaj/amazon-sales-dataset/dataset-metadata.json'],\n",
       "             'vi': ['michaelminhpham',\n",
       "              '../datasets/raw_datasets/kaggle/michaelminhpham/vietnamese-tiki-e-commerce-dataset/dataset-metadata.json']})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e4ba3-627c-442f-9245-7c780939bd04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2dba22e1-55b0-4ec6-b80e-3dd6ba48c441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>number of reccords with key</th>\n",
       "      <th>number of empty recpords</th>\n",
       "      <th>min_size</th>\n",
       "      <th>max_size</th>\n",
       "      <th>avg_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.887709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_no</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datasetSlugNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.887709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ownerUserNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usabilityRatingNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>titleNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4.063733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subtitleNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>8.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>descriptionNullable</td>\n",
       "      <td>659</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>11140</td>\n",
       "      <td>369.340753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>datasetId</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>datasetSlug</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.887709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hasDatasetSlug</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ownerUser</td>\n",
       "      <td>659</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hasOwnerUser</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>usabilityRating</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hasUsabilityRating</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>totalViews</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>totalVotes</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>totalDownloads</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>title</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4.063733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hasTitle</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>subtitle</td>\n",
       "      <td>659</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>8.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hasSubtitle</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>description</td>\n",
       "      <td>659</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>11140</td>\n",
       "      <td>369.340753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hasDescription</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>isPrivate</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>keywords</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>6.162367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>licenses</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>7.283763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>collaborators</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1.180577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>data</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   key_name  number of reccords with key  \\\n",
       "0                        id                          659   \n",
       "1                     id_no                          659   \n",
       "2       datasetSlugNullable                          659   \n",
       "3         ownerUserNullable                          659   \n",
       "4   usabilityRatingNullable                          659   \n",
       "5             titleNullable                          659   \n",
       "6          subtitleNullable                          659   \n",
       "7       descriptionNullable                          659   \n",
       "8                 datasetId                          659   \n",
       "9               datasetSlug                          659   \n",
       "10           hasDatasetSlug                          659   \n",
       "11                ownerUser                          659   \n",
       "12             hasOwnerUser                          659   \n",
       "13          usabilityRating                          659   \n",
       "14       hasUsabilityRating                          659   \n",
       "15               totalViews                          659   \n",
       "16               totalVotes                          659   \n",
       "17           totalDownloads                          659   \n",
       "18                    title                          659   \n",
       "19                 hasTitle                          659   \n",
       "20                 subtitle                          659   \n",
       "21              hasSubtitle                          659   \n",
       "22              description                          659   \n",
       "23           hasDescription                          659   \n",
       "24                isPrivate                          659   \n",
       "25                 keywords                          659   \n",
       "26                 licenses                          659   \n",
       "27            collaborators                          659   \n",
       "28                     data                          659   \n",
       "\n",
       "    number of empty recpords  min_size  max_size    avg_size  \n",
       "0                          0         1        10    3.887709  \n",
       "1                          0         1         1    1.000000  \n",
       "2                          0         1        10    3.887709  \n",
       "3                          0         1         1    1.000000  \n",
       "4                          0         1         1    1.000000  \n",
       "5                          0         1        12    4.063733  \n",
       "6                         79         1        19    8.275862  \n",
       "7                         75         4     11140  369.340753  \n",
       "8                          0         1         1    1.000000  \n",
       "9                          0         1        10    3.887709  \n",
       "10                         0         1         1    1.000000  \n",
       "11                         6         1         1    1.000000  \n",
       "12                         0         1         1    1.000000  \n",
       "13                         0         1         1    1.000000  \n",
       "14                         0         1         1    1.000000  \n",
       "15                         0         1         1    1.000000  \n",
       "16                         0         1         1    1.000000  \n",
       "17                         0         1         1    1.000000  \n",
       "18                         0         1        12    4.063733  \n",
       "19                         0         1         1    1.000000  \n",
       "20                        79         1        19    8.275862  \n",
       "21                         0         1         1    1.000000  \n",
       "22                        75         4     11140  369.340753  \n",
       "23                         0         1         1    1.000000  \n",
       "24                         0         1         1    1.000000  \n",
       "25                         0         1        17    6.162367  \n",
       "26                         0         6        22    7.283763  \n",
       "27                         0         1        16    1.180577  \n",
       "28                         0         1         1    1.000000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3084b-2c65-4c79-82e0-f63847fa3920",
   "metadata": {},
   "source": [
    "## find empty elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "63e908aa-3e10-4240-8539-fbcdb8732dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_empty_index(data_index):\n",
    "    \"\"\"\n",
    "    This function search for datasets which contains empty fields\n",
    "    Paramteres:\n",
    "    ----------\n",
    "        data_index: pd.DataFrame\n",
    "            with \"user_folder_name\",\"data_folder_name\",\n",
    "                 \"path_dataset\",\"path_metadata\"\n",
    "    \"\"\"\n",
    "    is_empty = []\n",
    "\n",
    "    path_columns = \"path_metadata\"\n",
    "    if \"path_metadata\" in data_index.columns:\n",
    "        path_column = \"path_metadata\"\n",
    "    else:\n",
    "        path_column = \"path to metadata\"\n",
    "    \n",
    "    for file_path in data_index[path_column]:\n",
    "        file_dict = load_metadata(file_path)\n",
    "        any_elem_empty = False\n",
    "        column_name = \"\"\n",
    "        for k in file_dict.keys():\n",
    "            if len(str(file_dict[k]))==0:\n",
    "                any_elem_empty=True\n",
    "                column_name=k\n",
    "                break\n",
    "        is_empty.append(any_elem_empty)    \n",
    "    data_index[\"is_empty\"] = is_empty\n",
    "    return data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c45214-8e48-4600-82da-7c51c7e3e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_index = set_empty_index(data_index)\n",
    "# print(data_index.is_empty.sum())\n",
    "# data_index = data_index[data_index.is_empty==False]\n",
    "# data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f5dd9dcc-b4b4-4331-98cb-c048297ac3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "0.10159651669085631\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>path to metadata</th>\n",
       "      <th>is_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>19</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mental health</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>mental health</td>\n",
       "      <td>18</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>dataset related to mental health</td>\n",
       "      <td>9</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>emotion detection and sentiment classification</td>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>text classification</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>text classification</td>\n",
       "      <td>14</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>classification</td>\n",
       "      <td>6</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>classification</td>\n",
       "      <td>14</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         query name  rating_hottest  \\\n",
       "18                          Top 1000 Movies Dataset              19   \n",
       "23                                    mental health               3   \n",
       "38                                    mental health              18   \n",
       "49                 dataset related to mental health               9   \n",
       "65   emotion detection and sentiment classification               5   \n",
       "..                                              ...             ...   \n",
       "653                             text classification               4   \n",
       "663                             text classification              14   \n",
       "670                                  classification               1   \n",
       "675                                  classification               6   \n",
       "683                                  classification              14   \n",
       "\n",
       "                                      path to metadata  is_empty  \n",
       "18   ../datasets/raw_datasets/metadata-for-search/a...      True  \n",
       "23   ../datasets/raw_datasets/metadata-for-search/o...      True  \n",
       "38   ../datasets/raw_datasets/metadata-for-search/o...      True  \n",
       "49   ../datasets/raw_datasets/metadata-for-search/c...      True  \n",
       "65   ../datasets/raw_datasets/metadata-for-search/m...      True  \n",
       "..                                                 ...       ...  \n",
       "653  ../datasets/raw_datasets/metadata-for-search/t...      True  \n",
       "663  ../datasets/raw_datasets/metadata-for-search/s...      True  \n",
       "670  ../datasets/raw_datasets/metadata-for-search/u...      True  \n",
       "675  ../datasets/raw_datasets/metadata-for-search/u...      True  \n",
       "683  ../datasets/raw_datasets/metadata-for-search/u...      True  \n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_data_index_hot\n",
    "search_data_index_hot = set_empty_index(search_data_index_hot)\n",
    "print(search_data_index_hot.is_empty.sum())\n",
    "print(search_data_index_hot.is_empty.sum()/len(search_data_index_hot))\n",
    "search_data_index_hot[search_data_index_hot.is_empty==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e0df8962-a393-4f72-8f43-43e2c0adba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/raw_datasets/metadata-for-search/adityaghuse/imdb-top-1000/dataset-metadata.json'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = search_data_index_hot[search_data_index_hot.is_empty==True]\n",
    "temp['path to metadata'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5b4a771-1222-40ac-b76b-863ef293e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "0.3304157549234136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_active</th>\n",
       "      <th>path to metadata</th>\n",
       "      <th>is_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/e...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/k...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>7</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>8</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/w...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>classification</td>\n",
       "      <td>13</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>classification</td>\n",
       "      <td>14</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>classification</td>\n",
       "      <td>17</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>classification</td>\n",
       "      <td>19</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/h...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>classification</td>\n",
       "      <td>20</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  query name  rating_active  \\\n",
       "1    Top 1000 Movies Dataset              2   \n",
       "2    Top 1000 Movies Dataset              3   \n",
       "3    Top 1000 Movies Dataset              4   \n",
       "6    Top 1000 Movies Dataset              7   \n",
       "7    Top 1000 Movies Dataset              8   \n",
       "..                       ...            ...   \n",
       "449           classification             13   \n",
       "450           classification             14   \n",
       "453           classification             17   \n",
       "455           classification             19   \n",
       "456           classification             20   \n",
       "\n",
       "                                      path to metadata  is_empty  \n",
       "1    ../datasets/raw_datasets/metadata-for-search/e...      True  \n",
       "2    ../datasets/raw_datasets/metadata-for-search/s...      True  \n",
       "3    ../datasets/raw_datasets/metadata-for-search/k...      True  \n",
       "6    ../datasets/raw_datasets/metadata-for-search/i...      True  \n",
       "7    ../datasets/raw_datasets/metadata-for-search/w...      True  \n",
       "..                                                 ...       ...  \n",
       "449  ../datasets/raw_datasets/metadata-for-search/n...      True  \n",
       "450  ../datasets/raw_datasets/metadata-for-search/n...      True  \n",
       "453  ../datasets/raw_datasets/metadata-for-search/s...      True  \n",
       "455  ../datasets/raw_datasets/metadata-for-search/h...      True  \n",
       "456  ../datasets/raw_datasets/metadata-for-search/m...      True  \n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_data_index_active\n",
    "search_data_index_active = set_empty_index(search_data_index_active)\n",
    "search_data_index_active = set_empty_index(search_data_index_active)\n",
    "print(search_data_index_active.is_empty.sum())\n",
    "print(search_data_index_active.is_empty.sum()/len(search_data_index_active))\n",
    "search_data_index_active[search_data_index_active.is_empty==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "54327ced-a844-489b-af64-2d2779c16b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "0.14879906268306972\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating</th>\n",
       "      <th>path to metadata</th>\n",
       "      <th>is_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>9</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>18</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>19</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/e...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>22</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>24</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>classification</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>classification</td>\n",
       "      <td>7</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>classification</td>\n",
       "      <td>8</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/z...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>classification</td>\n",
       "      <td>11</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>classification</td>\n",
       "      <td>36</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query name  rating  \\\n",
       "11    Top 1000 Movies Dataset       9   \n",
       "20    Top 1000 Movies Dataset      18   \n",
       "21    Top 1000 Movies Dataset      19   \n",
       "24    Top 1000 Movies Dataset      22   \n",
       "26    Top 1000 Movies Dataset      24   \n",
       "...                       ...     ...   \n",
       "1657           classification       4   \n",
       "1660           classification       7   \n",
       "1661           classification       8   \n",
       "1664           classification      11   \n",
       "1689           classification      36   \n",
       "\n",
       "                                       path to metadata  is_empty  \n",
       "11    ../datasets/raw_datasets/metadata-for-search/t...      True  \n",
       "20    ../datasets/raw_datasets/metadata-for-search/a...      True  \n",
       "21    ../datasets/raw_datasets/metadata-for-search/e...      True  \n",
       "24    ../datasets/raw_datasets/metadata-for-search/i...      True  \n",
       "26    ../datasets/raw_datasets/metadata-for-search/a...      True  \n",
       "...                                                 ...       ...  \n",
       "1657  ../datasets/raw_datasets/metadata-for-search/o...      True  \n",
       "1660  ../datasets/raw_datasets/metadata-for-search/u...      True  \n",
       "1661  ../datasets/raw_datasets/metadata-for-search/z...      True  \n",
       "1664  ../datasets/raw_datasets/metadata-for-search/u...      True  \n",
       "1689  ../datasets/raw_datasets/metadata-for-search/n...      True  \n",
       "\n",
       "[254 rows x 4 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_data_index_vote\n",
    "search_data_index_vote = set_empty_index(search_data_index_vote)\n",
    "print(search_data_index_vote.is_empty.sum())\n",
    "print(search_data_index_vote.is_empty.sum()/len(search_data_index_vote))\n",
    "search_data_index_vote[search_data_index_vote.is_empty==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e057a8-dab7-40b5-ac95-87578aac4bbb",
   "metadata": {},
   "source": [
    "## Dataset to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516e5f9-e894-40e5-b2a6-17012da6a4da",
   "metadata": {},
   "source": [
    "Сolumns used:\n",
    "\n",
    "\"title\"\n",
    "\"subtitle\"\n",
    "\"description\"\n",
    "\"keywords\"\n",
    "\"titleNullable\"\n",
    "\"subtitleNullable\"\n",
    "\"descriptionNullable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "514cc1b6-cdca-4ded-98f3-0597942bef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def collect_text_columns_to_single_text(used_data_columns,text_dict):\n",
    "    \"\"\"\n",
    "    Join all texts elemnt into single string\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    for c in used_data_columns:\n",
    "        if c == \"keywords\":\n",
    "            full_text = full_text + \" \" + \" \".join(text_dict[c])\n",
    "            continue\n",
    "        full_text = full_text + \" \" + text_dict[c]\n",
    "        \n",
    "    return full_text\n",
    "\n",
    "# data_index:pd.DataFrame,process_text: funct    \n",
    "def collect_data_to_csv(data_index,process_text,is_tokenized=True):\n",
    "    \"\"\"\n",
    "    load json dataset into csv file (all texts are preprocessed with process_text)\n",
    "    we use fileds used_data_columns ([\"title\", \"subtitle\",\"description\",\n",
    "                  \"keywords\", \"titleNullable\", \"subtitleNullable\",\n",
    "                  \"descriptionNullable\"])\n",
    "    on csv file we store\n",
    "                    list of words for each used_data_columns,\n",
    "                    list of tokens for each used_data_columns,\n",
    "                    list of words and tokens for full text\n",
    "    \"\"\"\n",
    "    dataset = defaultdict(list)\n",
    "    used_data_columns = [\"title\", \"subtitle\",\"description\",\n",
    "                         \"keywords\", \"titleNullable\", \"subtitleNullable\",\n",
    "                         \"descriptionNullable\"]\n",
    "    #list of columns to join into one text\n",
    "    used_data_columns_text = [\"title\", \"subtitle\",\"description\",\n",
    "                         \"keywords\"]\n",
    "    init_vocabulary()\n",
    "\n",
    "    \n",
    "    for id,row_elem in data_index.iterrows():\n",
    "        # if row_elem[\"is_empty\"]:\n",
    "        #     continue\n",
    "        file_path = row_elem[\"path to metadata\"]\n",
    "\n",
    "        \n",
    "        file = load_metadata(file_path)\n",
    "        dataset[\"path\"].append(file_path)\n",
    "        # add id\n",
    "        dataset[\"id\"].append(id)\n",
    "        # load to new dataset column which has same name in file and in new dataset\n",
    "        for column in used_data_columns:\n",
    "            text = process_text(file[column],text_source=column)\n",
    "            dataset[column].append(text)\n",
    "            \n",
    "            # create tokens for columm\n",
    "            if is_tokenized:\n",
    "                append_vocabulary(text_list=text)\n",
    "                text_tokens = tokenize_text(text_list = text)\n",
    "                dataset[column+\"_token_id\"].append(text_tokens)\n",
    "\n",
    "        # load to new dataset column which is absent in file\n",
    "        full_text = collect_text_columns_to_single_text(used_data_columns_text,file)\n",
    "        full_text = process_text(full_text,text_source=column)\n",
    "        dataset[\"text\"].append(full_text)\n",
    "        \n",
    "        # create tokens full text tokens\n",
    "        if is_tokenized:\n",
    "            append_vocabulary(text_list=full_text)\n",
    "            tokens = tokenize_text(text_list=full_text)\n",
    "            dataset[\"token_id\"].append(tokens)\n",
    "\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f2581c54-4b37-467b-b998-872aedd07c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_index = pd.read_csv(DATA_INDEX_PATH + \"data_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "967612cf-91ee-4b6f-bd7c-e1c13b2f6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = collect_data_to_csv(data_index,preprocess_text)\n",
    "# dataset = collect_data_to_csv(data_index,preprocess_stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5babd9c-fc83-4011-9ab1-0b5e4d7bf809",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title</th>\n",
       "      <th>title_token_id</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>subtitle_token_id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_token_id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_token_id</th>\n",
       "      <th>titleNullable</th>\n",
       "      <th>titleNullable_token_id</th>\n",
       "      <th>subtitleNullable</th>\n",
       "      <th>subtitleNullable_token_id</th>\n",
       "      <th>descriptionNullable</th>\n",
       "      <th>descriptionNullable_token_id</th>\n",
       "      <th>text</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/raw_datasets/kaggle/vijendersingh4...</td>\n",
       "      <td>[Research, Paper]</td>\n",
       "      <td>[3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[NLP, Based, Research, Papers]</td>\n",
       "      <td>[5, 4, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[#, #, #, Context, The, dataset, consists, of,...</td>\n",
       "      <td>[7, 7, 7, 20, 33, 56, 49, 81, 106, 94, 89, 68,...</td>\n",
       "      <td>[research, earth and nature, education, nlp]</td>\n",
       "      <td>[94, 119, 120, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[Research, Paper]</td>\n",
       "      <td>[3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[NLP, Based, Research, Papers]</td>\n",
       "      <td>[5, 4, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[#, #, #, Context, The, dataset, consists, of,...</td>\n",
       "      <td>[7, 7, 7, 20, 33, 56, 49, 81, 106, 94, 89, 68,...</td>\n",
       "      <td>[Research, Paper, NLP, Based, Research, Papers...</td>\n",
       "      <td>[3, 2, 5, 4, 3, 6, 7, 7, 7, 20, 33, 56, 49, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/raw_datasets/kaggle/kamalesh1997/t...</td>\n",
       "      <td>[Top, Rated, movies, -TMDB, data, set]</td>\n",
       "      <td>[126, 125, 127, 124, 55, 128, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[Dataset, of, Top, rated, movies, from, the, T...</td>\n",
       "      <td>[129, 81, 126, 133, 127, 132, 103, 130, 131, 0...</td>\n",
       "      <td>[Context, User, can, make, use, of, the, datas...</td>\n",
       "      <td>[20, 140, 46, 75, 146, 81, 103, 56, 68, 144, 1...</td>\n",
       "      <td>[arts and entertainment, beginner, intermediat...</td>\n",
       "      <td>[147, 148, 149, 150, 151, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[Top, Rated, movies, -TMDB, data, set]</td>\n",
       "      <td>[126, 125, 127, 124, 55, 128, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[Dataset, of, Top, rated, movies, from, the, T...</td>\n",
       "      <td>[129, 81, 126, 133, 127, 132, 103, 130, 131, 0...</td>\n",
       "      <td>[Context, User, can, make, use, of, the, datas...</td>\n",
       "      <td>[20, 140, 46, 75, 146, 81, 103, 56, 68, 144, 1...</td>\n",
       "      <td>[Top, Rated, movies, -TMDB, data, set, Dataset...</td>\n",
       "      <td>[126, 125, 127, 124, 55, 128, 129, 81, 126, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/raw_datasets/kaggle/charanjilagam/...</td>\n",
       "      <td>[IPL_WIN_PREDICTION, (, 98, %, ACCURACY, )]</td>\n",
       "      <td>[161, 157, 159, 156, 160, 158, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[IPL, WIN, PREDICTION, FROM, 2008, TO, 2016]</td>\n",
       "      <td>[165, 168, 166, 164, 162, 167, 163, 0, 0, 0, 0...</td>\n",
       "      <td>[With, the, preprocessed, dataset, at, hand, ,...</td>\n",
       "      <td>[188, 103, 275, 56, 199, 237, 9, 110, 46, 263,...</td>\n",
       "      <td>[cricket]</td>\n",
       "      <td>[324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[IPL_WIN_PREDICTION, (, 98, %, ACCURACY, )]</td>\n",
       "      <td>[161, 157, 159, 156, 160, 158, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[IPL, WIN, PREDICTION, FROM, 2008, TO, 2016]</td>\n",
       "      <td>[165, 168, 166, 164, 162, 167, 163, 0, 0, 0, 0...</td>\n",
       "      <td>[With, the, preprocessed, dataset, at, hand, ,...</td>\n",
       "      <td>[188, 103, 275, 56, 199, 237, 9, 110, 46, 263,...</td>\n",
       "      <td>[IPL_WIN_PREDICTION, (, 98, %, ACCURACY, ), IP...</td>\n",
       "      <td>[161, 157, 159, 156, 160, 158, 165, 168, 166, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/raw_datasets/kaggle/michaelbryantd...</td>\n",
       "      <td>[FiveThirtyEight, Election, Polls, Dataset]</td>\n",
       "      <td>[326, 325, 327, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[For, presidential, ,, house, ,, senate, ,, an...</td>\n",
       "      <td>[328, 332, 9, 331, 9, 333, 9, 39, 330, 329, 0,...</td>\n",
       "      <td>[FiveThirtyEight, ,, sometimes, rendered, as, ...</td>\n",
       "      <td>[326, 9, 378, 377, 197, 338, 9, 70, 356, 340, ...</td>\n",
       "      <td>[politics, exploratory data analysis, classifi...</td>\n",
       "      <td>[372, 384, 206, 282, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[FiveThirtyEight, Election, Polls, Dataset]</td>\n",
       "      <td>[326, 325, 327, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[For, presidential, ,, house, ,, senate, ,, an...</td>\n",
       "      <td>[328, 332, 9, 331, 9, 333, 9, 39, 330, 329, 0,...</td>\n",
       "      <td>[FiveThirtyEight, ,, sometimes, rendered, as, ...</td>\n",
       "      <td>[326, 9, 378, 377, 197, 338, 9, 70, 356, 340, ...</td>\n",
       "      <td>[FiveThirtyEight, Election, Polls, Dataset, Fo...</td>\n",
       "      <td>[326, 325, 327, 129, 328, 332, 9, 331, 9, 333,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/raw_datasets/kaggle/sonialikhan/ho...</td>\n",
       "      <td>[Household, Electric, Power, Consumption]</td>\n",
       "      <td>[388, 387, 389, 386, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[time, series, analysis-, regression, /, clust...</td>\n",
       "      <td>[393, 392, 391, 282, 390, 208, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[*, *, About, Dataset, *, *, I, need, help, to...</td>\n",
       "      <td>[395, 395, 418, 129, 395, 395, 27, 261, 65, 10...</td>\n",
       "      <td>[electricity]</td>\n",
       "      <td>[546, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[Household, Electric, Power, Consumption]</td>\n",
       "      <td>[388, 387, 389, 386, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[time, series, analysis-, regression, /, clust...</td>\n",
       "      <td>[393, 392, 391, 282, 390, 208, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[*, *, About, Dataset, *, *, I, need, help, to...</td>\n",
       "      <td>[395, 395, 418, 129, 395, 395, 27, 261, 65, 10...</td>\n",
       "      <td>[Household, Electric, Power, Consumption, time...</td>\n",
       "      <td>[388, 387, 389, 386, 393, 392, 391, 282, 390, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  ../datasets/raw_datasets/kaggle/vijendersingh4...   \n",
       "1  ../datasets/raw_datasets/kaggle/kamalesh1997/t...   \n",
       "2  ../datasets/raw_datasets/kaggle/charanjilagam/...   \n",
       "3  ../datasets/raw_datasets/kaggle/michaelbryantd...   \n",
       "4  ../datasets/raw_datasets/kaggle/sonialikhan/ho...   \n",
       "\n",
       "                                         title  \\\n",
       "0                            [Research, Paper]   \n",
       "1       [Top, Rated, movies, -TMDB, data, set]   \n",
       "2  [IPL_WIN_PREDICTION, (, 98, %, ACCURACY, )]   \n",
       "3  [FiveThirtyEight, Election, Polls, Dataset]   \n",
       "4    [Household, Electric, Power, Consumption]   \n",
       "\n",
       "                                      title_token_id  \\\n",
       "0  [3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [126, 125, 127, 124, 55, 128, 0, 0, 0, 0, 0, 0...   \n",
       "2  [161, 157, 159, 156, 160, 158, 0, 0, 0, 0, 0, ...   \n",
       "3  [326, 325, 327, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [388, 387, 389, 386, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0                     [NLP, Based, Research, Papers]   \n",
       "1  [Dataset, of, Top, rated, movies, from, the, T...   \n",
       "2       [IPL, WIN, PREDICTION, FROM, 2008, TO, 2016]   \n",
       "3  [For, presidential, ,, house, ,, senate, ,, an...   \n",
       "4  [time, series, analysis-, regression, /, clust...   \n",
       "\n",
       "                                   subtitle_token_id  \\\n",
       "0  [5, 4, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [129, 81, 126, 133, 127, 132, 103, 130, 131, 0...   \n",
       "2  [165, 168, 166, 164, 162, 167, 163, 0, 0, 0, 0...   \n",
       "3  [328, 332, 9, 331, 9, 333, 9, 39, 330, 329, 0,...   \n",
       "4  [393, 392, 391, 282, 390, 208, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  [#, #, #, Context, The, dataset, consists, of,...   \n",
       "1  [Context, User, can, make, use, of, the, datas...   \n",
       "2  [With, the, preprocessed, dataset, at, hand, ,...   \n",
       "3  [FiveThirtyEight, ,, sometimes, rendered, as, ...   \n",
       "4  [*, *, About, Dataset, *, *, I, need, help, to...   \n",
       "\n",
       "                                description_token_id  \\\n",
       "0  [7, 7, 7, 20, 33, 56, 49, 81, 106, 94, 89, 68,...   \n",
       "1  [20, 140, 46, 75, 146, 81, 103, 56, 68, 144, 1...   \n",
       "2  [188, 103, 275, 56, 199, 237, 9, 110, 46, 263,...   \n",
       "3  [326, 9, 378, 377, 197, 338, 9, 70, 356, 340, ...   \n",
       "4  [395, 395, 418, 129, 395, 395, 27, 261, 65, 10...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0       [research, earth and nature, education, nlp]   \n",
       "1  [arts and entertainment, beginner, intermediat...   \n",
       "2                                          [cricket]   \n",
       "3  [politics, exploratory data analysis, classifi...   \n",
       "4                                      [electricity]   \n",
       "\n",
       "                                   keywords_token_id  \\\n",
       "0  [94, 119, 120, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [147, 148, 149, 150, 151, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [372, 384, 206, 282, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [546, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                 titleNullable  \\\n",
       "0                            [Research, Paper]   \n",
       "1       [Top, Rated, movies, -TMDB, data, set]   \n",
       "2  [IPL_WIN_PREDICTION, (, 98, %, ACCURACY, )]   \n",
       "3  [FiveThirtyEight, Election, Polls, Dataset]   \n",
       "4    [Household, Electric, Power, Consumption]   \n",
       "\n",
       "                              titleNullable_token_id  \\\n",
       "0  [3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [126, 125, 127, 124, 55, 128, 0, 0, 0, 0, 0, 0...   \n",
       "2  [161, 157, 159, 156, 160, 158, 0, 0, 0, 0, 0, ...   \n",
       "3  [326, 325, 327, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [388, 387, 389, 386, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                    subtitleNullable  \\\n",
       "0                     [NLP, Based, Research, Papers]   \n",
       "1  [Dataset, of, Top, rated, movies, from, the, T...   \n",
       "2       [IPL, WIN, PREDICTION, FROM, 2008, TO, 2016]   \n",
       "3  [For, presidential, ,, house, ,, senate, ,, an...   \n",
       "4  [time, series, analysis-, regression, /, clust...   \n",
       "\n",
       "                           subtitleNullable_token_id  \\\n",
       "0  [5, 4, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [129, 81, 126, 133, 127, 132, 103, 130, 131, 0...   \n",
       "2  [165, 168, 166, 164, 162, 167, 163, 0, 0, 0, 0...   \n",
       "3  [328, 332, 9, 331, 9, 333, 9, 39, 330, 329, 0,...   \n",
       "4  [393, 392, 391, 282, 390, 208, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                 descriptionNullable  \\\n",
       "0  [#, #, #, Context, The, dataset, consists, of,...   \n",
       "1  [Context, User, can, make, use, of, the, datas...   \n",
       "2  [With, the, preprocessed, dataset, at, hand, ,...   \n",
       "3  [FiveThirtyEight, ,, sometimes, rendered, as, ...   \n",
       "4  [*, *, About, Dataset, *, *, I, need, help, to...   \n",
       "\n",
       "                        descriptionNullable_token_id  \\\n",
       "0  [7, 7, 7, 20, 33, 56, 49, 81, 106, 94, 89, 68,...   \n",
       "1  [20, 140, 46, 75, 146, 81, 103, 56, 68, 144, 1...   \n",
       "2  [188, 103, 275, 56, 199, 237, 9, 110, 46, 263,...   \n",
       "3  [326, 9, 378, 377, 197, 338, 9, 70, 356, 340, ...   \n",
       "4  [395, 395, 418, 129, 395, 395, 27, 261, 65, 10...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [Research, Paper, NLP, Based, Research, Papers...   \n",
       "1  [Top, Rated, movies, -TMDB, data, set, Dataset...   \n",
       "2  [IPL_WIN_PREDICTION, (, 98, %, ACCURACY, ), IP...   \n",
       "3  [FiveThirtyEight, Election, Polls, Dataset, Fo...   \n",
       "4  [Household, Electric, Power, Consumption, time...   \n",
       "\n",
       "                                            token_id  \n",
       "0  [3, 2, 5, 4, 3, 6, 7, 7, 7, 20, 33, 56, 49, 81...  \n",
       "1  [126, 125, 127, 124, 55, 128, 129, 81, 126, 13...  \n",
       "2  [161, 157, 159, 156, 160, 158, 165, 168, 166, ...  \n",
       "3  [326, 325, 327, 129, 328, 332, 9, 331, 9, 333,...  \n",
       "4  [388, 387, 389, 386, 393, 392, 391, 282, 390, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "736aee68-7c7c-4345-95ab-e259e208d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_active = collect_data_to_csv(search_data_index_active,no_process_text,is_tokenized=False)\n",
    "dataset_hot = collect_data_to_csv(search_data_index_hot,no_process_text,is_tokenized=False)\n",
    "dataset_vote = collect_data_to_csv(search_data_index_vote,no_process_text,is_tokenized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3451cfdd-0018-48cb-aa24-47051f5962d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_active</th>\n",
       "      <th>path to metadata</th>\n",
       "      <th>is_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/e...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/k...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>classification</td>\n",
       "      <td>16</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>classification</td>\n",
       "      <td>17</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>classification</td>\n",
       "      <td>18</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>classification</td>\n",
       "      <td>19</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/h...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>classification</td>\n",
       "      <td>20</td>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  query name  rating_active  \\\n",
       "0    Top 1000 Movies Dataset              1   \n",
       "1    Top 1000 Movies Dataset              2   \n",
       "2    Top 1000 Movies Dataset              3   \n",
       "3    Top 1000 Movies Dataset              4   \n",
       "4    Top 1000 Movies Dataset              5   \n",
       "..                       ...            ...   \n",
       "452           classification             16   \n",
       "453           classification             17   \n",
       "454           classification             18   \n",
       "455           classification             19   \n",
       "456           classification             20   \n",
       "\n",
       "                                      path to metadata  is_empty  \n",
       "0    ../datasets/raw_datasets/metadata-for-search/r...     False  \n",
       "1    ../datasets/raw_datasets/metadata-for-search/e...      True  \n",
       "2    ../datasets/raw_datasets/metadata-for-search/s...      True  \n",
       "3    ../datasets/raw_datasets/metadata-for-search/k...      True  \n",
       "4    ../datasets/raw_datasets/metadata-for-search/d...     False  \n",
       "..                                                 ...       ...  \n",
       "452  ../datasets/raw_datasets/metadata-for-search/n...     False  \n",
       "453  ../datasets/raw_datasets/metadata-for-search/s...      True  \n",
       "454  ../datasets/raw_datasets/metadata-for-search/c...     False  \n",
       "455  ../datasets/raw_datasets/metadata-for-search/h...      True  \n",
       "456  ../datasets/raw_datasets/metadata-for-search/m...      True  \n",
       "\n",
       "[457 rows x 4 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3a5ef-0306-4b7e-b570-8ea44244e38e",
   "metadata": {},
   "source": [
    "### load to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2cfaa838-7569-4748-97f4-d98a62c1b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hot.to_json(PROCESSED_DATA_PATH+\"/search_dataset_hot.json\")\n",
    "dataset_vote.to_json(PROCESSED_DATA_PATH+\"/search_dataset_vote.json\")\n",
    "dataset_active.to_json(PROCESSED_DATA_PATH+\"/search_dataset_active.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
