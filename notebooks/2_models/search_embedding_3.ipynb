{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147d63d6-cc6d-41ed-a329-585711d4261c",
   "metadata": {},
   "source": [
    "# Search embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61597329-7942-491f-b5c8-eee51c55ffee",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf23b1b-ca71-4e1b-888e-c836454146c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store constant varables with path to our datasets\n",
    "# SEARCH_DATA_INDEX_PATH = \"../../datasets\" #\"../../datasets/metadata_for_search\"\n",
    "# MODEL_PATH = \"../../models/\"\n",
    "\n",
    "# SEARCH_DATA_PATH = \"../../datasets\"\n",
    "# Here we store constant varables with path to our datasets\n",
    "SEARCH_DATA_INDEX_PATH = \"../../datasets/processed_datasets\"\n",
    "MODEL_PATH = \"../../models/\"\n",
    "\n",
    "SEARCH_DATA_PATH = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877337e-b83f-48eb-9c91-bb62a7122fce",
   "metadata": {},
   "source": [
    "#### load search data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc98c701-030f-4171-b22c-2aa82fa4cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# here we load metadata (related to source of dataset)\n",
    "data_vote = pd.read_json(SEARCH_DATA_PATH+\"/search_dataset_vote.json\")\n",
    "data_active = pd.read_json(SEARCH_DATA_PATH+\"/search_dataset_active.json\")\n",
    "data_hot = pd.read_json(SEARCH_DATA_PATH+\"/search_dataset_hot.json\")\n",
    "\n",
    "# here we load datasets (texts)\n",
    "search_data_index_hot = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info-hottest-rating.csv\")\n",
    "search_data_index_vote = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info.csv\")\n",
    "search_data_index_active = pd.read_csv(SEARCH_DATA_INDEX_PATH+\"/metadata-for-search-info-active-rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a4607ef-49bc-4dca-879c-a36864a00a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_data_index_hot['id'] = search_data_index_hot.index\n",
    "search_data_index_vote['id'] = search_data_index_vote.index\n",
    "search_data_index_active['id'] = search_data_index_active.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427e9640-0a4f-46c3-94a1-3c2cde41db9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>titleNullable</th>\n",
       "      <th>subtitleNullable</th>\n",
       "      <th>descriptionNullable</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>[arts and entertainment, movies and tv shows, ...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset Top 10000 Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "      <td>1</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset \"Release Year, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>2</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset Discover the Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>3</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB) In this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>4</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>[movies and tv shows, beginner, intermediate, ...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies Top 1000 Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>685</td>\n",
       "      <td>Scene Classification</td>\n",
       "      <td>Contains ~25K images from a wide range of natu...</td>\n",
       "      <td>This dataset contains about ~25k images from a...</td>\n",
       "      <td>[movies and tv shows, computer vision, classif...</td>\n",
       "      <td>Scene Classification</td>\n",
       "      <td>Contains ~25K images from a wide range of natu...</td>\n",
       "      <td>This dataset contains about ~25k images from a...</td>\n",
       "      <td>Scene Classification Contains ~25K images fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>686</td>\n",
       "      <td>Plants Classification</td>\n",
       "      <td>30 Types of Plants Image Classification</td>\n",
       "      <td>The dataset contains 30 types of plants images...</td>\n",
       "      <td>[biology, classification, image]</td>\n",
       "      <td>Plants Classification</td>\n",
       "      <td>30 Types of Plants Image Classification</td>\n",
       "      <td>The dataset contains 30 types of plants images...</td>\n",
       "      <td>Plants Classification 30 Types of Plants Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>687</td>\n",
       "      <td>Flower Classification</td>\n",
       "      <td>14 Types of Flower Image Classification</td>\n",
       "      <td>The dataset contains 14 types of flower images...</td>\n",
       "      <td>[classification, image, chinese]</td>\n",
       "      <td>Flower Classification</td>\n",
       "      <td>14 Types of Flower Image Classification</td>\n",
       "      <td>The dataset contains 14 types of flower images...</td>\n",
       "      <td>Flower Classification 14 Types of Flower Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/g...</td>\n",
       "      <td>688</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION</td>\n",
       "      <td>525 species, 84635 train, 2625 test, 2625 vali...</td>\n",
       "      <td>This version of the dataset adds 10 new specie...</td>\n",
       "      <td>[biology, computer vision, classification, cnn...</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION</td>\n",
       "      <td>525 species, 84635 train, 2625 test, 2625 vali...</td>\n",
       "      <td>This version of the dataset adds 10 new specie...</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>689</td>\n",
       "      <td>Brain Tumor Classification (MRI)</td>\n",
       "      <td>Classify MRI images into four classes</td>\n",
       "      <td># Contribute to OpenSource\\n##Repo: [GitHub](h...</td>\n",
       "      <td>[health, medicine, classification, health cond...</td>\n",
       "      <td>Brain Tumor Classification (MRI)</td>\n",
       "      <td>Classify MRI images into four classes</td>\n",
       "      <td># Contribute to OpenSource\\n##Repo: [GitHub](h...</td>\n",
       "      <td>Brain Tumor Classification (MRI) Classify MRI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path   id  \\\n",
       "0    ../datasets/raw_datasets/metadata-for-search/o...    0   \n",
       "1    ../datasets/raw_datasets/metadata-for-search/r...    1   \n",
       "2    ../datasets/raw_datasets/metadata-for-search/i...    2   \n",
       "3    ../datasets/raw_datasets/metadata-for-search/u...    3   \n",
       "4    ../datasets/raw_datasets/metadata-for-search/s...    4   \n",
       "..                                                 ...  ...   \n",
       "684  ../datasets/raw_datasets/metadata-for-search/n...  685   \n",
       "685  ../datasets/raw_datasets/metadata-for-search/m...  686   \n",
       "686  ../datasets/raw_datasets/metadata-for-search/m...  687   \n",
       "687  ../datasets/raw_datasets/metadata-for-search/g...  688   \n",
       "688  ../datasets/raw_datasets/metadata-for-search/s...  689   \n",
       "\n",
       "                                        title  \\\n",
       "0            Top 10000 Popular Movies Dataset   \n",
       "1                IMDB Top 1000 Movies Dataset   \n",
       "2                Top 1000 IMDb Movies Dataset   \n",
       "3       Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4            Top 1000 Highest Grossing Movies   \n",
       "..                                        ...   \n",
       "684                      Scene Classification   \n",
       "685                     Plants Classification   \n",
       "686                     Flower Classification   \n",
       "687  BIRDS 525  SPECIES- IMAGE CLASSIFICATION   \n",
       "688          Brain Tumor Classification (MRI)   \n",
       "\n",
       "                                              subtitle  \\\n",
       "0       Top 10000 Popular movies based on TMDB ratings   \n",
       "1    \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2    Discover the Greatest Movies of All Time - IMD...   \n",
       "3    In this dataset the top movies data is given b...   \n",
       "4            Top 1000 Highest Grossing Holywood Movies   \n",
       "..                                                 ...   \n",
       "684  Contains ~25K images from a wide range of natu...   \n",
       "685            30 Types of Plants Image Classification   \n",
       "686            14 Types of Flower Image Classification   \n",
       "687  525 species, 84635 train, 2625 test, 2625 vali...   \n",
       "688              Classify MRI images into four classes   \n",
       "\n",
       "                                           description  \\\n",
       "0    ### Context\\n\\nRecommendation systems are used...   \n",
       "1    This dataset provides comprehensive informatio...   \n",
       "2    Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3    \\nIn this dataset all time top  movies data is...   \n",
       "4    Context\\nThis dataset contains information abo...   \n",
       "..                                                 ...   \n",
       "684  This dataset contains about ~25k images from a...   \n",
       "685  The dataset contains 30 types of plants images...   \n",
       "686  The dataset contains 14 types of flower images...   \n",
       "687  This version of the dataset adds 10 new specie...   \n",
       "688  # Contribute to OpenSource\\n##Repo: [GitHub](h...   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    [arts and entertainment, movies and tv shows, ...   \n",
       "1                                [movies and tv shows]   \n",
       "2                                [movies and tv shows]   \n",
       "3                                [movies and tv shows]   \n",
       "4    [movies and tv shows, beginner, intermediate, ...   \n",
       "..                                                 ...   \n",
       "684  [movies and tv shows, computer vision, classif...   \n",
       "685                   [biology, classification, image]   \n",
       "686                   [classification, image, chinese]   \n",
       "687  [biology, computer vision, classification, cnn...   \n",
       "688  [health, medicine, classification, health cond...   \n",
       "\n",
       "                                titleNullable  \\\n",
       "0            Top 10000 Popular Movies Dataset   \n",
       "1                IMDB Top 1000 Movies Dataset   \n",
       "2                Top 1000 IMDb Movies Dataset   \n",
       "3       Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4            Top 1000 Highest Grossing Movies   \n",
       "..                                        ...   \n",
       "684                      Scene Classification   \n",
       "685                     Plants Classification   \n",
       "686                     Flower Classification   \n",
       "687  BIRDS 525  SPECIES- IMAGE CLASSIFICATION   \n",
       "688          Brain Tumor Classification (MRI)   \n",
       "\n",
       "                                      subtitleNullable  \\\n",
       "0       Top 10000 Popular movies based on TMDB ratings   \n",
       "1    \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2    Discover the Greatest Movies of All Time - IMD...   \n",
       "3    In this dataset the top movies data is given b...   \n",
       "4            Top 1000 Highest Grossing Holywood Movies   \n",
       "..                                                 ...   \n",
       "684  Contains ~25K images from a wide range of natu...   \n",
       "685            30 Types of Plants Image Classification   \n",
       "686            14 Types of Flower Image Classification   \n",
       "687  525 species, 84635 train, 2625 test, 2625 vali...   \n",
       "688              Classify MRI images into four classes   \n",
       "\n",
       "                                   descriptionNullable  \\\n",
       "0    ### Context\\n\\nRecommendation systems are used...   \n",
       "1    This dataset provides comprehensive informatio...   \n",
       "2    Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3    \\nIn this dataset all time top  movies data is...   \n",
       "4    Context\\nThis dataset contains information abo...   \n",
       "..                                                 ...   \n",
       "684  This dataset contains about ~25k images from a...   \n",
       "685  The dataset contains 30 types of plants images...   \n",
       "686  The dataset contains 14 types of flower images...   \n",
       "687  This version of the dataset adds 10 new specie...   \n",
       "688  # Contribute to OpenSource\\n##Repo: [GitHub](h...   \n",
       "\n",
       "                                                  text  \n",
       "0     Top 10000 Popular Movies Dataset Top 10000 Po...  \n",
       "1     IMDB Top 1000 Movies Dataset \"Release Year, D...  \n",
       "2     Top 1000 IMDb Movies Dataset Discover the Gre...  \n",
       "3     Top Rated 10000 Movies Dataset (IMDB) In this...  \n",
       "4     Top 1000 Highest Grossing Movies Top 1000 Hig...  \n",
       "..                                                 ...  \n",
       "684   Scene Classification Contains ~25K images fro...  \n",
       "685   Plants Classification 30 Types of Plants Imag...  \n",
       "686   Flower Classification 14 Types of Flower Imag...  \n",
       "687   BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...  \n",
       "688   Brain Tumor Classification (MRI) Classify MRI...  \n",
       "\n",
       "[689 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01610cd-272e-4173-9d58-38065acdf2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_hot\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_hot' is not defined"
     ]
    }
   ],
   "source": [
    "len(data_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6b2cd2-d7f1-4fbf-96e2-5d42452ccbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0        0\n",
       " 1        1\n",
       " 2        2\n",
       " 3        3\n",
       " 4        4\n",
       "       ... \n",
       " 684    685\n",
       " 685    686\n",
       " 686    687\n",
       " 687    688\n",
       " 688    689\n",
       " Name: id, Length: 689, dtype: int64,\n",
       " 0       Top 10000 Popular Movies Dataset Top 10000 Po...\n",
       " 1       IMDB Top 1000 Movies Dataset \"Release Year, D...\n",
       " 2       Top 1000 IMDb Movies Dataset Discover the Gre...\n",
       " 3       Top Rated 10000 Movies Dataset (IMDB) In this...\n",
       " 4       Top 1000 Highest Grossing Movies Top 1000 Hig...\n",
       "                              ...                        \n",
       " 684     Scene Classification Contains ~25K images fro...\n",
       " 685     Plants Classification 30 Types of Plants Imag...\n",
       " 686     Flower Classification 14 Types of Flower Imag...\n",
       " 687     BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...\n",
       " 688     Brain Tumor Classification (MRI) Classify MRI...\n",
       " Name: text, Length: 689, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hot['id'], data_hot['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6396c90a-545d-4069-956c-91e47fa015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = search_data_index_vote['query name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecafd6e-8a62-41a6-919b-4ea7063b7dbf",
   "metadata": {},
   "source": [
    "#### load dataset for traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fcdf985-0f5e-46be-bec7-0cc230e23bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a028ea-efc2-48bb-b513-701715004307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>path to metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>omkarborikar/top-10000-popular-movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>ritiksharma07/imdb-top-1000-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>inductiveanks/top-1000-imdb-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>umangdobariya1436/top-rated-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5</td>\n",
       "      <td>sanjeetsinghnaik/top-1000-highest-grossing-movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>classification</td>\n",
       "      <td>16</td>\n",
       "      <td>nitishabharathi/scene-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>classification</td>\n",
       "      <td>17</td>\n",
       "      <td>marquis03/plants-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>classification</td>\n",
       "      <td>18</td>\n",
       "      <td>marquis03/flower-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>classification</td>\n",
       "      <td>19</td>\n",
       "      <td>gpiosenka/100-bird-species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>classification</td>\n",
       "      <td>20</td>\n",
       "      <td>sartajbhuvaji/brain-tumor-classification-mri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  query name  rating_hottest  \\\n",
       "0    Top 1000 Movies Dataset               1   \n",
       "1    Top 1000 Movies Dataset               2   \n",
       "2    Top 1000 Movies Dataset               3   \n",
       "3    Top 1000 Movies Dataset               4   \n",
       "4    Top 1000 Movies Dataset               5   \n",
       "..                       ...             ...   \n",
       "685           classification              16   \n",
       "686           classification              17   \n",
       "687           classification              18   \n",
       "688           classification              19   \n",
       "689           classification              20   \n",
       "\n",
       "                                      path to metadata  \n",
       "0                omkarborikar/top-10000-popular-movies  \n",
       "1           ritiksharma07/imdb-top-1000-movies-dataset  \n",
       "2           inductiveanks/top-1000-imdb-movies-dataset  \n",
       "3           umangdobariya1436/top-rated-movies-dataset  \n",
       "4    sanjeetsinghnaik/top-1000-highest-grossing-movies  \n",
       "..                                                 ...  \n",
       "685               nitishabharathi/scene-classification  \n",
       "686                    marquis03/plants-classification  \n",
       "687                    marquis03/flower-classification  \n",
       "688                         gpiosenka/100-bird-species  \n",
       "689       sartajbhuvaji/brain-tumor-classification-mri  \n",
       "\n",
       "[690 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1bdc75e-bedc-4383-901f-f26750e6c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query name', 'rating_hottest', 'path to metadata'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data_index_hot.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4493963-1c3d-4307-bcb6-e5277de2e86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>titleNullable</th>\n",
       "      <th>subtitleNullable</th>\n",
       "      <th>descriptionNullable</th>\n",
       "      <th>text</th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>path to metadata</th>\n",
       "      <th>rating_active</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>[arts and entertainment, movies and tv shows, ...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset Top 10000 Po...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1.0</td>\n",
       "      <td>omkarborikar/top-10000-popular-movies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "      <td>1</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset \"Release Year, D...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ritiksharma07/imdb-top-1000-movies-dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>2</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset Discover the Gre...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3.0</td>\n",
       "      <td>inductiveanks/top-1000-imdb-movies-dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>3</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB) In this...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4.0</td>\n",
       "      <td>umangdobariya1436/top-rated-movies-dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>4</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>[movies and tv shows, beginner, intermediate, ...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies Top 1000 Hig...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5.0</td>\n",
       "      <td>sanjeetsinghnaik/top-1000-highest-grossing-movies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/t...</td>\n",
       "      <td>1704</td>\n",
       "      <td>Airline Passenger Satisfaction</td>\n",
       "      <td>What factors lead to customer satisfaction for...</td>\n",
       "      <td># **Context**\\nThis dataset contains an airlin...</td>\n",
       "      <td>[classification, clustering, tabular, binary c...</td>\n",
       "      <td>Airline Passenger Satisfaction</td>\n",
       "      <td>What factors lead to customer satisfaction for...</td>\n",
       "      <td># **Context**\\nThis dataset contains an airlin...</td>\n",
       "      <td>Airline Passenger Satisfaction What factors l...</td>\n",
       "      <td>classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>teejmahal20/airline-passenger-satisfaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/a...</td>\n",
       "      <td>1705</td>\n",
       "      <td>Sign Language Digits Dataset</td>\n",
       "      <td>Turkey Ankara Ayrancı Anadolu High School's Si...</td>\n",
       "      <td>### Context\\n\\nSign languages (also known as s...</td>\n",
       "      <td>[languages, education, image, primary and seco...</td>\n",
       "      <td>Sign Language Digits Dataset</td>\n",
       "      <td>Turkey Ankara Ayrancı Anadolu High School's Si...</td>\n",
       "      <td>### Context\\n\\nSign languages (also known as s...</td>\n",
       "      <td>Sign Language Digits Dataset Turkey Ankara Ay...</td>\n",
       "      <td>classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ardamavi/sign-language-digits-dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/a...</td>\n",
       "      <td>1706</td>\n",
       "      <td>Animals-10</td>\n",
       "      <td>Animal pictures of 10 different categories tak...</td>\n",
       "      <td>Hello everyone! \\n\\nThis is the dataset I have...</td>\n",
       "      <td>[categorical, animals, computer vision, classi...</td>\n",
       "      <td>Animals-10</td>\n",
       "      <td>Animal pictures of 10 different categories tak...</td>\n",
       "      <td>Hello everyone! \\n\\nThis is the dataset I have...</td>\n",
       "      <td>Animals-10 Animal pictures of 10 different ca...</td>\n",
       "      <td>classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alessiocorrado99/animals10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/p...</td>\n",
       "      <td>1707</td>\n",
       "      <td>Blood Cell Images</td>\n",
       "      <td>12,500 images: 4 different cell types</td>\n",
       "      <td>### Context\\n\\nThe diagnosis of blood-based di...</td>\n",
       "      <td>[biology, medicine, classification, image, hea...</td>\n",
       "      <td>Blood Cell Images</td>\n",
       "      <td>12,500 images: 4 different cell types</td>\n",
       "      <td>### Context\\n\\nThe diagnosis of blood-based di...</td>\n",
       "      <td>Blood Cell Images 12,500 images: 4 different ...</td>\n",
       "      <td>classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paultimothymooney/blood-cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/k...</td>\n",
       "      <td>1708</td>\n",
       "      <td>Indicators of Heart Disease (2022 UPDATE)</td>\n",
       "      <td>2022 annual CDC survey data of 400k+ adults re...</td>\n",
       "      <td># Key Indicators of Heart Disease\\n## 2022 ann...</td>\n",
       "      <td>[healthcare, health, data visualization, data ...</td>\n",
       "      <td>Indicators of Heart Disease (2022 UPDATE)</td>\n",
       "      <td>2022 annual CDC survey data of 400k+ adults re...</td>\n",
       "      <td># Key Indicators of Heart Disease\\n## 2022 ann...</td>\n",
       "      <td>Indicators of Heart Disease (2022 UPDATE) 202...</td>\n",
       "      <td>classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kamilpytlak/personal-key-indicators-of-heart-d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2853 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path    id  \\\n",
       "0     ../datasets/raw_datasets/metadata-for-search/o...     0   \n",
       "1     ../datasets/raw_datasets/metadata-for-search/r...     1   \n",
       "2     ../datasets/raw_datasets/metadata-for-search/i...     2   \n",
       "3     ../datasets/raw_datasets/metadata-for-search/u...     3   \n",
       "4     ../datasets/raw_datasets/metadata-for-search/s...     4   \n",
       "...                                                 ...   ...   \n",
       "1702  ../datasets/raw_datasets/metadata-for-search/t...  1704   \n",
       "1703  ../datasets/raw_datasets/metadata-for-search/a...  1705   \n",
       "1704  ../datasets/raw_datasets/metadata-for-search/a...  1706   \n",
       "1705  ../datasets/raw_datasets/metadata-for-search/p...  1707   \n",
       "1706  ../datasets/raw_datasets/metadata-for-search/k...  1708   \n",
       "\n",
       "                                          title  \\\n",
       "0              Top 10000 Popular Movies Dataset   \n",
       "1                  IMDB Top 1000 Movies Dataset   \n",
       "2                  Top 1000 IMDb Movies Dataset   \n",
       "3         Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4              Top 1000 Highest Grossing Movies   \n",
       "...                                         ...   \n",
       "1702             Airline Passenger Satisfaction   \n",
       "1703               Sign Language Digits Dataset   \n",
       "1704                                 Animals-10   \n",
       "1705                          Blood Cell Images   \n",
       "1706  Indicators of Heart Disease (2022 UPDATE)   \n",
       "\n",
       "                                               subtitle  \\\n",
       "0        Top 10000 Popular movies based on TMDB ratings   \n",
       "1     \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2     Discover the Greatest Movies of All Time - IMD...   \n",
       "3     In this dataset the top movies data is given b...   \n",
       "4             Top 1000 Highest Grossing Holywood Movies   \n",
       "...                                                 ...   \n",
       "1702  What factors lead to customer satisfaction for...   \n",
       "1703  Turkey Ankara Ayrancı Anadolu High School's Si...   \n",
       "1704  Animal pictures of 10 different categories tak...   \n",
       "1705              12,500 images: 4 different cell types   \n",
       "1706  2022 annual CDC survey data of 400k+ adults re...   \n",
       "\n",
       "                                            description  \\\n",
       "0     ### Context\\n\\nRecommendation systems are used...   \n",
       "1     This dataset provides comprehensive informatio...   \n",
       "2     Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3     \\nIn this dataset all time top  movies data is...   \n",
       "4     Context\\nThis dataset contains information abo...   \n",
       "...                                                 ...   \n",
       "1702  # **Context**\\nThis dataset contains an airlin...   \n",
       "1703  ### Context\\n\\nSign languages (also known as s...   \n",
       "1704  Hello everyone! \\n\\nThis is the dataset I have...   \n",
       "1705  ### Context\\n\\nThe diagnosis of blood-based di...   \n",
       "1706  # Key Indicators of Heart Disease\\n## 2022 ann...   \n",
       "\n",
       "                                               keywords  \\\n",
       "0     [arts and entertainment, movies and tv shows, ...   \n",
       "1                                 [movies and tv shows]   \n",
       "2                                 [movies and tv shows]   \n",
       "3                                 [movies and tv shows]   \n",
       "4     [movies and tv shows, beginner, intermediate, ...   \n",
       "...                                                 ...   \n",
       "1702  [classification, clustering, tabular, binary c...   \n",
       "1703  [languages, education, image, primary and seco...   \n",
       "1704  [categorical, animals, computer vision, classi...   \n",
       "1705  [biology, medicine, classification, image, hea...   \n",
       "1706  [healthcare, health, data visualization, data ...   \n",
       "\n",
       "                                  titleNullable  \\\n",
       "0              Top 10000 Popular Movies Dataset   \n",
       "1                  IMDB Top 1000 Movies Dataset   \n",
       "2                  Top 1000 IMDb Movies Dataset   \n",
       "3         Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4              Top 1000 Highest Grossing Movies   \n",
       "...                                         ...   \n",
       "1702             Airline Passenger Satisfaction   \n",
       "1703               Sign Language Digits Dataset   \n",
       "1704                                 Animals-10   \n",
       "1705                          Blood Cell Images   \n",
       "1706  Indicators of Heart Disease (2022 UPDATE)   \n",
       "\n",
       "                                       subtitleNullable  \\\n",
       "0        Top 10000 Popular movies based on TMDB ratings   \n",
       "1     \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2     Discover the Greatest Movies of All Time - IMD...   \n",
       "3     In this dataset the top movies data is given b...   \n",
       "4             Top 1000 Highest Grossing Holywood Movies   \n",
       "...                                                 ...   \n",
       "1702  What factors lead to customer satisfaction for...   \n",
       "1703  Turkey Ankara Ayrancı Anadolu High School's Si...   \n",
       "1704  Animal pictures of 10 different categories tak...   \n",
       "1705              12,500 images: 4 different cell types   \n",
       "1706  2022 annual CDC survey data of 400k+ adults re...   \n",
       "\n",
       "                                    descriptionNullable  \\\n",
       "0     ### Context\\n\\nRecommendation systems are used...   \n",
       "1     This dataset provides comprehensive informatio...   \n",
       "2     Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3     \\nIn this dataset all time top  movies data is...   \n",
       "4     Context\\nThis dataset contains information abo...   \n",
       "...                                                 ...   \n",
       "1702  # **Context**\\nThis dataset contains an airlin...   \n",
       "1703  ### Context\\n\\nSign languages (also known as s...   \n",
       "1704  Hello everyone! \\n\\nThis is the dataset I have...   \n",
       "1705  ### Context\\n\\nThe diagnosis of blood-based di...   \n",
       "1706  # Key Indicators of Heart Disease\\n## 2022 ann...   \n",
       "\n",
       "                                                   text  \\\n",
       "0      Top 10000 Popular Movies Dataset Top 10000 Po...   \n",
       "1      IMDB Top 1000 Movies Dataset \"Release Year, D...   \n",
       "2      Top 1000 IMDb Movies Dataset Discover the Gre...   \n",
       "3      Top Rated 10000 Movies Dataset (IMDB) In this...   \n",
       "4      Top 1000 Highest Grossing Movies Top 1000 Hig...   \n",
       "...                                                 ...   \n",
       "1702   Airline Passenger Satisfaction What factors l...   \n",
       "1703   Sign Language Digits Dataset Turkey Ankara Ay...   \n",
       "1704   Animals-10 Animal pictures of 10 different ca...   \n",
       "1705   Blood Cell Images 12,500 images: 4 different ...   \n",
       "1706   Indicators of Heart Disease (2022 UPDATE) 202...   \n",
       "\n",
       "                   query name  rating_hottest  \\\n",
       "0     Top 1000 Movies Dataset             1.0   \n",
       "1     Top 1000 Movies Dataset             2.0   \n",
       "2     Top 1000 Movies Dataset             3.0   \n",
       "3     Top 1000 Movies Dataset             4.0   \n",
       "4     Top 1000 Movies Dataset             5.0   \n",
       "...                       ...             ...   \n",
       "1702           classification             NaN   \n",
       "1703           classification             NaN   \n",
       "1704           classification             NaN   \n",
       "1705           classification             NaN   \n",
       "1706           classification             NaN   \n",
       "\n",
       "                                       path to metadata  rating_active  rating  \n",
       "0                 omkarborikar/top-10000-popular-movies            NaN     NaN  \n",
       "1            ritiksharma07/imdb-top-1000-movies-dataset            NaN     NaN  \n",
       "2            inductiveanks/top-1000-imdb-movies-dataset            NaN     NaN  \n",
       "3            umangdobariya1436/top-rated-movies-dataset            NaN     NaN  \n",
       "4     sanjeetsinghnaik/top-1000-highest-grossing-movies            NaN     NaN  \n",
       "...                                                 ...            ...     ...  \n",
       "1702         teejmahal20/airline-passenger-satisfaction            NaN    51.0  \n",
       "1703              ardamavi/sign-language-digits-dataset            NaN    52.0  \n",
       "1704                         alessiocorrado99/animals10            NaN    53.0  \n",
       "1705                      paultimothymooney/blood-cells            NaN    54.0  \n",
       "1706  kamilpytlak/personal-key-indicators-of-heart-d...            NaN    55.0  \n",
       "\n",
       "[2853 rows x 15 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = pd.merge(data_hot, search_data_index_hot, how=\"left\", on=[\"id\", \"id\"])\n",
    "result2 = pd.merge(data_active, search_data_index_active, how=\"left\", on=[\"id\", \"id\"])\n",
    "result3 = pd.merge(data_vote, search_data_index_vote, how=\"left\", on=[\"id\", \"id\"])\n",
    "\n",
    "result = pd.concat([result1,result2,result3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23148ac6-67f2-46b6-8a10-10319fdc0fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>titleNullable</th>\n",
       "      <th>subtitleNullable</th>\n",
       "      <th>descriptionNullable</th>\n",
       "      <th>text</th>\n",
       "      <th>query name</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>path to metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>[arts and entertainment, movies and tv shows, ...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset</td>\n",
       "      <td>Top 10000 Popular movies based on TMDB ratings</td>\n",
       "      <td>### Context\\n\\nRecommendation systems are used...</td>\n",
       "      <td>Top 10000 Popular Movies Dataset Top 10000 Po...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>omkarborikar/top-10000-popular-movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/r...</td>\n",
       "      <td>1</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset</td>\n",
       "      <td>\"Release Year, Duration, Ratings, Metascores, ...</td>\n",
       "      <td>This dataset provides comprehensive informatio...</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset \"Release Year, D...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>ritiksharma07/imdb-top-1000-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/i...</td>\n",
       "      <td>2</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset</td>\n",
       "      <td>Discover the Greatest Movies of All Time - IMD...</td>\n",
       "      <td>Welcome to the \"Top 1000 IMDb Movies Dataset\" ...</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset Discover the Gre...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>3</td>\n",
       "      <td>inductiveanks/top-1000-imdb-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/u...</td>\n",
       "      <td>3</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>[movies and tv shows]</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB)</td>\n",
       "      <td>In this dataset the top movies data is given b...</td>\n",
       "      <td>\\nIn this dataset all time top  movies data is...</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB) In this...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>4</td>\n",
       "      <td>umangdobariya1436/top-rated-movies-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>4</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>[movies and tv shows, beginner, intermediate, ...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies</td>\n",
       "      <td>Top 1000 Highest Grossing Holywood Movies</td>\n",
       "      <td>Context\\nThis dataset contains information abo...</td>\n",
       "      <td>Top 1000 Highest Grossing Movies Top 1000 Hig...</td>\n",
       "      <td>Top 1000 Movies Dataset</td>\n",
       "      <td>5</td>\n",
       "      <td>sanjeetsinghnaik/top-1000-highest-grossing-movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/n...</td>\n",
       "      <td>685</td>\n",
       "      <td>Scene Classification</td>\n",
       "      <td>Contains ~25K images from a wide range of natu...</td>\n",
       "      <td>This dataset contains about ~25k images from a...</td>\n",
       "      <td>[movies and tv shows, computer vision, classif...</td>\n",
       "      <td>Scene Classification</td>\n",
       "      <td>Contains ~25K images from a wide range of natu...</td>\n",
       "      <td>This dataset contains about ~25k images from a...</td>\n",
       "      <td>Scene Classification Contains ~25K images fro...</td>\n",
       "      <td>classification</td>\n",
       "      <td>16</td>\n",
       "      <td>nitishabharathi/scene-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>686</td>\n",
       "      <td>Plants Classification</td>\n",
       "      <td>30 Types of Plants Image Classification</td>\n",
       "      <td>The dataset contains 30 types of plants images...</td>\n",
       "      <td>[biology, classification, image]</td>\n",
       "      <td>Plants Classification</td>\n",
       "      <td>30 Types of Plants Image Classification</td>\n",
       "      <td>The dataset contains 30 types of plants images...</td>\n",
       "      <td>Plants Classification 30 Types of Plants Imag...</td>\n",
       "      <td>classification</td>\n",
       "      <td>17</td>\n",
       "      <td>marquis03/plants-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/m...</td>\n",
       "      <td>687</td>\n",
       "      <td>Flower Classification</td>\n",
       "      <td>14 Types of Flower Image Classification</td>\n",
       "      <td>The dataset contains 14 types of flower images...</td>\n",
       "      <td>[classification, image, chinese]</td>\n",
       "      <td>Flower Classification</td>\n",
       "      <td>14 Types of Flower Image Classification</td>\n",
       "      <td>The dataset contains 14 types of flower images...</td>\n",
       "      <td>Flower Classification 14 Types of Flower Imag...</td>\n",
       "      <td>classification</td>\n",
       "      <td>18</td>\n",
       "      <td>marquis03/flower-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/g...</td>\n",
       "      <td>688</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION</td>\n",
       "      <td>525 species, 84635 train, 2625 test, 2625 vali...</td>\n",
       "      <td>This version of the dataset adds 10 new specie...</td>\n",
       "      <td>[biology, computer vision, classification, cnn...</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION</td>\n",
       "      <td>525 species, 84635 train, 2625 test, 2625 vali...</td>\n",
       "      <td>This version of the dataset adds 10 new specie...</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...</td>\n",
       "      <td>classification</td>\n",
       "      <td>19</td>\n",
       "      <td>gpiosenka/100-bird-species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>../datasets/raw_datasets/metadata-for-search/s...</td>\n",
       "      <td>689</td>\n",
       "      <td>Brain Tumor Classification (MRI)</td>\n",
       "      <td>Classify MRI images into four classes</td>\n",
       "      <td># Contribute to OpenSource\\n##Repo: [GitHub](h...</td>\n",
       "      <td>[health, medicine, classification, health cond...</td>\n",
       "      <td>Brain Tumor Classification (MRI)</td>\n",
       "      <td>Classify MRI images into four classes</td>\n",
       "      <td># Contribute to OpenSource\\n##Repo: [GitHub](h...</td>\n",
       "      <td>Brain Tumor Classification (MRI) Classify MRI...</td>\n",
       "      <td>classification</td>\n",
       "      <td>20</td>\n",
       "      <td>sartajbhuvaji/brain-tumor-classification-mri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path   id  \\\n",
       "0    ../datasets/raw_datasets/metadata-for-search/o...    0   \n",
       "1    ../datasets/raw_datasets/metadata-for-search/r...    1   \n",
       "2    ../datasets/raw_datasets/metadata-for-search/i...    2   \n",
       "3    ../datasets/raw_datasets/metadata-for-search/u...    3   \n",
       "4    ../datasets/raw_datasets/metadata-for-search/s...    4   \n",
       "..                                                 ...  ...   \n",
       "684  ../datasets/raw_datasets/metadata-for-search/n...  685   \n",
       "685  ../datasets/raw_datasets/metadata-for-search/m...  686   \n",
       "686  ../datasets/raw_datasets/metadata-for-search/m...  687   \n",
       "687  ../datasets/raw_datasets/metadata-for-search/g...  688   \n",
       "688  ../datasets/raw_datasets/metadata-for-search/s...  689   \n",
       "\n",
       "                                        title  \\\n",
       "0            Top 10000 Popular Movies Dataset   \n",
       "1                IMDB Top 1000 Movies Dataset   \n",
       "2                Top 1000 IMDb Movies Dataset   \n",
       "3       Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4            Top 1000 Highest Grossing Movies   \n",
       "..                                        ...   \n",
       "684                      Scene Classification   \n",
       "685                     Plants Classification   \n",
       "686                     Flower Classification   \n",
       "687  BIRDS 525  SPECIES- IMAGE CLASSIFICATION   \n",
       "688          Brain Tumor Classification (MRI)   \n",
       "\n",
       "                                              subtitle  \\\n",
       "0       Top 10000 Popular movies based on TMDB ratings   \n",
       "1    \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2    Discover the Greatest Movies of All Time - IMD...   \n",
       "3    In this dataset the top movies data is given b...   \n",
       "4            Top 1000 Highest Grossing Holywood Movies   \n",
       "..                                                 ...   \n",
       "684  Contains ~25K images from a wide range of natu...   \n",
       "685            30 Types of Plants Image Classification   \n",
       "686            14 Types of Flower Image Classification   \n",
       "687  525 species, 84635 train, 2625 test, 2625 vali...   \n",
       "688              Classify MRI images into four classes   \n",
       "\n",
       "                                           description  \\\n",
       "0    ### Context\\n\\nRecommendation systems are used...   \n",
       "1    This dataset provides comprehensive informatio...   \n",
       "2    Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3    \\nIn this dataset all time top  movies data is...   \n",
       "4    Context\\nThis dataset contains information abo...   \n",
       "..                                                 ...   \n",
       "684  This dataset contains about ~25k images from a...   \n",
       "685  The dataset contains 30 types of plants images...   \n",
       "686  The dataset contains 14 types of flower images...   \n",
       "687  This version of the dataset adds 10 new specie...   \n",
       "688  # Contribute to OpenSource\\n##Repo: [GitHub](h...   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    [arts and entertainment, movies and tv shows, ...   \n",
       "1                                [movies and tv shows]   \n",
       "2                                [movies and tv shows]   \n",
       "3                                [movies and tv shows]   \n",
       "4    [movies and tv shows, beginner, intermediate, ...   \n",
       "..                                                 ...   \n",
       "684  [movies and tv shows, computer vision, classif...   \n",
       "685                   [biology, classification, image]   \n",
       "686                   [classification, image, chinese]   \n",
       "687  [biology, computer vision, classification, cnn...   \n",
       "688  [health, medicine, classification, health cond...   \n",
       "\n",
       "                                titleNullable  \\\n",
       "0            Top 10000 Popular Movies Dataset   \n",
       "1                IMDB Top 1000 Movies Dataset   \n",
       "2                Top 1000 IMDb Movies Dataset   \n",
       "3       Top Rated 10000 Movies Dataset (IMDB)   \n",
       "4            Top 1000 Highest Grossing Movies   \n",
       "..                                        ...   \n",
       "684                      Scene Classification   \n",
       "685                     Plants Classification   \n",
       "686                     Flower Classification   \n",
       "687  BIRDS 525  SPECIES- IMAGE CLASSIFICATION   \n",
       "688          Brain Tumor Classification (MRI)   \n",
       "\n",
       "                                      subtitleNullable  \\\n",
       "0       Top 10000 Popular movies based on TMDB ratings   \n",
       "1    \"Release Year, Duration, Ratings, Metascores, ...   \n",
       "2    Discover the Greatest Movies of All Time - IMD...   \n",
       "3    In this dataset the top movies data is given b...   \n",
       "4            Top 1000 Highest Grossing Holywood Movies   \n",
       "..                                                 ...   \n",
       "684  Contains ~25K images from a wide range of natu...   \n",
       "685            30 Types of Plants Image Classification   \n",
       "686            14 Types of Flower Image Classification   \n",
       "687  525 species, 84635 train, 2625 test, 2625 vali...   \n",
       "688              Classify MRI images into four classes   \n",
       "\n",
       "                                   descriptionNullable  \\\n",
       "0    ### Context\\n\\nRecommendation systems are used...   \n",
       "1    This dataset provides comprehensive informatio...   \n",
       "2    Welcome to the \"Top 1000 IMDb Movies Dataset\" ...   \n",
       "3    \\nIn this dataset all time top  movies data is...   \n",
       "4    Context\\nThis dataset contains information abo...   \n",
       "..                                                 ...   \n",
       "684  This dataset contains about ~25k images from a...   \n",
       "685  The dataset contains 30 types of plants images...   \n",
       "686  The dataset contains 14 types of flower images...   \n",
       "687  This version of the dataset adds 10 new specie...   \n",
       "688  # Contribute to OpenSource\\n##Repo: [GitHub](h...   \n",
       "\n",
       "                                                  text  \\\n",
       "0     Top 10000 Popular Movies Dataset Top 10000 Po...   \n",
       "1     IMDB Top 1000 Movies Dataset \"Release Year, D...   \n",
       "2     Top 1000 IMDb Movies Dataset Discover the Gre...   \n",
       "3     Top Rated 10000 Movies Dataset (IMDB) In this...   \n",
       "4     Top 1000 Highest Grossing Movies Top 1000 Hig...   \n",
       "..                                                 ...   \n",
       "684   Scene Classification Contains ~25K images fro...   \n",
       "685   Plants Classification 30 Types of Plants Imag...   \n",
       "686   Flower Classification 14 Types of Flower Imag...   \n",
       "687   BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...   \n",
       "688   Brain Tumor Classification (MRI) Classify MRI...   \n",
       "\n",
       "                  query name  rating_hottest  \\\n",
       "0    Top 1000 Movies Dataset               1   \n",
       "1    Top 1000 Movies Dataset               2   \n",
       "2    Top 1000 Movies Dataset               3   \n",
       "3    Top 1000 Movies Dataset               4   \n",
       "4    Top 1000 Movies Dataset               5   \n",
       "..                       ...             ...   \n",
       "684           classification              16   \n",
       "685           classification              17   \n",
       "686           classification              18   \n",
       "687           classification              19   \n",
       "688           classification              20   \n",
       "\n",
       "                                      path to metadata  \n",
       "0                omkarborikar/top-10000-popular-movies  \n",
       "1           ritiksharma07/imdb-top-1000-movies-dataset  \n",
       "2           inductiveanks/top-1000-imdb-movies-dataset  \n",
       "3           umangdobariya1436/top-rated-movies-dataset  \n",
       "4    sanjeetsinghnaik/top-1000-highest-grossing-movies  \n",
       "..                                                 ...  \n",
       "684               nitishabharathi/scene-classification  \n",
       "685                    marquis03/plants-classification  \n",
       "686                    marquis03/flower-classification  \n",
       "687                         gpiosenka/100-bird-species  \n",
       "688       sartajbhuvaji/brain-tumor-classification-mri  \n",
       "\n",
       "[689 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "092b642e-fd2d-482d-8a45-a92b44f6f059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rating_hottest</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Top 10000 Popular Movies Dataset Top 10000 Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IMDB Top 1000 Movies Dataset \"Release Year, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Top 1000 IMDb Movies Dataset Discover the Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Top Rated 10000 Movies Dataset (IMDB) In this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Top 1000 Highest Grossing Movies Top 1000 Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>685</td>\n",
       "      <td>16</td>\n",
       "      <td>Scene Classification Contains ~25K images fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>686</td>\n",
       "      <td>17</td>\n",
       "      <td>Plants Classification 30 Types of Plants Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>687</td>\n",
       "      <td>18</td>\n",
       "      <td>Flower Classification 14 Types of Flower Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>688</td>\n",
       "      <td>19</td>\n",
       "      <td>BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>689</td>\n",
       "      <td>20</td>\n",
       "      <td>Brain Tumor Classification (MRI) Classify MRI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  rating_hottest                                               text\n",
       "0      0               1   Top 10000 Popular Movies Dataset Top 10000 Po...\n",
       "1      1               2   IMDB Top 1000 Movies Dataset \"Release Year, D...\n",
       "2      2               3   Top 1000 IMDb Movies Dataset Discover the Gre...\n",
       "3      3               4   Top Rated 10000 Movies Dataset (IMDB) In this...\n",
       "4      4               5   Top 1000 Highest Grossing Movies Top 1000 Hig...\n",
       "..   ...             ...                                                ...\n",
       "684  685              16   Scene Classification Contains ~25K images fro...\n",
       "685  686              17   Plants Classification 30 Types of Plants Imag...\n",
       "686  687              18   Flower Classification 14 Types of Flower Imag...\n",
       "687  688              19   BIRDS 525  SPECIES- IMAGE CLASSIFICATION 525 ...\n",
       "688  689              20   Brain Tumor Classification (MRI) Classify MRI...\n",
       "\n",
       "[689 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result[['id','rating_hottest','text']]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2d55ee-8023-4d61-a3fe-98f4399c8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(sample):\n",
    "    sample['responce'] = sample['positive']+sample['negative']\n",
    "    sample['target'] = list(range(len(sample['responce'])))\n",
    "    return sample\n",
    "ask_dataset1 = ask_dataset.map(process_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96b153-e606-4ccf-b8d2-91ab7c0d456a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba447cd7-f1dc-48ef-8ad8-261476281eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danil/Text_search_with_LLM/sample/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a93a46e0b47138e8a5155db03f7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae23a166dee1422f92679270e514c7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb34e499bbf4be1a38131f45f5bc1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4369020c0fd446ca8bb7b188113f40ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc855bc0edc14925a80465da736fb156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8f83a5437a4c698d0641731b0832c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef95623243146ffac8b940907621879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857f6213f984496a9c789dd02d75fd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dc9989cf9c4a56b39f43e850393459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd0c6b00e3648d3aa7c2837798af20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d659ebe8a49847a59a67113e1c3c37aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b81c8-4e01-45ca-83e8-3dd82e0bf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Initialize the CoSENTLoss\n",
    "# This loss requires pairs of text and a floating point similarity score as a label\n",
    "loss = CoSENTLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "train_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train\")\n",
    "\"\"\"\n",
    "Dataset({\n",
    "    features: ['sentence1', 'sentence2', 'label'],\n",
    "    num_rows: 942069\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d037a9f-14a4-4c64-a656-64b11a960a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/mpnet-base-all-nli-triplet\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"mpnet-base-all-nli-triplet\",  # Used in W&B if `wandb` is installed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485093-939e-4c8e-a91e-e5eddbc22e1f",
   "metadata": {},
   "source": [
    "### Inference batching utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d3b30fd-d769-4f2a-866f-7cbba27b2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function return batches of our datasets\n",
    "def iterate_batch(dataset,batch_size=128):\n",
    "    for i in range(0,len(dataset),batch_size):\n",
    "        yield dataset[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea71676-1978-46ff-95ab-f362bb256af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iterate_batch at 0x7f52ac12a570>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_batch([1,2,3,4,5,6,7,8,9,10],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ef28e-9ecc-4523-8b88-960e3edb3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d0abf6-4929-4b43-a2b0-7141e850137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danil/Text_search_with_LLM/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.138 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unsloth/qwen2-0.5b-bnb-4bit does not have a padding token! Will use pad_token = <|PAD_TOKEN|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen2-0.5b-bnb-4bit\",\n",
    "]\n",
    "model1, tokenizer1 = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2-0.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1ae7ae-1358-4e78-a5bf-6e40c7f3cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.7 patched 24 layers with 0 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model1 = FastLanguageModel.get_peft_model(\n",
    "    model1,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16f8cf8-16be-4538-abd2-bb981e4d59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "EOS_TOKEN = tokenizer1.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bae5dba-33ca-4158-a080-22cfe84e30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 11.6k/11.6k [00:00<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|██████████| 44.3M/44.3M [00:01<00:00, 28.7MB/s]\n",
      "Generating train split: 100%|██████████| 51760/51760 [00:00<00:00, 111364.89 examples/s]\n",
      "Map: 100%|██████████| 51760/51760 [00:00<00:00, 87507.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd6639e-3cfb-43ad-8156-6afe957beb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 51760/51760 [00:15<00:00, 3401.23 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model = model1,\n",
    "    tokenizer = tokenizer1,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps = 20,\n",
    "        max_steps = 120,\n",
    "        learning_rate = 5e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3e539c-920c-4756-b89b-0c21c9c7ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 120\n",
      " \"-____-\"     Number of trainable parameters = 8,798,208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.258900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e673c26-3bc3-4b04-96ce-90fe1911e88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### qween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa08e1a-335a-4184-a666-6cab7f2812d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, cast\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from chromadb.api.types import EmbeddingFunction, Documents, Embeddings\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class TransformerEmbeddingFunction_Qween(EmbeddingFunction[Documents]):    \n",
    "    \"\"\"\n",
    "    The TransformerEmbeddingFunction_Qween class is an embedding function\n",
    "    that uses a pre-trained transformer model to generate embeddings for a\n",
    "    given set of documents. It is designed to work with the ChromaDB\n",
    "    library for building vector search engines.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Load a pre-trained tokenizer and model from the 'Alibaba-NLP/gte-Qwen2-7B-instruct' model\n",
    "        checkpoint\n",
    "        It also sets the max_length parameter, which is the maximum length of input sequences\n",
    "        that the model can handle.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-7B-instruct', trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained('Alibaba-NLP/gte-Qwen2-7B-instruct', trust_remote_code=True)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = 8192\n",
    "        return\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"\n",
    "        The __call__ method is the main entry point for generating embeddings. It takes a list of documents as input and returns a list of embeddings.\n",
    "        It first defines a helper function called last_token_pool that extracts the embedding of the last token from the model's output. This is done by checking if the input sequence has left padding and selecting the appropriate token index.\n",
    "        The input documents are tokenized using the tokenizer and passed through the model to obtain the last hidden states.\n",
    "        The last_token_pool function is applied to the last hidden states to extract the final embeddings.\n",
    "        The embeddings are converted to a list and returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        def last_token_pool(last_hidden_states: Tensor,attention_mask: Tensor) -> Tensor:\n",
    "            left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "            if left_padding:\n",
    "                return last_hidden_states[:, -1]\n",
    "            else:\n",
    "                sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "                batch_size = last_hidden_states.shape[0]\n",
    "                return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "        embeddings = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in iterate_batch(input,3):\n",
    "                embedding=0\n",
    "            \n",
    "                batch_dict = self.tokenizer(batch, max_length=self.max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                outputs = self.model(**batch_dict)\n",
    "                embedding = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "                \n",
    "                if type(embeddings) is int:\n",
    "                        embeddings = embedding\n",
    "                else:\n",
    "                    embeddings = torch.cat((embeddings,embedding),dim=0)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def preprocess_query(self,text_list):\n",
    "        \"\"\"\n",
    "        The preprocess_query method is used to preprocess the input queries before generating embeddings.\n",
    "        It defines a task description and a helper function called get_detailed_instruct that appends the task description to each query.\n",
    "        The method applies the get_detailed_instruct function to each query in the input list and returns the preprocessed queries.\n",
    "        Overall, this code provides a convenient way to use a pre-trained transformer model for generating document embeddings in the context of a vector search engine built with ChromaDB.        \n",
    "        \"\"\"\n",
    "        \n",
    "        task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "            return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "        text_list = [get_detailed_instruct(task, text) for text in text_list]\n",
    "        return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587d51d-3289-4c12-b364-33c67bf1a39f",
   "metadata": {},
   "source": [
    "### Load vector database for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74672989-37e9-462b-b545-2b474b023a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6ae56f94c44c23b5da31c210bd93a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_embed = TransformerEmbeddingFunction_Qween()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a0d4f35-48f8-4e38-9216-e331ae466673",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_hot = []\n",
    "all_results_vote = []\n",
    "all_results_active = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c09080-4f88-499f-9a4b-30041eab66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e235ef-8854-4dc3-ab91-9328e54dbe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qween\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2b3b1d-915d-4c1c-96d4-7e3ffd4872dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_time = 0\n",
    "search_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf6a649-e242-4c17-a17a-e1708a6a5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"seach_db_vote\")\n",
    "# client.delete_collection(\"seach_db_active\")\n",
    "# client.delete_collection(\"seach_db_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf114de5-3996-40b3-bbd7-175462f5ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f044fc91-e511-4e43-a9a2-894f8cdb31d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import time\n",
    "\n",
    "metrics[model_name] = {\"active\":{},\"vote\":{},\"hot\":{}}\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection_vote = client.create_collection(\"seach_db_vote\",get_or_create=False,embedding_function=model_embed)\n",
    "collection_active = client.create_collection(\"seach_db_active\",get_or_create=False,embedding_function=model_embed)\n",
    "collection_hot = client.create_collection(\"seach_db_hot\",get_or_create=False,embedding_function=model_embed)\n",
    "# collection = client.create_collection(\"test_seach_fb\",embedding_function,get_or_create=True)\n",
    "\n",
    "# load\n",
    "load_time = time.time()\n",
    "collection_vote.add(\n",
    "    documents=list(data_vote['text']),\n",
    "    ids=[str(i) for i in data_vote['id']]\n",
    ")\n",
    "load_time = time.time() - load_time\n",
    "metrics[model_name]['vote']['load_time'] = load_time\n",
    "\n",
    "load_time = time.time()\n",
    "collection_active.add(\n",
    "    documents=list(data_active['text']),\n",
    "    ids=[str(i) for i in data_active['id']]\n",
    ")\n",
    "load_time = time.time() - load_time\n",
    "metrics[model_name]['active']['load_time'] = load_time\n",
    "\n",
    "load_time = time.time()\n",
    "collection_hot.add(\n",
    "    documents=list(data_hot['text']),\n",
    "    ids=[str(i) for i in data_hot['id']]\n",
    ")\n",
    "load_time = time.time() - load_time\n",
    "metrics[model_name]['hot']['load_time'] = load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7d8523-22d6-4148-8c90-cfc768afcd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_time = time.time()\n",
    "queries = list(search_data_index_vote['query name'].unique())\n",
    "queries = model_embed.preprocess_query(queries)\n",
    "results_vote = collection_vote.query(\n",
    "    query_texts=queries, # Chroma will embed this for you\n",
    "    n_results=40 # how many results to return\n",
    ")\n",
    "search_time = time.time() - search_time\n",
    "metrics[model_name]['vote']['search_time'] = search_time\n",
    "\n",
    "search_time = time.time()\n",
    "queries = list(search_data_index_hot['query name'].unique())\n",
    "queries = model_embed.preprocess_query(queries)\n",
    "results_hot = collection_hot.query(\n",
    "    query_texts=queries, # Chroma will embed this for you\n",
    "    n_results=40 # how many results to return\n",
    ")\n",
    "search_time = time.time() - search_time\n",
    "metrics[model_name]['hot']['search_time'] = search_time\n",
    "\n",
    "search_time = time.time()\n",
    "queries = list(search_data_index_active['query name'].unique())\n",
    "queries = model_embed.preprocess_query(queries)\n",
    "results_active = collection_active.query(\n",
    "    query_texts=queries, # Chroma will embed this for you\n",
    "    n_results=40 # how many results to return\n",
    ")\n",
    "search_time = time.time() - search_time\n",
    "metrics[model_name]['active']['search_time'] = search_time\n",
    "\n",
    "client.delete_collection(\"seach_db_vote\")\n",
    "client.delete_collection(\"seach_db_active\")\n",
    "client.delete_collection(\"seach_db_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "943594fe-03bd-4faf-87d3-49950daeea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['query', 'positive', 'negative', 'responce', 'target'],\n",
       "        num_rows: 375\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13580-9c62-45dd-a441-5a0f1539a941",
   "metadata": {},
   "source": [
    "### load vector base for ask ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775eeaaf-298d-4641-9652-d3fc2dcb7adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e7f152-4290-4c14-878e-b11e2e62928c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmetrics2\u001b[49m[model_name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mubuntu\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}}\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m      8\u001b[0m collection_ubuntu \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseach_db_ubuntu\u001b[39m\u001b[38;5;124m\"\u001b[39m,get_or_create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,embedding_function\u001b[38;5;241m=\u001b[39mmodel_embed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics2' is not defined"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import time\n",
    "\n",
    "metrics2[model_name] = {\"ubuntu\":{}}\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection_ubuntu = client.create_collection(\"seach_db_ubuntu\",get_or_create=False,embedding_function=model_embed)\n",
    "# collection = client.create_collection(\"test_seach_fb\",embedding_function,get_or_create=True)\n",
    "\n",
    "# load\n",
    "load_time = time.time()\n",
    "collection_vote.add(\n",
    "    documents=list(ask_dataset1['responce'])\n",
    ")\n",
    "load_time = time.time() - load_time\n",
    "metrics[model_name]['vote']['load_time'] = load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fea093-75eb-4d46-8625-64ec57b83a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time = time.time()\n",
    "queries = list(search_data_index_vote['query name'].unique())\n",
    "queries = model_embed.preprocess_query(queries)\n",
    "results_vote = collection_vote.query(\n",
    "    query_texts=queries, # Chroma will embed this for you\n",
    "    n_results=40 # how many results to return\n",
    ")\n",
    "search_time = time.time() - search_time\n",
    "metrics[model_name]['vote']['search_time'] = search_time\n",
    "\n",
    "client.delete_collection(\"seach_db_hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f77150-cabb-46f3-a3c7-051dd053fd99",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00c2405c-5209-4fb0-9dc8-8ae212f55408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12771d54-dcdb-4f7c-9c45-6f0d2f8bd608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea1f37-3342-4fab-aa24-f21dd61b4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_algs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b335fe02-1596-4c01-9626-6501b5d55704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danil/Text_search_with_LLM/sample/lib/python3.10/site-packages/ranx/metrics/recall.py:29: NumbaTypeSafetyWarning: unsafe cast from uint64 to int64. Precision may be lost.\n",
      "  scores[i] = _recall(qrels[i], run[i], k, rel_lvl)\n"
     ]
    }
   ],
   "source": [
    "metrics[model_name]['vote']['metrics'] = evaluate_algs(results_vote,search_data_index_vote,column_name='rating')\n",
    "metrics[model_name]['active']['metrics'] = evaluate_algs(results_active,search_data_index_active,column_name='rating_active')\n",
    "metrics[model_name]['hot']['metrics'] = evaluate_algs(results_hot,search_data_index_hot,column_name='rating_hottest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7273f380-f929-4a8b-b745-5b682a87e4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.2288268800739379,\n",
       " 'precision@10': 0.5782608695652174,\n",
       " 'hits@10': 5.782608695652174,\n",
       " 'map@10': 0.18642717258326347,\n",
       " 'mrr@10': 0.7815217391304349,\n",
       " 'ndcg@10': 0.390860835402863,\n",
       " 'recall@40': 0.5568091062932357,\n",
       " 'precision@40': 0.46141304347826095,\n",
       " 'hits@40': 18.456521739130434,\n",
       " 'map@40': 0.40018413730541147,\n",
       " 'mrr@40': 0.7826659038901601,\n",
       " 'ndcg@40': 0.48348643757160337}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[model_name]['vote']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d6e91d8-8226-45b5-958c-f93e7fdc1e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.537359022556391,\n",
       " 'precision@10': 0.531578947368421,\n",
       " 'hits@10': 5.315789473684211,\n",
       " 'map@10': 0.42852861319966573,\n",
       " 'mrr@10': 0.7907894736842105,\n",
       " 'ndcg@10': 0.5269621411880194,\n",
       " 'recall@40': 0.8498746867167922,\n",
       " 'precision@40': 0.2513157894736842,\n",
       " 'hits@40': 10.052631578947368,\n",
       " 'map@40': 0.6052456815282098,\n",
       " 'mrr@40': 0.7953748006379586,\n",
       " 'ndcg@40': 0.6456429264912914}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[model_name]['active']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b201ca17-587d-4b8b-9d30-99d50e90407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.4683898803216984,\n",
       " 'precision@10': 0.6249999999999999,\n",
       " 'hits@10': 6.25,\n",
       " 'map@10': 0.3976503272160739,\n",
       " 'mrr@10': 0.8358585858585859,\n",
       " 'ndcg@10': 0.44386353572764725,\n",
       " 'recall@40': 0.8556416437098256,\n",
       " 'precision@40': 0.32784090909090907,\n",
       " 'hits@40': 13.113636363636363,\n",
       " 'map@40': 0.6334733570628028,\n",
       " 'mrr@40': 0.8369408369408369,\n",
       " 'ndcg@40': 0.6110369817475614}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[model_name]['hot']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710caa53-7dee-4bd9-947a-251726bab1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample",
   "language": "python",
   "name": "sample"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
