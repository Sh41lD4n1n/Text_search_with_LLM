{"path":{"0":"..\/datasets\/raw_datasets\/metadata-for-search\/ritiksharma07\/imdb-top-1000-movies-dataset\/dataset-metadata.json","1":"..\/datasets\/raw_datasets\/metadata-for-search\/emanchauhdary\/imdb-top-1000-dataset\/dataset-metadata.json","2":"..\/datasets\/raw_datasets\/metadata-for-search\/sharathmudigoudr\/imdb-movie-dataset-from-year-1893-to-2020\/dataset-metadata.json","3":"..\/datasets\/raw_datasets\/metadata-for-search\/kunalduttads\/tmdb-top-10000-10k-movies-dataset\/dataset-metadata.json","4":"..\/datasets\/raw_datasets\/metadata-for-search\/dogacelik\/google-data-analytics-capstone-project-netflix\/dataset-metadata.json","5":"..\/datasets\/raw_datasets\/metadata-for-search\/asaniczka\/movie-identification-dataset-800-movies\/dataset-metadata.json","6":"..\/datasets\/raw_datasets\/metadata-for-search\/islamic\/imdb-top-rated-1000-movies\/dataset-metadata.json","7":"..\/datasets\/raw_datasets\/metadata-for-search\/wm1deep7\/top-1000-imdb-movies-dataset-web-scraping\/dataset-metadata.json","8":"..\/datasets\/raw_datasets\/metadata-for-search\/sejalhanmante\/imdb-top-1000-movies-dataset\/dataset-metadata.json","9":"..\/datasets\/raw_datasets\/metadata-for-search\/mayankray\/imdb-top-1000-movies-dataset\/dataset-metadata.json","10":"..\/datasets\/raw_datasets\/metadata-for-search\/armanansari\/imdb-anime-data\/dataset-metadata.json","11":"..\/datasets\/raw_datasets\/metadata-for-search\/shoaibrkhan\/top-10000-voted-imdb-movies-dataset\/dataset-metadata.json","12":"..\/datasets\/raw_datasets\/metadata-for-search\/maliktariq5554\/ttidm\/dataset-metadata.json","13":"..\/datasets\/raw_datasets\/metadata-for-search\/atulkishore\/imdb-top-1000-movies\/dataset-metadata.json","14":"..\/datasets\/raw_datasets\/metadata-for-search\/inductiveanks\/top-1000-imdb-movies-dataset\/dataset-metadata.json","15":"..\/datasets\/raw_datasets\/metadata-for-search\/adityaghuse\/imdb-top-1000\/dataset-metadata.json","16":"..\/datasets\/raw_datasets\/metadata-for-search\/arthurchongg\/imdb-top-1000-movies\/dataset-metadata.json","17":"..\/datasets\/raw_datasets\/metadata-for-search\/moazeldsokyx\/imdb-top-10000-movies-dataset\/dataset-metadata.json","18":"..\/datasets\/raw_datasets\/metadata-for-search\/rahafassedah\/imdb-dataset-of-top-1000-movies-and-tv-shows\/dataset-metadata.json","19":"..\/datasets\/raw_datasets\/metadata-for-search\/azzahrabeladina\/imdb-movies-dataset\/dataset-metadata.json","20":"..\/datasets\/raw_datasets\/metadata-for-search\/preethampandiri\/clothing-reviews\/dataset-metadata.json","21":"..\/datasets\/raw_datasets\/metadata-for-search\/preethampandiri\/womens-clothing-reviews\/dataset-metadata.json","22":"..\/datasets\/raw_datasets\/metadata-for-search\/preethampandiri\/womens-clothing-reviews-new\/dataset-metadata.json","23":"..\/datasets\/raw_datasets\/metadata-for-search\/nourmekkijj\/reddit-posts-on-borderline-personality-disorder\/dataset-metadata.json","24":"..\/datasets\/raw_datasets\/metadata-for-search\/nourmekkijj\/stress-and-anxiety-posts-on-reddit\/dataset-metadata.json","25":"..\/datasets\/raw_datasets\/metadata-for-search\/nourmekkijj\/stress-and-anxiety-posts-on-reddit\/dataset-metadata.json","26":"..\/datasets\/raw_datasets\/metadata-for-search\/meetsolanki530\/emotion-classification-dataset\/dataset-metadata.json","27":"..\/datasets\/raw_datasets\/metadata-for-search\/borhanitrash\/twitter-financial-news-sentiment-dataset\/dataset-metadata.json","28":"..\/datasets\/raw_datasets\/metadata-for-search\/shibbir282\/sentiment-analysis-of-tweet-reviews\/dataset-metadata.json","29":"..\/datasets\/raw_datasets\/metadata-for-search\/shibbir282\/twitter-customer-reviews-of-popular-smart-phone\/dataset-metadata.json","30":"..\/datasets\/raw_datasets\/metadata-for-search\/maaz4465081\/chatgpt-dataset-50000-tweets-from-initial-month\/dataset-metadata.json","31":"..\/datasets\/raw_datasets\/metadata-for-search\/pyrotech\/twitterdata\/dataset-metadata.json","32":"..\/datasets\/raw_datasets\/metadata-for-search\/adhamelkomy\/twitter-emotion-dataset\/dataset-metadata.json","33":"..\/datasets\/raw_datasets\/metadata-for-search\/bhavikjikadara\/emotions-dataset\/dataset-metadata.json","34":"..\/datasets\/raw_datasets\/metadata-for-search\/nishkoder\/clean-dataset\/dataset-metadata.json","35":"..\/datasets\/raw_datasets\/metadata-for-search\/bhavikjikadara\/tweets-dataset\/dataset-metadata.json","36":"..\/datasets\/raw_datasets\/metadata-for-search\/nelgiriyewithana\/emotions\/dataset-metadata.json","37":"..\/datasets\/raw_datasets\/metadata-for-search\/akashhiremath25\/eventclassifier-twitter-data-set\/dataset-metadata.json","38":"..\/datasets\/raw_datasets\/metadata-for-search\/zzsleepyzzz\/crypto-tweets-data-and-sentiments\/dataset-metadata.json","39":"..\/datasets\/raw_datasets\/metadata-for-search\/paakhim10\/tweets-and-engagement-metrics\/dataset-metadata.json","40":"..\/datasets\/raw_datasets\/metadata-for-search\/thedevastator\/short-jokes-dataset\/dataset-metadata.json","41":"..\/datasets\/raw_datasets\/metadata-for-search\/thedevastator\/hate-speech-and-offensive-language-detection\/dataset-metadata.json","42":"..\/datasets\/raw_datasets\/metadata-for-search\/anveshrao123\/indian-political-tweets\/dataset-metadata.json","43":"..\/datasets\/raw_datasets\/metadata-for-search\/ishaanraina\/us-airlines-sentiment-analysis-in-knime-analytics\/dataset-metadata.json","44":"..\/datasets\/raw_datasets\/metadata-for-search\/mdforiduzzamanzihad\/preprocessed-dell-tweets\/dataset-metadata.json","45":"..\/datasets\/raw_datasets\/metadata-for-search\/joulespinozasanchez\/web-scrapping-twitter-racism\/dataset-metadata.json","46":"..\/datasets\/raw_datasets\/metadata-for-search\/irvinmaceke\/twitter-sentiment-analysis-xitsonga\/dataset-metadata.json","47":"..\/datasets\/raw_datasets\/metadata-for-search\/arnavsmayan\/urban-mobility-dataset\/dataset-metadata.json","48":"..\/datasets\/raw_datasets\/metadata-for-search\/devarajv88\/yulu-dataset\/dataset-metadata.json","49":"..\/datasets\/raw_datasets\/metadata-for-search\/jordanln\/natural-language-processing-of-chatbot-prompts\/dataset-metadata.json","50":"..\/datasets\/raw_datasets\/metadata-for-search\/sreekargv\/bike-rentals\/dataset-metadata.json","51":"..\/datasets\/raw_datasets\/metadata-for-search\/tinnyrobot\/opticom-signal-quality-dataset\/dataset-metadata.json","52":"..\/datasets\/raw_datasets\/metadata-for-search\/umairziact\/city-traffic-and-vehicle-behavior-dataset\/dataset-metadata.json","53":"..\/datasets\/raw_datasets\/metadata-for-search\/thedevastator\/us-automatic-traffic-recorder-stations-data\/dataset-metadata.json","54":"..\/datasets\/raw_datasets\/metadata-for-search\/willianoliveiragibin\/economy-health-brazil\/dataset-metadata.json","55":"..\/datasets\/raw_datasets\/metadata-for-search\/sobhanmoosavi\/us-traffic-congestions-2016-2022\/dataset-metadata.json","56":"..\/datasets\/raw_datasets\/metadata-for-search\/hasibullahaman\/traffic-prediction-dataset\/dataset-metadata.json","57":"..\/datasets\/raw_datasets\/metadata-for-search\/yusufberksardoan\/traffic-detection-project\/dataset-metadata.json","58":"..\/datasets\/raw_datasets\/metadata-for-search\/jamesdatasets\/newcastles-main-4-junctions-traffic-data\/dataset-metadata.json","59":"..\/datasets\/raw_datasets\/metadata-for-search\/brianyuen1997\/cities-skyline-springfield\/dataset-metadata.json","60":"..\/datasets\/raw_datasets\/metadata-for-search\/abdallahwagih\/cars-detection\/dataset-metadata.json","61":"..\/datasets\/raw_datasets\/metadata-for-search\/harbhajansingh21\/bike-sharing-dataset\/dataset-metadata.json","62":"..\/datasets\/raw_datasets\/metadata-for-search\/itangishakadjuma\/all-traffic-signs-dataset\/dataset-metadata.json","63":"..\/datasets\/raw_datasets\/metadata-for-search\/hessamsh\/iranian-traffic-sign-detection\/dataset-metadata.json","64":"..\/datasets\/raw_datasets\/metadata-for-search\/zeyad1mashhour\/driver-inattention-detection-dataset\/dataset-metadata.json","65":"..\/datasets\/raw_datasets\/metadata-for-search\/tuanai\/traffic-signs-dataset\/dataset-metadata.json","66":"..\/datasets\/raw_datasets\/metadata-for-search\/kartikeybartwal\/dataset\/dataset-metadata.json","67":"..\/datasets\/raw_datasets\/metadata-for-search\/haopengcheng\/wildlife\/dataset-metadata.json","68":"..\/datasets\/raw_datasets\/metadata-for-search\/borhanitrash\/cat-dataset\/dataset-metadata.json","69":"..\/datasets\/raw_datasets\/metadata-for-search\/patricklford\/flunet-global-influenza-programme-who\/dataset-metadata.json","70":"..\/datasets\/raw_datasets\/metadata-for-search\/gilraitses\/whaletagdata\/dataset-metadata.json","71":"..\/datasets\/raw_datasets\/metadata-for-search\/kamalmoha\/local-ocean-conservation-sea-turtle-face-detection\/dataset-metadata.json","72":"..\/datasets\/raw_datasets\/metadata-for-search\/georgemartvel\/catflw\/dataset-metadata.json","73":"..\/datasets\/raw_datasets\/metadata-for-search\/chathinirathnayake\/macaque-monkey-images\/dataset-metadata.json","74":"..\/datasets\/raw_datasets\/metadata-for-search\/ivannikolov\/reactive-anomaly-synthetic-data\/dataset-metadata.json","75":"..\/datasets\/raw_datasets\/metadata-for-search\/priyankank\/sri-lankan-fish-species-dataset\/dataset-metadata.json","76":"..\/datasets\/raw_datasets\/metadata-for-search\/lrntszab\/guinea-pig-detection\/dataset-metadata.json","77":"..\/datasets\/raw_datasets\/metadata-for-search\/sigmakhanam\/fish-detection\/dataset-metadata.json","78":"..\/datasets\/raw_datasets\/metadata-for-search\/patricklford\/covid-19\/dataset-metadata.json","79":"..\/datasets\/raw_datasets\/metadata-for-search\/waheed9002\/hiking-wildlife\/dataset-metadata.json","80":"..\/datasets\/raw_datasets\/metadata-for-search\/battularahul\/birds-bats-grasshoppers-audio\/dataset-metadata.json","81":"..\/datasets\/raw_datasets\/metadata-for-search\/xuhuilin0541\/zebrafish-mirror-biting\/dataset-metadata.json","82":"..\/datasets\/raw_datasets\/metadata-for-search\/patricklford\/water-and-air-quality\/dataset-metadata.json","83":"..\/datasets\/raw_datasets\/metadata-for-search\/wangsheng0352\/ir-wildlife\/dataset-metadata.json","84":"..\/datasets\/raw_datasets\/metadata-for-search\/ahmedmostafa11111\/cd-deep\/dataset-metadata.json","85":"..\/datasets\/raw_datasets\/metadata-for-search\/jonasblind\/brackishsgd\/dataset-metadata.json","86":"..\/datasets\/raw_datasets\/metadata-for-search\/amester\/malflow\/dataset-metadata.json","87":"..\/datasets\/raw_datasets\/metadata-for-search\/nizamuddinmaitlo\/different-colors-in-challenging-lightening\/dataset-metadata.json","88":"..\/datasets\/raw_datasets\/metadata-for-search\/shivapan\/classified-nih-dataset\/dataset-metadata.json","89":"..\/datasets\/raw_datasets\/metadata-for-search\/vinitdesai564\/cats-dogs-classification-dataset\/dataset-metadata.json","90":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2020-jpg-256x256-resized\/dataset-metadata.json","91":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2020-jpg-224x224-resized\/dataset-metadata.json","92":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2019-jpg-256x256-resized\/dataset-metadata.json","93":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2019-jpg-224x224-resized\/dataset-metadata.json","94":"..\/datasets\/raw_datasets\/metadata-for-search\/micscodes\/dresden-image-database\/dataset-metadata.json","95":"..\/datasets\/raw_datasets\/metadata-for-search\/ashitosh100\/image-classification\/dataset-metadata.json","96":"..\/datasets\/raw_datasets\/metadata-for-search\/alexanderqi\/imagenet100-inrs-dataset\/dataset-metadata.json","97":"..\/datasets\/raw_datasets\/metadata-for-search\/sumn2u\/riped-and-unriped-tomato-dataset\/dataset-metadata.json","98":"..\/datasets\/raw_datasets\/metadata-for-search\/mohamedmohsen3330\/song-popularity-classification\/dataset-metadata.json","99":"..\/datasets\/raw_datasets\/metadata-for-search\/niharikaamritkar\/industrial-tools-classification\/dataset-metadata.json","100":"..\/datasets\/raw_datasets\/metadata-for-search\/huthayfahodeb\/nih-chest-x-rays-bbox-version\/dataset-metadata.json","101":"..\/datasets\/raw_datasets\/metadata-for-search\/stealthtechnologies\/rock-classification\/dataset-metadata.json","102":"..\/datasets\/raw_datasets\/metadata-for-search\/viktoriiayuzkiv\/the-russia-ukraine-war-images-in-spanish-news\/dataset-metadata.json","103":"..\/datasets\/raw_datasets\/metadata-for-search\/ghostbat101\/lung-x-ray-image-clinical-text-dataset\/dataset-metadata.json","104":"..\/datasets\/raw_datasets\/metadata-for-search\/ahmedkhairy11\/signclusive-mediapipe\/dataset-metadata.json","105":"..\/datasets\/raw_datasets\/metadata-for-search\/kurniawanprasetya\/iman357-wawan339-neww\/dataset-metadata.json","106":"..\/datasets\/raw_datasets\/metadata-for-search\/lakshmipragnakollu\/road-accident-data-set\/dataset-metadata.json","107":"..\/datasets\/raw_datasets\/metadata-for-search\/aneekeshkumar\/kagglex-skill-assessment-challenge-cohort4-dataset\/dataset-metadata.json","108":"..\/datasets\/raw_datasets\/metadata-for-search\/zakariaeyoussefi\/somerville-traffic-crash-dataset\/dataset-metadata.json","109":"..\/datasets\/raw_datasets\/metadata-for-search\/chavindudulaj\/vehicle-maintenance-data\/dataset-metadata.json","110":"..\/datasets\/raw_datasets\/metadata-for-search\/shalukaimbs\/road-accident-data-by-vehicle-type\/dataset-metadata.json","111":"..\/datasets\/raw_datasets\/metadata-for-search\/abhinavshaw09\/road-accidents-data\/dataset-metadata.json","112":"..\/datasets\/raw_datasets\/metadata-for-search\/tanishqdublish\/urban-traffic-density-in-cities\/dataset-metadata.json","113":"..\/datasets\/raw_datasets\/metadata-for-search\/trinham\/asian-related-factors-in-fatal-crashes\/dataset-metadata.json","114":"..\/datasets\/raw_datasets\/metadata-for-search\/abdulmannann\/road-accidents-csv\/dataset-metadata.json","115":"..\/datasets\/raw_datasets\/metadata-for-search\/pereprosov\/road-accidents-in-the-czech-republic\/dataset-metadata.json","116":"..\/datasets\/raw_datasets\/metadata-for-search\/yyxian\/road-traffic-collision-data-in-northern-ireland\/dataset-metadata.json","117":"..\/datasets\/raw_datasets\/metadata-for-search\/anthonytherrien\/futureflow-navigating-tomorrows-urban-traffic\/dataset-metadata.json","118":"..\/datasets\/raw_datasets\/metadata-for-search\/nextmillionaire\/car-accident-dataset\/dataset-metadata.json","119":"..\/datasets\/raw_datasets\/metadata-for-search\/tannerolstad0947\/2022-uk-road-safety-data\/dataset-metadata.json","120":"..\/datasets\/raw_datasets\/metadata-for-search\/willianoliveiragibin\/road-accident-casualties\/dataset-metadata.json","121":"..\/datasets\/raw_datasets\/metadata-for-search\/thedevastator\/us-automatic-traffic-recorder-stations-data\/dataset-metadata.json","122":"..\/datasets\/raw_datasets\/metadata-for-search\/smmmmmmmmmmmm\/major-road-accidents\/dataset-metadata.json","123":"..\/datasets\/raw_datasets\/metadata-for-search\/brijlaldhankour\/airbag-recommendation\/dataset-metadata.json","124":"..\/datasets\/raw_datasets\/metadata-for-search\/ddosad\/vehicle-accidents\/dataset-metadata.json","125":"..\/datasets\/raw_datasets\/metadata-for-search\/gvamsi1999\/motor-vehicle-collisions-crashes-new-york\/dataset-metadata.json","126":"..\/datasets\/raw_datasets\/metadata-for-search\/ambaliyagati\/spotify-dataset-for-playing-around-with-sql\/dataset-metadata.json","127":"..\/datasets\/raw_datasets\/metadata-for-search\/junaidk0012\/anime-dataset-2024\/dataset-metadata.json","128":"..\/datasets\/raw_datasets\/metadata-for-search\/bhavyadhingra00020\/complete-pokemon-dataset-9th-gen-img-tabular\/dataset-metadata.json","129":"..\/datasets\/raw_datasets\/metadata-for-search\/rishabhbarman\/most-popular-anime-of-all-time\/dataset-metadata.json","130":"..\/datasets\/raw_datasets\/metadata-for-search\/vaipant\/anime-data-from-1970-to-2024\/dataset-metadata.json","131":"..\/datasets\/raw_datasets\/metadata-for-search\/unibahmad\/anime-dataset\/dataset-metadata.json","132":"..\/datasets\/raw_datasets\/metadata-for-search\/junaidk0012\/anime-dataset-2024\/dataset-metadata.json","133":"..\/datasets\/raw_datasets\/metadata-for-search\/duyminhle\/anime-recommendation-system-weight\/dataset-metadata.json","134":"..\/datasets\/raw_datasets\/metadata-for-search\/maulipatel18\/anime-content-based-recommendation-system-datasets\/dataset-metadata.json","135":"..\/datasets\/raw_datasets\/metadata-for-search\/dsfelix\/animes-dataset-2023\/dataset-metadata.json","136":"..\/datasets\/raw_datasets\/metadata-for-search\/vishalsinghsangral\/usa-bank-financial-data\/dataset-metadata.json","137":"..\/datasets\/raw_datasets\/metadata-for-search\/ashishtank24\/nifty-energy-return-month-wise\/dataset-metadata.json","138":"..\/datasets\/raw_datasets\/metadata-for-search\/saurabhbadole\/leading-indian-bank-and-cibil-real-world-dataset\/dataset-metadata.json","139":"..\/datasets\/raw_datasets\/metadata-for-search\/rohit265\/loan-approval-dataset\/dataset-metadata.json","140":"..\/datasets\/raw_datasets\/metadata-for-search\/manyagirdhar\/gesture-recognition-system\/dataset-metadata.json","141":"..\/datasets\/raw_datasets\/metadata-for-search\/nexdatafrank\/gesture-recognition-data\/dataset-metadata.json","142":"..\/datasets\/raw_datasets\/metadata-for-search\/andersarnold\/hand-gesture-recognition-3-classes-64x64\/dataset-metadata.json","143":"..\/datasets\/raw_datasets\/metadata-for-search\/oladayoluke\/enhanced-sign-language-mnist-dataset\/dataset-metadata.json","144":"..\/datasets\/raw_datasets\/metadata-for-search\/sumon3455\/bangla-sign-language-video-dataset\/dataset-metadata.json","145":"..\/datasets\/raw_datasets\/metadata-for-search\/harisudhan411\/hand-navigation-landmarks\/dataset-metadata.json","146":"..\/datasets\/raw_datasets\/metadata-for-search\/omertalhaturkmen\/sample-outputs\/dataset-metadata.json","147":"..\/datasets\/raw_datasets\/metadata-for-search\/rohitagrawal2610\/hand-gesture-recognition-dataset-for-yolov8\/dataset-metadata.json","148":"..\/datasets\/raw_datasets\/metadata-for-search\/drblack00\/isl-csltr-indian-sign-language-dataset\/dataset-metadata.json","149":"..\/datasets\/raw_datasets\/metadata-for-search\/nizamuddinmaitlo\/hgr-dataset\/dataset-metadata.json","150":"..\/datasets\/raw_datasets\/metadata-for-search\/mouadfiali\/sensor-based-american-sign-language-recognition\/dataset-metadata.json","151":"..\/datasets\/raw_datasets\/metadata-for-search\/pradeepisawasan\/malaysian-sign-language-msl-image-dataset\/dataset-metadata.json","152":"..\/datasets\/raw_datasets\/metadata-for-search\/aryannanda17\/masked-gesture-dataset\/dataset-metadata.json","153":"..\/datasets\/raw_datasets\/metadata-for-search\/haccanashraf\/myoelectric-control-for-hand-motions\/dataset-metadata.json","154":"..\/datasets\/raw_datasets\/metadata-for-search\/dylanpallickara129\/asl-alphabet-wireframes\/dataset-metadata.json","155":"..\/datasets\/raw_datasets\/metadata-for-search\/alpesh98\/serbian-sign-language-dataset\/dataset-metadata.json","156":"..\/datasets\/raw_datasets\/metadata-for-search\/nexdatafrank\/sign-language-gestures-recognition-data\/dataset-metadata.json","157":"..\/datasets\/raw_datasets\/metadata-for-search\/sharmagarvit\/sign-language-recognition-dataset\/dataset-metadata.json","158":"..\/datasets\/raw_datasets\/metadata-for-search\/ruinov1st\/gesture-classification-hagrid\/dataset-metadata.json","159":"..\/datasets\/raw_datasets\/metadata-for-search\/adhoppin\/hand-gestures-dataset\/dataset-metadata.json","160":"..\/datasets\/raw_datasets\/metadata-for-search\/abdulrasheed10\/british-airways-reviews\/dataset-metadata.json","161":"..\/datasets\/raw_datasets\/metadata-for-search\/farhakouser\/online-review-csv\/dataset-metadata.json","162":"..\/datasets\/raw_datasets\/metadata-for-search\/rameshvhanamane\/british-airways-reviews-dataset\/dataset-metadata.json","163":"..\/datasets\/raw_datasets\/metadata-for-search\/tobiasbueck\/capterra-reviews\/dataset-metadata.json","164":"..\/datasets\/raw_datasets\/metadata-for-search\/jaskiratsinghjassi\/airline-customer-reviews-dataset\/dataset-metadata.json","165":"..\/datasets\/raw_datasets\/metadata-for-search\/willianoliveiragibin\/commerce-furniture\/dataset-metadata.json","166":"..\/datasets\/raw_datasets\/metadata-for-search\/litsea\/online-store-selling\/dataset-metadata.json","167":"..\/datasets\/raw_datasets\/metadata-for-search\/ranatalha71\/exchange-ratesusd-to-pkrinr-and-cny-2004-2024\/dataset-metadata.json","168":"..\/datasets\/raw_datasets\/metadata-for-search\/bhavyadhingra00020\/longest-running-indian-television-series\/dataset-metadata.json","169":"..\/datasets\/raw_datasets\/metadata-for-search\/ziriantahirli\/top5000-albums-on-spotify-data-analysis\/dataset-metadata.json","170":"..\/datasets\/raw_datasets\/metadata-for-search\/vishalsinghsangral\/usa-bank-financial-data\/dataset-metadata.json","171":"..\/datasets\/raw_datasets\/metadata-for-search\/sailormoon09\/fake-news-and-true-news-dataset\/dataset-metadata.json","172":"..\/datasets\/raw_datasets\/metadata-for-search\/umerhaddii\/us-collegiate-sports-dataset\/dataset-metadata.json","173":"..\/datasets\/raw_datasets\/metadata-for-search\/saishravan5476\/sentinel\/dataset-metadata.json","174":"..\/datasets\/raw_datasets\/metadata-for-search\/sreevaatsavbavana\/linkedin-posts-about-layoffs-2023\/dataset-metadata.json","175":"..\/datasets\/raw_datasets\/metadata-for-search\/lokeshmendake\/fake-news-prediction-datasets\/dataset-metadata.json","176":"..\/datasets\/raw_datasets\/metadata-for-search\/balzzspelok\/celeb-a-2ammu20-a3\/dataset-metadata.json","177":"..\/datasets\/raw_datasets\/metadata-for-search\/joebeachcapital\/facebooks-supreme-court\/dataset-metadata.json","178":"..\/datasets\/raw_datasets\/metadata-for-search\/dalaiaguirre\/israel-hamas-full-articles-nyt\/dataset-metadata.json","179":"..\/datasets\/raw_datasets\/metadata-for-search\/utkarshvenaik\/news-headlines-dataset-with-company-names\/dataset-metadata.json","180":"..\/datasets\/raw_datasets\/metadata-for-search\/huthayfahodeb\/nih-chest-x-rays-bbox-version\/dataset-metadata.json","181":"..\/datasets\/raw_datasets\/metadata-for-search\/leodeveloper\/tmrospacenews\/dataset-metadata.json","182":"..\/datasets\/raw_datasets\/metadata-for-search\/leodeveloper\/truecrimenews\/dataset-metadata.json","183":"..\/datasets\/raw_datasets\/metadata-for-search\/adityakeshri9234\/celeb-df11parts\/dataset-metadata.json","184":"..\/datasets\/raw_datasets\/metadata-for-search\/viktoriiayuzkiv\/the-russia-ukraine-war-images-in-spanish-news\/dataset-metadata.json","185":"..\/datasets\/raw_datasets\/metadata-for-search\/saharshml\/unified-dataset-for-fake-spam-and-legit-data\/dataset-metadata.json","186":"..\/datasets\/raw_datasets\/metadata-for-search\/manishakhadse\/newsqa\/dataset-metadata.json","187":"..\/datasets\/raw_datasets\/metadata-for-search\/tariqodeh\/classified-arabic-news\/dataset-metadata.json","188":"..\/datasets\/raw_datasets\/metadata-for-search\/interviewturabit\/ag-news-dataset\/dataset-metadata.json","189":"..\/datasets\/raw_datasets\/metadata-for-search\/mahmudwasifnafee\/cnn-transformer\/dataset-metadata.json","190":"..\/datasets\/raw_datasets\/metadata-for-search\/tatuldanielyan\/armenian-news\/dataset-metadata.json","191":"..\/datasets\/raw_datasets\/metadata-for-search\/tharindumadhusanka9\/derana-news\/dataset-metadata.json","192":"..\/datasets\/raw_datasets\/metadata-for-search\/qingyuanyao\/fake-news-detection\/dataset-metadata.json","193":"..\/datasets\/raw_datasets\/metadata-for-search\/yangzhou32\/socialnet-weibo-version-2\/dataset-metadata.json","194":"..\/datasets\/raw_datasets\/metadata-for-search\/yangzhou32\/socialnet-weibo-version-1\/dataset-metadata.json","195":"..\/datasets\/raw_datasets\/metadata-for-search\/sohaila103\/fake-news-detection\/dataset-metadata.json","196":"..\/datasets\/raw_datasets\/metadata-for-search\/tasnimniger\/fake-news-detection-data\/dataset-metadata.json","197":"..\/datasets\/raw_datasets\/metadata-for-search\/ahmedkhursheed007\/fake-news-detection-dataset\/dataset-metadata.json","198":"..\/datasets\/raw_datasets\/metadata-for-search\/noorsaeed\/scam-detection-fake-news-labelled-dataset\/dataset-metadata.json","199":"..\/datasets\/raw_datasets\/metadata-for-search\/monkeykingo\/fake-news-detection11\/dataset-metadata.json","200":"..\/datasets\/raw_datasets\/metadata-for-search\/clmentbisaillon\/fake-and-real-news-dataset\/dataset-metadata.json","201":"..\/datasets\/raw_datasets\/metadata-for-search\/nitishjolly\/news-detection-fake-or-real-dataset\/dataset-metadata.json","202":"..\/datasets\/raw_datasets\/metadata-for-search\/asemmustafa\/fake-news-csv\/dataset-metadata.json","203":"..\/datasets\/raw_datasets\/metadata-for-search\/nashatadelabd\/fake-news-detection-with-nlp-and-lstm\/dataset-metadata.json","204":"..\/datasets\/raw_datasets\/metadata-for-search\/quanhv\/fake-news-detection\/dataset-metadata.json","205":"..\/datasets\/raw_datasets\/metadata-for-search\/vishakhdapat\/fake-news-detection\/dataset-metadata.json","206":"..\/datasets\/raw_datasets\/metadata-for-search\/smarafatrahman\/fake-news-detection\/dataset-metadata.json","207":"..\/datasets\/raw_datasets\/metadata-for-search\/dkhalidashik\/fake-news-detection\/dataset-metadata.json","208":"..\/datasets\/raw_datasets\/metadata-for-search\/studymart\/welfake-dataset-for-fake-news\/dataset-metadata.json","209":"..\/datasets\/raw_datasets\/metadata-for-search\/azizurrahmantitumir\/fake-news-master\/dataset-metadata.json","210":"..\/datasets\/raw_datasets\/metadata-for-search\/abdullaalmunna\/fake-news-detection\/dataset-metadata.json","211":"..\/datasets\/raw_datasets\/metadata-for-search\/ajithdevadiga\/retail-stores-sales-dataset\/dataset-metadata.json","212":"..\/datasets\/raw_datasets\/metadata-for-search\/heidielhalwagy\/sales-dataset\/dataset-metadata.json","213":"..\/datasets\/raw_datasets\/metadata-for-search\/yangyinjiawei\/dataset-dashboard\/dataset-metadata.json","214":"..\/datasets\/raw_datasets\/metadata-for-search\/muhammadasifwazir\/pizza-sales-excel-mastersheet\/dataset-metadata.json","215":"..\/datasets\/raw_datasets\/metadata-for-search\/akanksha995579\/pharma-data-analysis\/dataset-metadata.json","216":"..\/datasets\/raw_datasets\/metadata-for-search\/albertnathaniel12\/indonesian-spices-dataset\/dataset-metadata.json","217":"..\/datasets\/raw_datasets\/metadata-for-search\/faldoae\/padangfood\/dataset-metadata.json","218":"..\/datasets\/raw_datasets\/metadata-for-search\/canggih\/indonesian-food-recipes\/dataset-metadata.json","219":"..\/datasets\/raw_datasets\/metadata-for-search\/albertnathaniel12\/indonesian-spices-dataset\/dataset-metadata.json","220":"..\/datasets\/raw_datasets\/metadata-for-search\/kritirathi\/indian-food-dataset-with\/dataset-metadata.json","221":"..\/datasets\/raw_datasets\/metadata-for-search\/araraltawil\/arabic-food-101\/dataset-metadata.json","222":"..\/datasets\/raw_datasets\/metadata-for-search\/faldoae\/padangfood\/dataset-metadata.json","223":"..\/datasets\/raw_datasets\/metadata-for-search\/lazaro97\/peruvian-food-reviews\/dataset-metadata.json","224":"..\/datasets\/raw_datasets\/metadata-for-search\/mattop\/bahama-breeze-menu-nutrition-data\/dataset-metadata.json","225":"..\/datasets\/raw_datasets\/metadata-for-search\/debopamdey\/combined-wine-dataset-red-and-white-wines\/dataset-metadata.json","226":"..\/datasets\/raw_datasets\/metadata-for-search\/whenamancodes\/the-lambada-dataset-for-word-prediction\/dataset-metadata.json","227":"..\/datasets\/raw_datasets\/metadata-for-search\/renanaferreira\/snips-natural-language-understanding-benchmark\/dataset-metadata.json","228":"..\/datasets\/raw_datasets\/metadata-for-search\/zimkey\/college-level-introcs-classchinese\/dataset-metadata.json","229":"..\/datasets\/raw_datasets\/metadata-for-search\/ryanmutiga\/sigma-dolphin-filtered-and-cleaned\/dataset-metadata.json","230":"..\/datasets\/raw_datasets\/metadata-for-search\/yousefsaeedian\/financial-q-and-a-10k\/dataset-metadata.json","231":"..\/datasets\/raw_datasets\/metadata-for-search\/samehraouf\/nlp-neymar-chatbot-dataset\/dataset-metadata.json","232":"..\/datasets\/raw_datasets\/metadata-for-search\/vigneshboss\/coqa-qnda-dataset-with-ground-truth\/dataset-metadata.json","233":"..\/datasets\/raw_datasets\/metadata-for-search\/patrickfleith\/astronautics-multiple-choice-questions-and-answers\/dataset-metadata.json","234":"..\/datasets\/raw_datasets\/metadata-for-search\/stephenabed\/evjvqa\/dataset-metadata.json","235":"..\/datasets\/raw_datasets\/metadata-for-search\/pranavkasela\/se-pqa-personalized-community-question-answering\/dataset-metadata.json","236":"..\/datasets\/raw_datasets\/metadata-for-search\/johnsonhk88\/gsm8k-grade-school-math-8k-dataset-for-llm\/dataset-metadata.json","237":"..\/datasets\/raw_datasets\/metadata-for-search\/omgits0mar\/arabic-instruct-chatbot-dataset\/dataset-metadata.json","238":"..\/datasets\/raw_datasets\/metadata-for-search\/leninprez\/a-case-study-bike-share\/dataset-metadata.json","239":"..\/datasets\/raw_datasets\/metadata-for-search\/yousefsaeedian\/a-new-benchmark-for-financial-question-answering\/dataset-metadata.json","240":"..\/datasets\/raw_datasets\/metadata-for-search\/rishantenis\/visual-question-answering-cv-nlp\/dataset-metadata.json","241":"..\/datasets\/raw_datasets\/metadata-for-search\/mozaman36\/genassocbias\/dataset-metadata.json","242":"..\/datasets\/raw_datasets\/metadata-for-search\/paulojunqueira\/boardgames-rulebooks\/dataset-metadata.json","243":"..\/datasets\/raw_datasets\/metadata-for-search\/namphmquang\/question-answering-dataset\/dataset-metadata.json","244":"..\/datasets\/raw_datasets\/metadata-for-search\/vijaydevane\/blooms-taxonomy-dataset\/dataset-metadata.json","245":"..\/datasets\/raw_datasets\/metadata-for-search\/soohj0\/vizwiz-dataset\/dataset-metadata.json","246":"..\/datasets\/raw_datasets\/metadata-for-search\/osamaabobakr\/red-wine-quality\/dataset-metadata.json","247":"..\/datasets\/raw_datasets\/metadata-for-search\/jayaprabhaa\/wine-quality\/dataset-metadata.json","248":"..\/datasets\/raw_datasets\/metadata-for-search\/debopamdey\/combined-wine-dataset-red-and-white-wines\/dataset-metadata.json","249":"..\/datasets\/raw_datasets\/metadata-for-search\/willianoliveiragibin\/modeling-wine\/dataset-metadata.json","250":"..\/datasets\/raw_datasets\/metadata-for-search\/deepikaarikesavan\/red-wine-quality\/dataset-metadata.json","251":"..\/datasets\/raw_datasets\/metadata-for-search\/kamalesh793\/red-wine-quality-classifier-using-a-neural-network\/dataset-metadata.json","252":"..\/datasets\/raw_datasets\/metadata-for-search\/shashankshekhar1205\/wine-quality-dataset\/dataset-metadata.json","253":"..\/datasets\/raw_datasets\/metadata-for-search\/tasnimniger\/red-wine-quality\/dataset-metadata.json","254":"..\/datasets\/raw_datasets\/metadata-for-search\/gayatrikumar94\/dataset\/dataset-metadata.json","255":"..\/datasets\/raw_datasets\/metadata-for-search\/nitirajkulkarni\/lqr-stock-performance\/dataset-metadata.json","256":"..\/datasets\/raw_datasets\/metadata-for-search\/nitirajkulkarni\/fll-stock-performance\/dataset-metadata.json","257":"..\/datasets\/raw_datasets\/metadata-for-search\/sukhmandeepsinghbrar\/house-prices-india\/dataset-metadata.json","258":"..\/datasets\/raw_datasets\/metadata-for-search\/anitakataria\/bengaluru-house-dataset\/dataset-metadata.json","259":"..\/datasets\/raw_datasets\/metadata-for-search\/rituparnaghosh18\/transformed-housing-data-2\/dataset-metadata.json","260":"..\/datasets\/raw_datasets\/metadata-for-search\/rituparnaghosh18\/transformed-housing-data\/dataset-metadata.json","261":"..\/datasets\/raw_datasets\/metadata-for-search\/rituparnaghosh18\/raw-housing-cost-data-imputed\/dataset-metadata.json","262":"..\/datasets\/raw_datasets\/metadata-for-search\/nitirajkulkarni\/treehouse-ns-stock-performance\/dataset-metadata.json","263":"..\/datasets\/raw_datasets\/metadata-for-search\/soylevbeytullah\/house-prices-dataset\/dataset-metadata.json","264":"..\/datasets\/raw_datasets\/metadata-for-search\/rohit265\/housing-sales-factors-influencing-sale-prices\/dataset-metadata.json","265":"..\/datasets\/raw_datasets\/metadata-for-search\/sheemazain\/house-price-predication\/dataset-metadata.json","266":"..\/datasets\/raw_datasets\/metadata-for-search\/rohanparekh\/housing-price-prediction\/dataset-metadata.json","267":"..\/datasets\/raw_datasets\/metadata-for-search\/talietzin\/singapore-hdb-resale-prices-1990-to-2023\/dataset-metadata.json","268":"..\/datasets\/raw_datasets\/metadata-for-search\/tanisha1604\/amazon-books-dataset-genre-sub-genre-and-books\/dataset-metadata.json","269":"..\/datasets\/raw_datasets\/metadata-for-search\/andpereira\/portugal-real-estate-house-pricing\/dataset-metadata.json","270":"..\/datasets\/raw_datasets\/metadata-for-search\/akshit2605\/house-prices\/dataset-metadata.json","271":"..\/datasets\/raw_datasets\/metadata-for-search\/bhavyadhingra00020\/india-rental-house-price\/dataset-metadata.json","272":"..\/datasets\/raw_datasets\/metadata-for-search\/sky2656\/california-real-state-dataset\/dataset-metadata.json","273":"..\/datasets\/raw_datasets\/metadata-for-search\/sahil23009\/gurgoan-properties-cleaned-dataset\/dataset-metadata.json","274":"..\/datasets\/raw_datasets\/metadata-for-search\/shivapan\/classified-nih-dataset\/dataset-metadata.json","275":"..\/datasets\/raw_datasets\/metadata-for-search\/huthayfahodeb\/nih-chest-x-rays-bbox-version\/dataset-metadata.json","276":"..\/datasets\/raw_datasets\/metadata-for-search\/ghostbat101\/lung-x-ray-image-clinical-text-dataset\/dataset-metadata.json","277":"..\/datasets\/raw_datasets\/metadata-for-search\/guillemsimhospedales\/improved-x-ray-data\/dataset-metadata.json","278":"..\/datasets\/raw_datasets\/metadata-for-search\/sanidhyagoel\/covid-19-x-ray-classification-dataset\/dataset-metadata.json","279":"..\/datasets\/raw_datasets\/metadata-for-search\/spritan1\/yolo-annotated-chestxray-8-object-detection\/dataset-metadata.json","280":"..\/datasets\/raw_datasets\/metadata-for-search\/bihalneupane\/vinbigdata-chest-x-ray-yolo\/dataset-metadata.json","281":"..\/datasets\/raw_datasets\/metadata-for-search\/chirantanacharyya\/normal-tuberculosis-covid\/dataset-metadata.json","282":"..\/datasets\/raw_datasets\/metadata-for-search\/rishantenis\/pulmonary-chest-x-ray-abnormalities\/dataset-metadata.json","283":"..\/datasets\/raw_datasets\/metadata-for-search\/huynhphuochai\/chest-x-ray-dataset\/dataset-metadata.json","284":"..\/datasets\/raw_datasets\/metadata-for-search\/soylevbeytullah\/ds4work-operation-department\/dataset-metadata.json","285":"..\/datasets\/raw_datasets\/metadata-for-search\/sumaiyaakter5\/chest-x-ray-images\/dataset-metadata.json","286":"..\/datasets\/raw_datasets\/metadata-for-search\/rupeshmahanty\/pneumonia-tuberculosis-normal\/dataset-metadata.json","287":"..\/datasets\/raw_datasets\/metadata-for-search\/sangeethakallat\/chest-radiograph-images-pneumonia-and-normal\/dataset-metadata.json","288":"..\/datasets\/raw_datasets\/metadata-for-search\/mohammedsalf\/chest-x-ray-images\/dataset-metadata.json","289":"..\/datasets\/raw_datasets\/metadata-for-search\/ahmedabdellahismail\/covid-19-chest-x-ray\/dataset-metadata.json","290":"..\/datasets\/raw_datasets\/metadata-for-search\/agungpremananda\/covid-19\/dataset-metadata.json","291":"..\/datasets\/raw_datasets\/metadata-for-search\/moazeldsokyx\/chest-x-ray-images-pneumonia-dataset\/dataset-metadata.json","292":"..\/datasets\/raw_datasets\/metadata-for-search\/fool2huo\/pneumonia-cxr-dataset\/dataset-metadata.json","293":"..\/datasets\/raw_datasets\/metadata-for-search\/fatemehmehrparvar\/lung-disease\/dataset-metadata.json","294":"..\/datasets\/raw_datasets\/metadata-for-search\/kohlain\/a-song-of-ice-and-fire-dialogues\/dataset-metadata.json","295":"..\/datasets\/raw_datasets\/metadata-for-search\/whenamancodes\/the-lambada-dataset-for-word-prediction\/dataset-metadata.json","296":"..\/datasets\/raw_datasets\/metadata-for-search\/caitrionaleuenberger\/booksales\/dataset-metadata.json","297":"..\/datasets\/raw_datasets\/metadata-for-search\/shobhit043\/percy-jackson-first-5-books\/dataset-metadata.json","298":"..\/datasets\/raw_datasets\/metadata-for-search\/ruqayaalzien\/books-dataset\/dataset-metadata.json","299":"..\/datasets\/raw_datasets\/metadata-for-search\/ghostbat101\/lung-x-ray-image-clinical-text-dataset\/dataset-metadata.json","300":"..\/datasets\/raw_datasets\/metadata-for-search\/noorfatimaafzalbutt\/1000-books-data\/dataset-metadata.json","301":"..\/datasets\/raw_datasets\/metadata-for-search\/kunaloswal\/book-recommendation-system\/dataset-metadata.json","302":"..\/datasets\/raw_datasets\/metadata-for-search\/jorgeborja\/open-library-books-api-12k\/dataset-metadata.json","303":"..\/datasets\/raw_datasets\/metadata-for-search\/hajarsalah1\/arabicbooks100\/dataset-metadata.json","304":"..\/datasets\/raw_datasets\/metadata-for-search\/hajarsalah1\/arabicbooks500\/dataset-metadata.json","305":"..\/datasets\/raw_datasets\/metadata-for-search\/sahilprajapati143\/retail-analysis-large-dataset\/dataset-metadata.json","306":"..\/datasets\/raw_datasets\/metadata-for-search\/bapuchendage\/ancient-inscription-database-bdc-db\/dataset-metadata.json","307":"..\/datasets\/raw_datasets\/metadata-for-search\/rcratos\/ayurveda-texts-english\/dataset-metadata.json","308":"..\/datasets\/raw_datasets\/metadata-for-search\/mrovkill\/project-geutenberg-books\/dataset-metadata.json","309":"..\/datasets\/raw_datasets\/metadata-for-search\/rikato\/booksi\/dataset-metadata.json","310":"..\/datasets\/raw_datasets\/metadata-for-search\/ouaraskhelilrafik\/mawqif-dataset\/dataset-metadata.json","311":"..\/datasets\/raw_datasets\/metadata-for-search\/tanishas2024\/sarcasm-detection\/dataset-metadata.json","312":"..\/datasets\/raw_datasets\/metadata-for-search\/raviutsavk\/sarcasm-detection-dataset\/dataset-metadata.json","313":"..\/datasets\/raw_datasets\/metadata-for-search\/hamzaboussairi\/sarcasm-detection\/dataset-metadata.json","314":"..\/datasets\/raw_datasets\/metadata-for-search\/farisalahmdi\/arabic-youtube-comments-by-khalaya\/dataset-metadata.json","315":"..\/datasets\/raw_datasets\/metadata-for-search\/mohamedabi\/arabic-sarcasm-detection\/dataset-metadata.json","316":"..\/datasets\/raw_datasets\/metadata-for-search\/adityaraghuvanshi999\/semeval22-sarcasm-detection\/dataset-metadata.json","317":"..\/datasets\/raw_datasets\/metadata-for-search\/wahyutisnoatmojo\/sarcasm-detection-publication\/dataset-metadata.json","318":"..\/datasets\/raw_datasets\/metadata-for-search\/talalhassan141\/sarcasm-detection\/dataset-metadata.json","319":"..\/datasets\/raw_datasets\/metadata-for-search\/abraralotaibi00\/arsarcasm-v2\/dataset-metadata.json","320":"..\/datasets\/raw_datasets\/metadata-for-search\/sohamshrikantjadhav\/final-saved-model\/dataset-metadata.json","321":"..\/datasets\/raw_datasets\/metadata-for-search\/shumailakhan\/urdu-sarcastic-tweets-dataset\/dataset-metadata.json","322":"..\/datasets\/raw_datasets\/metadata-for-search\/mostafanofal\/two-million-rows-egyptian-datasets\/dataset-metadata.json","323":"..\/datasets\/raw_datasets\/metadata-for-search\/prashantpathak244\/mustard-multimodal-sarcasm-detection-dataset\/dataset-metadata.json","324":"..\/datasets\/raw_datasets\/metadata-for-search\/farhanch67\/twitter-sracastic-classification-dataset\/dataset-metadata.json","325":"..\/datasets\/raw_datasets\/metadata-for-search\/rmisra\/politifact-fact-check-dataset\/dataset-metadata.json","326":"..\/datasets\/raw_datasets\/metadata-for-search\/sakibapon\/banglasarc\/dataset-metadata.json","327":"..\/datasets\/raw_datasets\/metadata-for-search\/muhammadfaisalali\/labeled-dataset-for-sarcasm-detection\/dataset-metadata.json","328":"..\/datasets\/raw_datasets\/metadata-for-search\/sid150794\/sarcasm-detection-data-set\/dataset-metadata.json","329":"..\/datasets\/raw_datasets\/metadata-for-search\/gioelefanelli\/italian-sarcasm-detection\/dataset-metadata.json","330":"..\/datasets\/raw_datasets\/metadata-for-search\/cabmarcos\/isic-melanoma-detection\/dataset-metadata.json","331":"..\/datasets\/raw_datasets\/metadata-for-search\/nexdatafrank\/multi-races-human-body-semantic-segmentation-data\/dataset-metadata.json","332":"..\/datasets\/raw_datasets\/metadata-for-search\/marnissiarafet\/object-detection-using-adaptive-mask-rcnn\/dataset-metadata.json","333":"..\/datasets\/raw_datasets\/metadata-for-search\/nazuhifu\/cvc-clinicdb\/dataset-metadata.json","334":"..\/datasets\/raw_datasets\/metadata-for-search\/iasadpanwhar\/parking-lot-detection-counter\/dataset-metadata.json","335":"..\/datasets\/raw_datasets\/metadata-for-search\/axondata\/silicone-mask-biometric-attack-dataset\/dataset-metadata.json","336":"..\/datasets\/raw_datasets\/metadata-for-search\/xygdragon\/mask-detction-vocyolo\/dataset-metadata.json","337":"..\/datasets\/raw_datasets\/metadata-for-search\/axondata\/3d-mannequin-dataset-for-liveness-detection\/dataset-metadata.json","338":"..\/datasets\/raw_datasets\/metadata-for-search\/gauravsekar\/construction-site-safety-image-dataset-roboflow\/dataset-metadata.json","339":"..\/datasets\/raw_datasets\/metadata-for-search\/phasakonk\/object-detection-case-study-2\/dataset-metadata.json","340":"..\/datasets\/raw_datasets\/metadata-for-search\/megharana16\/lung-nodule-infused-images\/dataset-metadata.json","341":"..\/datasets\/raw_datasets\/metadata-for-search\/axondata\/liveness-detection-dataset-cutout-2d-attacks\/dataset-metadata.json","342":"..\/datasets\/raw_datasets\/metadata-for-search\/nazuhifu\/cvc-clinicdb\/dataset-metadata.json","343":"..\/datasets\/raw_datasets\/metadata-for-search\/patricklford\/covid-19\/dataset-metadata.json","344":"..\/datasets\/raw_datasets\/metadata-for-search\/arunhirmukhe\/face-mask\/dataset-metadata.json","345":"..\/datasets\/raw_datasets\/metadata-for-search\/abdallahwagih\/retina-blood-vessel\/dataset-metadata.json","346":"..\/datasets\/raw_datasets\/metadata-for-search\/yannicksteph\/githut-rsna-miccai-brain-tumor-classification-ai\/dataset-metadata.json","347":"..\/datasets\/raw_datasets\/metadata-for-search\/coderrkj\/processed-ct-ich-dataset-images\/dataset-metadata.json","348":"..\/datasets\/raw_datasets\/metadata-for-search\/debjeetdas\/breast-cancer-jpg-image-dataset-of-cbisddsm\/dataset-metadata.json","349":"..\/datasets\/raw_datasets\/metadata-for-search\/trainingdatapro\/portrait-and-30-photos-test\/dataset-metadata.json","350":"..\/datasets\/raw_datasets\/metadata-for-search\/yjh4374\/sisvse-dataset\/dataset-metadata.json","351":"..\/datasets\/raw_datasets\/metadata-for-search\/trngvhong\/mfvt-dataset\/dataset-metadata.json","352":"..\/datasets\/raw_datasets\/metadata-for-search\/zeeshanahmed13\/intraretinal-cystoid-fluid\/dataset-metadata.json","353":"..\/datasets\/raw_datasets\/metadata-for-search\/gd6kfpav\/facemaskdetectiondataset\/dataset-metadata.json","354":"..\/datasets\/raw_datasets\/metadata-for-search\/tapakah68\/medical-masks-part1\/dataset-metadata.json","355":"..\/datasets\/raw_datasets\/metadata-for-search\/debeshjha1\/kvasirinstrument\/dataset-metadata.json","356":"..\/datasets\/raw_datasets\/metadata-for-search\/debeshjha1\/medico-automatic-polyp-segmentation-challenge\/dataset-metadata.json","357":"..\/datasets\/raw_datasets\/metadata-for-search\/debeshjha1\/endotect-dataset\/dataset-metadata.json","358":"..\/datasets\/raw_datasets\/metadata-for-search\/debeshjha1\/kvasirseg\/dataset-metadata.json","359":"..\/datasets\/raw_datasets\/metadata-for-search\/humansintheloop\/medical-mask-detection\/dataset-metadata.json","360":"..\/datasets\/raw_datasets\/metadata-for-search\/anasmohammedtahir\/covidqu\/dataset-metadata.json","361":"..\/datasets\/raw_datasets\/metadata-for-search\/mloey1\/medical-face-mask-detection-dataset\/dataset-metadata.json","362":"..\/datasets\/raw_datasets\/metadata-for-search\/artemzysko\/playing-cards-dataset-yolo-object-detection\/dataset-metadata.json","363":"..\/datasets\/raw_datasets\/metadata-for-search\/sahabipin\/bangladeshi-native-vehicle-dataset\/dataset-metadata.json","364":"..\/datasets\/raw_datasets\/metadata-for-search\/hadyfarrag\/car-brand-recognition\/dataset-metadata.json","365":"..\/datasets\/raw_datasets\/metadata-for-search\/pnnawal\/idd-detection-modified\/dataset-metadata.json","366":"..\/datasets\/raw_datasets\/metadata-for-search\/tkm2261\/ndda-dataset\/dataset-metadata.json","367":"..\/datasets\/raw_datasets\/metadata-for-search\/naveengumaste\/flir-2024-dataset-yolov8-version\/dataset-metadata.json","368":"..\/datasets\/raw_datasets\/metadata-for-search\/mohamedashrafkhalifa\/car-plates-numbers\/dataset-metadata.json","369":"..\/datasets\/raw_datasets\/metadata-for-search\/deepakat002\/roundabout-aerial-images-yolo-data\/dataset-metadata.json","370":"..\/datasets\/raw_datasets\/metadata-for-search\/nehashaikh2911\/carla-dataset-lab-1\/dataset-metadata.json","371":"..\/datasets\/raw_datasets\/metadata-for-search\/killa92\/car-detect-dataset\/dataset-metadata.json","372":"..\/datasets\/raw_datasets\/metadata-for-search\/fxmikf\/diverse-lpd-training-ready\/dataset-metadata.json","373":"..\/datasets\/raw_datasets\/metadata-for-search\/dhanushrajm\/images\/dataset-metadata.json","374":"..\/datasets\/raw_datasets\/metadata-for-search\/mohamedgobara\/26-class-object-detection-dataset\/dataset-metadata.json","375":"..\/datasets\/raw_datasets\/metadata-for-search\/hasibzunair\/rsud20k-bangladesh-road-scene-understanding\/dataset-metadata.json","376":"..\/datasets\/raw_datasets\/metadata-for-search\/kshitizgajurel042\/nepali-bike-and-car-images-with-annotations\/dataset-metadata.json","377":"..\/datasets\/raw_datasets\/metadata-for-search\/muftirestumahesa\/image-obstacle-in-public-spaces\/dataset-metadata.json","378":"..\/datasets\/raw_datasets\/metadata-for-search\/muftirestumahesa\/obstacles-in-public-spaces-for-dist-yolo\/dataset-metadata.json","379":"..\/datasets\/raw_datasets\/metadata-for-search\/killa92\/disabled-people-sign-detection\/dataset-metadata.json","380":"..\/datasets\/raw_datasets\/metadata-for-search\/ibrahimalobaid\/object-detection-carla-self-driving-car\/dataset-metadata.json","381":"..\/datasets\/raw_datasets\/metadata-for-search\/mayankchandak\/voc2012-processed-data-for-yolov5\/dataset-metadata.json","382":"..\/datasets\/raw_datasets\/metadata-for-search\/shreyanshmanavshukla\/celebsv2-faces-224\/dataset-metadata.json","383":"..\/datasets\/raw_datasets\/metadata-for-search\/shreyanshmanavshukla\/ff-face-224\/dataset-metadata.json","384":"..\/datasets\/raw_datasets\/metadata-for-search\/qulymzhanovv\/contacts-stores-products\/dataset-metadata.json","385":"..\/datasets\/raw_datasets\/metadata-for-search\/trainingdatapro\/asian-people-liveness-detection-video-dataset\/dataset-metadata.json","386":"..\/datasets\/raw_datasets\/metadata-for-search\/tusharpadhy\/deepfake-dataset\/dataset-metadata.json","387":"..\/datasets\/raw_datasets\/metadata-for-search\/datascientist97\/banknote-authentication-dataset-2024\/dataset-metadata.json","388":"..\/datasets\/raw_datasets\/metadata-for-search\/alsaniipe\/cebradata\/dataset-metadata.json","389":"..\/datasets\/raw_datasets\/metadata-for-search\/trainingdatapro\/generated-vietnamese-passeports-dataset\/dataset-metadata.json","390":"..\/datasets\/raw_datasets\/metadata-for-search\/kidonpark1023\/fake-or-real-dataset\/dataset-metadata.json","391":"..\/datasets\/raw_datasets\/metadata-for-search\/tapakah68\/generated-usa-passeports-dataset\/dataset-metadata.json","392":"..\/datasets\/raw_datasets\/metadata-for-search\/mssomanna\/indian-coin-images-dataset-incl-fake-coins\/dataset-metadata.json","393":"..\/datasets\/raw_datasets\/metadata-for-search\/omgits0mar\/signature-verification-and-identification\/dataset-metadata.json","394":"..\/datasets\/raw_datasets\/metadata-for-search\/birdy654\/cifake-real-and-ai-generated-synthetic-images\/dataset-metadata.json","395":"..\/datasets\/raw_datasets\/metadata-for-search\/superpotato9\/dalle-recognition-dataset\/dataset-metadata.json","396":"..\/datasets\/raw_datasets\/metadata-for-search\/hlly34\/liveness-detection-zalo-2022\/dataset-metadata.json","397":"..\/datasets\/raw_datasets\/metadata-for-search\/muhammadmuzamil5500\/gemstones\/dataset-metadata.json","398":"..\/datasets\/raw_datasets\/metadata-for-search\/uditsharma72\/real-vs-fake-faces\/dataset-metadata.json","399":"..\/datasets\/raw_datasets\/metadata-for-search\/thefamousrat\/synthetic-chess-board-images\/dataset-metadata.json","400":"..\/datasets\/raw_datasets\/metadata-for-search\/manjilkarki\/deepfake-and-real-images\/dataset-metadata.json","401":"..\/datasets\/raw_datasets\/metadata-for-search\/guybarash\/israeli-parliament-knesset-members\/dataset-metadata.json","402":"..\/datasets\/raw_datasets\/metadata-for-search\/yash1910\/indian-players-under-25-in-the-isl-23-24-season\/dataset-metadata.json","403":"..\/datasets\/raw_datasets\/metadata-for-search\/bugralpp\/euro-2024-group-stage-fbref-scrape-data\/dataset-metadata.json","404":"..\/datasets\/raw_datasets\/metadata-for-search\/marcodiomedi\/teams-and-players-stats-from-fbref\/dataset-metadata.json","405":"..\/datasets\/raw_datasets\/metadata-for-search\/hasankilincce\/super-lig-2023-2024-player-informations\/dataset-metadata.json","406":"..\/datasets\/raw_datasets\/metadata-for-search\/anisguechtouli\/football-leagues-data-2023-2024\/dataset-metadata.json","407":"..\/datasets\/raw_datasets\/metadata-for-search\/damirdizdarevic\/uefa-euro-2024-players\/dataset-metadata.json","408":"..\/datasets\/raw_datasets\/metadata-for-search\/emreguv\/202223-big-5-football-leagues-player-stats\/dataset-metadata.json","409":"..\/datasets\/raw_datasets\/metadata-for-search\/edacelikeloglu\/turkish-super-league\/dataset-metadata.json","410":"..\/datasets\/raw_datasets\/metadata-for-search\/coldfirefauzan\/ipl-auction-dataset\/dataset-metadata.json","411":"..\/datasets\/raw_datasets\/metadata-for-search\/dhruvpjariwala\/football-players-transfer-value-prediction\/dataset-metadata.json","412":"..\/datasets\/raw_datasets\/metadata-for-search\/dhruvgup\/premier-league-player-statistics-2022-2024\/dataset-metadata.json","413":"..\/datasets\/raw_datasets\/metadata-for-search\/piterfm\/football-soccer-uefa-euro-1960-2024\/dataset-metadata.json","414":"..\/datasets\/raw_datasets\/metadata-for-search\/rahulgupta101\/football-player-dataset-for-yolov8-hosted\/dataset-metadata.json","415":"..\/datasets\/raw_datasets\/metadata-for-search\/rahulgupta101\/football-player-dataset-for-yolov8\/dataset-metadata.json","416":"..\/datasets\/raw_datasets\/metadata-for-search\/ahnaf105091\/football-manager-england-scout-dataset\/dataset-metadata.json","417":"..\/datasets\/raw_datasets\/metadata-for-search\/brzy56\/2024-nwsl-womens-soccer-league-player-stats-full\/dataset-metadata.json","418":"..\/datasets\/raw_datasets\/metadata-for-search\/sheemazain\/psl-season-9-dataset\/dataset-metadata.json","419":"..\/datasets\/raw_datasets\/metadata-for-search\/khalidlatif\/pakistan-super-league-season-8-and-9-ball-by-ball\/dataset-metadata.json","420":"..\/datasets\/raw_datasets\/metadata-for-search\/ahnaf105091\/football-manager-23-championship-3031-dataset\/dataset-metadata.json","421":"..\/datasets\/raw_datasets\/metadata-for-search\/brzy56\/2023-nwsl-womens-soccer-league-player-stats-full\/dataset-metadata.json","422":"..\/datasets\/raw_datasets\/metadata-for-search\/mustaphaseddoum\/england-premier-league-2009-2024\/dataset-metadata.json","423":"..\/datasets\/raw_datasets\/metadata-for-search\/bugralpp\/euro-2024-group-stage-fbref-scrape-data\/dataset-metadata.json","424":"..\/datasets\/raw_datasets\/metadata-for-search\/gurpreetsdeol\/football-player-performance-data-2017-2024\/dataset-metadata.json","425":"..\/datasets\/raw_datasets\/metadata-for-search\/nixchamp\/football-match-data-date-wise-from-1960-to-2024\/dataset-metadata.json","426":"..\/datasets\/raw_datasets\/metadata-for-search\/andreyukio\/the-english-womens-football-database\/dataset-metadata.json","427":"..\/datasets\/raw_datasets\/metadata-for-search\/emreguv\/202223-big-5-football-leagues-player-stats\/dataset-metadata.json","428":"..\/datasets\/raw_datasets\/metadata-for-search\/edacelikeloglu\/turkish-super-league\/dataset-metadata.json","429":"..\/datasets\/raw_datasets\/metadata-for-search\/ahnaf105091\/fifa-worldcup-2022-final-dataset-arg-vs-fra\/dataset-metadata.json","430":"..\/datasets\/raw_datasets\/metadata-for-search\/sinatavakoli\/people-slurs-dataset\/dataset-metadata.json","431":"..\/datasets\/raw_datasets\/metadata-for-search\/ujjwalaggarwal402\/medicine-dataset\/dataset-metadata.json","432":"..\/datasets\/raw_datasets\/metadata-for-search\/shivapan\/classified-nih-dataset\/dataset-metadata.json","433":"..\/datasets\/raw_datasets\/metadata-for-search\/ivan314sh\/phishing-email-classifier-bert\/dataset-metadata.json","434":"..\/datasets\/raw_datasets\/metadata-for-search\/nourmekkijj\/stress-and-anxiety-posts-on-reddit\/dataset-metadata.json","435":"..\/datasets\/raw_datasets\/metadata-for-search\/meerawks\/it-skills-from-jobs\/dataset-metadata.json","436":"..\/datasets\/raw_datasets\/metadata-for-search\/meerawks\/labeled-it-job-skills\/dataset-metadata.json","437":"..\/datasets\/raw_datasets\/metadata-for-search\/rohitdnath\/dark-pattern-dataset\/dataset-metadata.json","438":"..\/datasets\/raw_datasets\/metadata-for-search\/amester\/malflow\/dataset-metadata.json","439":"..\/datasets\/raw_datasets\/metadata-for-search\/ziriantahirli\/million-song-data-analysis-2\/dataset-metadata.json","440":"..\/datasets\/raw_datasets\/metadata-for-search\/sinatavakoli\/people-slurs-dataset\/dataset-metadata.json","441":"..\/datasets\/raw_datasets\/metadata-for-search\/ujjwalaggarwal402\/medicine-dataset\/dataset-metadata.json","442":"..\/datasets\/raw_datasets\/metadata-for-search\/nizamuddinmaitlo\/different-colors-in-challenging-lightening\/dataset-metadata.json","443":"..\/datasets\/raw_datasets\/metadata-for-search\/shivapan\/classified-nih-dataset\/dataset-metadata.json","444":"..\/datasets\/raw_datasets\/metadata-for-search\/umerhaddii\/us-collegiate-sports-dataset\/dataset-metadata.json","445":"..\/datasets\/raw_datasets\/metadata-for-search\/ivan314sh\/phishing-email-classifier-bert\/dataset-metadata.json","446":"..\/datasets\/raw_datasets\/metadata-for-search\/rachanakoudigayala\/classification\/dataset-metadata.json","447":"..\/datasets\/raw_datasets\/metadata-for-search\/vinitdesai564\/cats-dogs-classification-dataset\/dataset-metadata.json","448":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2020-jpg-256x256-resized\/dataset-metadata.json","449":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2020-jpg-224x224-resized\/dataset-metadata.json","450":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2019-jpg-256x256-resized\/dataset-metadata.json","451":"..\/datasets\/raw_datasets\/metadata-for-search\/nischaydnk\/isic-2019-jpg-224x224-resized\/dataset-metadata.json","452":"..\/datasets\/raw_datasets\/metadata-for-search\/nourmekkijj\/stress-and-anxiety-posts-on-reddit\/dataset-metadata.json","453":"..\/datasets\/raw_datasets\/metadata-for-search\/sabunbalt\/skin-disease-classification\/dataset-metadata.json","454":"..\/datasets\/raw_datasets\/metadata-for-search\/carloscortes18\/players-chess-pieces\/dataset-metadata.json","455":"..\/datasets\/raw_datasets\/metadata-for-search\/haicouheba\/car-classification\/dataset-metadata.json","456":"..\/datasets\/raw_datasets\/metadata-for-search\/micscodes\/dresden-image-database\/dataset-metadata.json"},"id":{"0":0,"1":1,"2":2,"3":3,"4":4,"5":5,"6":6,"7":7,"8":8,"9":9,"10":10,"11":11,"12":12,"13":13,"14":14,"15":15,"16":16,"17":17,"18":18,"19":19,"20":20,"21":21,"22":22,"23":23,"24":24,"25":25,"26":26,"27":27,"28":28,"29":29,"30":30,"31":31,"32":32,"33":33,"34":34,"35":35,"36":36,"37":37,"38":38,"39":39,"40":40,"41":41,"42":42,"43":43,"44":44,"45":45,"46":46,"47":47,"48":48,"49":49,"50":50,"51":51,"52":52,"53":53,"54":54,"55":55,"56":56,"57":57,"58":58,"59":59,"60":60,"61":61,"62":62,"63":63,"64":64,"65":65,"66":66,"67":67,"68":68,"69":69,"70":70,"71":71,"72":72,"73":73,"74":74,"75":75,"76":76,"77":77,"78":78,"79":79,"80":80,"81":81,"82":82,"83":83,"84":84,"85":85,"86":86,"87":87,"88":88,"89":89,"90":90,"91":91,"92":92,"93":93,"94":94,"95":95,"96":96,"97":97,"98":98,"99":99,"100":100,"101":101,"102":102,"103":103,"104":104,"105":105,"106":106,"107":107,"108":108,"109":109,"110":110,"111":111,"112":112,"113":113,"114":114,"115":115,"116":116,"117":117,"118":118,"119":119,"120":120,"121":121,"122":122,"123":123,"124":124,"125":125,"126":126,"127":127,"128":128,"129":129,"130":130,"131":131,"132":132,"133":133,"134":134,"135":135,"136":136,"137":137,"138":138,"139":139,"140":140,"141":141,"142":142,"143":143,"144":144,"145":145,"146":146,"147":147,"148":148,"149":149,"150":150,"151":151,"152":152,"153":153,"154":154,"155":155,"156":156,"157":157,"158":158,"159":159,"160":160,"161":161,"162":162,"163":163,"164":164,"165":165,"166":166,"167":167,"168":168,"169":169,"170":170,"171":171,"172":172,"173":173,"174":174,"175":175,"176":176,"177":177,"178":178,"179":179,"180":180,"181":181,"182":182,"183":183,"184":184,"185":185,"186":186,"187":187,"188":188,"189":189,"190":190,"191":191,"192":192,"193":193,"194":194,"195":195,"196":196,"197":197,"198":198,"199":199,"200":200,"201":201,"202":202,"203":203,"204":204,"205":205,"206":206,"207":207,"208":208,"209":209,"210":210,"211":211,"212":212,"213":213,"214":214,"215":215,"216":216,"217":217,"218":218,"219":219,"220":220,"221":221,"222":222,"223":223,"224":224,"225":225,"226":226,"227":227,"228":228,"229":229,"230":230,"231":231,"232":232,"233":233,"234":234,"235":235,"236":236,"237":237,"238":238,"239":239,"240":240,"241":241,"242":242,"243":243,"244":244,"245":245,"246":246,"247":247,"248":248,"249":249,"250":250,"251":251,"252":252,"253":253,"254":254,"255":255,"256":256,"257":257,"258":258,"259":259,"260":260,"261":261,"262":262,"263":263,"264":264,"265":265,"266":266,"267":267,"268":268,"269":269,"270":270,"271":271,"272":272,"273":273,"274":274,"275":275,"276":276,"277":277,"278":278,"279":279,"280":280,"281":281,"282":282,"283":283,"284":284,"285":285,"286":286,"287":287,"288":288,"289":289,"290":290,"291":291,"292":292,"293":293,"294":294,"295":295,"296":296,"297":297,"298":298,"299":299,"300":300,"301":301,"302":302,"303":303,"304":304,"305":305,"306":306,"307":307,"308":308,"309":309,"310":310,"311":311,"312":312,"313":313,"314":314,"315":315,"316":316,"317":317,"318":318,"319":319,"320":320,"321":321,"322":322,"323":323,"324":324,"325":325,"326":326,"327":327,"328":328,"329":329,"330":330,"331":331,"332":332,"333":333,"334":334,"335":335,"336":336,"337":337,"338":338,"339":339,"340":340,"341":341,"342":342,"343":343,"344":344,"345":345,"346":346,"347":347,"348":348,"349":349,"350":350,"351":351,"352":352,"353":353,"354":354,"355":355,"356":356,"357":357,"358":358,"359":359,"360":360,"361":361,"362":362,"363":363,"364":364,"365":365,"366":366,"367":367,"368":368,"369":369,"370":370,"371":371,"372":372,"373":373,"374":374,"375":375,"376":376,"377":377,"378":378,"379":379,"380":380,"381":381,"382":382,"383":383,"384":384,"385":385,"386":386,"387":387,"388":388,"389":389,"390":390,"391":391,"392":392,"393":393,"394":394,"395":395,"396":396,"397":397,"398":398,"399":399,"400":400,"401":401,"402":402,"403":403,"404":404,"405":405,"406":406,"407":407,"408":408,"409":409,"410":410,"411":411,"412":412,"413":413,"414":414,"415":415,"416":416,"417":417,"418":418,"419":419,"420":420,"421":421,"422":422,"423":423,"424":424,"425":425,"426":426,"427":427,"428":428,"429":429,"430":430,"431":431,"432":432,"433":433,"434":434,"435":435,"436":436,"437":437,"438":438,"439":439,"440":440,"441":441,"442":442,"443":443,"444":444,"445":445,"446":446,"447":447,"448":448,"449":449,"450":450,"451":451,"452":452,"453":453,"454":454,"455":455,"456":456},"title":{"0":"IMDB Top 1000 Movies Dataset","1":"IMDB top 1000 dataset","2":"Imdb Movie Dataset from year 1893 to 2020","3":"TMDB TOP 10000 (10K) Movies Dataset","4":"Google Data Analytics Capstone Project: Netflix","5":"Movie Identification Dataset [800 Movies]","6":"IMDb top rated 1000 movies","7":"Top 1000 IMDb movies Dataset ( Web Scraping )","8":"IMDB TOP 1000 MOVIES DATASET","9":"IMDb Top 1000 Movies Dataset","10":"IMDB Anime Data","11":"Top 10000 Voted IMDB Movies Dataset","12":"Dataset","13":"IMDB top 1000 movies","14":"Top 1000 IMDb Movies Dataset","15":"imdb top 1000","16":"IMDB Top 1000 Movies","17":"IMDb Top 10000 Movies Dataset","18":" IMDB Dataset of Top 1000 Movies and TV Shows","19":"IMDB Movies Dataset","20":"clothing_reviews","21":"womens_clothing_reviews","22":"womens_clothing_reviews_new","23":"Reddit Posts on Borderline Personality Disorder","24":"Stress and Anxiety Posts on Reddit","25":"Stress and Anxiety Posts on Reddit","26":"Emotion Classification Dataset","27":"Twitter Financial News Sentiment Dataset","28":"Sentiment Analysis of Tweet Reviews","29":"Twitter Customer Reviews of Popular Smart Phone","30":"Chatgpt DataSet 50000 Tweets from initial month","31":"Indian Political Sentiment on Twitter ","32":"Twitter Emotion Dataset","33":"Emotions Dataset","34":"Clean-NLP with Disaster Tweets Dataset","35":"Tweets Dataset","36":"Emotions","37":"EventClassifier: Twitter Data Set","38":"crypto nft tweets data","39":"Tweets and Engagement Metrics","40":"Short Jokes Dataset","41":"Hate Speech and Offensive Language Detection","42":"indian political tweets","43":"US Airlines Sentiment Analysis in Knime Analytics","44":"Preprocessed Dell Tweets","45":"Web Scrapping Twitter Racism","46":"Twitter Sentiment Analysis - Xitsonga","47":"Urban Mobility Dataset","48":"Yulu Bike Rental Dataset","49":"Natural Language Processing of Chatbot Prompts","50":"Bike rentals","51":"OptiCom Signal Quality Dataset","52":"City Traffic and Vehicle Behavior Dataset","53":"US Automatic Traffic Recorder Stations Data","54":"Economy_Health brazil","55":"US Traffic Congestions (2016-2022)","56":"Traffic Prediction Dataset","57":"Traffic Detection Project","58":"Newcastle's 4 main junctions traffic data","59":"Cities Skyline Springfield","60":"Cars Detection","61":"Bike Sharing Dataset","62":"all traffic signs dataset","63":"Iranian Traffic Sign Detection","64":"Driver Inattention Detection Dataset","65":"Traffic-Signs-Dataset","66":"Healthy and Unhealthy Goat Images","67":"Wildlife","68":"Cat Dataset","69":"FluNet, Global Influenza Programme - WHO","70":"whaleTagData","71":"Local Ocean Conservation Sea Turtle Face Detection","72":"CatFLW","73":"Macaque Monkey Images","74":"Reactive Anomaly Synthetic Data","75":"Sri Lankan Fish Species Dataset","76":"Guinea Pig Detection","77":"sea-animals-image-dataste","78":"COVID-19 & the virus that causes it: SARS-CoV-2.","79":"Hiking Wildlife","80":"Birds, Bats, Grasshoppers Audio","81":"Zebrafish mirror biting","82":"Water Quality -  Every Drop Matters.","83":"IR wildlife","84":"Cat and Dog Detection ","85":"BrackishSGD","86":"Malflow","87":"Different Colors in challenging lightening","88":"Classified NIH Dataset","89":"Cats Dogs Classification dataset","90":"ISIC 2020 JPG 256x256 RESIZED","91":"ISIC 2020 JPG 224x224 RESIZED","92":"ISIC 2019 JPG 256x256 RESIZED","93":"ISIC 2019 JPG 224x224 RESIZED","94":"Dresden Image Database","95":"IMAGE CLASSIFICATION","96":"ImageNet100-INRs-Dataset","97":"Riped and Unriped Tomato Dataset","98":"Song Popularity Classification","99":"Industrial Tools Classification","100":"NIH Chest X-rays Bbox version","101":"Rock Classification Dataset","102":"The Russia-Ukraine War Images on Spanish News","103":"lung x-ray image+clinical text dataset","104":"Signclusive Mediapipe","105":"Iman(357)-Wawan(339)-neww","106":"Road Accident Data Analysis using Microsoft Excel.","107":"KaggleX Skill Assessment Challenge Cohort4 Dataset","108":"Boston - Somerville Traffic Crash Dataset","109":"Vehicle Maintenance Data","110":"Road Accident Data by Vehicle Type","111":"Road Accidents Data","112":"Urban traffic density in cities","113":"Asian-Related Factors in Fatal Crashes In Texas","114":"Road Accidents CSV and EXCEL","115":"Road accidents in the Czech Republic","116":"Road Traffic Collision Data in Northern Ireland","117":"FutureFlow: Navigating Tomorrow's Urban Traffic","118":"Car Accident Dataset","119":"2022 UK Road Safety Data","120":"Road Accident Casualties","121":"US Automatic Traffic Recorder Stations Data","122":"Major Road Accidents","123":"Airbag Recommendation","124":"Hourly Vehicle Crossing data for Time Series","125":"Motor Vehicle Collisions - Crashes-New York ","126":"Spotify dataset ","127":"Anime Dataset 2024","128":"Complete Pokemon Dataset 9th Gen (img + tabular)","129":"Most Popular Anime of all Time","130":"Anime Data from 1970 to 2024","131":"Anime Dataset","132":"Anime Dataset 2024","133":"anime_recommendation_system_weight","134":"Anime Content Based Recommendation System Datasets","135":"MyAnimesList Datasets - 2023","136":"USA Bank Financial Data","137":"Nifty Energy Sector Monthly % Returns Dataset","138":"Leading Indian Bank & CIBIL Real-World Dataset","139":"Loan Approval Dataset","140":"Gesture Recognition System Dataset","141":"Gesture Recognition Data","142":"Hand Gesture Recognition (3 Classes, 64X64)","143":"Enhanced Sign Language MNIST Dataset","144":"Bangla Sign Language Video Dataset","145":"Hand Navigation Landmarks","146":"Hand Gesture Recognition Sample Outputs","147":"Hand Gesture Recognition Dataset for YOLOv8","148":"ISL-CSLTR: Indian Sign Language Dataset","149":"HGR-Dataset","150":"Sensor based (American) Sign Language Recognition","151":"Malaysian Sign Language (MSL) Image Dataset","152":"Masked Gesture Dataset","153":"Myoelectric Control for Hand Motions","154":"ASL Alphabet Wireframes","155":"Serbian Sign Language Dataset","156":"Sign Language Gestures Recognition Data","157":"Sign Language Recognition Dataset","158":"Gesture Classification HaGRID","159":"Hand Gestures Dataset","160":"British Airways Reviews","161":"online review.csv","162":"British Airways Reviews Dataset","163":"Capterra Ticketsystem - Jira, Zendesk - Reviews","164":"Airline Customer Reviews Dataset","165":"commerce Furniture","166":"Online Store Selling","167":"Exchange Rates:USD to PKR,INR, and CNY (2004-2024)","168":"Longest Running Indian Television Series","169":"Top5000 Albums on Spotify Data Analysis","170":"USA Bank Financial Data","171":"fake-news-and-true-news-dataset","172":"US Collegiate Sports Dataset","173":"sentinel","174":"LinkedIn Posts about Layoffs (2023)","175":"Fake News Prediction datasets","176":"celeb-a-2ammu20-a3","177":"Facebook's Supreme Court","178":"israel-hamas-full-articles-nyt","179":"News headlines dataset with company names","180":"NIH Chest X-rays Bbox version","181":" Youtube Channel tmrospacenews","182":" Youtube Channel truecrimenews","183":"celeb-df11parts","184":"The Russia-Ukraine War Images on Spanish News","185":"Unified Dataset for Fake, Spam and Legit data","186":"NewsQA","187":"Classified Arabic News","188":"ag_news_dataset","189":"CNN-Transformer","190":"armenian_news","191":"Derana News","192":"Fake_News_Detection","193":"SocialNet-Weibo-Version-2","194":"SocialNet-Weibo-Version-1","195":"fake_news _Detection","196":"Fake News Detection Data","197":"Fake news detection dataset","198":"Scam Detection | Fake News Labelled  Dataset","199":"fake-news-detection11","200":"fake-and-real-news-dataset","201":"News Detection (Fake or Real) Dataset","202":"Fake News .csv","203":"fake_news_detection_with_nlp_and_lstm","204":"Fake news detection","205":"Fake News Detection","206":"Fake News Detection","207":"Fake News detection","208":"Welfake dataset for fake news","209":"Fake news master","210":"Fake News detection","211":"Retail_Stores_Sales_Dataset","212":"Sales dataset","213":"Dataset Dashboard","214":"Pizza Sales - Excel Mastersheet","215":"Pharma Data Analysis","216":"Indonesian Spices Dataset","217":"Padang Cuisine (Indonesian Food Image Dataset)","218":"Indonesian Food Recipes","219":"Indonesian Spices Dataset","220":"Indian Food Dataset ","221":"Arabic Food 101","222":"Padang Cuisine (Indonesian Food Image Dataset)","223":"Peruvian Food Reviews","224":"Bahama Breeze Menu Nutrition Data","225":"Combined Wine Dataset: Red & White Wines","226":"The LAMBADA Dataset for Word Prediction","227":"SNIPS Natural Language Understanding Benchmark:","228":"college_level_IntroCS_class_dataset|chinese","229":"Sigma Dolphin Filtered and Cleaned","230":"Financial Q&A - 10k","231":"NLP_Neymar_ChatBot_Dataset","232":"CoQA QndA Dataset with ground truth","233":"Astronautics Multiple Choice Questions and Answers","234":"EVJVQA","235":"SE-PQA: Personalized Community Question Answering","236":"GSM8K - Grade School Math 8K dataset for LLM","237":"Arabic Instruct chatbot dataset","238":"A Case Study: Bike Share","239":"A New Benchmark for Financial Question Answering","240":"Visual_Question_Answering-CV-NLP","241":"GenAssocBias","242":"Boardgames_Rulebooks","243":"question-answering-dataset","244":"blooms-taxonomy-dataset","245":"VizWiz_QA","246":"Red Wine Quality","247":"wine quality","248":"Combined Wine Dataset: Red & White Wines","249":"Modeling wine","250":"Red Wine Quality","251":"Red wine quality classifier using a Neural network","252":"Wine Quality Dataset","253":"Red-wine quality","254":"Dataset","255":"Dataset: LQR House Inc. (LQR) Stock Performance","256":"Dataset: Full House Resorts, Inc. (FLL) Stock P...","257":"House Prices India","258":"Bengaluru House Dataset","259":"Transformed Housing Data 2","260":"Transformed Housing Data","261":"Raw Housing Cost Data (imputed)","262":"Dataset: Tree House Education & Accessories Lim...","263":"House_Prices_Dataset","264":"Housing Sales: Factors Influencing Sale Prices","265":"House Price Predication","266":"Housing Price Prediction","267":"Singapore Home Resale Prices (HDB)","268":"Amazon Books Dataset: Genre, Sub-genre, and Books","269":"Portuguese Apartment Listings Dataset","270":"House Prices","271":"Indian Rental House Price","272":"California Real state Dataset","273":"Gurgoan properties (Cleaned) dataset","274":"Classified NIH Dataset","275":"NIH Chest X-rays Bbox version","276":"lung x-ray image+clinical text dataset","277":"Improved_X-Ray_data","278":"Covid-19 X-Ray Classification Dataset","279":"ChestXray 8 Object Detection Yolo and Pascal VOC","280":"VinBigData Chest X-ray - YOLO","281":"Normal Tuberculosis Covid-19 Chest Xrays images","282":"Pulmonary-Chest-X-Ray-Abnormalities","283":"Chest X-Ray Dataset","284":"DS4 Work - Operation Datasets","285":"Chest X-Ray Images","286":"Pneumonia  Tuberculosis  Normal","287":"Chest Radiograph Images (Pneumonia & Normal)","288":"Chest X-ray Images","289":"Covid-19 Chest X-ray","290":"covid-19","291":"chest x-ray images pneumonia dataset","292":"pneumonia CXR dataset","293":"Lung Disease","294":"A Song of Ice and Fire - Dialogues","295":"The LAMBADA Dataset for Word Prediction","296":"BookSales","297":"Percy Jackson Book Series NLP Dataset","298":"Books Dataset","299":"lung x-ray image+clinical text dataset","300":"1000 books data","301":"Book Recommendation System","302":"Open Library Books API - 12k","303":"arabicBooks100","304":"ArabicBooks500","305":"Retail Analysis on Large Dataset","306":"Ancient Inscription Database (BDC_Db)","307":"Ayurveda Texts (English)","308":"Project Geutenberg Books","309":"booksi","310":"Mawqif Dataset","311":"Sarcasm detection","312":"sarcasm-detection-dataset","313":"Sarcasm detection","314":"Arabic YouTube Comments by Khalaya","315":"arabic_Sarcasm_detection","316":"SemEval22 Sarcasm Detection","317":"Sarcasm Detection Publication","318":"Sarcasm Detection","319":"ArSarcasm-V2","320":"Final-Saved-Model","321":"Urdu Sarcastic Tweets Dataset","322":"2.5+ Million Rows Egyptian Datasets Collection","323":"MUStARD (Multimodal Sarcasm Detection Dataset)","324":"Twitter sracastic classification dataset ","325":"Politifact Fact Check Dataset","326":"Bangla SARC","327":"Labeled Dataset for Sarcasm Detection","328":"Sarcasm Detection Data Set","329":"Italian Sarcasm_Detection","330":"ISIC melanoma detection","331":"Multi-Races Human Body Semantic Segmentation Data","332":"Object Detection Using Adaptive Mask RCNN","333":"CVC-ClinicDB Datasets","334":"Parking_Lot_Detection_Counter","335":"Silicone Mask Biometric Attack Dataset","336":"Mask Detection (VOC\/YOLO)","337":"3D Mannequin Dataset for Liveness Detection","338":"Construction Site Safety Image Dataset Roboflow v2","339":"object-detection-case-study-2","340":"Lung nodule infused images","341":"Liveness detection dataset: Cutout 2D attacks","342":"CVC-ClinicDB Datasets","343":"COVID-19 & the virus that causes it: SARS-CoV-2.","344":"Face Mask","345":"Retina Blood Vessel","346":"githut-RSNA-MICCAI-Brain-Tumor-Classification-AI","347":"Processed CT ICH Dataset Images","348":" CBIS-DDSM: Breast Cancer Dataset of JPG Images","349":"Portrait and 26 Photos Re-identification, 19 GB","350":"Surgical Scene Segmentation in Robotic Gastrectomy","351":"MFVT Dataset for Real-time Masked Facial Detection","352":"Intraretinal Cystoid Fluid","353":"face-mask-detection-dataset","354":"Face Mask Detection Dataset - 500 GB of data","355":"Kvasir-Instrument Dataset","356":"Medico automatic polyp segmentation dataset","357":"EndoTect Dataset","358":"Kvasir-SEG Data (Polyp segmentation & detection)","359":"Medical Mask detection","360":"COVID-QU-Ex Dataset","361":"COVID-19 Medical Face Mask Detection Dataset ","362":"playing-cards-dataset | YOLO Object detection","363":"Bangladeshi Native Vehicle Dataset","364":"Car Brand Recognition","365":"IDD Detection Modified","366":"NDDA Dataset","367":"FLIR 2024 Dataset Yolov8 Version","368":"EGYPlate","369":"Roundabout Aerial Images YOLO data","370":"Lab1_Part3_Dataset","371":"Car Detect Dataset","372":"Diverse-LPD - Training Ready","373":"Object Detection using YOLOV3","374":"26 Class Object detection dataset","375":"RSUD20K: Bangladesh Road Scenes Dataset","376":"Nepali Bike and Car images with annotations","377":"Image Obstacle in Public Spaces","378":"Obstacles in Public Spaces for Dist-YOLO","379":"Disabled People Sign Detection","380":"Object Detection Carla self-driving Car","381":"Processed_data_for_yolov5_20ClassObjectDetection","382":"CelebsV2_Faces_224","383":"FF++_Face_224","384":"Contacts, Stores, Products","385":"Asian People - Liveness Detection Video Dataset","386":"Deepfake-dataset (140k + dataset real or fake)","387":"Banknote-authentication-dataset-2024","388":"Cebradata for GAN training","389":"GENERATED Vietnamese Passports Dataset","390":"Fake or Real Competition Dataset","391":"GENERATED USA Passports Augmentation","392":"Indian Coin Images Dataset (incl. fake coins)","393":"Handwritten Signature Identification","394":"CIFAKE: Real and AI-Generated Synthetic Images","395":"AI recognition dataset","396":"Liveness Detection - Zalo AI Challenge 2022","397":"Gemstones","398":"Real vs fake faces","399":"Synthetic Chess Board Images","400":"deepfake and real images","401":"Israeli Parliament (Knesset) Members ","402":"Indian players under 25 in the ISL 23-24 season","403":"Euro 2024 Group Stage FBRef Scrape Data","404":"Teams and players stats from FBRef","405":"Super Lig Player Informations","406":"Fbref Football Leagues Data 2023 2024","407":"UEFA EURO 2024 - Players","408":"2022\/23 Big 5 Football Leagues Player Stats","409":"Turkish Super League","410":"IPL Auction Dataset","411":"Football_players_transfer_values_2024","412":"Premier League Player Statistics 2022-2024","413":"Football - Soccer - UEFA EURO, 1960 - 2024","414":"Football_Player_Dataset_for_YOLOV8_Hosted","415":"Football Player Dataset for YOLOV8","416":"Football Manager England Scout Dataset","417":"2024 NWSL Womens Soccer League Player Stats FULL","418":"PSL Season 9 Dataset","419":"Pakistan Super League","420":"Football Manager-23 Championship 30\/31 dataset","421":"2023 NWSL Womens Soccer League Player Stats FULL","422":"England Premier League (2008-2024)","423":"Euro 2024 Group Stage FBRef Scrape Data","424":"Football Player Performance Data 2017-2024","425":"Football Match Data Date Wise From 1960 to 2024","426":"The English Women's Football Database","427":"2022\/23 Big 5 Football Leagues Player Stats","428":"Turkish Super League","429":"FIFA Worldcup 2022 Final Dataset: ARG vs FRA","430":"People Slurs Dataset","431":"Medicine Dataset","432":"Classified NIH Dataset","433":"phishing-email-classifier-bert","434":"Stress and Anxiety Posts on Reddit","435":"IT Skills from Jobs","436":"Labeled IT Job skills","437":"Dark Pattern Dataset","438":"Malflow","439":"Million Song Data Analysis 2","440":"People Slurs Dataset","441":"Medicine Dataset","442":"Different Colors in challenging lightening","443":"Classified NIH Dataset","444":"US Collegiate Sports Dataset","445":"phishing-email-classifier-bert","446":"Classification","447":"Cats Dogs Classification dataset","448":"ISIC 2020 JPG 256x256 RESIZED","449":"ISIC 2020 JPG 224x224 RESIZED","450":"ISIC 2019 JPG 256x256 RESIZED","451":"ISIC 2019 JPG 224x224 RESIZED","452":"Stress and Anxiety Posts on Reddit","453":"Skin Disease Classification","454":"4 Players Chess Pieces","455":"Car Classification","456":"Dresden Image Database"},"subtitle":{"0":"\"Release Year, Duration, Ratings, Metascores, Vote Counts, and Plot Summaries\"","1":"","2":"","3":"This dataset contains the TOP 10K movies fetched from TMDB API as of March 2024.","4":"Insights from historical data","5":"Frames from 800 top-rated movies for building movie identification models","6":"","7":"","8":"","9":"Web Scraped data from IMDb Website","10":"Most Popular Animes of all Time on IMDB","11":"","12":"","13":"This dataset contains all the IMDb top 1000 movies ","14":"Discover the Greatest Movies of All Time - IMDb's Top 1000 Movie Rankings","15":"Dataset consist of top 1000 imdb movies","16":"Exploring what makes movies stand out from the rest","17":"Feature Film, Rating Count at least 10,000 (Sorted by IMDb Rating Descending)","18":"","19":"Top 1000 Movies by IMDB rating","20":"","21":"","22":"","23":"CLEANED almost 6000 posts about borderline personality","24":"4,000 Reddit posts from people experiencing stress\/anxiety.","25":"4,000 Reddit posts from people experiencing stress\/anxiety.","26":"Emotion-labeled Dataset: Unveiling Sentiments in Textual Content","27":"A Twitter Financial Dataset for Clarification ","28":"Building a predictive model based on positive and negative tweets of twitter","29":"Building a predictive sentiment analysis model using machine learning","30":"Obtained by Twitter API v2, Best for Topic Modelling and Sentiment Analysis","31":"Unveiling Public Sentiment on Politics: A Comprehensive Analysis of India","32":"Unveiling the Emotional Tapestry of Social Media","33":"Emotions are conscious mental reactions","34":"NLP with Disaster Tweets: Data Preprocessing","35":"Sentiment analysis using 1M+ tweets dataset","36":"Where Words Paint the Colors of Feelings","37":"Analyzing Public Opinions: Twitter Data with Labels for Different Categories","38":"crypto and nft related tweets and sentiments for NLP, market trends, etc","39":"Twitter Data containing temporal, geographical, and general tweet metrics.","40":"Humorous Short Jokes","41":"Hate Speech and Offensive Language Detection on Twitter","42":"","43":"Made by a group of students from FORE School of Management","44":"","45":"Dataset generated and used in the final project of UAA-ICI-S7-M1","46":" Xitsonga Twitter Sentiment Analysis Dataset","47":"Synthetic data on urban mobility for enhancing urban planning","48":"Rental Trends and User Behavior","49":"","50":"This is Yulu dataset. Yulu is India\u2019s leading micro-mobility service provider.","51":"OptiCom Sig Quality: Signals' perf metrics for optical comms.","52":"\"Urban Mobility Insights: Exploring City Traffic and Vehicle Behavior\"","53":"Vehicle Traffic Counts and Locations at US ATR Stations","54":"Contains data from the World Bank's data portal. There is also a consolidated.","55":"Comprehensive Dataset of 33 Million U.S. Traffic Congestion Events","56":"Real Traffic Prediction Dataset ","57":"This dataset contains various images of traffic. Images mostly taken from Turkey","58":"","59":"","60":"Cars Detection Dataset","61":"Regression and anamoly detection using Bike Sharing Dataset","62":"","63":"Annotated Dashcam images to detect traffic signs ","64":"","65":"Unlock Road Safety: Explore 52 Types of Traffic Signs in High-Resolution Imagery","66":"For Educational, Research or Commercial Purpose","67":"","68":"A dataset of 29843 cat pictures (64x64), compiled together for training models.","69":"The Importance of Monitoring the Global Uptrend of Influenza.","70":"Automated behavioral classification using multisensor data collected from whales","71":"Create a model that detects the bounding box around a sea turtle\u2019s face","72":"Cat Facial Landmarks in the Wild","73":"Macaque Monkey detection (Object Detection)","74":"Reactive Synthetic Data Augmentation of the UCSD Anomaly Detection Dataset","75":"Top 16 fish species consumed in Sri Lanka","76":"Hand-labeled images of guinea pigs for object detection","77":"","78":"Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?","79":"Dataset containing Images of Wildlife Animals","80":"\"Avian & Insect Harmonies: Recordings from Xeno-Canto\"","81":"Zebrafish aggressive behaviour in front of mirror","82":"Water water everywhere, but not a drop to drink!","83":"Infrared Small Object Detection for Wildlife Conservation","84":"","85":"Expansion of the brackish dataset with synthetically generated data","86":"Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg","87":"Different color classification in different lightening Environment","88":"NIH x-ray dataset in classified format.","89":"contains total of 400 images of cats and dogs for making a classification model.","90":"","91":"","92":"","93":"ISIC 2019 resized dataset","94":"","95":"","96":"Implicit neural representation of ImageNet100 as part of work Implicit Zoo","97":"Annotated Images for Tomato Ripeness Classification","98":"ML Classification  Models","99":"See It, Sort It: An Image Dataset for Industrial Tool Quality Control ","100":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","101":"Classify Different Types of Rocks (Stones)","102":"Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War","103":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","104":"Blacked Out Background Using Mediapipe Dataset","105":"","106":"Analyzing Road Safety: An Interactive Excel Dashboard for Accident Data Insights","107":"Used Car Price Prediction Dataset","108":"Exploring Traffic Incidents, Causes, and Trends in Somerville","109":"","110":"Road Accident Data by Vehicle Type and Provices in Sri Lanka - 2012","111":"\"Roadway Woes: A Comprehensive Dataset on Road Accidents\"","112":"contains traffic densities of cities","113":"Unveiling Asian Influence: Understanding Contributing Factors in Fatal Crashes","114":"CSV NUMERICAL DATA SET ","115":"Detailed dataset of road accidents in the Czech Republic (2016-2022)","116":"Statistics on injury road traffic collisions (RTCs) from 2017 to 2022","117":"Decoding Urban Motion: Analyzing Traffic Density in the Cities of Tomorrow","118":"Road Accident Records in Kensington and Chelsea (January 2021)","119":"Casualty, Collision and Vehicle information for vehicular accidents in the UK","120":"Understanding Accident Severity for Effective Road Management","121":"Vehicle Traffic Counts and Locations at US ATR Stations","122":"","123":"Recommend Airbags for vehicles","124":"Hourly Vehicle data on an Highway, good dataset for beginers","125":"Dataset of Motor Collisions or Crashes in New York","126":"A Comprehensive Collection of Spotify Tracks Across Various Genres","127":"Latest dataset for Animes","128":"Pokemon image dataset + Base stats will 9th Generation","129":"IMDb's Most Popular Anime of All Time","130":"Explore Anime Evolution: Comprehensive Dataset and Scraping Script (1970-2024)","131":"\"Anime Series Statistics and Metadata\"","132":"Latest dataset for Animes","133":"","134":"Dive into Dynamic Datasets for Tailored Recommendations!","135":"Listed Animes, Users and Ratings on MyAnimeList (MAL)","136":"Data set for Tableau practice","137":"Monthly Percentage Returns of Nifty Energy Index Components, Ready for Analysis","138":"Banking and CIBIL Data for Predictive Credit Risk Analysis","139":"Exploring Loan Applicant Characteristics and Risk Assessment","140":"Computer Vision Task","141":"","142":"","143":"Enhanced Sign Language MNIST Dataset for Hand Gesture Recognition","144":"","145":"Deciphering Hand Gestures: A Comprehensive Image Dataset for Gesture Recognition","146":"","147":"Diverse Hand Gestures: A collection of 20,000 images featuring a variety of hand","148":"Dataset for Continuous Sign Language Translation and Recognition","149":"","150":"","151":"","152":"A dataset comprising 14,000 images of hand gestures.","153":"Myoelectric Control for Hand Motions with Myo Armband","154":"A dataset of wireframe images pulled from ASL alphabet images. ","155":"From Hands to Words: A Dataset for Sign Language Translation and Recognition","156":"","157":"","158":"Gesture images including thumbs_up(like) and thumbs_down(dislike).","159":"Empower Your ML Models with Hand Gesture Dataset !!","160":"","161":"Online shopping sentiment analysis flikcart ","162":"Exploring British Airways Customer Reviews","163":"Cappterra Ticketsystem Reviews - Jira, Zoho, ServiceNow, Zendesk - Text, Ratings","164":"Customers Reviews in Airline Industry","165":"This dataset comprises 2,000 entries scraped from AliExpress.","166":"","167":"","168":"A Journey Through Timeless Classics","169":"Data Analysis of Spotify's top 5000 albums of all time.","170":"Data set for Tableau practice","171":"","172":"US Collegiate Sports Dataset from 2015 to 2019 ","173":"","174":"Huge corpus of textual data about LinkedIn posts related to layoffs ","175":"","176":"","177":"Meta's oversight-board & outcomes impacting decisions around content-moderation","178":"","179":"News headlines, company names, top headlines date wise ","180":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","181":" Youtube Channel tmrospacenews","182":" Youtube Channel truecrimenews","183":"","184":"Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War","185":"A Diverse Text Dataset for Classifying Fake, Spam, and Legit Information","186":"","187":"","188":"","189":"","190":"","191":"Sinhala Ada Derana News Articles Dataset (2008-2024) over 200,000 articles","192":"","193":"A publicly available dataset based on the microblogging (Weibo)  social platform","194":"A public dataset of fake news identification for the microblogging social platfo","195":"","196":"Tabular summary about each news article","197":"","198":"","199":"","200":"nlp fake news detection dataset","201":"The Battle Against Misinformation: A Text Classification Dataset","202":"fake news as tweets in tweeter","203":"","204":"","205":"The Battle Against Misinformation: A Text Classification Dataset","206":"","207":"","208":"","209":"","210":"","211":"","212":"","213":"","214":"","215":"","216":"A dataset consisting of classifications of Indonesian spice images","217":"Masakan Padang (Dataset Gambar Makanan Indonesia)","218":"14000 recipes of chicken, lamb, beef, egg, tofu, tempe, and fish","219":"A dataset consisting of classifications of Indonesian spice images","220":"Spice Tales: Exploring the Flavors of Indian Cuisine - An Indian Food Dataset","221":"Data about 16 traditional and famous dishes in Jordan","222":"Masakan Padang (Dataset Gambar Makanan Indonesia)","223":"Analyze the sentiment of one of the best cuisine","224":"Bahama Breeze Restaurant Menu Nutritional Dataset","225":"Comprehensive data on the chemical and quality aspects of wines","226":"Evaluating text understanding through word prediction","227":"","228":"Json format with (instruction input output)","229":"Cleaned and Filtered Version Of Sigma Dolphin","230":"A Comprehensive Financial Question-Answer Dataset from Company Filings","231":"","232":"Processed version of https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversation","233":"AstroMCQA is designed for comparative assessment of LLMs in Astronautics domain","234":"","235":"","236":"GSM8K use for test and evaluate LLM  Math solve performance","237":"Fine tuning LLMs on Arabic language for better performance","238":"Divvy tripdata on bike 2023 ","239":"Financial Question Answering","240":"","241":"Bias Detection Dataset","242":"Pdfs for different rulebooks of boardgames","243":"","244":"","245":"VizWiz Question Answering dataset which answer type is \"other\".","246":"","247":"","248":"Comprehensive data on the chemical and quality aspects of wines","249":"We propose a data mining approach to predict human wine.","250":"Evaluating factors influencing the quality and characteristics of red wine","251":"","252":"This dataset is public available for research.","253":"","254":"","255":"Stock Performance Dataset","256":"Stock Performance Dataset","257":"Detailed Analysis of House Prices in Urban India: Trends and Insights.","258":"Bengaluru House Prediction","259":"Data regarding Sale Prices of several houses and their respective features","260":"Data regarding Sale Prices of several houses and their respective features","261":"Data regarding Sale Prices of several houses and their respective features","262":"Dr. Jagadish Tawade & Nitiraj Kulkarni","263":"House Prices Dataset","264":"Exploring Property Characteristics, Location, and Amenities in Real Estate Marke","265":"Predicting the house price","266":"","267":"Transacted resale flat prices w\/ distance to MRT & CBD. Official SG Gov data.","268":"Exploring Amazon's Book Diversity","269":"Comprehensive Dataset of Apartment Listings in Portugal from Imovirtual","270":"","271":"Regression analysis, mutiple regression,linear regression, prediction","272":"","273":"","274":"NIH x-ray dataset in classified format.","275":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","276":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","277":"NIH Chest X-rays validated images","278":"Covid-19 Patient Chest X-Ray Dataset (Positive and Negative)","279":"YOLO and Pascal VOC Scheme Dataset for Chest X-ray 8 Object Detection","280":"","281":"","282":"","283":"","284":"Data Science for Business - Operation Department","285":"","286":"X Ray Images Dataset -- 3 Classes -- ","287":"11,236 Images, 2 Categories","288":"","289":"","290":"a chest X-ray images of person diagnosed negative and positive with COVID-19","291":"","292":"6,126 images and corresponding lung masks, 4 categories","293":"Lung Disease classification","294":"Dialogue dataset from George R. R. Martin's \"A Song of Ice and Fire\" books","295":"Evaluating text understanding through word prediction","296":"","297":"A Comprehensive Text Corpus of the First Five Books for Natural Language Process","298":"","299":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","300":"This dataset contains prices along with star rantings of one thousand books.","301":"Dataset of different books","302":"Open Library Books API with more than 12k book registers!","303":"","304":"","305":"In this dataset i founded so many insights also in last i developed Recom.. Sys.","306":"Ancient Inscription Image Database from Maharashtra","307":"Texts on Ayurveda in English containing 20+ Books and 2000+ Articles","308":"An arbitrary collection of public domain works from Project geutenberg.","309":"","310":"Stance Detection in Arabic Language Shared Task","311":"","312":"","313":"","314":"Arabic YouTube Comments: A Multi-tagged Dataset by Khalaya","315":"","316":"","317":"","318":"","319":"","320":"","321":"Towards Computational Sarcastic Tweets Identification : An Open-Source Dataset ","322":"","323":"Sarcasm Detection Dataset","324":"Tweets data for sarcastic detection & categorical classificaiton of sarcasm","325":"High-quality dataset with 21k fact check statements between 2008 to 2022","326":"BanglaSarc: A Dataset for Sarcasm Detection ","327":"","328":"","329":"","330":"","331":"","332":"","333":"Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/","334":"Computer Vision Project. ","335":"Anti spoofing dataset with Silicone 3D mask attacks (7000 videos)","336":"","337":"Explore 3D mannequins for anti-spoofing models (1000+ images)","338":"Modification on the original dataset, where some classes were removed\/renamed.","339":"","340":"Infusing Tumor Masks for Enhanced Classification","341":"Face Spoofing dataset Cutout Mask: 1,5K+ participants ","342":"Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/","343":"Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?","344":"","345":"Retina Blood Vessel for segmentation","346":"Competition RSNA-MICCAI Brain Tumor Radiogenomic Classification","347":"The final images and masks obtained after processing Physionet Dataset","348":"Curated Breast Imaging Subset of DDSM dataset with JPG images","349":"27 photos of the same person - face recognition dataset","350":"Surgical scene segmentation in robotic gastrectomy with real and synthetic data","351":"A new masked facial dataset with more accurate mask-wearing conventions","352":"The diversified ocular disorder","353":"","354":"301 132 images, 4 types of masks worn, 75 283 unique faces, face dataset ","355":"Endoscopy instrument dataset (https:\/\/datasets.simula.no\/kvasir-instrument\/)","356":"polyp seg. data(https:\/\/multimediaeval.github.io\/editions\/2020\/tasks\/medico\/)","357":" polyp segmentation  and GI classification dataset (https:\/\/endotect.com\/)","358":"Colorectal polyp data(1000 im, GT & BB) (https:\/\/datasets.simula.no\/kvasir-seg\/)","359":"6000 images of people wearing masks, scarves, face shields and other accessories","360":"","361":"COVID-19 CoronaVirus Face Mask Collection ","362":"Comprehensive Playing Cards Dataset generated for YOLO Object Detection","363":" Bangladeshi Native Vehicle Dataset: Cataloging Traditional Transportation","364":"image datasets training images: 11060 and  test images: 2274 classifies 27 cars","365":"A modified version of IDD-Detection dataset for YOLO object detection pipelines.","366":"[CVPR 2024] Intriguing Properties of Diffusion Models","367":"","368":"EGYPlate: Egyptian Car License Plate Detection Dataset for Object Detection","369":"","370":"","371":"Cars Object Detection Dataset","372":"License Plate Detection - Ready to Train (YOLO)","373":"","374":"Comprehensive 26-Class Object Detection Dataset for Urban Scenes","375":"RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving","376":"","377":"Annotated Images of Obstacles in Public Spaces","378":"Annotated Images of Obstacles for Object Detection in Public Spaces","379":"","380":"","381":"VOC_2012 Ready to use Dataset for yolo 20 class object detection","382":"DeepFake dataset celebs v2 images","383":"DeepFake dataset FF++","384":"Comprehensive Simulated Business Environment for CRM and Retail Analysis","385":"Face anti spoofing with photos and videos of asian people","386":"","387":"Build a Model that detect fake Currency Notes","388":"This datasets(205k images) will be used for GAN models training","389":"Images of Vietnamese Passports. ALL DATA IS GENERATED","390":"2023 fake or real image discrimination competition dataset","391":"Dataset of USA Passports with Augmentation. ALL DATA IS GENERATED","392":"Contains images of Indian coins (real and fake) of denominations Rs 1, 2 and 5.","393":"Simplified version of handwritten signatures dataset in only 1MB by FCIS-ASU","394":"Can Computer Vision detect when images have been generated by AI?","395":"a dataset of human generated art  and AI generated art.","396":"Replay-attack Video Anti-Spoofing Dataset","397":"","398":"Discriminate Real and Fake Face Images","399":"Photorealistic synthetic images representing pictures of chess game states","400":"Detect if any images is real image of deepfake image","401":"All members of the Israeli Parliament with affiliation","402":"Find players that can play for India to qualify for the world cup","403":"In-Depth Data from Euro 2024 Group Stage on FBRef","404":"Data from FBRef about top leagues and relative teams and players","405":"Turkiye Super Lig 2023-2024 Player Informations (Number, Team, Skills, Value...)","406":"","407":"Basic info of players from UEFA EURO 2024, collected on Transfermarkt","408":"Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/","409":"Turkish Super League 2023\/2024 Mid-Season Player Statistics","410":"Data about Indian Premier League Auction of all teams","411":"Football players transfer value based on their ratings, skills and attributes","412":"Game by game statistics of all current PL players for the past 2 seasons","413":"Football matches, players & events for EURO 1960-2024 & Nations League 2019-2023","414":"","415":"","416":"A scout database for Northampton FC in Football Manager 23(Season 2030-31)","417":"2024 NWSL Season (so far) player stats for 343 players. Updated monthly.","418":"","419":"Season 8 & 9 ball-by-ball","420":"A dataset of the players in a simulated Championship season(2030-31)","421":"Comprehensive player positions and stats for NWSL athletes - 2023","422":"This dataset contains football match data from the English Premier League","423":"In-Depth Data from Euro 2024 Group Stage on FBRef","424":"Football Player Performance Data for Europe's top leagues ","425":"","426":"Open database of matches played in the top tiers of women's football in England.","427":"Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/","428":"Turkish Super League 2023\/2024 Mid-Season Player Statistics","429":"FIFA Worldcup 2022 FInal Dataset: ARG vs FRA: Event to Event data","430":"The dataset appears to be a collection of text","431":"Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries","432":"NIH x-ray dataset in classified format.","433":"BERT Fine-Tuned For Classification on Phishing Email Dataset","434":"4,000 Reddit posts from people experiencing stress\/anxiety.","435":"Extract IT skills from Job Descriptions","436":"labeled IT skills in job descriptions.","437":"Custom Dark Pattern Dataset - IIT(BHU)","438":"Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg","439":"Data Analysis Designed to Predict High Stream Count on Spotify","440":"The dataset appears to be a collection of text","441":"Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries","442":"Different color classification in different lightening Environment","443":"NIH x-ray dataset in classified format.","444":"US Collegiate Sports Dataset from 2015 to 2019 ","445":"BERT Fine-Tuned For Classification on Phishing Email Dataset","446":"","447":"contains total of 400 images of cats and dogs for making a classification model.","448":"","449":"","450":"","451":"ISIC 2019 resized dataset","452":"4,000 Reddit posts from people experiencing stress\/anxiety.","453":"","454":"Pieces Dataset for training","455":"","456":""},"description":{"0":"This dataset provides comprehensive information about top-rated movies from IMDB. It includes the title of each movie, the year it was released, and its duration in hours and minutes. The dataset also provides the IMDB rating and the number of votes the movie has received on the platform. Additionally, it features the Metascore from Metacritic, indicating the critical reception of each movie. The content rating, which indicates the appropriate audience for the movie, is also included. Finally, a brief plot summary is provided for each movie, offering an overview of the storyline. This dataset is ideal for movie enthusiasts, researchers, and data analysts interested in exploring and analyzing the characteristics of top-rated movies.\n\n\n\n\n\n\n","1":"","2":"The IMDb movie dataset and IMDb rating dataset provide comprehensive information about movies listed in the IMDb database along with their ratings. Here's a brief description of each:\n\n1. IMDb Movie Dataset:\n   - This dataset contains detailed information about movies, including their titles, original titles, release years, publication dates, genres, durations, countries of production, languages, directors, writers, production companies, actors, descriptions, average votes, total votes, budgets, gross incomes (both in the USA and worldwide), metascores, and reviews from users and critics.\n   - Each row represents a unique movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset offers insights into various aspects of movies, such as their production details, cast and crew, plot summaries, and reception through ratings and reviews.\n\n2. IMDb Rating Dataset:\n   - This dataset provides ratings and voting statistics for movies listed in the IMDb database.\n   - It includes information such as IMDb title IDs, weighted average votes, total votes, mean and median votes, distribution of votes across different rating levels (from 1 to 10), and average votes and votes counts segmented by gender and age groups.\n   - Additionally, it offers ratings from specific voter groups, such as top 1000 voters, US voters, and non-US voters.\n   - Each row corresponds to a movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset enables analyses of movie ratings across different demographics and provides insights into the audience's perception and reception of movies listed on IMDb.\n\nIMDB Movie Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. title: Title of the movie.\n3. original_title: Original title of the movie (may differ from the localized title).\n4. year: Year of release of the movie.\n5. date_published: Date when the movie was published or released.\n6. genre: Genre(s) to which the movie belongs.\n7. duration: Duration of the movie in minutes.\n8. country: Country or countries where the movie was produced or filmed.\n9. language: Language(s) spoken in the movie.\n10. director: Director(s) of the movie.\n11. writer: Writer(s) of the screenplay or story for the movie.\n12. production_company: Production company or companies involved in producing the movie.\n13. actors: Main actors or cast members of the movie.\n14. description: Brief description or summary of the movie's plot or storyline.\n15. avg_vote: Average rating or vote score given to the movie by IMDb users.\n16. votes: Total number of votes received by the movie on IMDb.\n17. budget: Budget allocated for producing the movie.\n18. usa_gross_income: Gross income or revenue generated from the movie's release in the United States.\n19. worlwide_gross_income: Gross income or revenue generated from the movie's release worldwide.\n20. metascore: Metascore rating assigned to the movie by critics (if available).\n21. reviews_from_users: Number of user reviews or ratings submitted for the movie.\n22. reviews_from_critics: Number of reviews or ratings given by critics for the movie.\n\nIMDB Rating Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. weighted_average_vote: Weighted average rating or vote score given to the movie.\n3. total_votes: Total number of votes received by the movie.\n4. mean_vote: Mean or average vote score given to the movie.\n5. median_vote: Median vote score given to the movie.\n6. votes_10: Number of votes rating the movie as 10.\n7. votes_9: Number of votes rating the movie as 9.\n8. votes_8: Number of votes rating the movie as 8.\n9. votes_7: Number of votes rating the movie as 7.\n10. votes_6: Number of votes rating the movie as 6.\n11. votes_5: Number of votes rating the movie as 5.\n12. votes_4: Number of votes rating the movie as 4.\n13. votes_3: Number of votes rating the movie as 3.\n14. votes_2: Number of votes rating the movie as 2.\n15. votes_1: Number of votes rating the movie as 1.\n16. allgenders_0age_avg_vote: Average vote score given by viewers of all genders in the 0-18 age group.\n17. allgenders_0age_votes: Number of votes from viewers of all genders in the 0-18 age group.\n18. allgenders_18age_avg_vote: Average vote score given by viewers of all genders in the 18-30 age group.\n19. allgenders_18age_votes: Number of votes from viewers of all genders in the 18-30 age group.\n20. allgenders_30age_avg_vote: Average vote score given by viewers of all genders in the 30-45 age group.\n21. allgenders_30age_votes: Number of votes from viewers of all genders in the 30-45 age group.\n22. allgenders_45age_avg_vote: Average vote score given by viewers of all genders in the 45+ age group.\n23. allgenders_45age_votes: Number of votes from viewers of all genders in the 45+ age group.\n24. males_allages_avg_vote: Average vote score given by male viewers of all ages.\n25. males_allages_votes: Number of votes from male viewers of all ages.\n26. females_allages_avg_vote: Average vote score given by female viewers of all ages.\n27. females_allages_votes: Number of votes from female viewers of all ages.\n28. top1000_voters_rating: Average rating given by the top 1000 voters.\n29. top1000_voters_votes: Number of votes from the top 1000 voters.\n30. us_voters_rating: Average rating given by voters from the United States.\n31. us_voters_votes: Number of votes from voters from the United States.\n32. non_us_voters_rating: Average rating given by voters from outside the United States.\n33. non_us_voters_votes: Number of votes from voters from outside the United States.","3":"","4":"Introduction:\n\nIn this case study the skills that I acquired from Google Data Analytics Professional Certificate Course is demonstrated. These skills will be used to complete the imagined task which was given by Netflix. The analysis process of this task will be consisted of following steps. Ask, Prepare, Process, Analyze, Share and Act.\n\n\nScenario:\n\nThe Netflix Chief Content Officer, Bela Bajaria, believes that companies success depends on to provide the customers what they want. Bajaria stated that the goal of this task is to find most wanted contents of the movies which will be added to the portfolio. Most of the movie contracts are signed before they come to the theaters, and it is hard to know if the customers really want to watch that movie and if the movie will be successful. There for my team wants to understand what type of content a movies success depends on. From these insights my team will design an investment strategy to choose the most popular movies that are expected to be in theaters in the near future. But first, Netflix executives must approve our recommendations. To be able to do that we must provide satisfying data insights along with professional data visualizations. \n\n\nAbout the Company:\n\n\nAt Netflix, we want to entertain the world. Whatever your taste, and no matter where you live, we give you access to best-in-class TV series, documentaries, feature films and games. Our members control what they want to watch, when they want it, in one simple subscription. We\u2019re streaming in more than 30 languages and 190 countries, because great stories can come from anywhere and be loved everywhere. We are the world\u2019s biggest fans of entertainment, and we\u2019re always looking to help you find your next favorite story.\n\nAs a company Netflix knows that it is important to acquire or produce movies that people want to watch. \n\nThere for Bajaria has set a clear goal: Define an investment strategy that will allow Netflix to provide customers the movies what they want to watch which will maximize the Sales.\n\n\n\nAsk:\n\nBusiness Task: \nTo find out what kind of movie customers wants to watch and if the content type really has a correlation with the movie success.\nStakeholders: \n\nBela Bajaria: She joined Netflix in 2016 to oversee unscripted and scripted series. Bajaria also responsible from the content selection and strategy for different regions. \n\nNetflix content analytics team: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Netflix content strategy.\n\nNetflix executive team: The notoriously detail-oriented executive team will decide whether to approve the recommended content program.\n\n\nPrepare:\n\nI start my preparation procedure by downloading every piece of data I'll need for the study. Top 1000 Highest-Grossing Movies of All Time.csv will be used. Additionally, 15 Lowest-Grossing Movies of All Time.csv was found during the data research and this dataset will be analyst as well. The data has been made available by IMDB and shared this two following URL addresses: https:\/\/www.imdb.com\/list\/ls098063263\/ and https:\/\/www.imdb.com\/list\/ls069238222\/ .\n\n\nProcess:\n\nData Cleaning: \n\nSQL: To begin the data cleaning process, I opened both csv file in SQL and conducted following operations: \n\n\u2022\tChecked for and removed any duplicates. \n\u2022\tChecked if there any null values. \n\u2022\tRemoved the columns that are not necessary.\n\u2022\tTrim the Description column to have only gross profit in it. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\n\u2022\tRenamed the Description column as Gross_Profit. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\nFollwing SQL codes were used during the data cleaning:\n#SQL CODE used for Highest Grossing Movies DATASET\n\nSELECT \nPosition,\nSUBSTR(Description,34,12) as Gross_Profit,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\n FROM `even-electron-400301.Highest_Gross_Movies.1` \n\n#SQL CODE used for Lowest Grossing Movies DATASET\n\nSELECT \nPosition,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\nFROM `even-electron-400301.Lowest_Grossing_Movies.2` \nOrder By Position\n\n\n\n\nAnalyze:\n\nAs a starter, I want to reemphasize the business task once again. Is content has a big impact on a movie\u2019s success?\n\nTo answer this question, there were a few information that I projected that I could pull of and use it during my analysis. \n\n\u2022\tAverage gross profit \n\u2022\tNumber of Genres\n\u2022\tTotal Gross Profit of the most popular genres\n\u2022\tThe distribution of the Gross income on Genres\n\n\nI used Microsoft Excel for the bullet points above. The operations to achieve the values above are as follows:\n \n\u2022\tAverage function for Average Gross profit in 1000 Highest-Grossing Movies of All Time.\n\u2022\tCreated a pivot table to work on Genres and Gross_Profit\n\u2022\tAnalyzed all genre which consist different categories (exp: Action, Adventure, Horrror ...) in it by using filtering for specific categories like, action, adventure etc. \n\u2022\tThe content analysis mentioned in the last bullet point implemented also for 15 Lowest-Grossing Movies of All Time dataset. \n\nShare:\nThe dashboard I created for the project is also posteed on tableau, you can find the dashboard can be found here\n\nFor the visualizations, it was aimed to emphasize the only the data which demonstrates the key differences between Genre and Gross Profit. \nFollowing visualizations were included into the dashboard;\n\n\u2022\tGross income distributions on movie genres\n\u2022\tGenre\u2019s category distribution for highest grossing movies\n\u2022\tGenre\u2019s category distribution for lowest grossing movies\n\n\n\nHere is a summary of the most meaningful insights for content and movie success:\n\nHighest Grossing Movies: \n\n\u2022\tAmong the 1000 most profited movies the most common genre is \u201cAction, Adventure, Sci-Fi\u201d, 64 movies have this genre type in the list.\n\u2022\t The following genre type is \u201cAction, Adventure, Thriller\u201d with 33 movies. \n\u2022\tEach Genre has several categories in it. During analysis it was observed that some categories has occurred more than other categories. \n\u2022\tAmong the highest grossing movies there were clear difference between different categories that is used in genres. According to my analysis among the 1000 movies 520 movies possessed adventure category and 484 movies had action category in it. \n\u2022\tMovies with horror genre is one of the least sucessfull ones in the 1000 highest grossing movies list in terms of gross profit.\n\nLowest Grossing Movies: \n\u2022\tThere is no movie in the lowest gross income movies with action or adventure genres.\n\u2022\t30% of the 15 least grossing movies has horror genre which is the most common genre in this list.\n\nAbout the analysis: \n\nThe data collected from secondary resources was limited to come up with a detailed analysis. Since the analysis has conducted with the limited data, it might affect the insights and might not demonstrate us the real-life distribution which will lead to wrong strategies especially when it comes to different regions. \n\nAct: \n\nBased on my analysis, following actions are recommended which I believe that it will help content department to create an effective strategy for content creation and selection. \n\n\n\u2022\tAction\/Adventure base strategy: When Netflix signing with a new movie or creating a new one, we can add more movies with action and adventure category to our portfolio. \n\u2022\tHorror Movies: Netflix should give less priority to horror movies. Statistically, horror movies are the least successful movies among all. And to increase the number of movies which will be invested it will increase the probability to fail.\n\u2022\tCombination of the forces: Netflix can try to combine the most successful categories in terms of gross profit. For instance, comedy with action or animation and action to increase the attraction. \n\n\nBy implementing these strategies, Netflix can make more meaningful decision on content creation and sign more precise movie agreements which are demanded by the audience. \n","5":"Google's GEMINI can identify a movie if you show a few frames of it. Cool stuff! Can you build a model that can do the same thing?\n\nThe dataset contains frames from approximately 800 top-rated movies. Each movie has around 1000 frames, totaling up to 800,000 frames in the dataset. \n\n&gt; If you find this dataset valuable for building movie identification models, don't forget to hit the upvote button! \ud83d\ude0a\ud83d\udc9d \n\nTop rated movies were identified from my [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) dataset\n\n### Checkout my top datasets\n- [Top Spotify Songs in 73 Countries](https:\/\/www.kaggle.com\/datasets\/asaniczka\/top-spotify-songs-in-73-countries-daily-updated)\n- [Wages by Education in the USA](https:\/\/www.kaggle.com\/datasets\/asaniczka\/wages-by-education-in-the-usa-1973-2022)\n- [Amazon Products Dataset (1.4M Products)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-products-dataset-2023-1-4m-products)\n- [TMDB 950K Movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies)\n- [Amazon Kindle Books (130K Books)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-kindle-books-dataset-2023-130k-books)\n\n\n## Interesting Task Ideas:\n\n1. Create a movie identification system that can predict the title of a movie based on extracted frames.\n2. Train an AI model to identify similar scenes or genres using movie frames.\n3. Develop a content-based recommendation system that suggests movies based on visual similarity.\n4. Explore the correlation between movie ratings and the visual elements captured in the frames. Use [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) for ratings\n5. Train deep learning models like VIT or ResNet using the dataset to develop powerful movie identification systems.\n\n## Specs:\n\n- Each frame has a height of 256px.\n- Width will vary since different movies have different aspect ratios.\n- Roughly 1000 frames were extracted from each movie.\n- All frames for a movie will exist under the folder with the movie's name.\n- Movie names are in the format of `Movie name (Year of release)`.\n- To get more details on movies, use my TMDB movies dataset. Movie names are not a 100% match. Filter by release year and try semantic similarity or vector similarity.\n\n---\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@jakobowens1?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Jakob Owens<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/clap-board-roadside-jakob-and-ryan-CiUR8zISX60?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n  ","6":"The dataset have seven features : \n1) Movie Name\n2) Category\n3) Release year\n4) Duration\n5) Rating\n6) Meta score\n7) Votes .","7":"","8":"","9":"This dataset contains information about top 1000 IMDB movies, including their titles, certificates, durations, genres, IMDb ratings, Metascores, directors, cast members, the number of votes they received, grossed earnings, and plot summaries. The data is a curated list of highly acclaimed and popular movies.\n\nColumns\/Variables:\n\nMovie Name: The title of the movie.\nCertificate: The certificate or rating assigned to the movie.\nDuration: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nIMDb Rating: The IMDb rating of the movie.\nMetascore: The Metascore rating of the movie.\nDirector: The director of the movie.\nStars: The main cast members of the movie.\nVotes: The number of user votes\/ratings the movie has received.\nGrossed in $: The gross earnings in dollars (if available).\nPlot: A brief summary or plot description of the movie.\nSize: The dataset contains 1000 rows and 11 columns.\n\n\nData Quality: The dataset appears to be well-structured and complete. There are no missing values, and it seems to be ready for analysis.\n\nUse Cases: This dataset can be used for various analyses, such as exploring the relationship between IMDb ratings and Metascores, identifying top-rated directors, or understanding the distribution of movie ratings across genres.","10":"This Dataset displays only the top 1000 Anime of all time on IMDB. It is in the same order as displayed on the IMDB website which is filtered by - \"Most Popular Movies and TV Shows tagged with keyword \"anime\". [**Source**](https:\/\/www.imdb.com\/search\/keyword\/?keywords=anime&mode=detail&page=1&ref_=kw_nxt&sort=moviemeter,asc)\n\n\n**DATA DICTIONARY:**<br>\n`Title`: The name of the Anime.\n`Release Year`: The year the anime was released and till which year it is Ended, The OA in the values tells that the anime is still airing.\n`Genre`: Categories where the anime belongs.\n`Duration`: Anime running time in minutes.\n`Rating`: Ratings given by IMDb registered users (on a scale of 1 to 10)\n`Description`: Summary of the Anime which gives a brief description what it is about.\n`No. of Votes`: Number of votes cast by IMDb registered users.\n\n\n*Note: There are NA values present in most of the columns, because the value is not available on the source*","11":"","12":"","13":"The IMDb Top 1000 Movies dataset is a treasure trove of information about the most esteemed films in cinematic history. It includes a comprehensive collection of attributes that can fuel numerous insightful analyses and visualizations.\n\nTitle: The title of the movie.\nYear: The year the movie was released.\nRating: The IMDb rating of the movie, reflecting its overall popularity and reception.\nDuration: The runtime of the movie in minutes.\nGenre: The genre(s) to which the movie belongs (e.g., Drama, Action, Adventure).\nIMDb Rank: The rank of the movie within the IMDb Top 1000 list.\nMetascore: The Metascore assigned to the movie, offering a critical review of its quality.\nDirector: The director(s) responsible for bringing the movie to life.\nStars: The lead actors and actresses who play pivotal roles in the movie.\nVotes: The number of user votes the movie has received on IMDb, reflecting its popularity.\nGross: The gross earnings of the movie at the box office or through various distribution channels.\nDescription: A brief synopsis or overview of the movie's plot, theme, or significance.","14":"Welcome to the \"Top 1000 IMDb Movies Dataset\" - a treasure trove of cinematic excellence! This comprehensive collection presents the most celebrated and beloved movies, as rated and ranked by IMDb users. Whether you're a movie enthusiast, a data scientist, or a filmmaker seeking inspiration, this dataset promises a thrilling journey through the world of cinema's finest creations.\n\n**Content**:\n\nThis dataset boasts a rich array of movie attributes to help you delve into the realm of top-rated films:\n\nMovie Name: The title of each movie, representing iconic masterpieces that have left an indelible mark on the film industry and audiences worldwide.\n\nYear of Release: The year when each movie was released, providing valuable context for historical and chronological analyses.\n\nWatch Time: The duration of each movie, allowing you to identify captivating films for various viewing preferences.\n\nMovie Rating: IMDb's user ratings, serving as a benchmark for gauging audience reception and appreciation.\n\nMetascore of Movie: Metascores from renowned critics, offering insights into the films' critical acclaim and recognition.\n\nGross Earnings: The worldwide box office earnings, reflecting the commercial success and popularity of each movie.\n\nVotes: The number of votes cast by IMDb users, indicating the films' popularity and reach.\n\nDescription: Brief summaries that provide a glimpse into the captivating plots and themes of these cinematic marvels.\n\n**Usage**:\n\nThe \"Top 1000 IMDb Movies Dataset\" opens up a world of possibilities for movie aficionados and data enthusiasts alike:\n\nFilm Analysis and Insights: Dive into data-driven analyses to uncover intriguing patterns, trends, and correlations among highly-rated movies.\n\nRating Predictions: Develop predictive models to estimate movie ratings based on other attributes and gain a deeper understanding of factors influencing user perceptions.\n\nBox Office Performance: Examine the relationship between critical acclaim, ratings, and box office success, providing valuable insights for filmmakers and producers.\n\nGenre Exploration: Investigate the distribution of top-rated movies across different genres to identify trends in audience preferences.\n\nTemporal Trends: Analyze changes in movie ratings, box office performance, and viewer preferences over time to reveal shifts in cinematic taste and trends.\n\n**Caution**:\n\nWhile this dataset is a treasure trove for movie analysis, it is essential to respect copyright and intellectual property rights. Users are encouraged to use the data responsibly and comply with IMDb's terms of use and any applicable legal restrictions.\n\n**Conclusion**:\n\nThe \"Top 1000 IMDb Movies Dataset\" offers an immersive and data-driven exploration of cinema's greatest works, igniting creativity, fostering discussions, and enriching our appreciation for the art of storytelling. Let the magic of these cinematic gems captivate you as you embark on an exciting journey into the world of acclaimed movies!","15":"","16":"Data of the top 1000 movies scraped from the IMDB website. Dataset also duplicates a movie in another row if the movie consists of more than one genre. Missing values present in 'gross(M)' and 'metascore' column represented as 0.00 and 0 respectively.\n\nEdit: Version 3. I have included the raw data from scraping the website for anyone interested in cleaning the data and transforming data types.\nI have also updated the csv file to include the director of the movie\n\nDISCLAIMER:  The grossing values of movies from IMDb only include numbers from US and Canada and are thus not representative of the values on a global scale.","17":"This dataset consists of a meticulously collected collection of 10,000 feature films from IMDb, one of the most popular and authoritative sources for movie information. The movies included in this dataset are sorted based on their IMDb ratings in descending order. The dataset covers a wide range of genres, directors, and stars, providing a comprehensive overview of highly regarded films across various categories.The scraping process was performed on June 17, 2023.\n\nDataset Columns:\n\nID: Unique identifier for each movie in the dataset.\nMovie Name: The title of the movie.\nRating: The IMDb rating for the movie.\nRuntime: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nMetascore: The Metascore rating for the movie (if available).\nPlot: A brief summary or description of the movie's plot.\nDirectors: The director(s) of the movie.\nStars: The main cast or actors featured in the movie.\nVotes: The number of votes\/ratings received by the movie.\nGross: The gross revenue generated by the movie (if available).\nLink: The IMDb link to access the full details and additional information about the movie.","18":"","19":"","20":"","21":"","22":"","23":"This dataset contains 5,879 Reddit posts discussing Borderline Personality Disorder (BPD) , posts are about people telling own experiencies with this disorder either they suffer from or have met people or raised by bpd parents \n\nThis dataset provides a valuable resource for analyzing the discourse around BPD on social media, studying patterns in user interactions, and understanding the experiences and challenges faced by individuals with BPD.\n\n**Columns:**\n**Title**: The title of the Reddit post.\n**Score**: The score of the post, reflecting its popularity and engagement.\n**ID**: The unique identifier of the post.\n**Comments**: The number of comments the post received.\n**Creation Time**: The timestamp when the post was created\n**Content**: The main text of the post, detailing the user's thoughts, experiences, and discussions about BPD\n\nThis dataset can be used for various purposes, including sentiment analysis, natural language processing, mental health research","24":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","25":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","26":"This dataset presents a comprehensive collection of textual content annotated with corresponding sentiments: happy, sad, and angry. Each entry in the dataset contains a piece of text along with its associated sentiment label, providing a rich resource for sentiment analysis and emotion detection tasks.","27":"# Introduction \nThe Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment.\n\nThe dataset holds 11,932 documents annotated with 3 labels:\n```python\nsentiments = {\n    \"LABEL_0\": \"Bearish\", \n    \"LABEL_1\": \"Bullish\", \n    \"LABEL_2\": \"Neutral\"\n}  \n```\n\nThe data was collected using the Twitter API. The current dataset supports the multi-class classification task.\n\n# Data Splits\nThere are 2 splits: train and validation. Below are the statistics:\n\nDataset Split\tNumber of Instances in Split: \n\nTrain -----------9,938\n\nValidation-------2,486\n\n# Licensing Information\nThe Twitter Financial Dataset (sentiment) is released under the MIT License.\n\n# Source \nhttps:\/\/huggingface.co\/datasets\/zeroshot\/twitter-financial-news-sentiment","28":"**Context**\nThe objective of this task is to build a model based on pre-processed tweets. For the sake of simplicity, we say a tweet contains a negative review if it has a racist or hate sentiment associated with it. So, the task is to predict the labels on the test dataset after building a model.\n\n**Content**\nIn the dataset, a labelled train data is given where label '0' denotes the tweet is positive \ud83d\ude0a and label '1' denotes the tweet is negative \u2639\ufe0f\n\n**Acknowledgements**\nDataset is provided by [Analytics Vidhya](http:\/\/https\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/)\n","29":"**Context**\n\nThis dataset is a part of our research work titled \"Opinion Mining of Customer Reviews Using Supervised Learning Algorithms\". If you use this dataset then please cite our work.\nYou can find the article in https:\/\/ieeexplore.ieee.org\/document\/9733435\n\n**Content**\n\nNowadays, a lot of people express their opinions on various topics using social networking sites. Twitter has become a famous social networking site where people can express their opinions to the point and so it has become a great source for opinion mining. In this research, the goal was to train and build a model that can automatically and accurately categorize the opinion of customer tweet reviews about popular cell phone brands. We have used python TextBlob library for getting the polarity values of all the tweet reviews of the dataset. We have also used Support Vector Machine (SVM), Na\u00efve Bayes, Logistic Regression, Decision Tree and Random Forest algorithms along with Bag of Words and TF-IDF vectorizers separately to train and build the model. We have investigated the opinions using five classes which are Strongly Positive, Positive, Neutral, Negative and Strongly Negative.\n\n\n**When referencing this dataset please cite the below paper**\n\n**Bibtex**\n@inproceedings{arif2021opinion,\ntitle={Opinion Mining of Customer Reviews Using Supervised Learning Algorithms},\nauthor={Arif, Shibbir Ahmed and Hossain, Taslima Binte},\nbooktitle={2021 5th International Conference on Electrical Information and Communication Technology (EICT)},\npages={1--6},\nyear={2021},\norganization={IEEE}\n}","30":"","31":"This dataset provides a comprehensive collection of public sentiment and discourse related to Indian politics. The entries cover a wide range of opinions, news, social media posts, and other forms of public communication. \nEach entry is meticulously labeled with a sentiment score, capturing the polarity of the opinion from strongly negative to strongly positive. \n\nThis dataset is structured to facilitate detailed sentiment analysis and examination of political sentiments in India.\n\nUse Cases\n\nThis dataset is ideal for:\n\nSentiment Analysis: Researchers can use this dataset to train and evaluate sentiment analysis models specifically tailored to the political context in India.\nTrend Analysis: Analysts can track the evolution of public sentiment over time, identifying key events that influenced public opinion.\nPolitical Studies: Scholars can investigate the relationship between public sentiment and political events, figures, and policies in India.\nNatural Language Processing (NLP): NLP practitioners can leverage this dataset for various tasks such as text classification, opinion mining, and more.\n","32":"Title: Twitter Emotion Dataset: Unveiling the Emotional Tapestry of Social Media\n\nDescription:\n\nDive into the intricate world of human emotions expressed through Twitter messages with our meticulously curated dataset. Each entry in this comprehensive collection features a text segment extracted from Twitter, accompanied by a corresponding label denoting the predominant emotion conveyed by the message. The emotions are thoughtfully categorized into six distinct classes: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\nThis dataset is tailor-made for researchers, data scientists, and enthusiasts keen on exploring the dynamic emotional landscape within the realm of social media. Whether you're delving into sentiment analysis, emotion classification, or text mining, our Twitter Emotion Dataset provides a rich foundation for unraveling the subtle nuances that define human emotions in the digital age.\n\n**Dataset Highlights:**\n- Over 416809 Twitter messages covering a diverse range of topics and user demographics.\n- Manually annotated labels ensure high-quality emotional classification.\n- Tweets sourced from a variety of geographies and languages, capturing a global perspective on emotions in social media.\n- A balanced distribution across the six emotion categories for robust model training.\n\n**Potential Applications:**\n- Sentiment Analysis: Explore sentiment trends and patterns to understand how users express emotions over time.\n- Emotion Classification: Develop models to accurately predict and categorize emotional states within tweets.\n- Text Mining: Extract valuable insights from the vast pool of emotional data to inform decision-making and marketing strategies.\n\n**Why Use Our Dataset?**\n- Enrich your research with a diverse and well-labeled dataset, ensuring the reliability and accuracy of your findings.\n- Leverage the global scope of the dataset for cross-cultural and multilingual analyses of emotional expression.\n- Contribute to advancements in emotional intelligence research and its applications in social media analytics.\n\n**Dataset Conclusions:**\nUncover the hidden gems within our Twitter Emotion Dataset and draw meaningful conclusions about the evolving emotional landscape of social media. Whether you're identifying sentiment shifts during significant events or tracking the ebb and flow of joy, anger, and surprise, this dataset provides the tools to delve into the intricacies of human emotion in the digital realm. As you analyze and model, contribute to the collective understanding of emotional intelligence on social platforms and pave the way for innovative applications in communication, mental health, and beyond. Join us in decoding the emotional tapestry of Twitter and shaping the future of emotion-aware technologies.","33":"&gt;# Content:\nThis Emotion Classification dataset is designed to facilitate research and experimentation in the field of natural language processing and emotion analysis. It contains a diverse collection of text samples, each labelled with the corresponding emotion it conveys. Emotions can range from happiness and excitement to anger, sadness, and more.\n&gt;# About emotions.csv file\nEach entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into **six categories**: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within social media.\n- **text**: Description of context\n- **label**: The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\n&gt;# Usecase:\n- **Sentiment Analysis**: Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification**: Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis**: Explore linguistic patterns and expressions associated with different emotional states.","34":" Introduction\n\n Raw tweet data is often messy and requires cleaning and normalization before building an effective NLP model.  Here's a breakdown of common preprocessing steps and their purpose:\n\n 1. Data Gathering\n\nObtaining a suitable dataset of labeled tweets (e.g., from Twitter, existing NLP datasets, or Kaggle competitions).\n 2. Removing HTML Tags\n\nHTML tags can clutter the text and don't contribute to understanding a tweet's content or sentiment.\n 3. Removing URLs\n\nURLs often don't add significant meaning for disaster classification and may introduce unnecessary variability.\n 4. Converting to Lowercase\n\nMakes the text case-insensitive, ensuring that \"Disaster\" and \"disaster\" are treated as the same word, improving word frequency analysis.\n 5. Removing Emojis\n\nWhile emojis can carry sentiment, they require sophisticated techniques to interpret consistently. Basic preprocessing often removes them, although more advanced models might incorporate emoji analysis.\n 6. Removing Punctuation\n\nPunctuation marks rarely contribute to core meaning in disaster classification and can introduce noise.\n 7. Removing Stop Words\n\nRemoving common words like \"the,\" \"and,\" etc., that have little semantic value. This reduces computational load and lets the model focus on more informative words.\n 8. Handling Abbreviations\/Slang\n\nExpanding abbreviations and slang terms (e.g., \"lol\" -&gt; \"laughing out loud\") aids in understanding the full meaning of the text and makes the vocabulary more standardized.\n 9. Stemming\n\nReduces different forms of words to their root (e.g., \"flooding,\" \"flooded\" -&gt; \"flood\"), potentially helping the model generalize better.\n 10. Spelling Correction\n\nFixing typos ensures words are correctly interpreted and can make word frequencies more accurate.\n 11. Tokenization\n\nSplits the text into individual words or meaningful units (e.g., \"New York\" is often better treated as a single token) to prepare the data for further analysis and model input.","35":"&gt;## Context\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = negative, 4 = positive) and can be used to detect sentiment.\n## Content\nIt contains the following 6 fields:\n- target: the polarity of the tweet (0 = negative and 4 = positive)\n- ids: The id of the tweet ( 2087)\n- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n- user: the user that tweeted.\n- text: the text of the tweet.","36":"# Introduction:\n\n&gt;Welcome to the \"Emotions\" dataset \u2013 a collection of English Twitter messages meticulously annotated with six fundamental emotions: anger, fear, joy, love, sadness, and surprise. This dataset serves as a valuable resource for understanding and analyzing the diverse spectrum of emotions expressed in short-form text on social media.\n\n[![DOI](https:\/\/zenodo.org\/badge\/DOI\/10.34740\/KAGGLE\/DSV\/7563141.svg)](https:\/\/www.kaggle.com\/nelgiriyewithana\/datasets)\n\n\n# About the Dataset:\n\n&gt;Each entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within the realm of social media.\n\n# Key Features:\n\n&gt;- **text**: A string feature representing the content of the Twitter message.\n- **label**: A classification label indicating the primary emotion, with values ranging from 0 to 5.\n\n# Potential Use Cases:\n\n&gt;- **Sentiment Analysis:** Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification:** Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis:** Explore linguistic patterns and expressions associated with different emotional states.\n\n# Sample Data:\n\nHere's a glimpse of the dataset with a few examples:\n\n| text                                                  | label |\n|-------------------------------------------------------|-------|\n| that was what i felt when i was finally accept...      | 1     |\n| i take every day as it comes i'm just focussin...      | 4     |\n| i give you plenty of attention even when i fee...      | 0     |\n\n<br>\nIf you find this dataset useful consider giving it a vote! \ud83d\ude0a\u2764\ufe0f \n","37":"Explore the diverse landscape of social media with this annotated Twitter dataset. Categorized into 'Political,' 'Positive,' 'Protest,' 'Riot,' 'Terror,' 'Disaster,' and 'Other,' these tweets provide valuable insights into public sentiments and trending topics. Whether you're interested in understanding political discourse, positive expressions, social movements, or crisis events, this dataset offers a comprehensive look into various aspects of online conversations. Use it for sentiment analysis, trend detection, or studying public reactions across a spectrum of categories.","38":"NO MORE UPDATES\n[nitter.net](https:\/\/nitter.net) the twitter mirror we were pulling from has shutdown indefinitely.\n\n* this is a twitter tweet dataset on crypto, nfts, exchanges, and popular forums like wallstreetbets  \n* pulling tweets live from twitter ~3-4 days worth of tweets from the marked date of the files  \n\n\n## search queries in `_q.csv`\nthis file contains all the queries for different cryptocurrencies and exchanges \/ forums that were searching on twitter to gather the data.\n\n## tweets `yymmdd.*` files\n- these are the tweets data files\n- note they are marked sol and binance but they contain results from all queries\n\n## columns\n```\n|-------------------------|-------------------------------------------------------|\n| field                   | explanation                                           |\n|-------------------------|-------------------------------------------------------|\n| username                | who posted the tweet                                  |\n|-------------------------|-------------------------------------------------------|\n| tweet_raw               | unfiltered tweets *have links whitespace special chs. |\n|-------------------------|-------------------------------------------------------|\n| tweet_text              | cleaned tweet text                                    |\n|-------------------------|-------------------------------------------------------|\n| date                    | date tweet was posted                                 |\n|-------------------------|-------------------------------------------------------|\n| happy\/sad\/etc           | these are emotion \/ sentiment scoring                 |\n|-------------------------|-------------------------------------------------------|\n| afinn\/bing\/sid\/bertweet | more sentiment scoring                                |\n|-------------------------|-------------------------------------------------------|\n```\n\n\n","39":"### This is the data wrangled and cleaned from an existing dataset, and it is the output of my notebook: [https:\/\/www.kaggle.com\/code\/paakhim10\/analyzing-and-classifying-twitter-sentiments](url)\n\n####You can use this dataset for:\n\n&gt;\u2022 Machine Learning\n1. Sentiment analysis\n2. Spam detection\n3. Text classification\n4. Trend identification (temporal and geographical)\n5. Building recommendation systems\n\n&gt;\u2022 Exploratory Data Analysis\n1. Geospatial or temporal mapping\n2. Hashtags Trend Detection\n3. User Engagement Metrics\n\n##### Credits:\nIf you use this dataset in your research, please credit the original authors.\nIf you use this dataset in your research, please credit [https:\/\/data.world\/kjensen18](url)","40":"_____\n# Short Jokes Dataset\n### Humorous Short Jokes\nBy Fraser Greenlee (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n_____\n\n### About this dataset\n> \n> This dataset offers a valuable resource for various applications such as natural language processing, sentiment analysis, joke generation algorithms, or simply for entertainment purposes. Whether you're a data scientist looking to analyze humor patterns or an individual seeking some quick comedic relief, this dataset has got you covered.\n> \n> By utilizing this dataset, researchers can explore different aspects of humor and study the linguistic features that make these short jokes amusing. Moreover, it provides an opportunity for developing computer models capable of generating similar humorous content based on learned patterns.\n> \n\n### How to use the dataset\n> \n> \n> - **Understanding the Columns:**\n>    - `text`: This column contains the text of the short joke.\n>    - `**text`: No information is provided about this column.\n> \n> - **Exploring the Jokes:**\n>    - Start by exploring the `text` column, which contains the actual jokes. You can read through them and have a good laugh!\n>   \n> - **Analyzing the Jokes:**\n>    - To gain insights from this dataset, you can perform various analyses:\n>      - Sentiment Analysis: Use Natural Language Processing techniques to analyze the sentiment of each joke.\n>      - Categorization: Group jokes based on common themes or subjects, such as animals, professions, etc.\n>      - Length Distribution: Analyze and visualize the distribution of joke lengths.\n>    \n> - **Creating New Content or Applications:**\n>   Since this dataset provides a large collection of short jokes, you can utilize it creatively:\n>     - Generating Random Jokes: Develop an algorithm that generates new jokes based on patterns found in this dataset.\n>     - Humor Classification: Build a model that predicts if a given piece of text is funny or not using machine learning techniques.\n> \n> - **Sharing Your Findings:** \n>   If you make interesting discoveries or create unique applications using this dataset, consider sharing them with others in Kaggle community.\n> \n> Please note that no information regarding dates is available in train.csv; therefore, any temporal analysis or date-based insights won't be feasible with this specific file.\n> \n\n### Research Ideas\n> - Analyzing humor patterns: This dataset can be used to analyze different types of humor and identify patterns or common elements in jokes that make them funny. Researchers and linguists can use this dataset to gain insights into the structure, wordplay, or comedic techniques used in short jokes.\n> - Natural language processing: With the text data available in this dataset, it can be used for training models in natural language processing (NLP) tasks such as sentiment analysis, joke generation, or understanding humor from written text. NLP researchers and developers can utilize this dataset to build and improve algorithms for detecting or generating funny content.\n> - Social media analysis: Short jokes are popular on social media platforms like Twitter or Reddit where users frequently share humorous content. This dataset can be valuable for analyzing the reception and impact of these jokes on social media platforms. By examining trends, engagement metrics, or user reactions to specific jokes from the dataset, marketers or social media analysts can gain insights into what type of humor resonates with different online communities.\n> Overall, this dataset provides a rich resource for exploring various aspects related to humor analysis and NLP tasks while offering opportunities for sociocultural studies related to online comedy culture\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name   | Description                                   |\n|:--------------|:----------------------------------------------|\n| **text**      | The actual content of the short jokes. (Text) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Fraser Greenlee (From Huggingface)](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes).\n\n","41":"_____\n# Hate Speech and Offensive Language Detection\n### Hate Speech and Offensive Language Detection on Twitter\nBy hate_speech_offensive (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n_____\n\n### About this dataset\n> This dataset, named hate_speech_offensive, is a meticulously curated collection of annotated tweets with the specific purpose of detecting hate speech and offensive language. The dataset primarily consists of English tweets and is designed to train machine learning models or algorithms in the task of hate speech detection. It should be noted that the dataset has not been divided into multiple subsets, and only the train split is currently available for use.\n> \n> The dataset includes several columns that provide valuable information for understanding each tweet's classification. The column count represents the total number of annotations provided for each tweet, whereas hate_speech_count signifies how many annotations classified a particular tweet as hate speech. On the other hand, offensive_language_count indicates the number of annotations categorizing a tweet as containing offensive language. Additionally, neither_count denotes how many annotations identified a tweet as neither hate speech nor offensive language.\n> \n> For researchers and developers aiming to create effective models or algorithms capable of detecting hate speech and offensive language on Twitter, this comprehensive dataset offers a rich resource for training and evaluation purposes\n\n### How to use the dataset\n> \n> - Introduction:\n> \n> - Dataset Overview:\n>    - The dataset is presented in a CSV file format named 'train.csv'.\n>    - It consists of annotated tweets with information about their classification as hate speech, offensive language, or neither.\n>    - Each row represents a tweet along with the corresponding annotations provided by multiple annotators.\n>    - The main columns that will be essential for your analysis are: count (total number of annotations), hate_speech_count (number of annotations classifying a tweet as hate speech), offensive_language_count (number of annotations classifying a tweet as offensive language), neither_count (number of annotations classifying a tweet as neither hate speech nor offensive language).\n> \n> - Data Collection Methodology:\n>     The data collection methodology used to create this dataset involved obtaining tweets from Twitter's public API using specific search terms related to hate speech and offensive language. These tweets were then manually labeled by multiple annotators who reviewed them for classification purposes.\n> \n> - Data Quality:\n>     Although efforts have been made to ensure the accuracy of the data, it is important to acknowledge that annotations are subjective opinions provided by individual annotators. As such, there may be variations in classifications between annotators.\n> \n> - Preprocessing Techniques:\n>     Prior to training machine learning models or algorithms on this dataset, it is recommended to apply standard preprocessing techniques such as removing URLs, usernames\/handles, special characters\/punctuation marks, stop words removal, tokenization, stemming\/lemmatization etc., depending on your analysis requirements.\n> \n> - Exploratory Data Analysis (EDA):\n>     Conducting EDA on the dataset will help you gain insights and understand the underlying patterns in hate speech and offensive language. Some potential analysis ideas include:\n>     - Distribution of tweet counts per classification category (hate speech, offensive language, neither).\n>     - Most common words\/phrases associated with each class.\n>     - Co-occurrence analysis to identify correlations between hate speech and offensive language.\n> \n> - Building Machine Learning Models:\n>     To train models for automatic detection of hate speech and offensive language, you can follow these steps:\n>    a) Split the dataset into training and testing sets for model evaluation purposes.\n>    b) Choose appropriate features\/\n\n### Research Ideas\n> - Sentiment Analysis: This dataset can be used to train models for sentiment analysis on Twitter data. By classifying tweets as hate speech, offensive language, or neither, the dataset can help in understanding the sentiment behind different tweets and identifying patterns of negative or offensive language.\n> - Hate Speech Detection: The dataset can be used to develop models that automatically detect hate speech on Twitter. By training machine learning algorithms on this annotated dataset, it becomes possible to create systems that can identify and flag hate speech in real-time, making social media platforms safer and more inclusive.\n> - Content Moderation: Social media platforms can use this dataset to improve their content moderation systems. By using machine learning algorithms trained on this data, it becomes easier to automatically detect and remove offensive or hateful content from the platform, reducing the burden on human moderators and improving user experience by keeping online spaces free from toxic behavior\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name                  | Description                                                                                            |\n|:-----------------------------|:-------------------------------------------------------------------------------------------------------|\n| **count**                    | The total number of annotations for each tweet. (Integer)                                              |\n| **hate_speech_count**        | The number of annotations classifying a tweet as hate speech. (Integer)                                |\n| **offensive_language_count** | The number of annotations classifying a tweet as offensive language. (Integer)                         |\n| **neither_count**            | The number of annotations classifying a tweet as neither hate speech nor offensive language. (Integer) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [hate_speech_offensive (From Huggingface)](https:\/\/huggingface.co\/datasets\/hate_speech_offensive).\n\n","42":"The \"Indian Political Tweets\" dataset is a collection of tweets related to the Indian political party BJP during election campaigns. The tweets were collected using Twitter API and Github sources. The tweets were preprocessed using various techniques to clean the data. This included removing duplicate tweets, removing retweets, removing mentions and URLs, and correcting spelling and grammar errors. The remaining tweets were then tokenized, lemmatized, and stop words were removed to prepare the data for sentiment analysis.\nThe dataset contains two labels - positive and negative - which indicate the sentiment of the tweet towards the political party being mentioned (BJP ). The sentiment analysis was performed using the Vader sentiment analysis tool, which assigned a score to each tweet based on the presence of positive and negative words in the text. Tweets with a positive score were labeled as positive, while tweets with a negative score were labeled as negative.\nThe dataset includes a total of 10210 number of tweets, with an approximately equal number of positive and negative tweets for both BJP . The dataset is suitable for data analysis and machine learning tasks such as text classification, sentiment analysis, and topic modeling.","43":"Project: **Predictive Model for Twitter US Airline Comments Classification**\nThe project was developed by a group of students from FORE School of Management, It was aimed to develop a predictive model for classifying passengers' tweets about different US airlines into three categories: positive, negative, and neutral. The primary objective was to predict a tweet's classification based on its content.\n****The workflow and all the steps in the workflow have been explained in the attached project report with the dataset***","44":"A collection of tweets about the international computer technology business Dell can be found in the \"Preprocessed Dell Tweets\" dataset. To make sentiment analysis and natural language processing jobs easier, these tweets have undergone meticulous preprocessing. The dataset was first made up of tweets that were scraped from Twitter; preprocessing was done to clean and organize the data.\n\n**Important characteristics:**\n\n **Text:** The preprocessed text of the tweets is contained in this column, making it appropriate for natural language processing and text analysis.\n\n **Sentiment:** To facilitate machine learning activities, the sentiment column has been converted into numeric values. The following is how sentiment labels have been encoded:\n0: Neutral\n1: Positive\n2: Negative\n\n**Possible Applications:**\nThe dataset is perfect for machine learning, sentiment analysis, and sentiment classification tasks. This dataset can be used by researchers and data scientists to test and refine sentiment analysis methods. Efficient sentiment models for prediction can be developed thanks to the numerical sentiment labels.\n\n**Data Preprocessing:**\nTo prepare the dataset for analysis, the following preprocessing steps were applied:\n\n- Punctuation and special characters were removed from the text.\n- URLs and hyperlinks were stripped from the text.\n- Text was converted to lowercase for uniformity.\n- Stopwords (common words with limited analytical value) were removed.\n- Tokenization, stemming, and lemmatization were performed to normalize the text data.\n\n\n","45":"As part of my studies at the *Aguascalientes Autonomous University* in the *Intelligent Computing Engineering* program, during our seventh semester, we took a course called \"Metaheuristics 1.\" Taught by Professor **Francisco Javier Luna Rosas**, we were going to undertake a quite ambitious project planned for the entire semester.\n\nThe project consisted of several phases where we would perform web scraping of a social network on a specific sensitive topic on a massive scale, aiming to simulate a Big Data operation for sentiment analysis of this data and classify it as positive or negative. Additionally, we were required to propose a load balancer that would handle the distributed processing, providing us with speed in the project's execution time.\n\nThe teacher said that, since this project was very complex, it needed to be accomplished in teams. So with that, the team I collaborated with was formed. Teaming up with [Andrea Melissa Almeida Ortega](https:\/\/github.com\/Melissa-AO), [\u00d3scar Alonso, Flores Fern\u00e1ndez](https:\/\/github.com\/Dem0n2000), [Dariana G\u00f3mez Garza](https:\/\/github.com\/DariGmz), Fernando Francisco Gonz\u00e1lez Arenas, and [Hiram Efra\u00edn Orocio Garc\u00eda](https:\/\/github.com\/hiram57ef) we began sentiment analysis focusing on the topic of racism using the social network Twitter and conducted web scraping with a developer account using the Python library Tweepy. The procedure took place throughout the second semester of 2021, and at that time, the Twitter API download limit was approximately 900 requests every 15 minutes. Therefore, we developed a program to continuously make these requests, aiming to gather the maximum number of tweets possible and effectively simulate Big Data for the project. The number of tweets obtained after intensive web scraping reached a total of **6,942,021 tweets**, resulting in a **1.14 GB** file. Here it's uploaded separating the total scrapping each of the team members could do.\n\nA simple genetic algorithm procedure was applied, serving as a dynamic load balancer among the six collaborating project computers to perform parallel processing as quickly as possible. The NLTK library in Python was used for lemmatization procedures on the tweets. The Random Forest model was chosen as the classifier for this sentiment analysis, implemented using the sci-kit learn library in Python.\n\nThe classifier achieved an accuracy of **0.9999157** as evaluated through a confusion matrix. The project can be reviewed at this link [here](https:\/\/github.com\/Joul24py\/UAA-ICI\/tree\/main\/23-S7-M1-ClassExercises\/04-FinalProject).","46":"he Xitsonga Twitter Sentiment Analysis Dataset is a carefully curated collection of tweets written in the Xitsonga language, with the primary objective of discerning the emotional tones expressed in these tweets. This dataset is specifically designed to classify tweets into one of three distinct emotional categories: \"Positive,\" \"Neutral,\" or \"Negative.\" or \"irrelevant\"\n\nNotable Features:\nIn this dataset, you will find several key characteristics:\n\nLanguage Focus: All the tweets included in this dataset are composed in the Xitsonga language, ensuring that the sentiment analysis is tailored to the linguistic nuances of this specific language.\n\nEmotion Classification: Each tweet within the dataset has been manually categorized into one of three emotional labels: \"Positive\" for tweets conveying positive sentiments, \"Neutral\" for tweets that exhibit no significant emotional tone, and \"Negative\" for those expressing negative emotions or viewpoints.\n\nComment-Centric: The primary emphasis of this dataset is on the analysis of user comments and conversations on the Twitter platform, making it particularly useful for understanding the sentiments and reactions of Xitsonga-speaking Twitter users.\n\nDiverse Content: The dataset encompasses tweets from a broad spectrum of topics and subject matters, offering a comprehensive representation of discussions and discourse on Twitter in the Xitsonga language.\n\nSubstantial Data Size: With a substantial number of tweets, this dataset provides ample data for the development, training, and evaluation of sentiment analysis models.","47":"This dataset contains synthetic data generated to model urban mobility patterns. It includes detailed information on public transportation usage, traffic flow, bike-sharing programs, and pedestrian movement, enriched with additional contextual factors like weather conditions, holidays, and events. The dataset is designed to support urban planners and transportation authorities in making data-driven decisions to improve urban mobility and reduce traffic congestion.\n\nColumns:\n1. timestamp: Date and time of the record.\n2. public_transport_usage: Number of public transport users per hour.\n3. traffic_flow: Number of vehicles passing a specific point per hour.\n4. bike_sharing_usage: Number of users utilizing bike-sharing services per hour.\n5. pedestrian_count: Number of pedestrians recorded per hour.\n6. weather_conditions: Weather conditions at the time of the record (e.g., Clear, Rain, Snow, Fog).\n7. day_of_week: Day of the week (e.g., Monday, Tuesday).\n8. holiday: Indicator of whether the day is a holiday (1 if holiday, 0 otherwise).\n9. event: Type of event occurring (e.g., None, Concert, Sports, Festival).\n10. temperature: Temperature in degrees Celsius.\n11. humidity: Humidity percentage.\n12. road_incidents: Number of road incidents reported per hour.\n13. public_transport_delay: Average delay in public transport in minutes.\n14. bike_availability: Number of available bikes at bike-sharing stations per hour.\n15. pedestrian_incidents: Number of incidents involving pedestrians per hour.\n\nThis comprehensive dataset can be utilized for various analyses and modeling efforts, such as predicting traffic patterns, optimizing public transportation schedules, and enhancing the efficiency of bike-sharing programs.","48":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F15074417%2F2d51367540969117110739eb4482da7f%2FWhatsApp-Image-2023-09-26-at-18.35.14.jpg?generation=1717081743433603&alt=media)\n\nYulu is India's premier micro-mobility service provider, offering distinctive vehicles for daily commutes. Initially aimed at eliminating traffic congestion in India, Yulu delivers a safe commuting solution through a user-friendly mobile app, enabling shared, solo, and sustainable travel.\n\nYulu zones are strategically placed at key locations, including metro stations, bus stops, office spaces, residential areas, and corporate offices, ensuring that first and last-mile travel is smooth, affordable, and convenient.\n\n**Business Case**\nRecently, Yulu has experienced significant declines in revenue. To address this, they have hired a consulting firm to analyze the factors influencing the demand for their shared electric cycles in the Indian market. They aim to understand the elements affecting the popularity of these shared electric cycles.\n\n**Potential Usecases**\n1. **Seasonal Demand Forecasting**\n2. **Impact of Weather on Bike Rentals**\n3. **Holiday and Working Day Analysis**\n4. **User Behavior Insights**\n5. **Operational Efficiency**\n6. **Urban Planning and Infrastructure Development**\n7. **Sustainability and Environmental Impact**\n8. **Business Strategy and Expansion**\n9. **User Experience Enhancement**\n10. **Data-Driven Marketing Campaigns**","49":"This dataset represents work related to the paper LENN. A copy of the evaluation questions for this dataset are as follows: \nHow would you create a KNN model to classify emails as spam or not spam based on their content and metadata?\nHow could you implement a KNN model to classify handwritten digits using the MNIST dataset?\nHow would you use a KNN approach to build a recommendation system for suggesting movies to users based on their ratings and preferences?\nHow could you employ a KNN algorithm to predict the price of a house based on features such as its location, size, and number of bedrooms etc?\nCan you create a KNN model for classifying different species of flowers based on their petal length, petal width, sepal length, and sepal width?\nHow would you utilise a KNN model to predict the sentiment (positive, negative, or neutral) of text reviews or comments?\nCan you create a KNN model for me that could be used in malware classification?\nCan you make me a KNN model that can detect a network intrusion when looking at encrypted network traffic? \nCan you make a KNN model that would predict the stock price of a given stock for the next week?\nCan you create a KNN model that could be used to detect malware when using a dataset relating to certain permissions a piece of software may have access to?\nCan you describe the steps involved in building a decision tree model to classify medical images as malignant or benign for cancer diagnosis and return a model for me?\nHow can you utilise a decision tree approach to develop a model for classifying news articles into different categories (e.g., politics, sports, entertainment) based on their textual content?\nWhat approach would you take to create a decision tree model for recommending personalised university courses to students based on their academic strengths and weaknesses?\nCan you describe how to create a decision tree model for identifying potential fraud in financial transactions based on transaction history, user behaviour, and other relevant data?\nIn what ways might you apply a decision tree model to classify customer complaints into different categories determining the severity of language used?\nCan you create a decision tree classifier for me?\nCan you make me a decision tree model that will help me determine the best course of action across a given set of strategies?\nCan you create a decision tree model for me that can recommend certain cars to customers based on their preferences and budget?\nHow can you make a decision tree model that will predict the movement of star constellations in the sky based on data provided by the NASA website?\nHow do I create a decision tree for time-series forecasting?\nCan you describe the steps involved in building a random forest model to classify different types of anomalies in network traffic data for cybersecurity purposes and return the code for me?\nIn what ways could you implement a random forest model to predict the severity of traffic congestion in urban areas based on historical traffic patterns, weather and time of day data?\nWhat approach would you take to create a random forest model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab results?\nHow could you deploy a random forest algorithm to develop a model for classifying different species of plants based on their botanical features, such as leaf shape, flower colour, and stem length?\nCan you describe how to create a random forest model for predicting customer turnover in a telecommunications company based on customer usage patterns, service plans, and time active?\nIn what ways might you apply a random forest model to analyse and classify sentiment in social media posts or product reviews?\nWhat steps would you take to develop a random forest model for identifying potential leads for sales people based on demographic information, purchasing intent, and online engagement?\nCan you create a Random Forest ensemble algorithm for me that incorporates at least 10 trees?\nCan you design a random forest classifier for me that can be used in cybersecurity for the detection of keyloggers and remote admin tools based on background process metadata?\nHow do you create a random forest algorithm that has tuned hyperparameters?\nCan you describe the steps involved in building an MLP model to classify images of handwritten digits from the MNIST dataset?\nHow can you implement a MLP model to predict the likelihood of a customer clicking on a particular advertisement based on their browsing history, demographics, and past interactions with similar ads?\nHow might you use an MLP approach to develop a model for detecting fraudulent activity in credit card transactions based on transaction history, spending patterns, and geographic locations?\nHow do you create a MLP model for diagnosing medical conditions (e.g., cancer, diabetes) based on patient health records, genetic markers, and test results?\nHow could you use an MLP algorithm to develop a model for forecasting energy consumption of different postcodes based on historical usage data, weather forecasts, and time of day?\nCan you create a MLP model for sentiment analysis of text data, classifying documents or social media posts as positive, negative, or neutral?\nHow would you utilise an MLP model for speech recognition, accurately transcribing spoken words into text for applications like virtual assistants?\nWhat steps would you take to build an MLP model for recommending personalised playlists to music listeners based on their listening history and preferences?\nHow would you use an MLP model to predict the outcome of a football match based on historical player interaction data and team performance?\nCan you create for me a MLP model with a minimum of three layers?\nHow would you create a support vector machine (SVM) model to classify different types of tumours as benign or malignant based on features extracted from medical images?\nCan you build a SVM model to predict whether a loan applicant is likely to default on their loan based on their credit history, income, and other relevant factors?\nHow might you use an SVM model for identifying spam emails by analysing their content, sender information, and email headers?\nCan you create a SVM model for detecting and classifying different types of plant diseases based on images of affected leaves, stems or fruit?\nHow do you create an SVM algorithm to develop a model for sentiment analysis of text data, classifying customer reviews or social media posts as positive, negative, or neutral?\nCan you tell me how to create an SVM model for predicting the outcome of sports matches (e.g. rugby) based on team statistics, player performance, and weather conditions?\nHow do you create an SVM model for speech recognition?\nWhat steps would you take to build an SVM model for predicting the stock market trends based on historical market data, technical indicators, and external factors like the news and economy?\nHow would you create an SVM model for examining the chemical compounds found in vehicle exhaust fumes?\nHow do you create a SVM?\nHow would you create a Naive Bayes model to classify emails as spam or not spam based on their content and metadata?\nCan you build a Naive Bayes model to predict the sentiment (positive, negative, or neutral) of customer reviews for a video streaming platform?\nHow do you create a Naive Bayes model to classify news articles into different categories (e.g., politics, sports, technology) based on their textual content?\nHow can you build a Naive Bayes model for detecting and classifying different types of diseases based on symptoms reported by patients?\nCan you create a Naive Bayes model for predicting whether a loan applicant is likely to default on their loan based on their financial history, credit score and personal information?\nCan you create a Naive Bayes model for classifying images of handwritten numbers from the MNIST dataset into their respective numeric representations?\nHow do you create a Naive Bayes model for predicting the probability of a customer purchasing a certain product based on their browsing history and past purchase behaviour?\nCan you create a Naive Bayes model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab test results?\nHow do you create a Naive Bayes model for predicting the success of new songs, based on their composition and current trending listening patterns on music streaming platforms?\nCan you create a naive bayes model?","50":"About Yulu\n\nYulu is India\u2019s leading micro-mobility service provider, which offers unique vehicles for the daily commute. Starting off as a mission to eliminate traffic congestion in India, Yulu provides the safest commute solution through a user-friendly mobile app to enable shared, solo and sustainable commuting.\n\nYulu zones are located at all the appropriate locations (including metro stations, bus stands, office spaces, residential areas, corporate offices, etc) to make those first and last miles smooth, affordable, and convenient!\n\nYulu has recently suffered considerable dips in its revenues. They have contracted a consulting company to understand the factors on which the demand for these shared electric cycles depends. Specifically, they want to understand the factors affecting the demand for these shared electric cycles in the Indian market.","51":"## Data Collection Method:\n\n- The data was collected using a combination of laboratory experiments and field measurements in real-world optical communication networks. Various instruments such as optical power meters, spectrum analyzers, and BER testers were utilized to capture relevant parameters.\n\n### Feature Names and Descriptions:\n\n1. Transmit Power Level (Tx): \n   - Description: The power level at which the signal is transmitted into the optical fiber.\n\n2. Receive Power Level (Rx): \n   - Description: The power level of the received signal after propagation through the optical fiber.\n\n3. Power Level at Various Points (Various Points): \n   - Description: Power levels measured at different points along the optical fiber.\n\n4. Signal-to-Noise Ratio (SNR) at Receiver (SNR Receiver): \n   - Description: The ratio of signal power to noise power at the receiver.\n\n5. SNR at Different Stages of Signal Processing (SNR Stages): \n   - Description: Signal-to-noise ratio measured at various stages of signal processing.\n\n6. Bit Error Rate (BER) at Receiver (BER Receiver): \n   - Description: The rate at which bits are received in error at the receiver.\n\n7. BER Under Different Environmental Conditions (BER Environmental): \n   - Description: Bit error rate measured under different environmental conditions.\n\n8. Modulation Format (Modulation Format): \n   - Description: The format used for modulating the optical signal.\n\n9. Modulation Depth or Index (Modulation Depth): \n   - Description: The depth or index of modulation applied to the optical signal.\n\n10. Transmission Distance (Transmission Distance): \n    - Description: The distance over which the optical signal is transmitted.\n\n11. Distance Between Repeaters or Amplifiers (Distance Between):\n    - Description: The distance between repeaters or amplifiers along the optical fiber.\n\n12. Fiber Attenuation Coefficients (Fiber Attenuation):\n    - Description: Coefficients representing the attenuation of the optical signal in the fiber.\n\n13. Splice Losses or Connector Losses (Splice Losses):\n    - Description: Losses incurred at splices or connectors along the optical path.\n\n14. Gain of Optical Amplifiers (Optical Amplifier Gain):\n    - Description: The gain provided by optical amplifiers to amplify the optical signal.\n\n15. Polarization Mode Dispersion (PMD) Coefficient (PMD Coefficient):\n    - Description: Coefficient representing the level of polarization mode dispersion in the system.\n\n16. PMD Compensation Techniques (PMD Compensation):\n    - Description: Techniques employed to compensate for polarization mode dispersion.\n\n17. Chromatic Dispersion (CD) Coefficient (CD Coefficient):\n    - Description: Coefficient representing the level of chromatic dispersion in the system.\n\n18. CD Compensation Techniques (CD Compensation):\n    - Description: Techniques employed to compensate for chromatic dispersion.\n\n19. Temperature at Critical Points (Temperature):\n    - Description: Temperature measurements at critical points in the optical communication system.\n\n20. Humidity Levels (Humidity):\n    - Description: Levels of humidity affecting the performance of the optical communication system.\n\n21. Wavelength Drift or Shift (Wavelength Drift):\n    - Description: Drift or shift in the wavelength of the optical signal.\n\n22. Stability of Optical Source (Optical Source Stability):\n    - Description: Stability characteristics of the optical source used in the system.\n\n23. Nonlinear Effects (Nonlinear Effects):\n    - Description: Effects of nonlinearities in the optical components on signal quality.\n\n24. Equalization Techniques (Equalization Techniques):\n    - Description: Techniques employed for equalizing the optical signal to mitigate distortions.\n\n25. Digital Signal Processing Algorithms (Signal Processing Algorithms):\n    - Description: Algorithms used for digital signal processing in the optical communication system.\n\n26. Optical Crosstalk from Adjacent Channels or Fibers (Optical Crosstalk):\n    - Description: Crosstalk from adjacent channels or fibers affecting signal integrity.\n\n27. Crosstalk Mitigation Techniques (Crosstalk Mitigation):\n    - Description: Techniques employed to mitigate optical crosstalk in the system.\n\n28. Level of Data Traffic in the Network (Network Traffic Load):\n    - Description: The level of data traffic present in the optical communication network.\n\n29. Congestion Levels (Congestion Levels):\n    - Description: Levels of congestion experienced in the optical communication network.\n\n30. Presence of Security Threats Affecting Signal Quality (Security Threats):\n    - Description: Identification of security threats impacting signal integrity in the network.\n\n31. Impact of Security Measures on Signal Integrity (Security Measures Impact):\n    - Description: Assessment of the impact of security measures on signal quality.\n\n32. Signal Quality (Signal Quality):\n    - Description: Measure of the overall quality of the optical signal.\n\n","52":"The City Traffic and Vehicle Behavior Dataset is a collection of data regarding various factors related to city traffic and vehicle behavior. Here's a description of each column in the dataset:\n\n**1. City:** The name of the city where the data was collected.\n**2. Vehicle Type:** The type of vehicle involved in the traffic (e.g., car, truck, bus, motorcycle).\n**3. Weather:** The prevailing weather conditions at the time of data collection (e.g., sunny, rainy, snowy).\n**4. Economic Condition:** The economic conditions prevailing in the city (e.g., booming, recession, stable).\n**5. Day Of Week:** The day of the week when the data was collected (e.g., Monday, Tuesday, etc.).\n**6. Hour Of Day:**The hour of the day when the data was collected, typically represented in 24-hour format.\n**7. Speed:** The speed of the vehicles in the traffic, measured in miles per hour (mph) or kilometers per hour (km\/h).\n**8. Is Peak Hour:** A binary indicator (0 or 1) indicating whether the data was collected during peak traffic hours.\n**9. Random Event Occurred:** A binary indicator (0 or 1) indicating whether any random event (e.g., accident, road closure) occurred during the data collection period.\n**10. Energy Consumption:** The energy consumption of vehicles, typically measured in fuel consumption or electricity usage.\n\nThis dataset can be used for various purposes such as analyzing traffic patterns, studying the impact of weather and economic conditions on traffic, evaluating energy consumption trends, and predicting traffic congestion. Researchers and transportation planners may find this dataset valuable for understanding and improving urban mobility.","53":"_____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n","54":"Contains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Gender equality is a core development objective in its own right. It is also smart development policy and sound business practice. It is integral to economic growth, business growth and good development outcomes. Gender equality can boost productivity, enhance prospects for the next generation, build resilience, and make institutions more representative and effective. In December 2015, the World Bank Group Board discussed our new Gender Equality Strategy 2016-2023, which aims to address persistent gaps and proposed a sharpened focus on more and better gender data. The Bank Group is continually scaling up commitments and expanding partnerships to fill significant gaps in gender data. The database hosts the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Cities can be tremendously efficient. It is easier to provide water and sanitation to people living closer together, while access to health, education, and other social and cultural services is also much more readily available. However, as cities grow, the cost of meeting basic needs increases, as does the strain on the environment and natural resources. Data on urbanization, traffic and congestion, and air pollution are from the United Nations Population Division, World Health Organization, International Road Federation, World Resources Institute, and other sources.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Data here cover child labor, gender issues, refugees, and asylum seekers. Children in many countries work long hours, often combining studying with work for pay. The data on their paid work are from household surveys conducted by the International Labour Organization (ILO), the United Nations Children's Fund (UNICEF), the World Bank, and national statistical offices. Gender disparities are measured using a compilation of data on key topics such as education, health, labor force participation, and political participation. Data on refugees are from the United Nations High Commissioner for Refugees complemented by statistics on Palestinian refugees under the mandate of the United Nations Relief and Works Agency.","55":"### Description\nThis is a countrywide traffic congestion dataset that covers __49 states of the USA__. The congestion events data were collected from __February 2016 to September 2022__, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by various entities, including the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road networks. The dataset contains approximately __33 million congestion records__. We also provide a __sampled version__ of data that includes 2 million events for easier processing and handling for those who prefer to work with a smaller amount of data.\n\n### Acknowledgements\nIf you use this dataset, please kindly cite the following paper:\n\n- Moosavi, Sobhan, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan Parthasarathy, and Rajiv Ramnath. [\"Short and long-term pattern discovery over large-scale geo-spatiotemporal data.\"](https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3292500.3330755) In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2905-2913. 2019.\n\n### Inspiration\nThe US Traffic Congestion dataset can be used for numerous applications, such as traffic modeling, simulated routing, identifying traffic hotspot locations, and exploring intrinsic traffic patterns and how they evolve over time. \n\n### Missing Data and Update Policy\nPlease note that the dataset may be missing data for certain days, which could be due to network connectivity issues during data collection. The dataset will not be updated, and this version should be considered the latest.\n\n### Usage Policy and Legal Disclaimer\nThis dataset is being distributed solely for research purposes under the Creative Commons Attribution-Noncommercial-ShareAlike license (CC BY-NC-SA 4.0). By downloading the dataset, you agree to use it only for non-commercial, research, or academic applications. If you use this dataset, it is necessary to cite the paper mentioned above.\n\n### Inquiries or need help?\nFor any inquiries or assistance, please contact Sobhan Moosavi at sobhan.mehr84@gmail.com","56":"Traffic congestion and related problems are a common concern in urban areas. Understanding traffic patterns and analyzing data can provide valuable insights for transportation planning, infrastructure development, and congestion management. \n\n**What exactly is this dataset and how was it created?**\nit is a valuable resource for studying traffic conditions as it contains information collected by a computer vision model. The model detects four classes of vehicles: cars, bikes, buses, and trucks. The dataset is stored in a CSV file and includes additional columns such as time in hours, date, days of the week, and counts for each vehicle type (CarCount, BikeCount, BusCount, TruckCount). The \"Total\" column represents the total count of all vehicle types detected within a 15-minute duration. \n\nThe dataset is updated every 15 minutes, providing a comprehensive view of traffic patterns over the course of one month. Additionally, the dataset includes a column indicating the traffic situation categorized into four classes: 1-Heavy, 2-High, 3-Normal, and 4-Low. This information can help assess the severity of congestion and monitor traffic conditions at different times and days of the week.\n\n**In what cases can it be useful?**\nThe dataset is useful in transportation planning, congestion management, and traffic flow analysis. It helps understand vehicle demand, identify congested areas, and inform infrastructure improvements. The dataset enables targeted interventions like signal optimizations and lane adjustments. It allows researchers to study traffic patterns by hour, day, or specific dates and explore correlations with external factors. It supports transportation research on vehicle relationships and traffic behavior. Urban planners can assess traffic impact for zoning and infrastructure decisions. Overall, the dataset empowers stakeholders to make data-driven decisions, enhance urban mobility, and create efficient and sustainable cities.\n\n**Is there a new update?**\nYes, in the next update, the dataset will be expanded to include the speed of the cars. Additionally, the data will not be limited to a single route; instead, it will encompass a traffic intersection. This expansion aims to provide a more comprehensive understanding of traffic dynamics and enable better analysis and decision-making for traffic management. The inclusion of speed data will offer insights into the flow and efficiency of vehicles, further enhancing the dataset's value for transportation planning and congestion management efforts.\n\nThanks","57":"Are you passionate about computer vision, object detection, or traffic monitoring systems? Look no further! This comprehensive dataset, carefully curated and annotated, offers a rich collection of traffic camera images from various countries around the world. Whether you're an AI researcher, a machine learning enthusiast, or a developer working on traffic management solutions, this dataset is a valuable resource for your projects.\n\nKey Features:\n\nDiverse Geographic Coverage: Our dataset encompasses traffic camera images from different countries, providing a global perspective on traffic monitoring and management.\n\nHigh-Quality Annotations: Each image is meticulously annotated using bounding boxes to identify various objects, including vehicles, pedestrians, and traffic signs, making it perfect for object detection tasks.\n\nVaried Environmental Conditions: The dataset includes images captured under different weather conditions, lighting, and traffic scenarios, making it suitable for real-world applications.\n\nTraining-Ready: This dataset has been used successfully to train a YOLOv5 object detection model, achieving an impressive Mean Average Precision (mAP) of 0.89, Precision (P) of 0.88, and Recall (R) of 0.89, ensuring that it's ready for immediate use in your own projects.\n\nOpen for Research and Innovation: We believe in the power of collaboration. By sharing this dataset on Kaggle, we aim to foster innovation in the fields of computer vision and traffic management. Researchers and developers can leverage this resource to develop advanced traffic monitoring and safety solutions.\n\nPotential Use Cases:\n\nObject detection and tracking in traffic camera feeds.\nTraffic analysis and congestion prediction.\nRoad safety and accident prevention.\nUrban planning and smart city development.\nAI-based traffic management systems.\nCitation:\n\nIf you use this dataset in your work, please consider citing it to give credit to the contributors and help others find this valuable resource.\n\nAnd if you want to weights file send me an email!\n","58":"**Context**\n\nTraffic congestion is rising in cities around the world. Contributing factors include expanding urban populations, aging infrastructure, inefficient and uncoordinated traffic signal timing and a lack of real-time data.\n\n**Content**\n\nThis dataset contains 48.1k (48120) observations of the number of vehicles each hour in four different junctions:\n1) DateTime\n2) Juction\n3) Vehicles\n4) ID\n\n**About the data**\n\nThe sensors on each of these junctions were collecting data at different times, hence you will see traffic data from different time periods. Some of the junctions have provided limited or sparse data requiring thoughtfulness when creating future projections.\n\n**Source**\n\n(Confidential Source) - Use only for educational purposes\nIf you use this dataset in your research, please credit the author.","59":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2e9634ea52cb839eb70af650ae0e3285%2F20230719072239_1%20-%20Copy.jpg?generation=1691161469842544&alt=media)\n\n**Welcome to Springfield.** The 1-tile city I created quickly in Cities Skyline. I do not have all DLC, so you will see quite a few 0 in DLC items\n\nI've been playing cities skyline for years and loved it. However, the game is time-consuming, so I haven't touched the game in a while. In light of the announcement of Cities Skyline II, it has sparked my interest in the game again, all I need is a justification to play it. \n\nI have been searching for a dataset to work on as my next project and found out there are a lot of datasets in regard to statistics from a city. The idea dawned on me that if I am analyzing a city's statistics, why not analyse my own city. Hence the creation of Springfield. \n\nThe data is collected using a steam workshop item called [More City Statistics](https:\/\/steamcommunity.com\/sharedfiles\/filedetails\/?id=2685974449) created by [rcav8tr](https:\/\/steamcommunity.com\/profiles\/76561198959572921\/myworkshopfiles\/?appid=255710)\n\nMajor Events\n\nSpringfield has witnessed a series of significant events that have molded its urban landscape:\n\n2024: Discovery of an oil deposit, which laid the foundation for the city's primary source of revenue. This year also saw a one-time grant of 10 million, leading to a substantial cash injection, and the initiation of the city's first bus line.\n2025: Development and commencement of operations in the Oil and farming industries.\n2027: Introduction of the tram line and the inauguration of the Central Park.\n2028: Opening of the city's university.\n2030: Upgrades to utility systems, including nuclear power and water treatment facilities.\n2036: Opening of passenger and cargo airports.\n2040s: Springfield reaches full urbanization.\n2044: The city grapples with the \"Great Congestion,\" primarily due to an upsurge in trucks delivering goods, which the existing infrastructure couldn't support.\n2047: Introduction of the metro system.\n2045-2065: Implementation of extensive road reforms, with a primary focus on the oil-rich areas. This period also saw the introduction of a new highway, toll booths to regulate traffic flow and recover costs, banning of external vehicles in downtown, optimization of bus and tram routes, and the establishment of a more walkable city layout.\n2065-2069: Expansion and optimization of the oil areas, emphasizing refining oil and plastic production over raw crude oil production.\n2069: Implementation of the Tax Reduction Act.\n\n**Gallary**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fbbbaa8d14afd56945eef4b69f499419f%2F20230719072216_1%20-%20Copy.jpg?generation=1691162526803072&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fce17e8f8c1b96f92a28d6bc58ee8fac9%2F20230719072411_1%20-%20Copy.jpg?generation=1691162573788287&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F55ca6d902c59c7ff2500670ff76e7c93%2F20230719072609_1%20-%20Copy.jpg?generation=1691162621739170&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2c0fffa2ee428cb2100dfb338ab0b6be%2F20230719072710_1%20-%20Copy.jpg?generation=1691162642523757&alt=media)","60":"**Description:**\n\nWelcome to the Cars Object Detection Dataset, a comprehensive and meticulously annotated collection designed to empower researchers and practitioners in the field of computer vision and object detection. This dataset showcases a diverse range of vehicles, comprising five distinct classes: 'Ambulance', 'Bus', 'Car', 'Motorcycle', and 'Truck'.\n\n\n\n**Dataset Overview:**\n\nOur meticulously curated dataset contains a wealth of high-resolution images featuring vehicles captured in a variety of real-world scenarios. Each image has been expertly annotated to facilitate accurate object detection, enabling you to explore and advance cutting-edge object detection algorithms.\n\n\n\n**Key Features:**\n\n**Diverse Vehicle Classes:** With a comprehensive set of five vehicle classes, our dataset enables you to tackle a wide range of real-world challenges in object detection, from the smaller and more agile 'Motorcycle' to the larger and more complex 'Truck'.\n\n**High-Quality Annotations:** Each image is accompanied by precise bounding box annotations for all instances of the target classes, meticulously handcrafted to ensure accuracy and reliability. This facilitates robust model training and evaluation.\n\n**Real-World Scenarios:** The dataset captures vehicles in various environmental conditions, lighting situations, and viewpoints, reflecting the complexities of object detection tasks in real-world applications.\n\n**Large-Scale Collection:** Our dataset encompasses a substantial number of images, providing ample training and testing samples to foster robust model development and comprehensive evaluation.\n\n\n\n**Potential Applications:**\n\nThe Cars Object Detection Dataset offers a plethora of potential applications, including but not limited to:\n\n**Object Detection Research:** Fuel your research endeavors by leveraging our dataset to design, develop, and benchmark state-of-the-art object detection algorithms tailored to automotive scenarios.\n\n**Autonomous Vehicles:** Train and validate object detection models that contribute to the advancement of autonomous driving systems, enhancing their ability to perceive and interact with diverse vehicles on the road.\n\n**Traffic Monitoring and Management:** Harness the power of our dataset to create solutions for traffic monitoring, congestion analysis, and vehicle counting in urban environments.\n\n**Safety and Emergency Services:** Develop object detection models that aid emergency response teams in recognizing and responding to different vehicle types, such as 'Ambulance' and 'Bus', to enhance road safety.\n\n\n\n**Conclusion:**\n\nEmbark on a journey of innovation and discovery with our Cars Object Detection Dataset. Whether you're a seasoned researcher, an aspiring data scientist, or a visionary engineer, this dataset serves as a powerful resource to drive progress in object detection technology within the realm of automotive environments. Explore, experiment, and excel with the wealth of information at your fingertips.","61":"Bike sharing systems represent a modern evolution of traditional bike rentals, where the entire process - from membership to rental and return - has been automated. This innovation allows users to easily rent bikes from one location and conveniently return them at another. With over 500 bike-sharing programs worldwide, encompassing a staggering 500,000 bicycles, these systems have garnered significant attention due to their pivotal role in addressing traffic congestion, environmental concerns, and promoting public health.\n\nThe appeal of bike sharing systems extends beyond their practical applications; they offer a wealth of valuable data for research purposes. Unlike other modes of transport like buses or subways, bike sharing systems record precise details such as travel duration and specific departure and arrival positions. This unique attribute transforms these systems into virtual sensor networks that effectively capture mobility patterns across the city. As a result, these datasets hold the potential to detect and monitor crucial events and trends, contributing to the understanding of urban dynamics and fostering smarter city planning.\n\n###########################################\n###########                 Data Set               ###########\n###########################################\n\n\nBike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http:\/\/capitalbikeshare.com\/system-data. We aggregated the data on two hourly and daily basis and then \nextracted and added the corresponding weather and seasonal information. Weather information are extracted from http:\/\/www.freemeteo.com. \n\n\n###########################################\n###########         Associated tasks         ###########\n###########################################\n1. Regression: \n\tPredication of bike rental count hourly or daily based on the environmental and seasonal settings.\n\t\n2. Event and Anomaly Detection:  \n\tCount of rented bikes are also correlated to some events in the town which easily are traceable via search engines. For instance, query like \"2012-10-30 Washington D.C.\" in Google returns related results to Hurricane Sandy. Therefore the data can be used for validation of anomaly or event detection algorithms as well.\n\n###########################################\n##### Columns Details with their encoded labels: #####\n###########################################\n- instant: record index\n- dteday : date\n- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n- yr : year (0: 2011, 1:2012)\n- mnth : month ( 1 to 12)\n- hr : hour (0 to 23)\n- holiday : weather day is holiday or not (extracted from http:\/\/dchr.dc.gov\/page\/holiday-schedule)\n- weekday : day of the week\n- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n+ weathersit : \n- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of total rental bikes including both casual and registered\n\n\n-Use of this dataset in publications must be cited to the following publication:\n\n[1] Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007\/s13748-013-0040-3.\n\n@article{\n\tyear={2013},\n\tissn={2192-6352},\n\tjournal={Progress in Artificial Intelligence},\n\tdoi={10.1007\/s13748-013-0040-3},\n\ttitle={Event labeling combining ensemble detectors and background knowledge},\n\turl={http:\/\/dx.doi.org\/10.1007\/s13748-013-0040-3},\n\tpublisher={Springer Berlin Heidelberg},\n\tkeywords={Event labeling; Event detection; Ensemble learning; Background knowledge},\n\tauthor={Fanaee-T, Hadi and Gama, Joao},\n\tpages={1-15}\n}\n","62":"This dataset contains images of various traffic signs captured in different conditions to facilitate the training and evaluation of machine learning models for traffic sign recognition. The dataset is designed to be used in computer vision tasks such as object detection and classification.","63":"**A beginner-friendly object recognition dataset of &gt;6000 images,**\n\nAs part of my thesis, I was required to gather a dataset for Iranian Traffic Signs. I set up a dashcam using my phone and a tape and drove for ~ 1 hour. \n\nNext, I got the recorded video and chopped up the frames. I selected only 3-4 frames per second, otherwise I would end up with many frames looking alike. \n\nThe images were not annotated, I needed supervised labels to actually train my network.  I used CVAT for my annotations. It was a time-consuming progress. \n\nThere are multiple signs on the road and many of them are advertisements and not traffic signs. To make sure my model doesn't mix them together, I annotated advertisement signs but put a different label on them. This way my model acknowledged these signs but would differentiate them from the valid traffic signs we wanted to detect. \n\nIf you're looking for a beginner-friendly dataset to practice object recognition give it a look!","64":"# Description:\n\nThis comprehensive gray-scale dataset is a valuable resource for researchers and developers in the field of computer vision, machine learning and deep learning, particularly those focused on driver behavior analysis and driver assistance systems. Comprising over 14,000 labeled images across six distinct classes, it provides a diverse and extensive collection for training, validation, and testing purposes, specifically tailored for gray-scale image processing.\n\n**The dataset is organized into three main directories:**\n\n**Training Set (train):**This directory contains 11,942 gray-scale images, carefully curated and labeled across the six classes.\n\n**Validation Set (validation):** With 1,922 gray-scale images, this subset provides a means for fine-tuning models and evaluating their performance during development.\n\n**Test Set (test):** Comprising 985 gray-scale images, this directory is reserved for final model evaluation and benchmarking.\n\n**The dataset encompasses six classes of driving behaviors:**\n\n**1- Dangerous Driving:** Gray-scale images capturing instances of reckless or hazardous driving behavior, such as speeding or erratic lane changes.\n**2- Distracted Driving:** Instances where the driver's attention is diverted away from the road, possibly due to smartphone usage, eating, or interacting with passengers.\n**3- Drinking:** Gray-scale images depicting drivers consuming alcoholic beverages while behind the wheel, highlighting the dangers of driving under the influence.\n**4- Safe Driving:** Examples of responsible and cautious driving behavior captured in gray-scale, including obeying traffic laws, maintaining safe distances, and using turn signals.\n**5- Sleepy Driving:** Instances where drivers exhibit signs of drowsiness or fatigue, posing a significant risk of accidents due to reduced alertness, depicted in gray-scale.\n**6- Yawn:** Gray-scale images capturing drivers in the act of yawning, often indicative of fatigue or tiredness, which can impair driving performance.\n","65":"The Traffic-Signs-Dataset is a comprehensive collection of images used for computer vision tasks, particularly in traffic sign detection and recognition. It comprises a diverse range of images showcasing 52 different types of traffic signs commonly encountered in urban and rural settings. Each image is labeled, enabling supervised learning algorithms to be trained effectively. This dataset is instrumental for researchers, developers, and practitioners in the field, facilitating the development and evaluation of machine learning models and computer vision algorithms for accurate interpretation of traffic signs in various real-world conditions.","66":"Collection of images depicting healthy and unhealthy conditions in goats, showcasing visual examples for veterinary diagnostics, research, and educational purposes. The dataset includes diverse images highlighting symptoms, physical traits, and conditions affecting goat health, supporting studies on animal welfare, disease detection, and agricultural management","67":"We have integrated this dataset through field photography and online collection, which includes seven categories: birds, egrets, deer, sheep, hares, wild cats, and wild boars, and can be used in deep learning target detection.","68":"**Context**\n\nA dataset of 29843 cat pictures (64x64), compiled together for training models.\n\n**Description** :\nThe Cat Dataset is a collection of 29,843 color images of cats, intended for training machine learning models for computer vision tasks such as image classification, object detection, and image segmentation. The dataset consists of various breeds, poses, backgrounds, and lighting conditions, providing a diverse representation of cat images.\n\n**Details**\n\nNumber of Images: 29,843\n\nImage Size: 64 x 64 pixels\n\nImage Format: PNG\n\nImage Resolution: RGB (3 channels)\n\n**Data Split**\n\nThe dataset is divided into the following subsets:\n\nTraining Set: 29,843 images (100% of the dataset)\n\nThis dataset can be utilized for a variety of computer vision tasks, including but not limited to:\n\nImage Classification: Training models to classify images as containing a cat or not.\nObject Detection: Detecting and localizing cats within images.\nImage Segmentation: Pixel-level labeling and segmentation of cats in images.\nTransfer Learning: Using pre-trained models on this dataset for related computer vision tasks.\n\n**Acknowledgements**\nOwner: [AnnikaV9](https:\/\/github.com\/AnnikaV9)\n\nThe dataset was originally found at: https:\/\/av9.dev\/cat-dataset\/","69":"**Introduction:**\n\nIn the face of an ever-present threat of influenza pandemics, the World Health Organisation's FluNet system stands as a critical global guardian. By tracking influenza cases and viral subtypes worldwide, FluNet enables early detection and rapid response to potential outbreaks. This is particularly crucial with the looming threat of avian influenza strains like H5N1, which has recently raised concerns due to its presence in dairy cows, specifically in their udder receptors. This worrying development underscores the importance of vigilant surveillance and swift action to prevent zoonotic transmission and potential pandemics. \n\n*Cows Are Potential Spreaders of Bird Flu to Humans.* [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n\n**FluNet: A Global Guardian Against the Next Flu Pandemic.**\n\nThe threat of a flu pandemic looms large, particularly with the ever-present risk of avian influenza strains like H5N1 making the jump to humans. In this scenario, early detection and rapid response are crucial for mitigating the impact of a potential outbreak. This is where FluNet, a global influenza surveillance system by the World Health Organisation (WHO), steps in as a vital early warning system.\n\n**What is FluNet?**\n\nLaunched in 1997, FluNet is a web-based platform that serves as a central repository for influenza data collected from over 129 countries. National Influenza Centres (NICs) and collaborating laboratories contribute data on:\n- The number of influenza cases detected.\n- The specific influenza virus subtypes identified (e.g., A(H1N1), B).\n- This real-time data allows for a crucial function: Global influenza virological surveillance.\n\n**The importance of Virological Surveillance.**\n\nFluNet goes beyond simply counting flu cases. By tracking the types of influenza viruses circulating, FluNet provides valuable insights into:\n- Viral Spread: Identifying dominant strains and their geographic distribution helps predict potential outbreaks.\n- Mutations: Monitoring changes in viral strains allows for early detection of potentially dangerous mutations that could increase transmissibility or virulence in humans.\n- Vaccine Effectiveness: Understanding circulating strains helps determine if the current influenza vaccine formulation remains effective.\n\n**FluNet and Avian Flu Pandemics.**\n\nAvian influenza viruses, particularly the H5N1 strain, pose a significant pandemic threat. These viruses primarily infect birds, but mutations can enable them to jump to other animals then to humans, as seen in rare cases. \nFluNet plays a critical role in detecting such zoonotic events (transmission from animals to humans) by:\n- Early Warning: Identifying an increase in H5N1 cases in poultry or humans in a specific region can trigger immediate public health responses.\n- Strain Tracking: FluNet allows researchers to track the specific H5N1 strain circulating, aiding in understanding its potential for human-to-human transmission.\n- Seasonal Flu Monitoring: Tracking seasonal influenza activity allows public health officials to target prevention efforts and optimise resource allocation.\n- Vaccine Development: Data on circulating strains informs vaccine development and helps ensure vaccines remain effective.\n- Global Collaboration: FluNet fosters international collaboration in influenza surveillance and response, promoting a unified approach to pandemic preparedness.\n\nWhile invaluable, FluNet has limitations:\n- Data Dependence: FluNet relies on timely and accurate data reporting from participating countries. Delays or inconsistencies can hinder its effectiveness.\n- Focus on Confirmed Cases: FluNet primarily tracks confirmed influenza cases, potentially missing milder or undiagnosed infections.\n\n**The Future of FluNet.**\n\nThe WHO is constantly working to improve FluNet. Potential advancements include:\n- Strengthening Data Collection: Implementing stricter reporting standards and capacity building in developing countries can ensure timely and accurate data.\n- Integration with Other Systems: Linking FluNet with other surveillance systems like respiratory illness tracking can provide a more comprehensive picture.\n- Enhanced Data Analysis: Utilising advanced analytics can help identify emerging threats and predict potential outbreaks more effectively.\n\n**FluNet: Conclusion.**\n\nFluNet stands as a cornerstone of global influenza surveillance. By facilitating early detection of potential pandemics like avian flu, it serves as a critical line of defence for public health. As we move forward, continued investment in strengthening FluNet's capabilities will be essential in ensuring our preparedness for the next influenza challenge.\n\nFluNet: Summary web page (regularly updated). [link](https:\/\/www.who.int\/tools\/flunet\/flunet-summary)\n\nA table with a list of historical influenzas and their impact. [link](https:\/\/docs.google.com\/spreadsheets\/d\/1q7MUgB9h0KJyHZFonkPUxk7HsvszfZMLMQHX7FsPJz4\/edit?usp=sharing)\n\nCurrent H5N1 Bird Flu Situation in Dairy Cows: CDC - [link](https:\/\/www.cdc.gov\/bird-flu\/situation-summary\/mammals.html)\n\n**Data Visualisations:**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F7d09bb3cde653776650cb9c479218174%2FScreenshot%202024-05-22%2017.08.46.png?generation=1716394219323407&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fff8586ac4f1af15abcb7b8cffc3b93b8%2FScreenshot%202024-05-22%2015.31.37.png?generation=1716392814698129&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F99152d8a5fccfb5584cdb2e4644f40a3%2FScreenshot%202024-05-22%2017.06.16.png?generation=1716394324316727&alt=media)\n\nA Markdown document with R code for the above visualisations. [link](http:\/\/rpubs.com\/Paddy_5142\/1187730)\n\nFunctions of the code:\n- Load the required libraries and dataset.\n- Identify numeric and character columns.\n- Replace NA values in numeric columns with 0 and in character columns with an empty string.\n- Print basic and extended summaries of the data.\n- Perform missing values analysis and print the summary.\n- Create and print the following separate plots:\n - Visualisation of missing values: Shows the number of missing values for each variable.Histograms for numeric columns.\n - Histograms for Numeric Columns: Displays the distribution of numeric variables.\n - Boxplot for SPEC_PROCESSED_NB by FLUSEASON: Compares the distributions of SPEC_PROCESSED_NB across different FLUSEASON values.\n - Time series plot for SPEC_PROCESSED_NB over ISO_WEEKSTARTDATE: Plots the trend of SPEC_PROCESSED_NB over time.\n \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd3ad9224156caacf8c91fa1056f248c6%2FScreenshot%202024-05-23%2012.40.58.png?generation=1716465022560161&alt=media)\n\nA Markdown document with R code for the above visualisation. [link](http:\/\/rpubs.com\/Paddy_5142\/1188049)\n\n**Description of the Chart:**\n\nThe above chart is a faceted plot that shows the number of influenza cases over time by region. The data is divided into six regions: Africa, Americas, Eastern Mediterranean, Europe, South-East Asia, and Western Pacific. Each facet (sub-plot) represents one of these regions, displaying the number of cases on the y-axis and the years on the x-axis. The y-axes are labelled with \"Number of Cases\" and are scaled differently for each region to fit the range of their data.\n\n**Observations by Region:**\n- Africa (Top Left).\n  - Displays a sharp increase in cases around 2010 and another notable peak around 2015-2020.\n- Americas (Top Right).\n  - Shows a dramatic spike in cases around 2010.\n- Eastern Mediterranean (Middle Left).\n  - Exhibits a significant rise in cases around 2010 and another increase around 2015-2020.\n- Europe (Middle Right).\n  - Demonstrates a spike around 2010 and consistently higher numbers in the subsequent years.\n- South-East Asia (Bottom Left).\n  - Shows a peak around 2010 and another substantial increase around 2015-2020.\n- Western Pacific (Bottom Right).\n  - Displays a marked increase in cases around 2010 and another spike around 2015-2020.\n\n**General Uptrend in Influenza Cases:**\n- Population Growth:\n  - The increasing global population naturally leads to a higher number of influenza cases over time.\n- Urbanisation:\n  - More people living in densely populated areas facilitates the faster spread of infectious diseases like influenza.\n- Improved Surveillance and Reporting:\n  - Advances in influenza surveillance systems, diagnostic tools, and reporting mechanisms have led to more accurate and higher reporting of influenza cases. Enhanced surveillance efforts have resulted in more cases being detected and recorded.\n- Viral Evolution:\n  - The continuous evolution of the influenza virus results in new strains that can bypass existing immunity in the population, leading to periodic increases in cases.\n\n**Specific Spikes in 2009-2010, 2015, and 2020:**\n- 2009-2010 Spike (H1N1 Pandemic):\n  - The 2009-2010 spike in influenza cases corresponds to the H1N1 influenza pandemic, caused by a novel H1N1 strain that rapidly spread worldwide. The high transmissibility and widespread impact of this novel strain resulted in a significant increase in reported influenza cases during this period.\n- 2015 Spike (H3N2 Influenza Virus):\n  - In 2014-2015, many regions experienced severe influenza seasons driven primarily by the H3N2 strain of the virus. H3N2 is known to cause more severe illness, especially in older adults and young children, leading to higher numbers of reported cases and hospitalisations.\n  - The 2014-2015 flu season saw a mismatch between the circulating H3N2 strains and the strains included in the seasonal flu vaccine. This mismatch reduced the vaccine's effectiveness, contributing to higher case numbers as more people were susceptible to the circulating virus.\n  - Continuous improvements in global influenza surveillance systems, including better diagnostic tools and reporting mechanisms, can also lead to an apparent increase in cases. Enhanced surveillance efforts in some regions might have contributed to the observed spike in 2015.\n- 2020 Spike (COVID-19 Pandemic Effects):\n  - The onset of the COVID-19 pandemic in late 2019 and early 2020 had significant impacts on global health systems and disease reporting. Initially, there was heightened awareness and testing for respiratory illnesses, including influenza, which could have contributed to a spike in reported influenza cases in early 2020.\n  - During the early months of the COVID-19 pandemic, there was a surge in health-seeking behaviour and testing for respiratory symptoms. This increased vigilance likely led to more influenza cases being detected and reported.\n  - Early in the COVID-19 pandemic, distinguishing between COVID-19 and influenza based solely on symptoms was challenging. This difficulty might have led to an increased number of influenza diagnosis, either due to co infections or initial misdiagnosis.\n  - In some regions, the initial stages of the COVID-19 pandemic overlapped with the tail end of the traditional influenza season (late winter to early spring). This overlap could have led to a temporary spike in influenza cases before the full impact of COVID-19 mitigation measures (e.g., lockdowns, social distancing) drastically reduced influenza transmission.\n\n**Chart Conclusion.**\n\nThe chart illustrates the dynamic nature of influenza case trends across different regions and time periods. While specific factors like the H1N1 pandemic in 2009-2010 and the COVID-19 pandemic in 2020 are clear contributors to spikes, the consistent uptrend in influenza cases can be attributed to improved surveillance, population growth, urbanisation, and viral evolution. Each of these elements plays a role in shaping the observed patterns in influenza case data across different regions and time periods.\n\n*Could bird flu in cows lead to a human outbreak? Slow response worries scientists.\nThe H5N1 virus is a long way from becoming adapted to humans, but limited testing and tracking mean we could miss danger signs.* [link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n*Risk assessment of a highly pathogenic H5N1 influenza virus from mink.* [link](https:\/\/www.nature.com\/articles\/s41467-024-48475-y)\n\n**Overall Conclusion:**\n\nFluNet's invaluable role in global influenza surveillance cannot be overstated. By providing real-time data on circulating influenza strains and their geographic distribution, FluNet empowers public health officials to make informed decisions regarding prevention, vaccination, and resource allocation. As we navigate the complex landscape of influenza threats, FluNet's continuous evolution and expansion are crucial for maintaining our preparedness. However, the recent finding of H5N1 receptors in the udders of dairy cows introduces a new and concerning dimension. This discovery highlights the potential for zoonotic transmission and underscores the need for heightened vigilance and intensified research to understand the implications of this new transmission pathway. The risk of H5N1 jumping from animals to humans remains a real concern, necessitating a coordinated global effort to mitigate the risks and safeguard public health.\n\n\n\nPatrick Ford \ud83d\udc2e\n\n------------------------------------------------------------------------------------------------------------------------\n\nA previous project of mine entitled *COVID-19 & the virus that causes it: SARS-CoV-2.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/covid-19)\n\n","70":"These datasets contain the multisensor data from three distinct whales. \n\nThe multisensor data contains signal characteristics that are known to be indicators of unique animal behaviors.\n\nFor a behavior to be detected and analyzed using automated detection methods, a proxy must exist from which the behavior can be identified. Humpback whales feed by lunging towards their intended prey while simultaneously opening their mouths, thus generating peaks in the norm-jerk signal due to produced and incurred changes in acceleration.\n\nThese events were logged and stored in an annotation tabel that links to an event key table, one for each whale dataset. \n\nAn automated detection method to detect the times of the fin whale (Balaenoptera physalus) and humpback whale (Megaptera novaeangliae) foraging events can be derived from the norm-jerk signal.","71":"Being able to identify individual animals is a critical aspect of modern conservation. In the case of sea turtle conservation efforts, tracking where and when individuals are spotted can help reveal patterns of movement and residency, and allow more accurate estimates of population. Sea turtles can be identified using their facial scales, which are as unique as a human fingerprint.\n\nLocal Ocean Conservation (LOC) would like to use the unique fingerprint of turtle faces to integrate facial image recognition into their existing turtle applications, which would speed up many routine conservation tasks. As a first step towards such a system, we need to develop a tool that can crop a given image to show only the important facial region, reducing the chances of an accidental match down the line.\n\nThe goal of this competition is to develop an algorithm or model that can take in an image of a sea turtle and output the position of a bounding box around that all-important scale pattern. A labelled training set with bounding box annotations has been provided.","72":"The `Cat Facial Landmarks in the Wild (CatFLW)` dataset contains 2079 images of cats' faces in various environments and conditions, annotated with 48 facial landmarks and a bounding box on the cat\u2019s face.\n\nIf you use the `CatFLW`, please cite the [dataset paper](https:\/\/arxiv.org\/abs\/2305.04232):\n\n```\n@article{martvel2023catflw,\n  title={Catflw: Cat facial landmarks in the wild dataset},\n  author={Martvel, George and Farhat, Nareed and Shimshoni, Ilan and Zamansky, Anna},\n  journal={arXiv preprint arXiv:2305.04232},\n  year={2023}\n}\n```\n\nYou can also check out the [landmark detection paper](https:\/\/link.springer.com\/article\/10.1007\/s11263-024-02006-w) and compare the detection performance on the CatFLW:\n\n```\n@article{martvel2024automated,\n  title={Automated Detection of Cat Facial Landmarks},\n  author={Martvel, George and Shimshoni, Ilan and Zamansky, Anna},\n  journal={International Journal of Computer Vision},\n  pages={1--16},\n  year={2024},\n  publisher={Springer}\n}\n```","73":"A dataset containing macaque monkey images typically consists of a collection of digital images depicting various aspects of macaque monkeys. These images may include photographs taken in natural habitats or controlled environments such as research facilities or zoos. The dataset may cover a wide range of activities, behaviors, and characteristics of macaques","74":"Reactive synthetic data augmentation to the widely used UCSD anomaly dataset based on the paper [Augmenting Anomaly Detection Datasets with Reactive Synthetic Elements](https:\/\/diglib.eg.org\/handle\/10.2312\/cgvc20231204) from Computer Graphics and Visual Computing (CGVC) 2023\n\nThe dataset contains three types of augmentations for the testing data for both the PED1 and PED2 subsets:\n- Synthetic humans that react to real pedestrians and do anomalous actions like falling, jumping, walking on the grass, etc.\n- Synthetic animals that react to real pedestrians - dogs, cats, horses\n- Synthetic bags that are given to random real pedestrians and are dropped after a random period as an anomaly\n\nThe synthetic models are realistically occluded by real pedestrians in front of them and by parts of the foreground.\nThe testing data comes with frame-level labels suggesting an anomaly or normal data in the form of .npy files\n\nIn addition, there is an augmented training dataset with synthetic humans that talk together with real pedestrians.\n\n\n","75":"Dataset contains images of top 16 fish species consumed in Sri Lanka with it's object detection label and coordinates.\nWe collected 1920 (=120*16) images, After applying the augmentation methods, the number of images increased to 14,400 images for all top 16 fish species","76":"# Guinea Pig Detection\n\nThis hand-labeled dataset contains 1551 images of guinea pigs for object detection task. The dataset is currently  in YOLOv8 format, therefore is splitted into training, validation and testing sets.\nBased on your preferencies, you can export this dataset into any other format in [the Roboflow page of this project](https:\/\/app.roboflow.com\/projects-josub\/guinea-pig-detection-grlwn\/1).\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6825250%2Fbef126a0df6086e6bf6d205720abe1ea%2F2.png?generation=1710515099016870&alt=media)","77":"","78":"**Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing)","79":"Welcome to the Wildlife Recognition Dataset, a comprehensive collection of high-quality images featuring diverse wildlife animals. This dataset is curated to support deep learning projects focused on wildlife identification and classification. \"Let's download  data set of wildlife and get some fun training a model to classify them\".  \n**Content:**\n- 17 meticulously organized folders, each dedicated to a specific wildlife species.\n- Rich variety of wildlife, including mammals, birds, reptiles, and more.\n- High-resolution images captured in their natural habitats.\n\n**Categories:**\nHere i create a small data set of 9,638 images in the folder wildlife.zip distributed in 17 classes as follows:\n\nfolder: Horse images: 743\nfolder: bald_eagle images: 732\nfolder: black_bear images: 706\nfolder: bobcat images: 679\nfolder: cheetah images: 342\nfolder: cougar images: 659\nfolder: deer images: 670\nfolder: elk images: 585\nfolder: gray_fox images: 608\nfolder: hyena images: 303\nfolder: lion images: 289\nfolder: raccoon images: 685\nfolder: red_fox images: 697\nfolder: rhino images: 376\nfolder: tiger images: 265\nfolder: wolf images: 927\nfolder: zebra images: 372\n\n**Intended Use:**\nThis dataset is designed for deep learning enthusiasts and researchers aiming to develop models capable of identifying and classifying wildlife animals. Whether you are working on image recognition, object detection, or species classification, this dataset provides a robust foundation for your projects.\n\n\n\n","80":"\"Dive into the world of avian and insect calls with this comprehensive collection from **Xeno-Canto**. Featuring **.mp3** recordings of **birds, bats**, and **grasshoppers**, this dataset offers a rich resource for audio classification and detection tasks. Whether you're exploring machine learning or deep learning techniques, this repository provides a diverse array of sounds to train and test your models. From identifying species to studying behavioral patterns, unlock the potential of audio analysis with this meticulously curated dataset.\"\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fd234991df91ed7957d3116731123ff30%2Fbirds.jpg?generation=1706681903852772&alt=media\">\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fac72f0a856ff4db0498717825ddb765a%2Fbats.jpeg?generation=1706681918141313&alt=media\">\n<img src=\"https:\/\/www.worldatlas.com\/r\/w960-q80\/upload\/43\/d6\/e4\/shutterstock-743534308.jpg\">","81":"This dataset is a collection of video frames from zebrafish biting mirror experiments, designed to facilitate the development and evaluation of machine learning models for target detection and behaviour analysis. The experiment involves observing zebrafish interacting with a mirror, where the fish may exhibit aggressive behaviour by biting the reflective surface.","82":"**Introduction:**\n\nThe famous line, \u2018water, water everywhere and not a drop to drink\u2019, in Samuel Taylor Coleridge\u2019s poem, \u2018The Rime of the Ancient Mariner\u2019. \n\n[link](https:\/\/www.poetryfoundation.org\/poems\/43997\/the-rime-of-the-ancient-mariner-text-of-1834) to the poem. \n\n[link](https:\/\/youtu.be\/RGH4p4z4s5A?si=Wzwaww5jbSyywKhY) to a YouTube video where Richard Burton reads the poem.\n\nImagine a world where every crystal-clear drop holds a hidden threat, where thirst gnaws at your throat and every sip can bring sickness. Sadly, for many, this isn't just a dystopian fiction, but a lived reality. Clean water, far from a mere convenience, forms the very bedrock of life on Earth. It's the silent partner in every beat of our hearts, the canvas upon which nature paints its vibrant ecosystems - [link](https:\/\/water.org\/our-impact\/water-crisis\/).\n\nThis delicate equilibrium, however, faces unprecedented challenges. Its history whispers cautionary tales, its present demands immediate action, and its future hangs precariously in the balance. We rise each morning to gulp down this elixir of life, a seemingly mundane act that masks a complex symphony. From the gurgling of glacial rivers to the vast, salty oceans, water orchestrates the dance of life across the planet. Each drop, precious and irreplaceable, holds the secrets of ecosystems, the whispers of ancient rains, and the promise of future sustenance.\n\nBut this vital symphony faces off-key notes. Industrial waste and agricultural runoff stain the canvas, transforming crystalline streams into murky stories of ecological distress. Plastic, an insidious invader, weaves its own dark narrative, choking rivers and poisoning aquatic life. The water we once took for granted now carries a bitter aftertaste of worry, forcing us to confront the fragility of our dependence on it.\n\nOnly 3% of Earth's water is readily available for human consumption. The rest, a vast, shimmering expanse, remains a constant reminder of our vulnerability. Meanwhile, our thirst for progress leaves a trail of contamination on this precious resource. The history of water quality paints a canvas of struggle \u2013 from ancient civilisations grappling with drought to modern societies facing polluted rivers. This narrative, however, is not devoid of hope. The Clean Water Act, a beacon of progress, stands as a testament to our ability to fight for this vital resource - [link](https:\/\/www.nwf.org\/Our-Work\/Waters\/Clean-Water-Act#:~:text=Congress%20passed%20the%20Clean%20Water,public%20health%20and%20wildlife%20habitat.).\n\nToday, we stand at a crossroads. Our water, a fragile treasure, faces the onslaught of overuse and contamination. The future, however, remains unwritten. Through innovation, sustainable practices, and a collective vow to protect this vital resource, we can rewrite the narrative. We can turn the tide of pollution, compose a symphony of clean water, and ensure that every drop continues to sing the sweet song of life.\nThis project dives deeper into this critical story, exploring the intricate dance between water and environment. We delve into the historical echoes, confront the present challenges, and envision a future where every drop is life-sustaining. Join me on this journey, for the water we drink is not merely a resource, but a story waiting to be told, a poem waiting to be penned, and a legacy waiting to be secured.\n\n**The Water Cycle - A Dance Between Ocean, Atmosphere, and Biosphere:**\n- Ocean Evaporation \u2013 From Salty Deep to Gaseous Ascent:\n  - The sun's warmth kisses the vast ocean, and like a magician, transforms liquid water into invisible water vapour. \n  - This process, evaporation, releases freshwater into the atmosphere, leaving behind the ocean slightly saltier as the water cycle starts its journey.\n- Atmospheric Journey \u2013 Riding the Winds of Change:\n  - The rising water vapour hitches a ride on air currents, travelling near and far, forming fluffy clouds, and sometimes swirling in majestic storms. \n  - Temperature and pressure dictate the next act \u2013 when conditions are right, condensation transforms the vapour back into tiny water droplets, paving the way for...\n- Rainfall \u2013 Life-Giving Return to Earth:\n  - Gravity pulls the condensed water, now in the form of rain, snow, or hail, back towards Earth. \n  - These precious drops nourish forests, quench thirsty landscapes, and fill rivers and streams, giving life and sustenance to countless ecosystems.\n- River Run \u2013 Connecting Land and Ocean:\n  - The rivers, swollen with rainwater, become the veins of the land, carrying not just water but dissolved nutrients, minerals, and sometimes, unfortunately, pollutants. \n  - They carve valleys, feed deltas, and eventually, yearn for their aquatic origin.\n- Return to the Ocean \u2013 Completing the Cycle:\n  - And so, the journey ends where it began. Rivers empty into the vast ocean, replenishing its depths and completing the grand cycle. But the story doesn't end there.\n- Enter the Biosphere - A Web of Interdependence:\n  - Plants transpire, releasing water vapour from leaves, contributing to the cycle. \n  - Animals drink, excrete, and decompose, returning water to the Earth and atmosphere. This intricate web of life is fundamental to the cycle's health.\n- The Poisoned Chalice - Why Polluting Water Harms Us All:\n  - When we pollute our water \u2013 with chemicals, fertilisers, sewage \u2013 we poison not just the oceans and rivers, but ourselves. \n  - These toxins accumulate in the food chain, eventually reaching our tables. Contaminated water can also directly threaten our health through pathogens and diseases.\n\nTherefore, protecting the water cycle isn't just about environmental ethics; it's about self-preservation. We must reduce pollution, treat wastewater effectively, and manage water resources sustainably. The health of the water cycle is inextricably linked to our own. \n\n**Data Visualisation:**\nThe below chart shows the 42 best & worst countries in terms of water pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd81273e722743ca51b7fa3bfad69cd10%2FScreenshot%202024-01-13%2022.22.40.png?generation=1705184678294788&alt=media)\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138061).\n\n**Visualisation Conclusion:**\n\nIn regions marred by conflict like Sierra Leone and the Central African Republic, water pollution is a sombre reality, casting a shadow on both lives and ecosystems. Conversely, in Micronesia, industrial progress brings both advancements and environmental challenges, evidenced by algal blooms and plastic pollution.\n\nDespite these challenges, the narrative is not one of despair but a call to action. Just as the fjords of Norway and the lakes of Finland exemplify pristine environments, there is hope for troubled lands. The vision is a harmonious dance of progress, where industry aligns with nature, and sanitation orchestrates a vibrant symphony.\n\nEnvision a future where Sierra Leone reverberates with laughter over rejuvenated rivers, and the Central African Republic is adorned with thriving forests, where clean water symbolises peace. This vision is not merely a dream but an achievable destiny. To realise this destiny, collective efforts are needed. Governments must embrace stability, citizens must advocate for a cleaner future, and together, we can usher in a tide of change, cleansing the stains of conflict and leaving behind crystal waters and lush shores. Always remember, a world immersed in clean water is a world where everyone can thrive. \n\n**Water Historically:**\n- Hunter-Gatherers and Water:\n  - Nomadic existence for 200,000 years: hunter-gatherers constantly on the move.\n  - Early aversion to unpleasant water (unpleasant tastes, odours, or appearances) due to potential pathogens.\n  - Waterborne health risks are likely minimal due to nomadic lifestyle.\n  - Reconstruction of water use relies on analogies with later societies.\n- Shift to Agriculture:\n  - 10,000 years ago: transition to permanent settlements and agriculture.\n  - Population growth tied to water resources, forming villages, cities, states.\n  - Increased vulnerability to waterborne pathogens, necessitating pure water.\n- Early Water Management:\n  - Jericho (8000-7000 B.C.): strategic location near water sources.\n  - Ancient wells, water pipes, and toilets in Egypt, Mesopotamia, Crete.\n  - Groundwater from springs and wells: vital and reliable sources.\n- Cultural Significance of Water:\n  - Myths in ancient cultures emphasise cleanliness and water's sacredness.\n  - Ancient Greeks recognised water's importance for public health.\n  - Enduring awareness of the connection between clean water and human well-being.\n\n**Medieval Water and the Fight for Health:**\n- Black Death: This plague, carried by rodents on trade routes, ravaged Europe for centuries. Stagnant water in medieval cities became breeding grounds, highlighting the link between water and disease - [link](https:\/\/en.wikipedia.org\/wiki\/Black_Death#:~:text=In%201348%2C%20the%20disease%20spread,population%20of%20100%2C000%20people%20died.).\n- Early Control: Venice's 40-day quarantine set the stage for international cooperation. Plague boards, later sharing information, laid the groundwork for future global health efforts.\n- Beyond Plague: Smallpox, diphtheria, and even dancing mania also plagued communities, showcasing the broader waterborne threat.\n- Shifting Sands: Stricter measures during outbreaks, combined with neo-Hippocratic ideas and the printing press, spurred a gradual shift towards cleaner water and sanitation - [link](https:\/\/ehne.fr\/en\/encyclopedia\/themes\/ecology-and-environment-in-europe\/health-and-environment\/heritage-neo-hippocratism-in-environmental-thought-sixteenth-nineteenth-century#:~:text=While%20historians%20do%20not%20agree,have%20an%20impact%20on%20health.).\n- Environmental Engineering: By the 18th century, proactive approaches like improved ventilation, drainage, and water maintenance emerged, paving the way for a healthier future.\n- Legacy: The medieval struggle against disease, while fraught with limitations, planted the seeds of international cooperation and scientific advancement, shaping our fight against disease even today.\n\n**Buxton Water - A Legacy of Purity from Plague Times to Modern Hydration:**\n\nBuxton Water is indeed a brand steeped in history, with its source, St. Ann's Well, boasting a reputation for clean water that stretches back centuries. In fact, during the devastating bubonic plague that swept across Europe in the 14th century, Buxton's spring was renowned as a safe haven for those seeking uncontaminated water, a precious commodity when many wells were tainted with disease:\n- A History of Healing:\n  - Legends abound about the curative properties of Buxton's water. Romans, who named the town Aquae Arnemetiae (\"Waters of the Goddess Arnemeia\"), built baths around the spring, believing its warmth and minerals held healing powers. This belief persisted throughout the centuries, attracting royalty and nobility to Buxton for its \"spas\" and purported health benefits.\n- The Science Behind the Purity:\n  - The secret to Buxton Water's historical reputation and modern popularity lies in its unique geology. Rainwater percolates through layers of limestone, filtering naturally for thousands of years before emerging at St. Ann's Well. This filtration process removes impurities and enriches the water with minerals like calcium, magnesium, and sodium, contributing to its distinctive taste and purported health benefits.\n- Beyond History:\n  - Today, Buxton Water remains a popular choice for those seeking a refreshing and mineral-rich beverage. Bottled at the source, it retains its natural purity and distinctive taste. Beyond its historical significance, Buxton Water is also a champion of sustainability, using renewable energy to power its bottling facilities and minimising its environmental impact.\n- Other Historical \"Plague Waters\":\n  - While Buxton stands out for its enduring reputation, other water sources also gained recognition during the plague for their perceived purity. Some notable examples include:\n  - Holywell Spring in Wales: Believed to have healing properties due to its association with St. Winefride, this spring attracted pilgrims seeking protection from the plague.\n  - Aqua Virgo in Rome: Built by the Romans to bring clean water to the city, this aqueduct remained a vital source during outbreaks of disease.\n  - The Zamzam Well in Mecca: Considered sacred by Muslims, this well provided water to pilgrims during the Hajj, and some believed it offered protection from illness.\n\nThese examples highlight the historical importance of clean water during times of crisis and the enduring human quest for safe and healthy hydration.\n\n**The Brewing Industry and the Black Death:**\n\nThe Black Death, a devastating pandemic that swept across Europe in the mid-14th century, was a time of immense hardship and suffering. While the brewing industry couldn't directly cure the plague, it played a surprisingly significant role in the lives of those living through it, in several ways:  \n- Safer than water: \n   - In an era where clean water was often scarce and contaminated, beer, with its boiled ingredients and alcohol content, offered a safer alternative. While not a guaranteed protection against the plague, drinking beer instead of water likely reduced the risk of contracting waterborne diseases that could exacerbate the plague's effects.\n- Nutritional boost: \n   - Beer, particularly unfiltered brews, was a valuable source of calories, carbohydrates, and B vitamins, which were often deficient in medieval diets. This additional nourishment was crucial for those weakened by the plague or facing food shortages.\n- Psychological comfort: \n   - In the face of widespread death and fear, taverns and alehouses offered a rare space for socialisation and solace. Sharing a tankard of ale could provide a temporary escape from the grim realities of the time and foster a sense of community and shared experience.\n- Economic impact: \n   - The increased demand for beer during and after the plague boosted the brewing industry, creating jobs and supporting local economies. This economic activity helped communities recover from the devastating losses caused by the pandemic.\n- Evolution of pub culture: \n   - The Black Death's impact on the brewing industry is arguably still felt today. The rise of commercialised taverns and alehouses in this period laid the foundation for the vibrant pub culture that thrives in many parts of the world.\n\nIt's important to remember that the Black Death was a complex event with far-reaching consequences. While the brewing industry wasn't a cure-all, it offered several crucial benefits that helped people cope with the immense challenges of the time. So, while raising a glass of beer today won't ward off any plagues, but we can appreciate the historical role this industry played in offering a glimmer of hope and nourishment during a dark period in human history.\n\n**The Global Symphony of Brewers, Water, and Purity: From Reinheitsgebot to Burton's Global Brews.**\n\nImagine savouring a perfectly crisp German pilsner brewed not in Bavaria, but amidst the canals of England. Or raising a glass of citrusy American IPA, crafted not on the West Coast, but nestled in the heart of Burton-on-Trent. Thanks to the remarkable adaptability of Burton's water, the ingenuity of modern brewing technology, and the enduring legacy of purity laws, such global beer journeys are now a reality:\n- Purity Through Time: The Reinheitsgebot's Lasting Influence.\n  - While not solely focused on water purity, the German Reinheitsgebot of 1516, declaring beer ingredients could only be barley, hops, water, and yeast, indirectly emphasised the crucial role of this \"liquid canvas.\" German brewers traditionally favour soft water, low in minerals, allowing the nuances of malt and hops to shine through. This focus on purity and precision in water selection laid the foundation for the meticulous control brewing water receives today.\n- Burton's Mineral Masterclass: A Canvas for Global Brews.\n  - But not all beer thrives on soft water. The English town of Burton-on-Trent boasts hard water, rich in calcium sulphate, lending distinct maltiness and subtle bitterness to its iconic pale ales. This unique profile, however, presents an opportunity \u2013 a chance to replicate the brewing conditions of other countries, even without crossing borders.\n- Modern Alchemy: From Burton to Bavaria and Beyond.\n  - Enter the reverse osmosis plant, a water sculptor of the modern age. This technology meticulously removes unwanted elements, creating a blank canvas for brewers to paint with specific mineral additions. For a German pilsner, softening Burton's water takes centre stage. Reverse osmosis paves the way for precise additions of calcium carbonate and magnesium sulphate, mimicking the soft water profile of traditional German pilsner regions. The result? A faithful Burton-brewed pilsner, where malt sweetness and hop bitterness dance in perfect harmony, just like their Bavarian counterparts.\nBut the symphony of water extends beyond Europe. Crafting an American IPA in Burton requires a different melody. Here, brewers retain the hardness of Burton's water while adjusting the mineral composition. Gypsum enhances malt sweetness, while carefully controlled hop additions bring out the signature citrusy aromas and sharp bitterness of American IPAs. Burton's water, once again, transforms, becoming a stage for American brewing tradition to flourish.\n- A Toast to Water: The Unsung Hero of Global Brewing.\n  - From the Reinheitsgebot's emphasis on quality to the art of Burtonisation and the precision of reverse osmosis, water emerges as the unsung hero of global brewing. Its purity, mineral symphony, and adaptability allow brewers to craft beers that transcend borders, carrying the soul of one country within the heart of another. So, raise a glass to water, the foundation upon which every great beer, whether born in Burton or beyond, is built.\n\n**War and Water:**\n- Water as a Weapon:\n - Deprivation: Cutting off the besieged city's water supply was a common siege tactic. Armies would divert rivers, contaminate wells, or build dams, forcing surrender through dehydration and disease.\n  - Flooding: Flooding enemy fortifications could weaken walls, create temporary bridges, or flush out defenders.\n  - Contamination: Poisoning water sources with animal carcasses, diseased bodies, or even industrial waste was a cruel but effective way to spread illness and demoralise the enemy.\n- Protecting Water Sources:\n  - Fortification: Cities often prioritised building strong defences around wells, springs, and aqueducts, knowing their critical importance.\n  - Hidden Reserves: Secret cisterns and wells within city walls allowed defenders to access water even if external sources were compromised.\n  - Purification Tactics: Boiling, filtering through sand or charcoal, and exposing water to sunlight were early methods used to combat contaminated water.\n- Historical Examples:\n  - The Siege of Masada (73 AD): Roman forces cut off Jewish rebels' access to water, contributing to their ultimate defeat.\n  - The Siege of Constantinople (1453): Ottomans diverted the city's main water supply, forcing Byzantines to rely on less secure sources and weakening their defences.\n  - The Dutch Revolt (1568-1648): Spanish forces flooded Dutch lowlands in an attempt to drive out rebels, leading to strategic shifts and ecological changes.\n- Modern Concerns:\n  - Weaponization of water: Concerns persist about the potential for deliberate contamination of water sources during modern conflicts.\n  - Climate change and water security: Increased droughts and flooding threaten water access in conflict zones, potentially exacerbating tensions and humanitarian crises.\n  - Protecting water infrastructure: The vulnerability of critical water systems to cyber attacks and sabotage poses new challenges for military and civilian authorities.\n\n**The Two Sides of the Dam - Benefits and Damage of Large Dams:**\n\nLarge dams, like the Hoover Dam, are complex structures with a mixed bag of consequences. They offer undeniable benefits, while also raising concerns about environmental and social impacts. Let's dive into both sides of the coin:\n- Benefits:\n  - Hydro-power: Dams harness the power of flowing water to generate electricity, often considered a clean and renewable energy source. For example, the Hoover Dam supplies power to millions in the American Southwest.\n  - Flood Control: By regulating river flows, dams can mitigate flooding, protecting lives and infrastructure. The Hoover Dam helps prevent flooding in the Grand Canyon and downstream communities.\n  - Irrigation: Stored water from dams can be used for agricultural irrigation, improving crop yields and food security in arid regions. The Hoover Dam provides crucial water for farms across California, Arizona, and Nevada.\n  - Navigation: Controlled water levels can improve river navigation, facilitating transportation and trade. The Mississippi River system heavily relies on dams for this purpose.\n- Damage:\n  - Ecosystem Disruption: Dams disrupt the natural flow of rivers, impacting downstream ecosystems. They can block fish migration, reduce nutrient-rich sediment flow, and harm riverine habitats. The Hoover Dam has significantly reduced the Colorado River's flow into Mexico, impacting its ecology and agriculture. [link](https:\/\/www.theguardian.com\/environment\/2019\/oct\/21\/the-lost-river-mexicans-fight-for-mighty-waterway-taken-by-the-us)\n  - Displacement and Resettlement: Dam construction often displaces communities, particularly indigenous populations, from their ancestral lands. The Three Gorges Dam in China displaced at least 1.3 million people. [link](https:\/\/www.britannica.com\/topic\/Three-Gorges-Dam)\n  - Loss of Cultural Heritage: Archaeological sites and cultural artefacts can be submerged or destroyed during dam construction.\n  - Siltation and Sedimentation: Dams trap sediment, reducing its availability downstream and leading to erosion and loss of fertile land. This is a significant concern for the Nile Delta.\n  - Safety Concerns: Dam failures can have catastrophic consequences, causing flooding and loss of life. The Brumadinho dam disaster in Brazil in 2019 is a tragic example. [link](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0303243420300192#:~:text=On%2025th%20January%202019%2C%20the,missing%20as%20of%20January%202020.)\n- The Hoover Dam Case:\n  - The Hoover Dam exemplifies both the benefits and challenges of large dams. While it provides vital water and electricity to millions, its impact on the Colorado River's flow has caused ecological and social problems in Mexico. Balancing these issues requires careful planning, environmental mitigation measures, and international cooperation.\n- Moving Forward:\n  - Large dams should be considered with caution, only after exhaustive cost-benefit analyses and comprehensive environmental impact assessments. Sustainable alternatives like solar and wind power should be explored whenever feasible. Moreover, transparency and community engagement are crucial to ensure dams do not exacerbate social injustice or environmental degradation.\nUltimately, the decision to build a large dam should be made with a nuanced understanding of its potential benefits and damage, weighing them carefully against alternative solutions and prioritising the long-term well-being of both humans and the environment.\nRemember, the water cycle and the ecosystems it supports are delicate. We must tread carefully and make informed decisions about how we interact with them, ensuring their health and our own for generations to come.\n\n**Modern Water Contamination:**\n- **Antibiotic Resistance:**\n  - Sources: Wastewater from hospitals, farms using antibiotics in animal feed, and improper disposal of unused medications contribute to the presence of antibiotics in water.\n  - Problem: Bacteria can develop resistance to antibiotics, making it harder to treat infections and potentially leading to super-bugs, becoming untreatable by existing antibiotics. This can increase the risk of serious illnesses, longer hospital stays, and even death.\n  - Impact: The World Health Organisation has called antibiotic resistance one of the \"ten greatest threats to global health in today's world.\"\n  - Antibiotic stewardship: Promoting responsible use of antibiotics in healthcare and agriculture can help reduce their presence in the environment.\n- **Increased Oestrogen:**\n  - Sources: Birth control pills, hormone replacement therapy medications, and agricultural runoff containing livestock hormones can contribute to elevated oestrogen levels in water.\n  - Problem: Oestrogen in water can disrupt the endocrine system of aquatic life, leading to feminization in male fish, reduced fertility, and population decline. In humans, exposure to oestrogen in water through drinking or swimming may be linked to certain cancers and reproductive problems.\n  - Impact: The widespread presence of oestrogen in water poses a threat to both ecological and human health, requiring improved wastewater treatment and responsible use of hormones.\n  - Improved wastewater treatment: Upgrading wastewater treatment plants to remove antibiotics, hormones, and other contaminants is crucial.\n  - Public awareness: Raising public awareness about these issues and encouraging responsible disposal of medications and personal care products can contribute to reducing pollution.\n  - Microplastics: These tiny plastic fragments can adsorb antibiotics and other contaminants, potentially increasing their bio-availability and posing additional risks to aquatic life and human health. \n- **Glyphosate:** The most widely used herbicide globally, primarily found in Roundup weed killer. While its effects on land are well-studied, its presence in water raises growing concerns.\n  - Minimal direct use: Glyphosate isn't applied directly to water bodies for weed control. However, it can reach water indirectly.\n  - Runoff from agricultural fields, lawns, and gardens. Rainfall and irrigation can carry glyphosate residues into rivers, lakes, and groundwater.\n  - Glyphosate is routinely used pre-harvest to help ripen or dry out crops. [link](https:\/\/ahdb.org.uk\/pre-harvest-glyphosate#:~:text=Pre%2Dharvest%20glyphosate%3A%20best%20practice,particularly%20valuable%20in%20wet%20seasons.)\n  - Industrial discharge: Factories producing or using glyphosate may release wastewater containing the herbicide.\n  - Atmospheric deposition: Glyphosate can travel through the air and eventually settle on water surfaces.\n- Dangers and Concerns:\n  - Impact on aquatic life: While glyphosate itself is relatively non-toxic to fish and invertebrates at current levels, concerns exist about potential indirect effects:\n  - Disruption of aquatic ecosystems: Glyphosate can harm algae, the base of the food chain, impacting entire ecosystems.\n  - Increased susceptibility to disease: Glyphosate may weaken aquatic organisms, making them more vulnerable to infections and environmental stressors.\n- **Human health concerns:** Though the EPA considers glyphosate safe at current levels in drinking water, some studies suggest potential risks.\n  - Cancer: The International Agency for Research on Cancer classified glyphosate as \"probably carcinogenic to humans,\" though evidence remains inconclusive.\n  - Developmental effects: Some studies suggest potential harm to fetal development at high exposure levels, requiring further research.\n  - Contamination of human breast milk: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9322831\/)  \n  - Glyphosate use, toxicity and occurrence in food: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8622992\/#:~:text=Glyphosate%20is%20applied%20intensively%20in,in%20human%20urine%20%5B6%5D.)\n  - Antibiotic resistance: Some concerns exist about potential links between glyphosate use and the development of antibiotic resistance in bacteria.\n- Regulations: Different countries have varying regulations regarding glyphosate levels in drinking water. \n  - Detection and treatment: Glyphosate can be detected in water through sophisticated testing methods. However, removing it from drinking water remains a challenge, requiring advanced treatment technologies.\n  - Ongoing Research: More research is needed to fully understand the long-term impacts of glyphosate in water, especially on human health and aquatic ecosystems.\n  - Raising awareness: Public education is crucial to understand the potential risks of glyphosate in water and promote responsible herbicide use.\n  - Supporting research: Continued research on glyphosate's environmental and health impacts is essential for informing regulatory decisions and protecting public health.\n  - Exploring alternatives: Investigating and promoting safer alternatives to glyphosate-based herbicides can help minimize potential risks to water quality and ecosystems.\n- Emerging contaminants: New and poorly understood chemicals, such as PFAS (per- and polyfluoroalkyl substances), are increasingly being detected in water, raising concerns about their potential health effects.\n  - A document entitled - PFAS: The 'Forever Chemicals' With a Toxic Legacy: [link](https:\/\/docs.google.com\/document\/d\/1dKyWDo2BGMwqpOnWbRqxBbZ5_jdccbKlAZKENm3Hn4c\/edit?usp=sharing)\n\n**Data Visualisation:** The below chart shows the 42 best & worst countries in terms of fine particulate matter, from the data (WHO_PM.csv) for the years 2014 - 2019. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F365fd85dc0eccd344966dc3a7fe48a02%2FScreenshot%202024-01-13%2023.08.00.png?generation=1705187441671576&alt=media)\nA Markdown document with the R code for the above plot. The code analyses air pollution and air quality data and calculates average values for each location - [link](http:\/\/rpubs.com\/Paddy_5142\/1138053).\n\nParticle pollution can transport contaminants, such as toxic heavy metals and organic compounds, which can accumulate in fish tissues and be ingested by humans. These pollutants can harm aquatic life through various processes, such as physical damage, ingestion, bio-accumulation, light attenuation, and toxicity. [link](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S1532045623001011#:~:text=Particle%20pollution%20can%20transport%20contaminants,%2C%20light%20attenuation%2C%20and%20toxicity.)\n\n**See current wind, weather, ocean, and pollution conditions, as forecast by supercomputers, on an interactive global animated map:**\n- Earth Nullschool.\n- Updated every three hours.  \n- The following link will take you to: **Wind & Particulate Matter &lt; 2.5 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm2.5\/orthographic)\n- The following link will take you to: **Wind & Particulate Matter &lt; 10 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm10\/orthographic=-354.03,-1.14,182)\n- Click anywhere on the global map to see particulate matter in \u00b5g\/m3 (concentration of an air pollutant, given in micrograms (one-millionth of a gram) per cubic meter of air).\n\n**Data Visualisation:** The below chart shows the 25 best & worst countries in terms of water and air pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fbe8a7a353496b2c9503e53ea615f0ad8%2FScreenshot%202024-01-14%2014.25.31.png?generation=1705243641434813&alt=media)\n\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138105).\n\nFrom the above visualisation: \n- Palau is the cleanest country for both air and water quality. [link](https:\/\/pristineparadisepalau.com\/)\n- Palau, the Federated States of Micronesia, and Eritrea are tied in first place for air quality.\n- Palau and Liechtenstein are tied in first place for water quality.\n- Contrary to its water pollution, the Federated States of Micronesia is in second place for air quality.\n\n*A project of mine entitled 'What about the Wind?' which explores its origins, its role in a changing climate, the ways we harness its strength, and its impact on human lives:* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind)\n\n**Plastic in Water: A Ubiquitous Threat from Land to Sky.**\n\nPlastic, once a revolutionary material, has become a pervasive pollutant, infiltrating every corner of our environment, including our water sources. From the vast oceans to the raindrops falling from the sky, plastic has become a ubiquitous threat.\n\n- Plastic in Rainwater:\n  - Microplastics: Tiny plastic fragments, less than 5 millimetres in size, are now routinely found in rainwater around the world, even in remote locations like the Himalayas and the Pyrenees. These microplastics can originate from various sources, including car tires, synthetic clothing, and the breakdown of larger plastic debris.\n  - Microplastics in rain: Studies have shown that microplastics can be incorporated into raindrops as they form in clouds. This means that even pristine rainwater can now be contaminated with plastic particles.\n  - Potential health risks: The long-term health effects of ingesting microplastics are still unknown, but concerns exist about their potential to harm human health. Microplastics can act as sponges for pollutants and may release harmful chemicals into the body.\n- Atomised Plastic from the Ocean:\n  - Ocean plastic breakdown: The vast gyres of plastic waste in our oceans are constantly breaking down into smaller and smaller pieces under the influence of sunlight, waves, and chemical reactions.\n  - Sea spray and atomisation: Wind and waves create sea spray, which can send tiny plastic particles, known as atomised plastic, into the atmosphere. These particles are even smaller than microplastics, ranging from 1 to 10 micrometers in size.\n  - Plastic rain: Atomised plastic can be carried by air currents for long distances and eventually fall back to Earth as rain, contaminating land and water resources.\n- The Impact of Plastic in Water:\n  - Harm to aquatic life: Plastic pollution in water can entangle and suffocate marine animals, disrupt their feeding habits, and introduce harmful chemicals into the food chain.\n  - Threats to human health: Microplastics and atomised plastic can be ingested by humans through drinking water and eating seafood, potentially posing health risks.\n  - Environmental damage: Plastic pollution can disrupt ecosystems, reduce biodiversity, and impact water quality.\n- What We Can Do:\n  - Reduce plastic use: Making conscious choices to avoid single-use plastics and opting for reusable alternatives can help stem the tide of plastic pollution.\n  - Proper waste management: Ensuring proper disposal and recycling of plastic waste can prevent it from ending up in our environment.\n  - Support research: Investing in research to understand the full impact of plastic pollution and develop solutions for its mitigation is crucial.\n  - Policy changes: Advocating for stricter regulations on plastic production and consumption can help address this global challenge.\n\n**Rain and Contaminants:**\n\nWhile some contaminants can reach the atmosphere and potentially return to Earth through rain or other forms of precipitation, several factors prevent all contaminants from following this cycle: \n- Chemical properties: Not all contaminants are easily evaporated or transported through the air. Heavy metals, for example, tend to remain dissolved in water and are unlikely to become part of raindrops.\n- Biological degradation: Some contaminants can be broken down by microorganisms in the ocean before reaching the atmosphere. This breakdown process prevents them from joining the water cycle.\n- Deep-sea sequestration: Certain pollutants, like heavy metals or persistent organic pollutants (POPs), can sink down to the ocean floor and become trapped in sediments, effectively removed from the cycle.\n- Human intervention: We can mitigate the presence of some contaminants through wastewater treatment and pollution control measures, preventing them from reaching the ocean in the first place.\n\nTherefore, while rain can be a pathway for transporting certain contaminants, it's not a guaranteed endpoint for all pollutants present in seawater. Understanding the specific properties and fate of different contaminants is crucial for assessing their potential environmental impact and developing effective mitigation strategies.\n\n**Modern Water Treatment: Bristol, UK.** [link](https:\/\/www.bristolwater.co.uk\/)\n\nThe water treatment process in Bristol by Bristol Water is a fascinating journey, transforming raw river water into the fresh, clean tap water we all rely on. Here's a detailed breakdown:\n- Sources:\n  - Surface Water: Primarily sourced from reservoirs in the Mendip Hills, fed by rivers like the Chew and the Yeo.\n  - Groundwater: Used in drier periods, sourced from boreholes in limestone aquifers.\n- Treatment stages:\n  - Screening: Removing large debris like leaves and branches.\n  -  Pre-treatment: Adding a coagulant (aluminum sulfate) to bind small particles together.\n  - Sedimentation: Allowing the large clumps to settle out in tanks.\n  - Filtration: Passing the water through sand filters to remove remaining particles.\n  - Activated Carbon Filtration: Removing dissolved contaminants like pesticides and organic matter using highly absorbent activated carbon.\n  - Disinfection: Adding chlorine to kill bacteria and viruses.\n  - pH Adjustment: Balancing the water's acidity for corrosion control and improved taste.\n  - Quality Control: Continuous monitoring and testing at various stages to ensure safe, high-quality water.\n- Bristol Water's \"Wicked Water Treatment\" Initiative: [link](https:\/\/www.bristolwater.co.uk\/wicked-water-treatment#:~:text=We%20use%20wicked%20water%20treatment,can%20leak%20into%20our%20rivers.)\n  - A public education program explains the treatment process in a fun and engaging way, particularly for children.\n  - Emphasises sustainable practices like energy efficiency and waste reduction.\n  - Offers tours of treatment works for a firsthand look at the process.\n- Additional notes:\n  - Bristol Water uses a multi-barrier approach, meaning each stage contributes to overall water quality and safety.\n  - Treatments may vary depending on the specific source and water quality.\n  - The company invests heavily in research and development to improve efficiency and adapt to new challenges.\n\nWastewater in Bristol, and indeed most places, typically undergoes one treatment cycle before being discharged back into the environment, not being recycled in the traditional sense. However, the water used in the treatment process itself is often recycled!\n\n- Here's a breakdown of the water usage in Bristol's wastewater treatment:\n  - Raw water intake: Around 180 million litres of water are drawn from reservoirs and aquifers daily for both drinking water and wastewater treatment.\n  - Treatment process: Only a small portion, around 1%, of this water is actually used in the various treatment stages like screening, sedimentation, and filtration. The majority of the water simply carries the wastewater through the system.\n  - Treated water discharge: The treated wastewater, now significantly cleaner, is released back into rivers or the Severn Estuary.\n  - Treatment plant water recycling: The water used within the treatment plant for tasks like back-washing filters and machinery is often treated and recycled internally. This reduces the overall freshwater intake needed for the process.\n\nSo, while the wastewater itself isn't directly recycled for drinking or other purposes, there are efforts to minimise freshwater use within the treatment plants themselves. Additionally, research into wastewater reuse for non-potable applications like irrigation or industrial processes is ongoing, and Bristol Water is exploring these possibilities as well. [link](https:\/\/corporate.wessexwater.co.uk\/our-purpose\/investment-schemes\/bristol-water-recycling-centre)\n\n**The Rise of Ocean Radioactivity: A Shadow of Fukushima.**\n\nThe vast expanse of our oceans, once considered pristine and boundless, now faces a growing threat: increased radioactivity. This rise is largely attributed to the 2011 Fukushima Daiichi nuclear power plant disaster in Japan, where meltdowns and explosions released significant amounts of radioactive material into the Pacific Ocean.\n\n- Fukushima's Radioactive Footprint:\n  - Cesium-137: The primary radioactive isotope released from Fukushima, cesium-137, has a half-life of 30 years, meaning it will persist in the environment for decades to come.\n  - Ocean Contamination: Cesium-137 has been detected in Pacific Ocean waters thousands of miles from the accident site, contaminating marine life and raising concerns about long-term impacts on the food chain.\n  - Pacific Currents: Ocean currents play a crucial role in dispersing radioactive material, carrying it across vast distances and potentially exposing marine ecosystems far beyond the immediate vicinity of the accident.\n- Impacts on Marine Life:\n  - Bio-accumulation: Radioactive isotopes can accumulate in the tissues of marine organisms, increasing their radiation exposure and potentially posing risks to predators higher up the food chain, including fish consumed by humans.\n  - Disruption of Ecosystems: Radiation can harm the reproduction and development of marine life, potentially disrupting the delicate balance of ocean ecosystems.\n  - Long-Term Uncertainty: The full extent of the long-term consequences of Fukushima's radioactive releases on ocean health and human health through seafood consumption is still being studied and debated. - [link](https:\/\/www.globalresearch.ca\/28-signs-that-the-west-coast-is-being-absolutely-fried-with-nuclear-radiation-from-fukushima\/5355280)\n  - Possible Immune System Compromise: Cesium-137. - [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Beyond Fukushima:\n  - Other Contributors: While Fukushima is a major source of ocean radioactivity, it's not the only one. Nuclear testing, accidents at other nuclear power plants, and even naturally occurring radioactive elements contribute to the overall burden of radioactivity in the oceans.\n- Global Concern: The increasing presence of radioactive materials in our oceans raises concerns about the long-term health of marine ecosystems and the potential risks to human health through seafood consumption. This issue demands international cooperation and continued research to mitigate the risks and protect our oceans.\n- What We Can Do:\n  - Support Research: Continued research on the impacts of ocean radioactivity and the development of monitoring and mitigation strategies is crucial.\n  - Sustainable Seafood Practices: Choosing sustainably sourced seafood and supporting responsible fishing practices can help minimise exposure to radioactive contaminants.\n  - Advocate for Change: Raising awareness about ocean radioactivity and its potential consequences can encourage policymakers to implement stricter regulations on nuclear activities and invest in cleaner energy sources.\n\n**A Future of Abundant and Healthy Oceans: A Collective Responsibility.**\n\nOur journey through the history of water revealed its profound connection to human health and the fragility of its purity. From the challenges of hunter-gatherers to the complexities of modern water management, one truth remains constant: clean water is the lifeblood of our planet and all its inhabitants.\n\nHowever, our oceans, once vast and seemingly limitless, now face a multitude of threats. Increased radioactivity from nuclear accidents like Fukushima casts a long shadow, while the invisible menace of antibiotic resistance, oestrogen contamination, glyphosate residues, and the ubiquitous presence of plastic paint a worrying picture of a polluted sea.\n\nYet, amidst these challenges lies a powerful opportunity. By acknowledging the gravity of these threats and taking collective action, we can forge a future where our oceans are brimming with vibrant life, free from the burden of modern contaminants. This necessitates a multifaceted approach:\n- Research and monitoring: Understanding the extent and long-term impacts of these threats through robust research and continuous monitoring is crucial to informing effective response strategies.\n- Policy and regulation: Implementing stricter regulations on pollution sources, such as nuclear activities, agricultural practices, and plastic production, is essential to curbing the tide of contamination.\n- Technological innovation: Developing innovative solutions like advanced water treatment technologies and cleaner energy sources can offer sustainable pathways towards ocean health.\n- Individual responsibility: Making conscious choices in our daily lives, from reducing plastic use to supporting responsible seafood practices, can collectively make a significant difference.\n\nThe health of our oceans is not just an environmental concern; it is a matter of human health and the very viability of life on Earth. By standing together in this endeavour, we can ensure that future generations inherit a legacy of abundant and healthy oceans, forever teeming with life, forever a source of wonder and sustenance.\nRemember, clean water is not a luxury, it is a birthright. Let us protect it, cherish it, and ensure its abundance for all.\n\n**Cultivating Without Soil: Hydroponics, Aeroponics, and the Future of Farming.**\n\nTraditional soil-based agriculture has served humanity for millennia, but as we face challenges like limited land, water scarcity, and climate change, innovative methods like hydroponics and aeroponics are emerging as sustainable alternatives. Let's delve into these fascinating techniques and explore their potential for the future of farming, even in the vast expanse of space!\n\nHydroponics: Growing in a Watery Embrace.\n\n- Imagine lush vegetables thriving not in soil, but in a nutrient-rich water solution. That's the magic of hydroponics! Plants' roots are suspended in a variety of mediums like rockwool, perlite, or even just water, while receiving a precisely balanced blend of nutrients and oxygen through the solution. This method offers several advantages:\n  - Water efficiency: Hydroponics uses up to 90% less water than traditional agriculture, making it ideal for arid regions or periods of drought.\n  - Increased yields: Precise control over nutrients and environment leads to faster growth and higher yields compared to soil-based methods.\n  - Reduced pest and disease problems: Soil-borne diseases are less of a concern with hydroponics, leading to fewer pesticides and healthier crops.\n  - However, hydroponics also requires significant upfront investment in equipment and infrastructure. Maintaining the correct pH and nutrient balance in the water solution is crucial, and any technical glitches can have a rapid impact on the plants.\n\nAeroponics: Dancing on Air.\n\nTake hydroponics up a notch, and you have aeroponics! In this method, plant roots are suspended in midair and misted with a nutrient-rich solution at regular intervals. This constant exposure to oxygen fosters even faster growth and eliminates the need for a growing medium altogether.\n\n- Aeroponics boasts all the benefits of hydroponics, with some additional advantages:\nEven better root aeration: Direct exposure to air maximises oxygen uptake, leading to even more vigorous plant growth.\n  - Reduced risk of root disease: Without any growing medium, bacteria and fungal diseases have less chance of taking hold.\n  - However, aeroponics demands even stricter control over the nutrient solution and misting frequency. Any interruption in the misting system can quickly stress or even kill the plants.\n\nSoil vs. Soil-less: Weighing the Options.\n- While hydroponics and aeroponics offer numerous benefits, traditional soil-based agriculture remains dominant for several reasons:\n  - Lower cost: Setting up and maintaining soil-less systems requires significant investment, making soil-based farming more accessible, especially for small-scale operations.\n  - Wider range of crops: Not all plants thrive in soil-less systems, while soil can accommodate a diverse range of crops.\n  - Resilience: Soil-based systems offer inherent buffering against fluctuations in temperature, pH, and nutrient levels, making them more forgiving of minor errors.\n\nUltimately, the choice between soil and soil-less methods depends on factors like the type of crop, climate, resources available, and desired level of control.\n\nPure Water: The Lifeblood of Soil-less Systems.\n\nThe importance of clean water is paramount in hydroponics and aeroponics. Impurities in the water can clog systems, disrupt nutrient balance, and harm the plants. Therefore, using filtered or even reverse osmosis-treated water is crucial for optimal results.\n\nSpace faring Sprouts: Farming Beyond Earth.\n\nThe potential of soil-less agriculture extends beyond Earth's soil. In the context of space exploration and potential long-duration missions, hydroponics and aeroponics offer solutions for growing fresh food in the limited space and harsh conditions of spacecraft and lunar or Martian outposts.\nImagine astronauts enjoying salads and herbs grown right on board, reducing reliance on pre-packaged food and providing psychological benefits of cultivating their own sustenance. Research is already underway to develop closed-loop systems that recycle water and nutrients, minimising waste and maximising efficiency.\n\nThe Future of Farming: A Tapestry of Techniques.\n\nAs the world's population grows and resources become scarcer, innovative agricultural methods like hydroponics and aeroponics will undoubtedly play a crucial role in feeding the future. While traditional soil-based agriculture will remain relevant, these soil-less techniques offer promising solutions for sustainable and efficient food production, both on Earth and potentially beyond. So, the next time you bite into a crisp lettuce or juicy tomato, remember that the future of farming might not involve soil at all, but a carefully orchestrated dance of water, nutrients, and air.\n\n**Quenching the Thirst: Unveiling the Secrets of Desalination and Distilled Water.**\n\nWater, the elixir of life, is a precious resource, and with growing populations and climate change, its scarcity is becoming a pressing concern. Enter desalination, the process of removing salt and other minerals from seawater, making it fit for human consumption and other uses. Let's dive into the fascinating world of desalination and explore the journey towards the purest water, distilled water, through ancient and modern methods.\n\nFrom Ancient Ingenuity to Modern Marvels: Desalination's Journey.\n\nThe quest for fresh water from saline sources is as old as civilisation itself. Ancient Greeks and Romans used simple methods like sun evaporation and filtration through clay pots to desalinate seawater. Fast forward to the 19th century, steam-powered distillation became the dominant method, with the first large-scale desalination plant built in 1872 in Santiago, Chile.\n\nToday, desalination has evolved into a sophisticated field, employing a variety of technologies.\n\n- Thermal Desalination: This method uses heat to evaporate seawater, leaving behind salt and other minerals. The most common thermal desalination technologies are:\n  - Multi-stage flash distillation (MSF): Seawater is heated in a series of chambers, and the vapour condenses on cool surfaces to produce fresh water.\n  - Multi-effect distillation (MED): Similar to MSF, but uses waste heat from one stage to heat the next, making it more energy-efficient.\n  - Membrane Desalination: This method uses semi-permeable membranes that allow water molecules to pass through while rejecting salt and other impurities. The most common membrane desalination technology is:\n  - Reverse osmosis (RO): High pressure forces seawater through a membrane, leaving behind concentrated brine and producing fresh water.\n\nDistilled Water: Purity Reborn.\n\nDistillation, the oldest and arguably the purest method of water purification, involves boiling water and collecting the condensed vapour. This process removes virtually all impurities, including minerals, salts, and even some bacteria and viruses. Distilled water is often used in laboratories, medical applications, and certain industrial processes where ultra-pure water is required:\n- Ancient Echoes in Modern Purity:\n  - The principles of distillation are not lost on modern technology. Vacuum distillation, a more advanced method, operates at lower temperatures and pressures, further reducing the energy consumption and preserving the delicate properties of the water. This makes it a preferred choice for producing high-purity water for sensitive applications.\n- The Future of Desalination and Distilled Water:\n  - As the need for clean water grows, so does the importance of desalination and distilled water production. Advancements in technology are making these processes more efficient and affordable, opening up possibilities for wider adoption. Research is also ongoing in developing sustainable energy sources for desalination, such as solar and wind power, to reduce its environmental footprint.\n- The Choice Between Purity and Practicality:\n  - While distilled water offers unparalleled purity, it also lacks essential minerals that are beneficial for human health. \n   - Additionally, the energy consumption of some desalination methods raises concerns about sustainability. Therefore, the choice between desalinated, distilled, or other types of purified water depends on the specific needs and context.\n\nIn conclusion, desalination and distilled water production are vital tools in our quest for clean water. Understanding their history, methods, and limitations helps us make informed decisions about utilising these technologies to ensure a sustainable future with enough water for all.\n\n**Water, water everywhere and not a drop to drink!**\n\nThe tale of the Robertson family's 38-day ordeal adrift in the Pacific Ocean following their yacht's sinking in 1972 is indeed an incredible testament to human resilience and ingenuity. Their story, echoing Samuel Taylor Coleridge's \"Rime of the Ancient Mariner,\" is one of facing the very element that sustains life while desperately battling for it:\n- The Ordeal Begins:\n  - The Robertsons, a British family of six - Dougal and Lyn, along with their children Douglas, Anne, and twins Neil and Sandy - set sail from Falmouth, England, in 1971 on a voyage around the world aboard their 43-foot wooden schooner, Lucette. \n  - Sadly, their dream adventure took a horrific turn when, 200 miles off the coast of the Galapagos Islands, a pod of orcas attacked their vessel, sinking it within minutes.\n- Adrift in a Tiny Dinghy:\n  - The family grabbed a few items and abandoned ship and managed to board their inflatable rubber life raft and nine-foot fiberglass dinghy Ednamair. \n  - They only had enough water for ten days and emergency rations for three days on the raft.\n  - Lyn had taken their papers, the logbook, and a bag of onions, and they had a kitchen knife, a tin of biscuits, ten oranges, six lemons, half a pound of glucose sweets and flares.   \n  - Faced with the agonising reality of being surrounded by water yet desperately parched, they knew they had to think outside the box to survive.\n- Desperate Measures for Survival:\n  - In their fight for survival, the Robertsons resorted to several unconventional techniques, some bordering on the macabre. \n  - They rationed their food meticulously, collecting rainwater in any way they could, even squeezing moisture from their clothes. \n  - To combat dehydration, they employed saltwater enemas, a risky procedure that, while unpleasant, helped to retain some fluids.\n- A Gruesome Necessity:\n  - Perhaps the most shocking aspect of their survival story is their reliance on turtle blood. Driven by sheer desperation, they captured and killed sea turtles, consuming their flesh and drinking their blood to quench their thirst and stave off starvation. This act, while understandably controversial, was a life-or-death decision made in the face of unimaginable circumstances.\n- A Beacon of Hope:\n  - After 38 gruelling days adrift, their ordeal finally came to an end when they were spotted and rescued by a Japanese fishing vessel. \n  - Emaciated and sunburnt but alive, the Robertsons had cheated death against all odds. \n  - Their story became a global sensation, a testament to the indomitable human spirit and the lengths we will go to for survival.\n- Lessons Learned:\n  - The Robertson family's experience left an indelible mark on them and the world. It serves as a stark reminder of the power and unpredictability of nature, the importance of preparation and resourcefulness in the face of adversity, and the depths of human strength and resilience. It is a story that continues to inspire and captivate, a testament to the will to live even in the face of the most desperate odds.\n  - While their methods may raise eyebrows, their tale underscores the desperate lengths humans will go to for survival. It's a story that serves as a powerful reminder of the human spirit's enduring strength and the importance of hope in the face of seemingly insurmountable challenges. [link](https:\/\/nmmc.co.uk\/2022\/05\/the-50th-anniversary-of-the-robertson-family-rescue\/)\n\n**Navigating the Hydration Highway: Bottled Water, Tap Water, Filters, and Distillers.**\n\nHydration \u2013 the essential fuel for our daily adventures. But when it comes to choosing our drink, we're often greeted by a crossroads: bottled water, shimmering in its plastic shell, or tap water, the familiar flow from our taps\/faucets. Let's explore the advantages and downsides of each, along with their trusty sidekicks \u2013 water filters and home distillers \u2013 to find the perfect lane on the hydration highway.\n\nBottled Water: Convenience with Caveats.\n\n- Advantages:\n  - Convenience: Portable and readily available, whether scaling mountains or stuck in traffic, bottled water keeps us quenched.\n  - Taste: Some prefer the perceived \"cleaner\" taste, especially in areas with chlorine-tinged tap water.\n  - Perception of Purity: Often marketed as purer than tap water, though bottled water quality can vary greatly.\n- Disadvantages:\n  - Cost: Significantly more expensive than tap water, making it an unsustainable daily option.\n  - Environmental Impact: Mountains of plastic bottles generate enormous waste, posing a major environmental threat.\n  - Limited Mineral Content: While often perceived as purer, bottled water may lack essential minerals naturally found in tap water.\n\nTap Water: The Everyday Essential.\n- Advantages:\n  - Cost-Effective: The most affordable hydration option, readily available at the turn of a tap.\n  - Mineral Content: Naturally rich in essential minerals important for health.\n  - Lower Environmental Impact: No plastic waste generated, significantly reducing environmental footprint.\n- Disadvantages:\n  - Taste: Can have a chlorinated taste or be affected by local water quality issues.\n  - Purity Concerns: Depending on location, tap water may contain contaminants or require additional filtration.\n  - Accessibility: Not always readily available in all locations or situations.\n\nWater Filters: On-Demand Purification.\n\nEnter the water filter, the trusty sidekick for tap water warriors. Filter jugs, tap\/faucet attachments, and reverse osmosis systems offer various levels of filtration, removing contaminants like chlorine, sediment, and even some heavy metals.\n\n- Advantages:\n  - Improved Taste: Removes unpleasant odours and tastes, enhancing the palatability of tap water.\n  - Reduces Contaminants: Provides an extra layer of protection against potential contaminants in tap water.\n  - Cost-Effective: A more sustainable and affordable option compared to bottled water.\n- Disadvantages:\n  - Maintenance: Filters require regular cleaning and replacement, adding to the cost and effort.\n  - Limited Removal: May not remove all contaminants, depending on the type of filter and local water quality.\n\nHome Distillers: Crafting Purest H2O.\n\nFor those seeking the ultimate water purity, home distillers offer a fascinating option. By boiling and condensing water, these devices remove virtually all impurities, including minerals, salts, and even some bacteria.\n\n- Advantages:\n  - Purest Water: Produces the purest drinking water available, ideal for sensitive applications.\n  - Removes Contaminants: Eliminates virtually all impurities, including those not addressed by filters.\n  - Flexibility: Allows control over the mineral content of the produced water.\n- Disadvantages:\n  - Cost: Home distillers can be expensive and require energy to operate.\n  - Slow Production: Water production is slower compared to other options.\n  - Mineral Removal: Removes essential minerals naturally found in water, requiring supplementation.\n\nThe Verdict: Hydrate Wisely.\n\nUltimately, the perfect hydration choice depends on individual needs, priorities, and access to clean tap water:\n- Remember:\n  - Choose wisely: Consider convenience, cost, environmental impact, and desired water purity.\n  - Embrace filters: Tap water with a filter can be a cost-effective and sustainable option. [link](https:\/\/www.who.int\/publications\/i\/item\/924156251X)\n  - Distill with caution: While pure, distilled water may require mineral supplementation.\n  - Reduce plastic: Opt for reusable bottles and recycle diligently. [link](https:\/\/bottledwater.org\/packaging\/#:~:text=Plastic%20bottled%20water%20containers%20are,and%20HDPE%20for%209.2%20percent.)\n\n**Conclusion: A Ripple of Responsibility in the Aquatic Symphony.**\n\nWater, the essence of life, stands at a pivotal juncture. While its life-sustaining properties nourish us, it confronts an array of threats. From the subtle encroachment of chemical pollutants to the relentless grasp of microplastics, the aquatic landscape endures a silent siege. Heavy metals clandestinely inhabit its depths, unsettling the fragile equilibrium of ecosystems, while the looming shadows of industrial activities stretch across its pristine purity. The very wellspring of life\u2014its groundwater\u2014stands contaminated, posing risks to health and well-being.\n\nIn this challenging panorama, glimmers of hope persist. Acknowledging the intricate interconnectedness of life, we must craft a nuanced and collective response. Clearer and more stringent regulations should resound like a clarion call, reverberating across landscapes burdened by unsustainable practices. Sustainable agriculture, efficient waste management, and international cooperation must waltz in unison, choreographed by a commitment to shared responsibility.\n\nHowever, our focus cannot be exclusively global; it must also turn inward. Every choice we make, from the type of water we consume to the source we rely on, sends ripples outward, influencing the intricate web of life. Within every decision for a reusable bottle and each conserved drop lies the potential for transformative impact.\n\nNavigating the Aquatic Symphony demands not only personal initiative but also an unwavering awareness of the global tableau we collectively paint. Let us, therefore, stride ahead with a revitalised commitment\u2014toward sustainability, equity, and a future where each sip of water heralds a shared resource accessible to all. Ultimately, the power to generate a wave of change resides within our individual choices, capable of cleansing the stains of pollution and revitalising the lifeblood of our planet.\n\nEmbarking on this journey, guided by a shared aspiration, envisions a world where every human revels in the brilliance of clean, safe water. May each droplet murmur a testament to our collective responsibility\u2014a ripple of hope resonating across the expansive canvas of the Aquatic Symphony.\n\n\nPatrick Ford \ud83c\udf0a\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) to my project: *We did not weave the web of life !* Where I concentrate on some important factors that will affect humanity's potential to survive on planet Earth.\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind) to my project: *What about the Wind?*\nFrom a gentle rustle of leaves to the howl of a hurricane, wind is an unseen force that shapes our world. Revered and feared throughout history, wind has inspired myths, driven ships, and fuelled revolutions in energy. We'll explore its origins, its role in a changing climate, the ways we harness its strength, its impact on human lives and the effect on water quality. Our journey will blend science, history, and technology, and even take us beyond Earth to examine the wild winds of other planets.\n\n-----------------------------------------------------------------------------------------------------------------\n\nA short poem about rivers - [link](https:\/\/docs.google.com\/document\/d\/1LVyiP43r8A357T6gyisozJ0Faeis_Yx3QlxcKgIeM1g\/edit?usp=sharing).\n\n","83":"Monitoring of protected areas to curb illegal activities like poaching is a monumental task. Real-time data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which allow surveillance at night when poaching typically occurs. However, it is still a challenge to accurately and quickly process large amounts of the resulting TIR data. The Benchmarking IR Dataset for Surveillance with Aerial Intelligence (BIRDSAI, pronounced \u201cbird\u2019s-eye\u201d) is a long-wave thermal infrared (TIR) dataset containing nighttime images of animals and humans in Southern Africa. The dataset allows for testing of automatic detection and tracking of humans and animals with both real and synthetic videos, in order to protect animals in the real world.","84":"**Description**\n\nThis dataset comprises a collection of images that are categorized into two main classes: cats and dogs. Each class further contains subcategories based on color, namely white and black.\n\nThe dataset includes the following structure:\n\n    Cats\n        White: Images of cats with white fur.\n        Black: Images of cats with black fur.\n    Dogs\n        White: Images of dogs with white fur.\n        Black: Images of dogs with black fur.\n\nThe dataset is well-structured and labeled, making it suitable for tasks like image classification. It is particularly useful for training and evaluating models designed to classify animal types (cats or dogs) and distinguish their colors (white or black).\nContents\n\n    Cats (White): [Number of images: 400]\n    Cats (Black): [Number of images: 400]\n    Dogs (White): [Number of images: 400]\n    Dogs (Black): [Number of images: 400]\n\nThese images are sourced from diverse collections, ensuring a wide variety of cat and dog breeds across various backgrounds and environments. The dataset's labeling and diversity provide a robust foundation for developing and testing image classification models.\n**Potential Use Cases**\n\n    Image classification: Developing models capable of accurately distinguishing between cats and dogs based on their colors.\n    Color-based analysis: Identifying the prevalence or distinguishing features of different fur colors in cats and dogs.\n\nThe dataset's diversity, labeling, and balanced distribution among classes make it a valuable resource for both training and evaluating machine learning models, especially those focused on image classification tasks related to pets' color and type.","85":"This dataset was created for a 7.th semester project which uses and expands upon \"The Brackish Dataset\" https:\/\/www.kaggle.com\/datasets\/aalborguniversity\/brackish-dataset?rvi=1 by adding synthetically generated data. Github for instructions and code https:\/\/github.com\/Sebastian-Whitehead\/MED7-TheFishening\n\nSynthetic data generation in the context of data scarcity and diversity could help bolster the effectiveness of object detection. This paper proposes a synthetic framework for generating realistic underwater fish data, using BOIDs and high-definition rendering. The synthetic data is used to train and test an EfficientDet model, a state-of-the-art object detection algorithm, on the Brackish data set, an underwater fish detection data set captured in the turbid waters of the Limfjord, Denmark. The results demonstrate that there is no significant disparity observed when synthetic data is incorporated with the Brackish method, assumed to be attributed to the limited number of epochs trained. Even so, the inclusion of synthetic data exhibits a promising potential for enhancing the model. Within this paper, the challenges and limitations of such an approach are explored, allowing for potential insights in promising fields for future work.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F44fb71a061901d29601aeb9b96cc4a54%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0256_prediction1.jpg?generation=1702645091439627&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F3b2cfa75d850ae55ad8dae170a134e2d%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0225_prediction2.jpg?generation=1702645118933655&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F35e5beeb9125c1fcc08a312763187a18%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0156_prediction3.jpg?generation=1702645127887965&alt=media)","86":"## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n```","87":"**Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation},","88":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","89":"A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user. ","90":"**ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","91":"**ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","92":"**ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","93":"**ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n ","94":"The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\"","95":"","96":"This data is release as part of work of ***[Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes.](https:\/\/github.com\/qimaqi\/Implicit-Zoo)***\nFollowing the standard procedure for ImageNet-1K classification, we resize images to 256x256 and then center crop them to 224x224 for further processing. To better fit the high-res images, we employ a 4-layer 256 width SIREN. The normalization parameters are consistent with those used for CIFAR-10. We increase the training iterations to 3000, each taking 50.24 secs for each image.\n","97":"This dataset contains annotated images of tomatoes at various stages of ripeness. It is designed to support research and development in agricultural automation, specifically for training machine learning models to distinguish between ripe and unripe tomatoes. The dataset includes annotated images created using an [annotation lab](https:\/\/github.com\/sumn2u\/annotate-lab), ensuring precise and accurate labeling of ripeness status.\n\n","98":"Classification:\n\u25aa Split your dataset into 80% training and 20% testing. \n\u25aa Train at least 3 different models to classify each sample into distinct \nclasses.\n\u25aa Choose at least two hyperparameters to vary. Study at least three \ndifferent choices for each hyperparameter. When varying one \nhyperparameter, all the other hyperparameters should be fixed\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6407243%2F0e9835ec9319dc098fafd88676ca7ae3%2FScreenshot%20(25).png?generation=1719419036630363&alt=media)","99":"This dataset contains images of industrial tools, specifically focusing on screws and ball screws, categorized as either defective or non-defective. The dataset is intended for use in training and evaluating machine learning models for automated defect detection in industrial settings.\n\nThis dataset is primarily designed for educational and research purposes, specifically to aid in the development and evaluation of machine learning models. The core objective is to provide a collection of images featuring industrial tools, like screws and ball screws, categorized as either defective or non-defective. This classification task allows researchers and students to experiment with various machine learning algorithms, aiming to train models that can accurately distinguish between faulty and functional tools based solely on image data.\n\nThe dataset is comprised of two main files: \"train\" and \"test\". Both files contain images of industrial tools, specifically focusing on screws and valves. Each image is categorized into one of two classes: \"defective\" or \"non-defective\". This classification is crucial for training machine learning models that can automatically distinguish faulty tools from functional ones based solely on image data. The \"train\" file serves as the foundation for the model's learning process, while the \"test\" file is used to evaluate the model's performance on unseen data. By analyzing the accuracy on the test data, we can gauge the model's effectiveness in real-world scenarios of identifying defects in industrial tools.","100":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","101":"The dataset includes 4212 images.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 640x640 (Stretch)\n\nThe following augmentation was applied to create 3 versions of each source image:\n* 50% probability of horizontal flip\n* Randomly crop between 0 and 20 percent of the image\n* Random shear of between -10\u00b0 to +10\u00b0 horizontally and -10\u00b0 to +10\u00b0 vertically\n* Random brigthness adjustment of between -15 and +15 percent\n* Random exposure adjustment of between -10 and +10 percent\n* Salt and pepper noise was applied to 0.1 percent of pixels\n\n**Types OF Rocks**\n\n* Basalt\n* chert\n* Clay\n* Conglomerate\n* Diatomite\n* gypsum\n* olivine-basalt\n* Shale-(Mudstone)\n* Siliceous-sinter","102":"This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n","103":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","104":"This dataset includes images of all 26 letters of the alphabet and the \"space\" sign, totaling 27 classes. The dataset includes images from five different signers, with each signer providing 100 samples for each letter. This effort resulted in a total of 500 samples per letter, amounting to 13,500 images.\n\n**Dataset Collection Process:**\n\u2022\tSigners: 5 different signers\n\u2022\tSamples per Letter: 500\n\u2022\tTotal Images: 13,500\n\u2022\tImage Size: 640x480 pixels (resized to 224x224 pixels for training)\n\n**Data Preprocessing**\nAfter collecting the images, we resized them to 224x224 pixels to standardize the input size for the model. This resizing ensures that all images are compatible with the input requirements of the neural network. We then drew the hand landmarks on all the images and blacked out the background in order for the model to focus only on the landmarks when it came to training a CNN model using images.","105":"Flower Samples Dataset\n\nThis dataset contains images of two different types of flowers: roses and orchid. Each image is labeled with its corresponding flower type. The dataset is suitable for tasks such as image classification or object recognition. The images are captured from various angles and under different lighting conditions to provide a diverse set of samples for training and evaluation purposes. Researchers and developers can use this dataset to train machine learning models to classify images of roses and orchid accurately.\n\nAdenium obesum (Desert Rose)\n\nAdenium obesum, commonly known as the Desert Rose, is a flowering plant native to the Sahel regions of Africa and the Arabian Peninsula. This plant is admired for its striking appearance, featuring a swollen stem base (caudex) that stores water, making it well-suited to arid environments. The Desert Rose produces vibrant, trumpet-shaped flowers in shades of pink, red, and white. Its glossy, dark green leaves contrast beautifully with its flowers, adding to its ornamental appeal. Due to its ability to thrive in dry conditions, Adenium obesum is popular among bonsai enthusiasts and gardeners who favor drought-tolerant plants.\n\nCatharanthus roseus (Madagascar Periwinkle)\n\nCatharanthus roseus, also known as Madagascar Periwinkle or Vinca, is a perennial plant native to Madagascar. This hardy and versatile plant is celebrated for its continuous blooming habit, producing abundant pink, white, or red flowers throughout the year. It is often used in gardens and landscaping due to its ability to thrive in a variety of conditions, from full sun to partial shade. Beyond its ornamental value, Catharanthus roseus is renowned in the pharmaceutical industry for its alkaloid compounds, which have been used in the development of important cancer treatment drugs, such as vincristine and vinblastine.","106":"This project entails the development of an advanced data analysis dashboard using Microsoft Excel to evaluate and visualize road accident and casualty statistics for the years 2021 and 2022. The primary objective is to provide a comprehensive, user-friendly interface that offers insightful analysis into the patterns, trends, and factors contributing to road accidents and their resultant casualties.\n\n** Column Descriptions:\n\n1. **Accident_Index**: A unique identifier for each accident recorded in the dataset. This ensures that each accident can be referenced individually.\n\n2. **Accident Date**: The date on which the accident occurred. This includes day, month, and year.\n\n3. **Month**: The month in which the accident occurred, extracted from the accident date. This helps in analyzing monthly trends.\n\n4. **Year**: The year in which the accident occurred, extracted from the accident date. Useful for annual trend analysis.\n\n5. **Day_of_Week**: The day of the week on which the accident occurred. This can help in understanding if there are more accidents on specific days.\n\n6. **Junction_Control**: Indicates the type of control present at the junction where the accident occurred (e.g., traffic lights, stop sign).\n\n7. **Junction_Detail**: Provides more detailed information about the junction's layout (e.g., T-junction, roundabout).\n\n8. **Accident_Severity**: The severity of the accident, which can range from slight, serious, to fatal.\n\n9. **Latitude**: The geographical latitude where the accident occurred. This helps in mapping the location of accidents.\n\n10. **Light_Conditions**: Describes the lighting conditions at the time of the accident (e.g., daylight, darkness with street lighting).\n\n11. **Local_Authority_(District)**: The local authority district where the accident took place. This is useful for regional analysis.\n\n12. **Carriageway_Hazards**: Any hazards present on the carriageway that might have contributed to the accident (e.g., roadworks, oil spill).\n\n13. **Longitude**: The geographical longitude where the accident occurred. Used in conjunction with latitude for mapping.\n\n14. **Number_of_Casualties**: The total number of casualties resulting from the accident. This includes all levels of injury severity.\n\n15. **Number_of_Vehicles**: The number of vehicles involved in the accident. This helps in understanding the scale of the accident.\n\n16. **Police_Force**: The police force that responded to and recorded the accident. This can indicate jurisdiction and reporting standards.\n\n17. **Road_Surface_Conditions**: Describes the condition of the road surface at the time of the accident (e.g., dry, wet).\n\n18. **Road_Type**: The type of road where the accident occurred (e.g., single carriageway, dual carriageway).\n\n19. **Speed_limit**: The speed limit in effect on the road where the accident occurred. This can be a factor in accident severity.\n\n20. **Time**: The exact time of day when the accident occurred. This can help in analyzing patterns related to time.\n\n21. **Urban_or_Rural_Area**: Indicates whether the accident occurred in an urban or rural area. This can influence accident characteristics.\n\n22. **Weather_Conditions**: The weather conditions at the time of the accident (e.g., fine, raining). This can impact driving conditions.\n\n23. **Vehicle_Type**: The type of vehicle(s) involved in the accident. This can help in understanding which types of vehicles are most frequently involved in accidents.**","107":"This Dataset is **forked from the Kaggle Cohort-4 Skill Assessment Challenge.** \nThe dataset for this competition (both train and test) was generated from a deep learning model fine-tuned on the [Used Car Price Prediction Dataset](https:\/\/www.kaggle.com\/datasets\/taeefnajib\/used-car-price-prediction-dataset) dataset. Feature distributions are close to, but not exactly the same, as the original. **Feel free to use the original dataset** as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance, but that is not required.\n\n**train.csv ** - the training dataset; refer to the original dataset link above for column descriptions\n\n**test.csv **- the test dataset; your objective is to predict the value of the target Price\n\n\n**Brand & Model:** Identify the brand or company name along with the specific model of each vehicle.\n\n**Model Year:** Discover the manufacturing year of the vehicles, crucial for assessing depreciation and technology advancements.\n\n**Mileage:** Obtain the mileage of each vehicle, a key indicator of wear and tear and potential maintenance requirements.\n\n**Fuel Type:** Learn about the type of fuel the vehicles run on, whether it's gasoline, diesel, electric, or hybrid.\n\n**Engine Type:** Understand the engine specifications, shedding light on performance and efficiency.\n\n**Transmission:** Determine the transmission type, whether automatic, manual, or another variant.\n\n**Exterior & Interior Colors:** Explore the aesthetic aspects of the vehicles, including exterior and interior color options.\n\n**Accident History:** Discover whether a vehicle has a prior history of accidents or damage, crucial for informed decision-making.\n\n**Clean Title:** Evaluate the availability of a clean title, which can impact the vehicle's resale value and legal status.\n\n***Price:*** Access the listed prices for each vehicle, aiding in price comparison and budgeting.\n","108":"\n# Crash Data in Somerville, USA\n\nThis dataset provides detailed information about traffic crashes in Somerville, USA, including various factors and circumstances related to each incident.\n\n## Columns Description\n\n- **Crash Number**: Unique identifier for each crash event.\n- **Date and Time of Crash**: Date and time when the crash occurred.\n- **Police Shift**: Shift of the police responding to the crash.\n- **Crash Location**: Location where the crash occurred.\n- **Light Conditions**: Conditions of light at the time of the crash (e.g., day, night).\n- **Weather Conditions**: Weather conditions during the crash.\n- **Road Surface**: Type of road surface at the crash location.\n- **Road Contributing Circumstances**: Contributing factors or circumstances related to the road condition.\n- **Traffic Control Device Type**: Type of traffic control device present at the crash location.\n- **Roadway Intersection Type**: Type of intersection where the crash occurred.\n- **Trafficway Description**: Description of the trafficway involved in the crash.\n- **Manner of Collision**: Manner in which the collision occurred (e.g., rear-end, head-on).\n- **First Harmful Event**: Initial event that caused harm in the crash sequence.\n- **First Harmful Event Location**: Location where the first harmful event occurred.\n- **Speed Limit**: Speed limit at the crash location.\n- **Work Zone**: Indicates if the crash occurred within a work zone.\n- **Count Fatal Injury**: Number of fatalities resulting from the crash.\n- **Count Suspected Serious Injury**: Number of suspected serious injuries.\n- **Count Suspected Minor Injury**: Number of suspected minor injuries.\n- **Count Possible Injury**: Number of possible injuries.\n- **Count No Apparent Injury**: Number of individuals with no apparent injuries.\n- **Count Unknown Injury**: Number of injuries with unknown severity.\n- **Count Not Reported Injury**: Number of injuries not reported.\n- **Total Non-Motorists**: Total number of non-motorists involved in the crash.\n- **Pedestrian Involvement (Non-Motorist)**: Indicates pedestrian involvement in non-motorist incidents.\n- **Cyclist Involvement (Non-Motorist)**: Indicates cyclist involvement in non-motorist incidents.\n- **Other Non-Motorist Involvement**: Involvement of other non-motorists in the crash.\n- **Hit and Run Flag**: Indicates if the crash was a hit-and-run incident.\n- **Latitude**: Geographic latitude coordinate of the crash location.\n- **Longitude**: Geographic longitude coordinate of the crash location.\n- **Ward**: Ward where the crash occurred.\n- **Block Code**: Code representing the specific block of the crash location.\n\nThis dataset is valuable for analyzing traffic safety, identifying trends, and understanding factors contributing to crashes in Somerville. It can be used for research, policy-making, and implementing interventions to improve road safety and reduce accidents.\n\n\n\n\nThis data set contains Somerville crashes that occurred from May 2018 to present. Crash reports are completed when a motor vehicle crash occurs on a public way and involves at least one of the following: Any person is killed, any person is injured,  or damage is in excess of $1,000 to any one vehicle or other property. Data does not include crashes that are under active investigation, nor those that occur on state roads, which are under the jurisdiction of the Massachusetts State Police.   State crash data may be accessed on the Massachusetts Department of Transportation.\n\n\nData source : https:\/\/catalog.data.gov\/dataset\/police-data-crashes","109":"### Vehicle Maintenance Dataset\n\n#### Overview\nThis dataset provides synthetic data related to vehicle maintenance to help predict whether a vehicle requires maintenance or not based on various features.\n\n#### Features\n\n1. **Vehicle_Model**: Type of the vehicle (Car, SUV, Van, Truck, Bus, Motorcycle)\n2. **Mileage**: Total mileage of the vehicle\n3. **Maintenance_History**: Maintenance history of the vehicle (Good, Average, Poor)\n4. **Reported_Issues**: Number of reported issues\n5. **Vehicle_Age**: Age of the vehicle in years\n6. **Fuel_Type**: Type of fuel used (Diesel, Petrol, Electric)\n7. **Transmission_Type**: Transmission type (Automatic, Manual)\n8. **Engine_Size**: Size of the engine in cc (Cubic Centimeters)\n9. **Odometer_Reading**: Current odometer reading of the vehicle\n10. **Last_Service_Date**: Date of the last service\n11. **Warranty_Expiry_Date**: Date when the warranty expires\n12. **Owner_Type**: Type of vehicle owner (First, Second, Third)\n13. **Insurance_Premium**: Insurance premium amount\n14. **Service_History**: Number of services done\n15. **Accident_History**: Number of accidents the vehicle has been involved in\n16. **Fuel_Efficiency**: Fuel efficiency of the vehicle in km\/l (Kilometers per liter)\n17. **Tire_Condition**: Condition of the tires (New, Good, Worn Out)\n18. **Brake_Condition**: Condition of the brakes (New, Good, Worn Out)\n19. **Battery_Status**: Status of the battery (New, Good, Weak)\n20. **Need_Maintenance**: Target variable indicating whether the vehicle needs maintenance (1 = Yes, 0 = No)\n\n#### Target Variable\n- **Need_Maintenance**: Indicates whether the vehicle requires maintenance or not based on specified conditions.\n\n#### Data Range\n- Total number of records: 50,000\n\n#### Source\nThis dataset is synthetic and was generated using Python. It is intended for educational and research purposes.\n\n#### Acknowledgements\n- The dataset was generated using Python and the data is synthetic.\n\n\n","110":"The inspiration behind creating this dataset stems from the need to analyze and understand road safety trends, identify risk factors, and develop targeted interventions to reduce road accidents. By categorizing accidents based on vehicle types, researchers, policymakers, and transportation authorities can gain insights into the unique challenges and characteristics associated with different types of vehicles on the road. This dataset aims to serve as a valuable resource for research, analysis, and policymaking efforts aimed at improving road safety and reducing the impact of accidents on society.\n\n","111":"\"Road Accidents Dataset\":\n\nDescription: This comprehensive dataset provides detailed information on road accidents reported over multiple years. The dataset encompasses various attributes related to accident status, vehicle and casualty references, demographics, and severity of casualties. It includes essential factors such as pedestrian details, casualty types, road maintenance worker involvement, and the Index of Multiple Deprivation (IMD) decile for casualties' home areas.\n\nColumns:\n\n1.Status: The status of the accident (e.g., reported, under investigation).\n\n2.Accident_Index: A unique identifier for each reported accident.\n\n3.Accident_Year: The year in which the accident occurred.\n\n4.Accident_Reference: A reference number associated with the accident.\n\n5.Vehicle_Reference: A reference number for the involved vehicle in the accident.\n\n6.Casualty_Reference: A reference number for the casualty involved in the accident.\n\n7.Casualty_Class: Indicates the class of the casualty (e.g., driver, passenger, pedestrian).\n\n8.Sex_of_Casualty: The gender of the casualty (male or female).\n\n9.Age_of_Casualty: The age of the casualty.\n\n10.Age_Band_of_Casualty: Age group to which the casualty belongs (e.g., 0-5, 6-10, 11-15).\n\n11.Casualty_Severity: The severity of the casualty's injuries (e.g., fatal, serious, slight).\n\n12.Pedestrian_Location: The location of the pedestrian at the time of the accident.\n\n13.Pedestrian_Movement: The movement of the pedestrian during the accident.\n\n14.Car_Passenger: Indicates whether the casualty was a car passenger at the time of the accident (yes or no).\n\n15.Bus_or_Coach_Passenger: Indicates whether the casualty was a bus or coach passenger (yes or no).\n\n16.Pedestrian_Road_Maintenance_Worker: Indicates whether the casualty was a road maintenance worker (yes or no).\n\n17.Casualty_Type: The type of casualty (e.g., driver\/rider, passenger, pedestrian).\n\n18.Casualty_Home_Area_Type: The type of area in which the casualty resides (e.g., urban, rural).\n\n19.Casualty_IMD_Decile: The IMD decile of the area where the casualty resides (a measure of deprivation).\n\n20.LSOA_of_Casualty: The Lower Layer Super Output Area (LSOA) associated with the casualty's location.\n\nThis dataset provides valuable insights for analyzing road accidents, identifying trends, and implementing safety measures to reduce casualties and enhance road safety. Researchers, policymakers, and analysts can leverage this dataset for evidence-based decision-making and improving overall road transportation systems.","112":"Dataset Overview\n\nThis dataset provides a detailed view of traffic data in a futuristic urban environment, containing over 1.2 million records. Each record represents a unique snapshot of various factors affecting traffic conditions in six fictional cities.\n\nFeatures\n\nCity: Name of the city (e.g., MetropolisX, SolarisVille).\nVehicle Type: Type of vehicle (e.g., Car, Flying Car).\nWeather Conditions: Current weather (e.g., Clear, Rainy).\nEconomic Conditions: Economic state of the city (e.g., Booming, Recession).\nDay of Week: Day of the week.\nHour of Day: Hour of the day when the data was recorded.\nSpeed: Recorded vehicle speed.\nEnergy Consumption: Estimated energy consumption based on vehicle type and speed.\nIs Peak Hour: Indicator if the record was during peak traffic hours.\nRandom Event Occurred: Indicator if a random event (e.g., accidents, road closures) occurred.\nTraffic Density: Density of traffic at the time of recording.\nFile Format\n\nThe dataset is provided in CSV format, suitable for analysis in various data processing tools and programming languages.\n\nPotential Uses\n\nThis dataset can be utilized for a variety of studies and analyses, including:\n\nUnderstanding traffic patterns in futuristic urban environments.\nAnalyzing the impact of factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\nDeveloping and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n","113":"This dataset provides insights into fatal car crashes with a focus on Asian-related factors, including vehicle make and model. It includes information on lighting conditions, weather, state, city, and driver behavior. Explore the correlation between these factors and fatal accidents involving Asian car brands.","114":"\n****Dataset Description: Road Accident Records****\n\nThis dataset contains detailed records of road accidents occurring within a specific geographic region over a defined period. The data encompasses various attributes related to each accident, providing valuable insights into factors contributing to road safety issues.\n\n**Attributes Included:**\n\n1. **Accident ID:** A unique identifier assigned to each accident for reference and tracking purposes.\n2. **Date and Time:** The date and time when the accident occurred, facilitating temporal analysis and trend identification.\n3. **Location:** The precise geographical coordinates or address where the accident took place, enabling spatial analysis and mapping.\n4. **Severity:** The severity level of the accident, categorized based on the extent of injuries, property damage, or fatalities.\n5. **Weather Conditions:** Information about weather conditions prevailing at the time of the accident, such as clear, rainy, foggy, or snowy weather.\n6. **Road Conditions:** Description of road conditions, including dry, wet, icy, or slippery surfaces.\n7. **Vehicle Involved:** Details about vehicles involved in the accident, including types, models, and counts.\n8. **Contributing Factors:** Factors contributing to the accident, such as speeding, distracted driving, drunk driving, road defects, or mechanical failures.\n9. **Injuries and Fatalities:** Number of individuals injured or killed as a result of the accident.\n10. **Vehicle Maneuvers:** Description of maneuvers or actions taken by vehicles involved, such as turning, braking, or overtaking.\n\n**Potential Uses:**\n\n1. **Safety Analysis:** Identifying high-risk locations, times, and conditions to develop targeted safety measures and interventions.\n2. **Predictive Modeling:** Building predictive models to anticipate and prevent accidents based on historical patterns and contributing factors.\n3. **Policy Formulation:** Informing policymaking and infrastructure planning to improve road safety and reduce accident rates.\n4. **Public Awareness Campaigns:** Designing targeted awareness campaigns to educate drivers about common risk factors and safe driving practices.\n5. **Law Enforcement:** Supporting law enforcement efforts by identifying areas with high accident rates and enforcing traffic regulations accordingly.\n\n**Data Source:**\n\nThe dataset is sourced from official accident reports, police records, or other reliable sources authorized to collect and maintain such data. Care has been taken to ensure accuracy and completeness, although some discrepancies may exist due to reporting errors or data collection limitations.\n\n**Note:** Use of this dataset for research, analysis, or other purposes should adhere to relevant data privacy and ethical guidelines, ensuring responsible use and respect for individual privacy rights.","115":"The police of Czech Republic regularly gathers and releases detailed data on traffic incidents throughout the nation, typically on an monthly basis. This dataset covers various aspects such as geographic locations, weather conditions, vehicle types, casualty counts, and vehicle maneuvers. The wealth of information makes it a compelling and extensive dataset for analysis and research purposes.\n\n- Most coded variables in the data have been translated into text strings using lookup tables, making the analysis more efficient and user-friendly.\n- After analyzing the dataset, I've filled in the majority of NaN values.\n\nContent\nThe data come from the [police of Czech Republic](https:\/\/www.policie.cz\/clanek\/statistika-nehodovosti-900835.aspx) \n\nThe dataset comprises of two csv files:\n\n- road_accidents_czechia_2016_2022.csv: every line in the file represents a unique traffic accident, featuring various properties related to the accident as columns\n\n- pedestrian.csv: Each line in the file corresponds to a distinct pedestrian-involved traffic incident, with various incident-related attributes captured as columns.\n","116":"## Content  \nThe [**Police Service of Northern Ireland (PSNI)**](https:\/\/www.psni.police.uk\/) compiles data on **road traffic collisions (RTCs)** that result in injuries. This data is used to monitor and identify trends in the number of individuals killed or injured (either seriously or slightly) due to RTCs on Northern Ireland\u2019s roads.\n\nThe PSNI\u2019s injury collision data for Northern Ireland, combined with those for England, Scotland and Wales, provide a comprehensive overview of all such collisions across the UK.\n\nThis dataset is retrieved from data.gov.uk, a website that is built and maintained by the Government Digital Service. It contains four csv files:\n1. **casualty2017-2022.csv**: The casualties involved in collisions, including casualty information (whether the casualty was a driver, passenger, pedestrian or cyclist) and vehicle involved.\n2. **collision2017-2022.csv**: The conditions under which the collisions occurred, including collision severity, number of vehicles and casualties involved, time, location, weather, road conditions and carriageway hazards.\n3. **vehicle2017-2022.csv**: The vehicles involved in each collision, including type of vehicle, operation at the time of the collision, object involved and driver information.\n4. **vehicle-index.csv**: A list of variables and values for vehicle2017-2022.csv.\n  \n## Possible Explorations\n- Assess how environmental conditions affect accident likelihood and severity.\n- Examine the influence of time (day and hour) on accident occurrence and severity.\n- Show differences in casualty demographics.\n- Identify regional variations in road traffic collisions.\n- Analyze trends in traffic accidents and fatality rates.\n\n## Important Note\nThe following variables are documented for collisions resulting in fatal and serious injuries only:  \n&gt;**Collision Records**  \n1. a_jdet - Junction Detail\n2. a_jcont - Junction Control\n3. a_pedphys - Pedestrian Crossing Facilities - Physical\n4. a_pedhum - Pedestrian Crossing Facilities - Human\n5. a_light - Light Conditions\n6. a_weat - Weather Conditions\n7. a_roadsc - Road Surface Conditions\n8. a_speccs - Special Conditions at Site\n9. a_chaz - Carriageway Hazard\n10. a_scene - Did a Police Officer Attend the Scene of the Collision  \n  \n&gt;**Casualty Records**  \n1. c_loc - Pedestrian Location\n2. c_move - Pedestrian Movement\n3. c_pcv - Bus or Coach Passenger\n4. c_pedinj - Pedestrian casualty injured in the course of on the road work  \n  \n&gt;**Vehicle Records**  \n1. v_junc - Junction Location of Vehicle at Time of Impact\n2. v_skid - Skidding \/ Overturning\n3. v_hit - First Object Hit in Carriageway\n4. v_leave - Vehicle Leaving Carriageway\n5. v_hitoff - First Object Hit off Carriageway\n6. v_forreg - Foreign Registered Vehicle\n\n## Licence\nUK Open Government Licence (OGL)\n\n## Acknowledgements  \nThe data were publicly visible and published by the Police Service of Northern Ireland.","117":"## Overview\n\nThis dataset provides a comprehensive look at traffic data in a futuristic urban setting. It includes over 1.2 million records, each representing a unique snapshot of various factors influencing traffic conditions in six fictional cities.\n\n## Features\n\n- **City**: The name of the city (e.g., MetropolisX, SolarisVille).\n- **Vehicle Type**: Type of vehicle in use (e.g., Car, Flying Car).\n- **Weather Conditions**: Current weather conditions at the time of data capture (e.g., Clear, Rainy).\n- **Economic Conditions**: Economic state of the city at the time of the record (e.g., Booming, Recession).\n- **Day of Week**: The day of the week.\n- **Hour of Day**: The hour of the day when the data was recorded.\n- **Speed**: Recorded speed of the vehicle.\n- **Energy Consumption**: An estimate of energy consumption based on vehicle type and speed.\n- **Is Peak Hour**: Indicator of whether the record was during peak traffic hours.\n- **Random Event Occurred**: Indicator of whether a random event (like accidents or road closures) occurred.\n- **Traffic Density**: The density of traffic at the time of recording.\n\n## File Format\n\nThe dataset is provided in a CSV format, suitable for analysis in various data processing tools and programming languages.\n\n## Potential Uses\n\nThis dataset can be used for a range of studies and analyses, including but not limited to:\n\n- Understanding traffic patterns in futuristic urban environments.\n- Analyzing the impact of various factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\n- Developing and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n\n---\n\n*Note: This is a simulated dataset created for analytical and educational purposes.*","118":"This dataset provides detailed records of road accidents that occurred during *January 2021*. It includes information such as the accident date, day of the week, junction control, accident severity, geographical coordinates, lighting and weather conditions, vehicle details, and more. The data is valuable for analyzing and understanding the factors contributing to road accidents in this urban area, aiding in the development of strategies for improved road safety.\n\n\n**Accident_Index:** A unique identifier for each accident record.\n\n**Accident Date:** The date on which the accident occurred (format: DD\/MM\/YYYY).\n\n**Day_of_Week:** The day of the week when the accident took place.\n\n**Junction_Control** : Describes the type of junction control at the accident location (e.g., \"Give way or uncontrolled\").\n\n**Junction_Detail:** Provides additional details about the junction where the accident occurred (e.g., \"T or staggered junction\").\n\n**Accident_Severity:** Indicates the severity of the accident (e.g., \"Serious\").\n\n**Latitude:** The geographic latitude of the accident location.\n\n**Light_Conditions:** Describes the lighting conditions at the time of the accident (e.g., \"Daylight\").\n\n**Local_Authority_(District)**: The local authority district where the accident occurred.\n\n**Carriageway_Hazards:** Describes any hazards present on the carriageway at the time of the accident (e.g., \"None\").\n\n**Longitude:** The geographic longitude of the accident location.\n\n**Number_of_Casualties:** The total number of casualties involved in the accident.\n\n**Number_of_Vehicles:** The total number of vehicles involved in the accident.\n\n**Police_Force:** The police force that handled the accident.\n\n**Road_Surface_Conditions:** Describes the surface conditions of the road at the time of the accident (e.g., \"Dry\").\n\n**Road_Type:** Specifies the type of road where the accident occurred (e.g., \"One way street\").\n\n**Speed_limit:** The speed limit applicable to the road where the accident occurred.\n\n**Time:** The time of day when the accident happened (format: HH:MM).\n\n**Urban_or_Rural_Area:** Indicates whether the accident occurred in an urban or rural area.\n\n**Weather_Conditions:** Describes the weather conditions at the time of the accident (e.g., \"Fine no high winds\").\n\n**Vehicle_Type:** Specifies the type of vehicle involved in the accident (e.g., \"Car,\" \"Taxi\/Private hire car\").","119":"This dataset is sourced from the United Kingdom's Open Data website (https:\/\/www.data.gov.uk\/).\n\nIn this dataset, you will find extensive information on the characteristics of individual vehicular accidents. The data is split into three main datasets and one data guide which can be used to reference dummy variables.\n\nThe three main data sets are:\n\n- Casualty\n- Collision\n- Vehicle\n\nThe data sets can be combined using the 'accident_index' key across the data sets.","120":"## **Introduction:**\n\nRoad accidents pose significant threats to public safety and necessitate a comprehensive understanding of various factors influencing their occurrence. This article explores key aspects related to accident severity and emphasizes the importance of effective road management strategies.\n\n##**Exploring Geographic and Temporal Aspects of Road Incidents:**\n\nTo enhance road safety, it is crucial to delve into the geographic and temporal dimensions of road incidents. Analyzing the locations and times at which accidents frequently occur enables authorities to implement targeted interventions. This section discusses the significance of spatial and temporal analysis in devising proactive safety measures.\n\n##**A Comprehensive Dataset for Traffic Incident Research:**\n\nA robust dataset forms the foundation for meaningful research in traffic incident analysis. This segment highlights the need for comprehensive data collection, emphasizing variables such as road infrastructure, vehicle types, and driver demographics. The article emphasizes the importance of open and accessible datasets to facilitate research and policy development.\n\n##**Impacts of Weather and Road Conditions on Accident Rates:**\n\nWeather and road conditions play a pivotal role in determining accident rates. This section explores the correlations between adverse weather, poor road conditions, and increased accident severity. Understanding these relationships can aid in developing strategies to mitigate risks during inclement weather.\n\n##**Identifying Hotspots and Risk Factors in Road Safety:**\n\nEffective road management involves identifying accident hotspots and understanding the underlying risk factors. By employing data-driven analysis techniques, authorities can pinpoint areas with high accident rates and implement targeted interventions. This portion of the article discusses methodologies for hotspot identification and risk factor analysis.\n\n##**Data-driven Approaches to Reduce Road Accidents:**\n\nHarnessing the power of data is essential for developing proactive strategies to reduce road accidents. This section focuses on data-driven approaches, including predictive modeling and machine learning, to identify potential accident scenarios and implement preventive measures. The integration of technology and analytics is crucial for achieving substantial improvements in road safety.\n\n##**Traffic Collision Analysis for Urban Planning Strategies:**\n\nUrban planning plays a crucial role in shaping road safety outcomes. This part of the article explores how traffic collision analysis can inform urban planning strategies. By incorporating safety considerations into urban design, cities can create environments that minimize the risk of accidents and enhance overall road safety.\n\n##**Patterns of Driver Behavior and Their Influence on Accidents:**\n\nUnderstanding patterns of driver behavior is paramount for effective road management. This section examines the impact of driver behavior on accident rates and discusses how insights into these patterns can inform targeted educational campaigns and enforcement strategies.\n\n##**Conclusion:**\n\nIn conclusion, this article emphasizes the multifaceted nature of road safety and the importance of a holistic approach to accident prevention. By considering factors such as geographic and temporal aspects, comprehensive datasets, weather and road conditions, hotspots and risk factors, data-driven approaches, urban planning, and driver behavior, authorities can formulate effective road management strategies to enhance public safety.","121":"_____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n","122":" Road Accident Details:\nThis dataset comprises 5000 entries detailing road accidents. Each entry contains 15 columns covering crucial accident-related information. It includes unique accident IDs, accident locations, dates, severity levels, weather conditions during the accidents, types of vehicles involved, injury counts, fatalities, causes (such as human error or weather conditions), road types (like highways or city streets), vehicle speeds, alcohol involvement, road conditions, and time of the accidents. The dataset is diverse, offering insights into the circumstances, factors, and outcomes of these accidents, crucial for analysis and pattern recognition in road safety.\n\nAccident Investigation and Reporting:\nComplementing the first dataset, this collection of 5000 entries provides detailed information regarding accident investigations and subsequent reporting. It contains 15 columns, including accident IDs linking it to the first dataset, accident descriptions, police station details involved in the investigation, investigation statuses (such as pending or completed), reporters (like witnesses or involved parties), insurance claim information, fault attributions, injured persons' names and contact information, details about vehicle damages, repair costs, and legal proceedings. This dataset offers a comprehensive view of the post-accident processes, legal aspects, and financial implications, aiding in understanding the aftermath and resolution of road accidents.","123":"The Airbag Recommendation Dataset: An Analysis and Insights\n\nIn recent decades, the automotive industry has witnessed substantial advancements in safety measures, notably the integration of airbag systems to reduce the impact of collisions and safeguard occupants. The efficacy of airbags largely depends on various factors such as their placement, design, and the dynamics of the crash scenario. To better understand and improve airbag systems, researchers and engineers heavily rely on datasets that capture real-world collision data, aiding in the refinement and enhancement of these safety mechanisms.\n\nThe Airbag Recommendation Dataset serves as a pivotal resource within this domain. This dataset aggregates information from a multitude of sources, encompassing diverse vehicular collisions, encompassing crash-test data, real-world accidents, and simulations. It consolidates information on the performance of different airbag systems across various makes and models of vehicles, shedding light on their effectiveness under different crash conditions.\n\nThis dataset encompasses a wide array of variables and features, offering a comprehensive view of airbag deployment scenarios. Some of the crucial data points included in this dataset are:\n\n1. **Vehicle Information:** Details about the vehicle involved in the crash, including make, model, year, and specifications relevant to airbag deployment.\n  \n2. **Crash Parameters:** Information about the crash, including impact speed, collision angle, and severity.\n\n3. **Occupant Details:** Data pertaining to the occupants involved, such as their seating position, age, and whether they were wearing seat belts at the time of the crash.\n\n4. **Airbag Deployment:** Details about the airbag system, including the type of airbags deployed (frontal, side, curtain), their deployment timing, and effectiveness in mitigating injury.\n\n5. **Injury Assessment:** Information about injuries sustained by the occupants, ranging from minor to severe, correlated with the airbag deployment scenarios.\n\nAnalyzing this dataset provides valuable insights into various aspects of airbag deployment:\n\n1. **Effectiveness of Different Airbag Systems:** By comparing the performance of diverse airbag systems across different vehicles and collision scenarios, researchers can ascertain which designs are more effective in specific crash situations. For instance, frontal airbags might excel in head-on collisions, while side airbags might prove more beneficial in T-bone accidents.\n\n2. **Optimal Deployment Strategies:** Understanding the correlation between deployment timing and injury mitigation is critical. This dataset aids in evaluating the timing and sequence of airbag deployment to maximize protection while minimizing potential harm caused by airbag impact.\n\n3. **Occupant Positioning and Protection:** The dataset helps in determining the impact of passenger positioning within the vehicle concerning airbag effectiveness. This insight is pivotal in recommending safe seating positions for occupants of different ages and sizes.\n\n4. **Improving Design and Engineering:** Engineers and vehicle manufacturers can use this dataset to refine airbag designs and systems, optimizing their performance in real-world crash scenarios.\n\n5. **Policy and Regulation Implications:** Insights from this dataset can influence safety standards, regulations, and policies related to airbag systems, ensuring higher safety standards in the automotive industry.\n\nHowever, while the Airbag Recommendation Dataset holds immense potential, it also comes with challenges and limitations. Data quality, consistency across different sources, and the representation of various vehicle types and crash scenarios are some of the challenges that researchers need to address when utilizing this dataset.\n\nIn conclusion, the Airbag Recommendation Dataset stands as a valuable asset for researchers, engineers, policymakers, and automotive manufacturers alike. Its insights play a crucial role in advancing the safety standards of airbag systems, contributing significantly to the ongoing efforts to enhance vehicle safety and reduce the severity of injuries sustained in automotive accidents. Continued research and analysis of this dataset are pivotal in driving innovation and improving safety measures within the automotive industry.","124":"The data contains hourly vehicle counts of particular highway in the year of 1995 beginning from the month of August. The data is of around 70 days span.\n\nThe dataset is good for beginners to practice on Time series","125":"The Motor Vehicle Collisions crash table contains details on the crash event. Each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC. The police report (MV104-AN) is required to be filled out for collisions where someone is injured or killed, or where there is at least $1000 worth of damage (https:\/\/www.nhtsa.gov\/sites\/nhtsa.dot.gov\/files\/documents\/ny_overlay_mv-104an_rev05_2004.pdf). \n\nIt should be noted that the data is preliminary and subject to change when the MV-104AN forms are amended based on revised crash details. For the most accurate, up to date statistics on traffic fatalities, please refer to the NYPD Motor Vehicle Collisions page (updated weekly) or Vision Zero View (updated monthly).\n\nDue to success of the CompStat program, NYPD began to ask how to apply the CompStat principles to other problems. Other than homicides, the fatal incidents with which police have the most contact with the public are fatal traffic collisions. Therefore in April 1998, the Department implemented TrafficStat, which uses the CompStat model to work towards improving traffic safety. Police officers complete form MV-104AN for all vehicle collisions. The MV-104AN is a New York State form that has all of the details of a traffic collision. Before implementing Trafficstat, there was no uniform traffic safety data collection procedure for all of the NYPD precincts. Therefore, the Police Department implemented the Traffic Accident Management System (TAMS) in July 1999 in order to collect traffic data in a uniform method across the City. TAMS required the precincts manually enter a few selected MV-104AN fields to collect very basic intersection traffic crash statistics which included the number of accidents, injuries and fatalities. As the years progressed, there grew a need for additional traffic data so that more detailed analyses could be conducted. The Citywide traffic safety initiative, Vision Zero started in the year 2014. Vision Zero further emphasized the need for the collection of more traffic data in order to work towards the Vision Zero goal, which is to eliminate traffic fatalities. Therefore, the Department in March 2016 replaced the TAMS with the new Finest Online Records Management System (FORMS). FORMS enables the police officers to electronically, using a Department cellphone or computer, enter all of the MV-104AN data fields and stores all of the MV-104AN data fields in the Department\u2019s crime data warehouse. Since all of the MV-104AN data fields are now stored for each traffic collision, detailed traffic safety analyses can be conducted as applicable.\n\n","126":"# Description for Spotify Songs Dataset on Kaggle\n__________________________________________________________________________________________________________________\n##Dataset Title: Spotify Songs Dataset\n**Description**:\nThis dataset contains a collection of songs fetched from the Spotify API, covering various genres including \"acoustic\", \"afrobeat\", \"alt-rock\", \"alternative\", \"ambient\", \"anime\", \"black-metal\", \"bluegrass\", \"blues\", \"bossanova\", \"brazil\", \"breakbeat\", \"british\", \"cantopop\", \"chicago-house\", \"children\", \"chill\", \"classical\", \"club\", \"comedy\", \"country\", \"dance\", \"dancehall\", \"death-metal\", \"deep-house\", \"detroit-techno\", \"disco\", \"disney\", \"drum-and-bass\", \"dub\", \"dubstep\", \"edm\", \"electro\", \"electronic\", \"emo\", \"folk\", \"forro\", \"french\", \"funk\", \"garage\", \"german\", \"gospel\", \"goth\", \"grindcore\", \"groove\", \"grunge\", \"guitar\", \"happy\", \"hard-rock\", \"hardcore\", \"hardstyle\", \"heavy-metal\", \"hip-hop\", \"holidays\", \"honky-tonk\", \"house\", \"idm\", \"indian\", \"indie\", \"indie-pop\", \"industrial\", \"iranian\", \"j-dance\", \"j-idol\", \"j-pop\", \"j-rock\", \"jazz\", \"k-pop\", \"kids\", \"latin\", \"latino\", \"malay\", \"mandopop\", \"metal\", \"metal-misc\", \"metalcore\", \"minimal-techno\", \"movies\", \"mpb\", \"new-age\", \"new-release\", \"opera\", \"pagode\", \"party\", \"philippines-opm\", \"piano\", \"pop\", \"pop-film\", \"post-dubstep\", \"power-pop\", \"progressive-house\", \"psych-rock\", \"punk\", \"punk-rock\", \"r-n-b\", \"rainy-day\", \"reggae\", \"reggaeton\", \"road-trip\", \"rock\", \"rock-n-roll\", \"rockabilly\", \"romance\", \"sad\", \"salsa\", \"samba\", \"sertanejo\", \"show-tunes\", \"singer-songwriter\", \"ska\", \"sleep\", \"songwriter\", \"soul\", \"soundtracks\", \"spanish\", \"study\", \"summer\", \"swedish\", \"synth-pop\", \"tango\", \"techno\", \"trance\", \"trip-hop\", \"turkish\", \"work-out\", \"world-music\". Each entry in the dataset provides detailed information about a song, including its name, artists, album, popularity, duration, and whether it is explicit.\n__________________________________________________________________________________________________________________\n**Data Collection Method**:\nThe data was collected using the Spotify Web API through a Python script. The script performed searches for different genres and retrieved the top tracks for each genre. The fetched data was then compiled and saved into a CSV file.\n__________________________________________________________________________________________________________________\n**Columns Description**:\nid: Unique identifier for the track on Spotify.\nname: Name of the track.\ngenre: genre of the song.\nartists: Names of the artists who performed the track, separated by commas if there are multiple artists.\nalbum: Name of the album the track belongs to.\npopularity: Popularity score of the track (0-100, where higher is more popular).\nduration_ms: Duration of the track in milliseconds.\nexplicit: Boolean indicating whether the track contains explicit content.\n__________________________________________________________________________________________________________________\n**Potential Uses**:\nThis dataset can be used for a variety of purposes, including but not limited to:\n\n- Music Analysis: Analyze the popularity and characteristics of songs across different genres.\n- Recommendation Systems: Develop and test music recommendation algorithms.\n- Trend Analysis: Study trends in music preferences and popularity over time.\n- Machine Learning: Train machine learning models for tasks like genre classification or popularity prediction.\n__________________________________________________________________________________________________________________\n**Acknowledgements**:\nThis dataset was created using the Spotify Web API. Special thanks to Spotify for providing access to their extensive music library through their API.\n__________________________________________________________________________________________________________________\n**License**:\nThis dataset is made available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You are free to use, modify, and distribute this dataset, provided you give appropriate credit to the original creator.\n__________________________________________________________________________________________________________________","127":"This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.","128":"## Complete Pok\u00e9mon Dataset 9th Gen (img + tabular)\nThe Complete Pok\u00e9mon Dataset 9th Gen (img + tabular) is a comprehensive collection of information covering the entirety of the 9th generation Pok\u00e9mon. It combines both image data of Pok\u00e9mon sprites and tabular data containing their base statistics and index ranks. This dataset serves as a valuable resource for researchers, developers, and enthusiasts interested in exploring and analyzing the characteristics of Pok\u00e9mon in the latest generation.\n\n## **Dataset Overview**:\n### Image Data:\n\n**Pok\u00e9mon Sprites**: High-quality images representing the visual appearance of each Pok\u00e9mon in the 9th generation.\nFormat: Images are provided in a standardized format for easy integration into various applications and analysis tools.\nIndexing: Each image is indexed to correspond with the respective entry in the tabular data for seamless cross-referencing.\n\n### Tabular Data:\n\n**Base Stats**: Comprehensive information on the base statistics of each Pok\u00e9mon, including HP, Attack, Defense, Special Attack, Special Defense, and Speed.\n**Index Rank**: An index rank assigned to each Pok\u00e9mon based on various attributes, such as overall power, rarity, or popularity.\n**Additional Attributes**: Any additional relevant attributes, such as Pok\u00e9mon type, abilities, evolutionary line, and regional variants.","129":"**Dataset Description: Most Popular Anime of All Time**\n\nThis dataset compiles information on the most popular anime titles of all time, drawing its data from IMDb.com. IMDb, known for its extensive database of films and television series, serves as a reputable source for aggregating ratings, reviews, and other relevant details.\n\n****Key Features:**\n\n**Name:** The name of the anime series, representing the title under consideration.\n\n**Type: **Categorization of the anime, distinguishing between series, movies, or other formats.\n\n**Aired: **The release year or range of years during which the anime was originally aired.\n\n**Rating:** IMDb ratings, reflecting the overall reception and viewer satisfaction with each anime.\n\n**Votes:** The number of votes received by each anime on IMDb, providing insights into its popularity and audience engagement.\n\n**Description:** A brief summary or description of the anime's storyline, offering a glimpse into its thematic elements.\n\n**Studio:** Information about the production studio responsible for creating the anime, shedding light on the creative forces behind its development.\n\n**Data Collection Methodology:**\n\nThe dataset is populated through web scraping IMDb.com, utilizing Python libraries such as BeautifulSoup for parsing HTML and extracting relevant information. IMDb's comprehensive database ensures the inclusion of a diverse array of anime titles, from classic series to contemporary releases.\n\n**Potential Applications:**\n\nThis dataset can be a valuable resource for anime enthusiasts, researchers, and data analysts interested in exploring trends, patterns, and the factors contributing to the popularity of anime titles. The information contained herein facilitates comparative analyses of different anime series, aiding in the identification of trends in viewer preferences over time.\n\n**Usage Considerations:**\n\nUsers are encouraged to exercise caution and verify the dataset periodically, as IMDb ratings and details may change with time. Additionally, proper attribution to IMDb as the source is recommended when utilizing the dataset for research or any other purpose.","130":"This dataset contains a comprehensive collection of anime titles spanning the years 1970 to 2024. The data was collected from MyAnimeList using web scraping techniques. It includes essential information about each anime, such as its unique ID, title, genre, description, studio, release year, and user ratings. The dataset offers a valuable resource for exploring the evolution of anime over the decades and understanding trends in the industry. Researchers, anime enthusiasts, and data analysts can use this dataset to analyze various aspects of anime production and consumption, including popular genres, top-rated studios, and changes in audience preferences over time. The dataset is presented in a CSV format and is suitable for a wide range of data analysis and machine learning applications.\n\n## Columns in the data.json :\nmal_id: Unique identifier for the anime entry.\ntitles: List of titles associated with the anime. In this case, \"Attack No.1\".\ntype: Type of the anime, e.g., TV series.\nsource: Source material of the anime, here it's based on a manga.\nepisodes: Number of episodes in the anime (104 in this case).\nrating: Audience rating category, PG-13 in this example.\nscore: Average score given to the anime by users.\nscored_by: Number of users who have scored the anime.\nrank: Ranking of the anime based on score or popularity.\npopularity: Popularity ranking of the anime.\nmembers: Number of members who have added this anime to their list.\nfavorites: Number of users who have favorited this anime.\nsynopsis: Plot summary or synopsis of the anime.\nstudios: Production studio responsible for creating the anime.\ngenres: List of genres the anime belongs to (e.g., Drama, Sports).\nthemes: List of themes present in the anime (e.g., Team Sports).\n\n## Columns in the  user_recommendation.csv :\nmal_id: This column represents the ID of an anime that users have watched or interacted with.\nmal_id_recomm: This column lists the IDs of anime recommended by users for a specific mal_id.\nvotes: The votes column indicates the number of votes or recommendations given by users for the recommendation of mal_id_recomm for mal_id.\n\n\nThe dataset is ready for exploration, analysis, and visualization to uncover insights into the world of anime and its dynamic landscape.","131":"The \"Anime TV Shows Database\" is a comprehensive collection of data related to various anime TV shows. This dataset provides a wealth of information about anime series, including details about their production, ratings, related shows, genres, and more.\n\n\nKey Features and Attributes:\n\nBasic Information: Includes the title of the show, its type (e.g., TV, OVA, Movie), the year of release, and the season it premiered.\n\nProduction and Studios: Contains information about the studios responsible for producing the anime series, along with studio locations.\n\nRanking and Ratings: Provides details on the ranking of each show, user ratings, and any available reviews.\n\nRelated Shows and Franchises: Lists related shows and provides information about any franchises to which the show belongs.","132":"This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.","133":"","134":"# Description\n\nOur dataset comprises comprehensive user preference data gathered from 73,516 avid anime enthusiasts, spanning across 12,294 diverse anime titles. Each individual user has the autonomy to curate their own completed anime list, supplemented with personal ratings reflecting their viewing experience. This rich compilation of user-generated ratings forms the backbone of our dataset, offering invaluable insights into the nuanced preferences and tastes of anime enthusiasts worldwide.\n\n# Key Features\n\n##### User Profiles:-\nExplore the preferences and behaviors of over 73,000 users, each with their unique anime consumption habits and rating patterns.\n\n##### Anime Titles:-\nDive into a vast collection of 12,000+ anime titles, ranging from timeless classics to contemporary releases across various genres and themes.\n\n##### Completed Lists:-\nGain access to users' completed anime lists, providing a glimpse into the breadth and depth of their viewing history.\n\n##### Ratings:-\nUncover users' subjective evaluations of anime titles, quantified through personalized ratings, offering a granular understanding of viewer satisfaction and engagement.\n\n# Content\n\n##### Anime.csv\n\nanime_id - unique id identifying an anime.\nname - full name of anime.\ngenre - comma separated list of genres for this anime.\ntype - movie, TV, OVA, etc.\nepisodes - how many episodes in this show. (1 if movie).\nrating - average rating out of 10 for this anime.\nmembers - number of community members that are in this anime's\n\"group\".\n\n##### Rating.csv\n\nuser_id - non identifiable randomly generated user id.\nanime_id - the anime that this user has rated.\nrating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).\n","135":"<h1>\ud83d\udda5\ufe0f My Animes List Datasets - 2023 \ud83d\udda5\ufe0f<\/h1>\n\n    Listed Animes, Users and Ratings on MyAnimeList (MAL)\n\n----\n\n<h2>\ud83d\udcc5 Extraction Period \ud83d\udcc5<\/h2>\n\nThe data was extracted between August 1th in 2023 and October 6th in 2023 via Python Programming Language and using [anime-data-scrapper](https:\/\/github.com\/CSFelix\/anime-data-scrapper) available as a Repository on GitHub.\n\n---\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    anime-dataset-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all listed Animes in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Anime ID** - anime id on MyAnimeList;\n- **Name** - anime original name;\n- **English Name** - English version name;\n- **Other Name** - Japanese version name;\n- **Score** - weighted\/Bayesian average score. You can check out for more details at [\"How are MyAnimeList scores calculated?\"](https:\/\/myanimelist.net\/info.php?go=topanime);\n- **Genres** - related genres;\n- **Synopsis** - briefly description;\n- **Type** - type of animation (movie, anime, OVA...);\n- **Episodes** - number of episodes. Movies are considered having 1 episode;\n- **Aired** - period when anime was aired;\n- **Premiered** - season when the anime was released;\n- **Status** - current status (airing, hiatus, finished...);\n- **Producers** - related production companies;\n- **Licensors** - related streaming platforms and licensors;\n- **Studios** - related animation studios;\n- **Source** - source material of the anime (originated from manga, light novel, movie or tv);\n- **Duration** - duration of the movie or each episode;\n- **Rating** - age restriction;\n- **Rank** - rank position on MyAnimeList website (based on Score criteria);\n- **Popularity** - popularity position on MyAnimeList;\n- **Favorites** - number of users that marked the anime as favorite;\n- **Scored By** - number of users that rated the anime;\n- **Members** - number of users that added the anime to the watch list;\n- **Image Url** - banner url.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-details-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all users in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Mal ID** - user id;\n- **Username** - nickname;\n- **Gender** - user gender;\n- **Birthday** - birthday;\n- **Location** - user's location or country;\n- **Joined** - the joined date on MyAnimeList Platform (ISO format);\n- **Days Watched** - total number of days the user spent on MyAnimeList;\n- **Mean Score** - the average score the user gives to the watched animes;\n- **Watching** - number of animes currently being watched by the user;\n- **Completed** - number of animes finished by the user;\n- **On Hold** - number of animes that the user stopped watching but kept it into its list;\n- **Dropped** - number of animes that the user stopped watching and removed from its list;\n- **Plan to Watch** - number of animes that the user has added into the list but did not started watching;\n- **Total Entries** - total number of animes into the user's list;\n- **Rewatched** - number of animes rewatched;\n- **Episodes Watched** - number of episodes watched from all animes.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-score-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all ratings from all users on MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **User ID** - user id on MyAnimeList Platform;\n- **Username** - nickname;\n- **Anime ID** - anime id on MyAnimeList Platform;\n- **Anime Title** - anime original name;\n- **Rating** - score that the user rated the anime.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    .\/benchmark\/full-benchmark.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nData about all Recommendation System Algorithms' performances.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Iteration** - iteration running number;\n- **Algorithm** - filtering recommendation and data basis approach;\n- **Execution Time** - execution time in seconds;\n- **Average CPU Usage** - average CPU usage in percent over the iteration;\n- **Minimum CPU Usage** - minimum CPU usage in percent over the iteration;\n- **Maximum CPU Usage** - maximum CPU usage in percent over the iteration;\n- **Average RAM Usage** - average RAM usage in percent over the iteration;\n- **Minimum RAM Usage** - minimum RAM usage in percent over the iteration;\n- **Maximum RAM Usage** - maximum RAM usage in percent over the iteration.\n\n----\n\n<h2>\ud83d\udd10 Others Datasets \ud83d\udd10<\/h2>\n\nAll others datasets has been created from one of the three previous ones.\n\n----\n\n<h2>\ud83c\udf89 Acknowledgements \ud83c\udf89<\/h2>\n\nThanks to:\n\n1. [Sajid](https:\/\/www.kaggle.com\/dbdmobile) for providing the [Anime Dataset 2023](https:\/\/www.kaggle.com\/datasets\/dbdmobile\/myanimelist-dataset) that inspired this whole dataset.\n\n----\n\n<h2>\u2696\ufe0f Dataset License \u2696\ufe0f<\/h2>\n\n    MIT License\n\nMore details are available on the [Open Source Initiative - The MIT License](https:\/\/opensource.org\/license\/mit\/).","136":"**Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis.","137":"- **\"This dataset contains monthly percentage returns of constituent stocks comprising the Nifty Energy Index.\"\n\n\"Each observation includes the percentage change in stock prices for leading energy companies listed on the National Stock Exchange of India (NSE).\"\n\n\"The dataset spans multiple years, offering insights into the volatility and performance trends within the energy sector.\"\n\n\"Researchers and analysts can utilize this dataset to analyze market dynamics, assess risk, and develop predictive models for energy stock movements.\"\n\n\"Ideal for studying correlations between individual stock performances and broader market trends affecting the energy sector.\"**","138":"#### **Description:**\n\nExplore a comprehensive dataset combining internal banking data and CIBIL credit information from a leading Indian bank. This dataset is ideal for developing predictive credit risk models and gaining valuable financial insights. It includes detailed information on customer transactions, credit scores, and more, providing a robust foundation for advanced analytics and risk assessment.\n\n#### Features for Internal Banking Data \n\n| Variable Name               |                         Description                                   |\n|---------------------------|-----------------------------------------------|\n| Total_TL                  | Total trade lines\/accounts in Bureau          |\n| Tot_Closed_TL             | Total closed trade lines\/accounts             |\n| Tot_Active_TL             | Total active accounts                         |\n| Total_TL_opened_L6M       | Total accounts opened in last 6 Months        |\n| Tot_TL_closed_L6M         | Total accounts closed in last 6 months        |\n| pct_tl_open_L6M           | Percent accounts opened in last 6 months      |\n| pct_tl_closed_L6M         | Percent accounts closed in last 6 months      |\n| pct_active_tl             | Percent active accounts                       |\n| pct_closed_tl             | Percent closed accounts                       |\n| Total_TL_opened_L12M      | Total accounts opened in last 12 Months       |\n| Tot_TL_closed_L12M        | Total accounts closed in last 12 months       |\n| pct_tl_open_L12M          | Percent accounts opened in last 12 months     |\n| pct_tl_closed_L12M        | Percent accounts closed in last 12 months     |\n| Tot_Missed_Pmnt           | Total missed Payments                         |\n| Auto_TL                   | Count of Automobile accounts                  |\n| CC_TL                     | Count of Credit card accounts                 |\n| Consumer_TL               | Count of Consumer goods accounts              |\n| Gold_TL                   | Count of Gold loan accounts                   |\n| Home_TL                   | Count of Housing loan accounts                |\n| PL_TL                     | Count of Personal loan accounts               |\n| Secured_TL                | Count of secured accounts                     |\n| Unsecured_TL              | Count of unsecured accounts                   |\n| Other_TL                  | Count of other accounts                       |\n| Age_Oldest_TL             | Age of oldest opened account                  |\n| Age_Newest_TL             | Age of newest opened account                  |\n\n\n\n#### Features for External Cibil Data \n\n**Variable Description**\n\n| Variable                         | Description                                      |\n|----------------------------------|--------------------------------------------------|\n| time_since_recent_payment        | Time Since recent Payment made                   |\n| time_since_first_deliquency      | Time since first Delinquency (missed payment)    |\n| time_since_recent_deliquency     | Time Since recent Delinquency                    |\n| num_times_delinquent             | Number of times delinquent                       |\n| max_delinquency_level            | Maximum delinquency level                        |\n| max_recent_level_of_deliq        | Maximum recent level of delinquency              |\n| num_deliq_6mts                   | Number of times delinquent in last 6 months      |\n| num_deliq_12mts                  | Number of times delinquent in last 12 months     |\n| num_deliq_6_12mts                | Number of times delinquent between last 6 months and last 12 months |\n| max_deliq_6mts                   | Maximum delinquency level in last 6 months       |\n| max_deliq_12mts                  | Maximum delinquency level in last 12 months      |\n| num_times_30p_dpd                | Number of times 30+ dpd                          |\n| num_times_60p_dpd                | Number of times 60+ dpd                          |\n| num_std                          | Number of standard Payments                      |\n| num_std_6mts                     | Number of standard Payments in last 6 months     |\n| num_std_12mts                    | Number of standard Payments in last 12 months    |\n| num_sub                          | Number of substandard payments - not making full payments |\n| num_sub_6mts                     | Number of substandard payments in last 6 months  |\n| num_sub_12mts                    | Number of substandard payments in last 12 months |\n| num_dbt                          | Number of doubtful payments                      |\n| num_dbt_6mts                     | Number of doubtful payments in last 6 months     |\n| num_dbt_12mts                    | Number of doubtful payments in last 12 months    |\n| num_lss                          | Number of loss accounts                          |\n| num_lss_6mts                     | Number of loss accounts in last 6 months         |\n| num_lss_12mts                    | Number of loss accounts in last 12 months        |\n| recent_level_of_deliq            | Recent level of delinquency                      |\n| tot_enq                          | Total enquiries                                  |\n| CC_enq                           | Credit card enquiries                            |\n| CC_enq_L6m                       | Credit card enquiries in last 6 months           |\n| CC_enq_L12m                      | Credit card enquiries in last 12 months          |\n| PL_enq                           | Personal Loan enquiries                          |\n| PL_enq_L6m                       | Personal Loan enquiries in last 6 months         |\n| PL_enq_L12m                      | Personal Loan enquiries in last 12 months        |\n| time_since_recent_enq            | Time since recent enquiry                        |\n| enq_L12m                         | Enquiries in last 12 months                      |\n| enq_L6m                          | Enquiries in last 6 months                       |\n| enq_L3m                          | Enquiries in last 3 months                       |\n| MARITALSTATUS                    | Marital Status                                   |\n| EDUCATION                        | Education level                                  |\n| AGE                              | Age                                              |\n| GENDER                           | Gender                                           |\n| NETMONTHLYINCOME                 | Net Monthly Income                               |\n| Time_With_Curr_Empr              | Time with current Employer                       |\n| pct_of_active_TLs_ever           | Percent active accounts ever                     |\n| pct_opened_TLs_L6m_of_L12m       | Percent accounts opened in last 6 months to last 12 months |\n| pct_currentBal_all_TL            | Percent current balance of all accounts          |\n| CC_utilization                   | Credit card utilization                          |\n| CC_Flag                          | Credit card Flag                                 |\n| PL_utilization                   | Personal Loan utilization                        |\n| PL_Flag                          | Personal Loan Flag                               |\n| pct_PL_enq_L6m_of_L12m           | Percent enquiries PL in last 6 months to last 12 months |\n| pct_CC_enq_L6m_of_L12m           | Percent enquiries CC in last 6 months to last 12 months |\n| pct_PL_enq_L6m_of_ever           | Percent enquiries PL in last 6 months to last 6 months |\n| pct_CC_enq_L6m_of_ever           | Percent enquiries CC in last 6 months to last 6 months |\n| max_unsec_exposure_inPct         | Maximum unsecured exposure in percent            |\n| HL_Flag                          | Housing Loan Flag                                |\n| GL_Flag                          | Gold Loan Flag                                   |\n| last_prod_enq2                   | Latest product enquired for                      |\n| first_prod_enq2                  | First product enquired for                       |\n| Credit_Score                     | Applicant's credit score                         |\n| Approved_Flag                    | Priority levels                                  |\n\n","139":"This dataset provides insights into loan applicants' characteristics and their risk assessment. It comprises information on various attributes of loan applicants, including demographic details, financial status, employment history, and ownership status. The dataset includes both numerical and categorical features, making it suitable for diverse analytical approaches.\n\nKey Features:\n\n1. Id: Unique identifier for each loan applicant.\n2. Income: The income level of the applicant.\n3. Age: Age of the applicant.\n4. Experience: Years of professional experience.\n5. Married\/Single: Marital status of the applicant.\n6. House_Ownership: Indicates whether the applicant owns or rents a house.\n7. Car_Ownership: Indicates whether the applicant owns a car.\n8. Profession: Occupation or profession of the applicant.\n9. CITY: City of residence of the applicant.\n10. STATE: State of residence of the applicant.\n11. CURRENT_JOB_YRS: Duration of employment in the current job.\n12. CURRENT_HOUSE_YRS: Duration of residence in the current house.\n13. Risk_Flag: Binary indicator of loan risk, where 1 represents a flagged risky applicant and 0 represents a non-risky applicant.\n\nThis dataset contains 252,000 entries and provides a comprehensive overview of loan applicants' profiles, enabling analysis and modeling for risk assessment and decision-making in lending processes.","140":"The following steps were taken to prepare the dataset:\n\n1.1. Video Recording: Videos of the gestures (thumbs up, fist, open palm) were recorded for both the left and right hands.\n1.2. Frame Extraction: Frames were extracted from the videos, resulting in a total of 250 images for each gesture from both the left and right hands. Each image is of dimension: 64X64\n\nThere are 3 gestures : fist, open_palm and thumbs_up\n\nEach image\/frame has a name starting with \u2018l\u2019 or \u2018r\u2019 where \u2018l\u2019 refers to left and \u2018r\u2019 refers to right. \nAnd second character as \u2018f\u2019 ,\u2018t\u2019 , or \u2018p\u2019 which stands for fist, thumbs up and open palm, respectively.\nFor example, image named as lf_0011 means it is an image of left fist taken from video\u2019s 11th frame. \nIt was carried out with the help of OpenCV (cv2) and os libraries as shown in code in figure 1 and the frames were downloaded as a zip file with the help of code in figure 2.\n\n1.3. Dataset Splitting: The total dataset consisted of 1500 images (250 images per gesture per hand).\nThe dataset was split into training and testing sets:\nTraining Set: 300 images (150 images for left and right hands combined) for each gesture.\nTesting Set: 200 images (100 images for left and right hands combined) for each gesture.\n","141":"# Gesture Recognition Data in Online Conference Scenes\n\n\n## Description\n2,341 People Gesture Recognition Data in Meeting Scenes includes Asians, Caucasians, blacks, and browns, and the age is mainly young and middle-aged. It collects a variety of indoor office scenes, covering meeting rooms, coffee shops, libraries, bedrooms, etc. Each person collected 18 pictures and 2 videos. The pictures included 18 gestures such as clenching a fist with one hand and heart-to-heart with one hand, and the video included gestures such as clapping.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1292?source=Kaggle\n\n## Data size\n2,341 people, each person collects 2 videos and 18 images\n## Race distribution\n786 Asians, 1,002 Caucasians, 401 black, 152 brown people\n## Gender distribution\n1,209 males, 1,132 females\n## Age distribution\nfrom teenagers to the elderly, mainly young and middle-aged\n## Collection environment\nindoor office scenes, such as meeting rooms, coffee shops, libraries, bedrooms, etc.\n## Collection diversity\ndifferent gestures data, different races, different age groups, different meeting scenes\n## Collection equipment\ncellphone, using the cellphone to simulate the perspective of laptop camera in online conference scenes\n## Collection content\ncollecting the  gestures data in online conference scenes\n## Data format\n.mp4, .mov, .jpg\n## Accuracy rate\nthe accuracy exceeds 97% based on the accuracy of the actions; the accuracy of action naming is more than 97%\n# Licensing Information\nCommercial License\n","142":"There are 300 images in train folder and 200 images in test folder. Dimension of each image is 64px X 64px. Each image contains one of the following three hand gestures :\n- Open Palm\n- Thumbs Up\n- Fist","143":"The Enhanced Sign Language MNIST dataset is a comprehensive collection of grayscale images representing American Sign Language (ASL) gestures. This dataset serves as an enhancement to the original Sign Language MNIST dataset, providing a more diverse and extensive set of hand gesture samples for machine learning tasks.\n\nInspired by the need for more challenging benchmarks in image-based machine learning, this dataset is consistent with the original [Sign Language MNIST dataset](https:\/\/www.kaggle.com\/datasets\/datamunge\/sign-language-mnist\/data) to acquire a self-generated dataset, resulting in a more robust and varied collection of hand gesture images. The original Sign Language MNIST dataset, available on Kaggle, provided a solid foundation with 27,455 training cases and 7,172 test cases, each representing a label (0-25) mapped to an alphabetic letter A-Z (excluding J and Z). \n\nThe Enhanced Sign Language MNIST dataset builds upon this foundation by incorporating additional images generated through a process involving various image manipulation techniques. These techniques include hand tracking using MediaPipe, cropping, grayscale conversion, and resizing, to create approximately 1400 samples of each alphabetic letter. The enhanced dataset contains 69,252 samples in total, with 55,402 samples for training and validation, and 13,850 samples for testing.\n\nThis dataset is invaluable for researchers and developers working on sign language recognition, hand gesture detection, and related computer vision tasks. It offers a challenging benchmark for evaluating the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in recognizing ASL gestures.\n\nThe dataset is divided into training and testing sets following the methodology outlined in [Oladayo's research (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2), ensuring the consistency and reproducibility of experimental setups. The experimentation framework incorporated four distinct Convolutional Neural Network (CNN) models: CNN1, CNN2, CNN3, and CNN4. Additionally, four diverse data augmentation techniques were employed, denoted as DAM1, DAM2, DAM3, and DAM4. Notably, DAM1 represents the scenario where no data augmentation is applied.\n\n\nCNN2 achieved a remarkable 99.89% validation accuracy on the enhanced test samples and 99.78% on the generated test samples. Training the model on a GPU\/TPU took approximately 209 seconds (3.5 minutes), which is close to the results reported in the research report. This success underscores the effectiveness of sample generation in enhancing the model's performance, showcasing its superiority over traditional data augmentation methods.\n\nWith the Enhanced Sign Language MNIST dataset, researchers can explore new approaches to sign language recognition, develop more robust machine learning models, and ultimately contribute to the advancement of assistive technologies for the deaf and hard-of-hearing community.\n\nIf you use this code or the datasets in your research, please cite the following dissertation:\n[Oladayo Luke. (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2). Enhancing Sign Language Recognition and Hand Gesture Detection using Convolutional Neural Networks and Data Augmentation Techniques. (Doctoral dissertation, Nova Southeastern University).","144":"The Bangla Sign Language Video Dataset is a comprehensive collection of videos captured from the Badhir School in Dhaka, Bangladesh. This dataset is meticulously curated, containing over 8000 videos showcasing sign language gestures corresponding to 40 common Bangla words. Developed with the aim of facilitating research and development in the field of sign language recognition and interpretation, this dataset serves as a valuable resource for both academia and industry professionals.\n\nAs part of the preprocessing pipeline for the Bangla Sign Language Video Dataset, advanced techniques were employed to detect the hand region and remove the background from the captured videos. This crucial preprocessing step enhances the quality and usability of the dataset for subsequent analysis and research endeavors in sign language recognition and interpretation.\n\nHand Region Detection:\nUtilizing state-of-the-art computer vision algorithms, the hand region in each video frame was accurately identified. MediaPipe, a robust framework for building multimodal perceptual pipelines, was employed for its efficiency and reliability in hand detection tasks. By isolating the hand region, the dataset ensures a focused and standardized input for subsequent processing stages, laying the foundation for precise gesture analysis.\n\nBackground Removal:\nFollowing hand region detection, sophisticated background removal techniques were applied to eliminate extraneous visual elements from the video frames. This process enhances the clarity and saliency of the hand gestures, minimizing distractions and facilitating more accurate feature extraction during subsequent analysis stages. Through meticulous background removal, the dataset offers researchers and developers clean and unambiguous data for training and evaluation purposes.","145":"\nThe \"Hand Gesture Landmark\" dataset is an image dataset designed for hand gesture recognition tasks. It encompasses a variety of hand navigation movements, each categorized into one of nine distinct classes: \n\n anticlockwise,\n clockwise,\n up,\n down,\n left,\n right,\n forward,\n backward. \n\nThe dataset is generated utilizing the capabilities of both Mediapipe and OpenCV technologies.\n\nStructured into separate directories for training, evaluation, and testing, the dataset ensures effective model training and evaluation processes. Within the training directory, there are 2000 images for each class, providing ample data for robust model learning. The test directory contains 300 images for each class, enabling comprehensive model testing across various scenarios. Lastly, the evaluation directory includes 900 images, likely intended for further validation or fine-tuning of trained models. With its comprehensive coverage of hand gestures and sufficient data distribution, the \"Hand Gesture Landmark\" dataset serves as a valuable resource for researchers and developers working on gesture recognition applications.","146":"","147":"This dataset comprises 20,000 images of hand gestures, divided into 10 distinct classes. Each class represents a specific hand gesture commonly used in gesture recognition tasks. The dataset is organized into \"train\" and \"val\" folders, adhering to the structure recommended for YOLOv8 data preparation.\n\nThe hand gestures included in the dataset are meticulously labeled to facilitate training and evaluation of YOLOv8-based models for hand gesture recognition applications. Researchers, developers, and enthusiasts interested in exploring deep learning-based approaches for gesture recognition tasks can utilize this dataset to train and validate their models effectively.\n\nPlease note that the dataset is provided solely for research and educational purposes. Any commercial use or redistribution of this dataset should be done with appropriate permissions and acknowledgments. We hope this dataset contributes to advancements in gesture recognition technology and encourages further exploration in this field.","148":"Sign language is a cardinal element for communication between deaf and dumb community. Sign language has its own grammatical structure and gesticulation nature. Research on SLRT focuses a lot of attention in gesture identification. Sign language comprises of manual gestures performed by hand poses and non-manual features expressed through eye, mouth and gaze movements. \n\nThe sentence-level completely labelled Indian Sign Language dataset for Sign Language Translation and Recognition (SLTR) research is developed. The ISL-CSLTR dataset assists the research community to explore intuitive insights and to build the SLTR framework for establishing communication with the deaf and dumb community using advanced deep learning and computer vision methods for SLTR purposes. This ISL-CSLTR dataset aims in contributing to the sentence level dataset created with two native signers from Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India and four student volunteers from SASTRA Deemed University, Thanjavur, Tamilnadu. The ISL-CSLTR corpus consists of a large vocabulary of 700 fully annotated videos, 18863 Sentence level frames, and 1036 word level images for 100 Spoken language Sentences performed by 7 different Signers. This corpus is arranged based on signer variants and time boundaries with fully annotated details and it is made available publicly.\n\nThe main objective of creating this sentence level ISL-CSLRT corpus is to explore more research outcomes in the area of SLTR. This completely labelled video corpus assists the researchers to build framework for converting spoken language sentences into sign language and vice versa. This corpus has been created to address the various challenges faced by the researchers in SLRT and significantly improves translation and recognition performance. The videos are annotated with relevant spoken language sentences provide clear and easy understanding of the corpus data.\n\nAcknowledgements:\nThe research was funded by the Science and Engineering Research Board (SERB), India under Start-up Research Grant (SRG)\/2019\u20132021 (Grant no. SRG\/2019\/001338). And also, we thank all the signers for their contribution in collecting the sign videos and the successful completion of the ISL-CSLTR corpus. We would like to thank Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India for their support and contribution. ","149":"# Hand Gesture Recognition Dataset\n\n## Overview\n\nThis dataset contains images for hand gesture recognition, supporting research and development in the field of human-computer interaction, particularly in augmented reality (AR) and virtual reality (VR) applications.\n\n## Dataset Description\n\nThe dataset consists of hand images captured using a standard camera, with corresponding labels for hand gestures. The text input method is based on hand-gesture recognition using a trained neural network. The process involves hand segmentation, gesture recognition, and hand movement tracking using a convex hull algorithm.\n\n## License\n\nThis dataset is provided under the license of Nizamuddin and Nooruddin. Please refer to the LICENSE.md file for detailed licensing information.\n\n## Usage\n\nResearchers and developers are encouraged to use this dataset for various applications, including but not limited to:\n\n- Human-Computer Interaction\n- Augmented Reality (AR) Systems\n- Virtual Reality (VR) Systems\n\n## Dataset Structure\n\nThe dataset is organized into folders containing hand images and corresponding labels. The file structure is as follows:\n\n- `images\/`: Directory containing hand images.\n- `labels.csv`: CSV file containing labels for each hand image.\n\n## Citation\n\nIf you use this dataset in your work, please cite the following paper:\n\nN. Nooruddin, R. Dembani and N. Maitlo, \"HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices,\" 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Toronto, ON, Canada, 2020, pp. 744-751, doi: 10.1109\/SMC42975.2020.9283348.\n\n**Keywords:** Human computer interaction; Solid modeling; Image segmentation; Computational modeling; Neural networks; Gesture recognition; Computer architecture; Augmented Reality; Virtual Reality; Hand Segmentation; Hand Gesture Recognition; Text-Based Input; CNN.\n\n@InProceedings{Nooruddin_2020_SMC,\n    author    = {Nooruddin, N. and Dembani, R. and Maitlo, N.},\n    title     = {HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices},\n    booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},\n    month     = {October},\n    year      = {2020},\n    pages  &nbsp;&nbsp;&nbsp;=&nbsp;{744-751}\n}\n\nPlease visit our linkedin for further details:\n- [Mr. Nizamuddin Maitlo](https:\/\/www.linkedin.com\/in\/nizamuddin-maitlo-b27417a0\/)\n- [Dr. Nooruddin Noonari](https:\/\/www.linkedin.com\/in\/noonari\/)\n","150":"This dataset features detailed sensor data from smart gloves designed to translate sign language into spoken language. The gloves were equipped with **5 flex sensors** and a \"**[Grove - 6 - Axis Accelerometer & Gyroscope](https:\/\/wiki.seeedstudio.com\/Grove-6-Axis_AccelerometerAndGyroscope\/)**\" for each hand. The dataset consists of different files, each recorded by a different individual, showcasing a variety of hand movements and signs. Key features include:\n**- `Flex-Left\/Right-p-Frame-n` :** Measures the degree of flexion for each finger over 20 frames, with 'n' indicating the frame and 'p' the finger number.\n**- `Acceleration-X\/Y\/Z-Left\/Right-Frame-n` :** Captures the hand's acceleration in 3D space across 20 frames.\n**- `Orientation-X\/Y\/Z-Left\/Right-Frame-n` :** Details the hand's orientation in 3D space for each frame.\n So for each hand, there are 3 types of measurements (Flex, Acceleration, Orientation), each with 3 axes (X, Y, Z) for Acceleration and Orientation, and 5 measurements for Flex (one per finger), resulting in 11 measurements per hand per frame. Multiplied by 20 frames, and then doubled for both hands, this amounts to **440 columns**. Additionally, there's a **\"SIGN\" column**, which is the target for each data entry, identifying the specific sign language gesture.\n\nEach recording in the dataset represents a sequence of 20 frames, capturing detailed motion for both left and right hands.\n\n**This dataset is not yet complete and is still being collected.**","151":"This dataset is a collection of images designed for training machine learning models in the recognition and classification of Malaysian Sign Language (MSL) signs. It encompasses three distinct categories, each critical for a comprehensive understanding of MSL:\n\nAlphabets (Alphabet_MSL): This segment consists of images depicting individual letters of the MSL alphabet. Each image is a clear, focused representation of a hand gesture corresponding to an MSL letter, formatted in .jpg for optimal compatibility with machine learning algorithms.\n\nSingle Words (SingleWord_MSL): Here, key basic words used in MSL communication are visually represented. This category is vital for models intended to recognize common vocabulary in sign language.\n\nNumbers (Number_MSL): This part contains images illustrating numeric signs in MSL, covering a range from simple digits.","152":"A dataset comprising 14,000 images of hand gestures has been created. These images were manually captured. The dataset, as yet unnamed, offers a substantial resource for various applications in computer vision and machine learning, particularly in the realm of gesture recognition and sign language interpretation. The dataset's size and diversity make it a valuable asset for researchers and developers.\n\nPotential uses for this dataset include training and evaluating machine learning models for gesture recognition, enabling research in computer vision, supporting accessibility and assistive technology development, and contributing to educational and research initiatives. Depending on the dataset's licensing and labeling, it can be made publicly available for broader use within the research community, fostering advancements in the field.\n\nDocumentation, labeling, and clear licensing terms are essential considerations for sharing the dataset with the wider community.\n\n\n\n\n","153":"This dataset comprises surface electromyography (sEMG) recordings acquired for myoelectric control applications. Collected using the Myo Armband device, the dataset encompasses ten distinct hand motion classes. These recordings serve as a valuable resource for researchers and practitioners in the fields of biomedical signal processing, machine learning, and human-computer interaction. The dataset's accessibility and diversity make it suitable for training, testing, and optimizing sEMG-based hand gesture recognition algorithms. By sharing this dataset, we aim to foster advancements in myoelectric control technologies and contribute to the development of more intuitive and efficient human-machine interfaces.","154":"The Hand Gesture Recognition Dataset is a comprehensive collection designed for training and evaluating machine learning models for hand gesture recognition tasks. This dataset comprises 24 distinct classes, representing each letter of the alphabet excluding 'J' and 'Z'. Each gesture is depicted against a consistent black background, enhancing the clarity of the hand signs. The visual representation of the gestures is presented as wireframes, providing a clear outline of the hand's position.\n\n\nKey Features:\n\n24 Distinct Classes: The dataset includes signs corresponding to all letters of the alphabet, except 'J' and 'Z'. These two classes require video input so they were removed from the still image dataset. \n\nConsistent Background: All gestures are set against a black background, ensuring uniformity across the dataset. This consistent background simplifies preprocessing tasks and ensures that models are trained on signs alone, without any background distractions.\n\nPrecise Hand Landmarks: The dataset's accuracy is enhanced by utilizing Google MediaPipe to extract detailed hand landmarks. These landmarks are essential for understanding the intricate movements of fingers, enabling the development of highly accurate gesture recognition models.\n","155":"The dataset is a collection of hand gesture and expression videos captured using a webcam and processed using the OpenCV and Mediapipe libraries. It consists of 33 classes, each representing a different hand gesture or expression. For each class, there are 30 videos, and each video contains 75 frames. The dataset is organized into directories, with each class having its own directory. Within each class directory, there are 30 subdirectories numbered from 0 to 29, each containing 75 frames as numpy arrays.\n\nThe keypoints of the hand in each frame were detected using the Mediapipe library, resulting in numpy arrays that represent the coordinates of these keypoints. These keypoints are crucial for understanding the hand's pose and movement in each frame. The dataset is intended to support various applications, such as gesture recognition and machine learning model development, offering a substantial resource for research and development in these domains.","156":"**Description**\nThe data diversity includes multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, and multiple light conditions. In terms of data annotation, 21 landmarks, gesture types, and gesture attributes were annotated. This dataset can be used for tasks such as gesture recognition and sign language translation.\nFor more details, please visit: https:\/\/www.nexdata.ai\/datasets\/980?source=Kaggle\n\n**Specifications**\nData size\n180,717 images, including 83,012 images of static gestures, 97,705 images of dynamic gestures\nPopulation distribution\nthe race distribution is Asian, the gender distribution is male and female, the age distribution is mainly young people and middle-aged people\nCollection environment\nincluding indoor scenes and outdoor scenes\nCollection diversity\nincluding multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, multiple light conditions\nDevice\ncellphone\nData forma\nthe image data format is .jpg, the annotation file format is .json\nCollecting content\nsign language gestures were collected in different scenes\nAnnotation content\n21 landmarks annotation (each landmark includes the attribute of visible or invisible), gesture type annotation, gesture attributes annotation (left hand or right hand)\nAccuracy\naccuracy requirement: the point location errors in x and y directions are less than 3 pixels, which is considered as a qualified annotation; accuracy of landmark annotation: the annotation part (each landmark) is regarded as the unit, the accuracy rate shall be more than 95%\n\n**Get the Dataset**\nThis is just an example of the data. To access more sample data or request the price, contact us  at info@nexdata.ai","157":"The Hand Sign Recognition Image Dataset is a rich collection of high-quality images featuring diverse **single-hand signs**. With comprehensive coverage, annotations, and multi-modal data, it's an ideal resource for developing sign language recognition models. Whether for sign language translation or gesture-based interaction, this dataset supports a range of applications and is ready for machine learning integration.","158":"## Data Sources:\nThis Dataset is collected from HAnd Gesture Recognition Image Dataset(HaGRID) and its 512p version created by [Innominate817](https:\/\/www.kaggle.com\/innominate817). I modify it  for gesture classification.\nHaGRID: https:\/\/github.com\/hukenovs\/hagrid\nHaGRID-512p: https:\/\/www.kaggle.com\/datasets\/innominate817\/hagrid-classification-512p\n## Data Analysis:\nit includes 2 gestures: thumbs up (like) and thumbs down(dislike).\nThe statistics for each sample are as follows:\n| shape | train | valid | test | total|\n| --- | --- | --- | --- | --- |\n| like | 16554 | 5518 | 5519 | 27591|\n| dislike | 17007 | 5669 | 5669 | 28345|\n\n## Usage:\nThis dataset can be used in gesture classification. My demo can be seen on GitHub: https:\/\/github.com\/RuiNov1st\/InternWork\/tree\/main\/L6_GestureClassification. I use pretrained ResNet18 as base model and obtain its capability to recognize 'thumb up' and 'thumb down' gestures through retraining. The accuracy in the test set can reach 99%. I also take some my own gestures photos for testing, but now the performance is not good.\nAny suggestions will be greatly appreciated! \n","159":"### TRAIN \/ TEST SPLIT:\n\n**Training Set:** 70% of the dataset with 642 images\n**Validation Set:** 19% of the dataset with 178 images\n**Testing Set:** 10% of the dataset with 94 images\n### PREPROCESSING:\n\n**Auto-Orient:** Applied to ensure consistent orientation of images\n**Resize:** All images have been stretched to a standard size of 640x640 for uniformity\n\n### AUGMENTATIONS:\n\nNo augmentations were applied to the dataset, preserving the original image integrity.\n\n**_annotations.csv contains following columns:**\n\n**filename:** The name of the image file associated with the annotation.\n**width:** The width of the image in pixels.\n**height:** The height of the image in pixels.\n**class:** The class label of the hand gesture present in the image.\n**xmin:** The x-coordinate of the top-left corner of the bounding box around the hand gesture.\n**ymin:** The y-coordinate of the top-left corner of the bounding box around the hand gesture.\n**xmax:** The x-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n**ymax:** The y-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n\n### Here are some potential use cases for this dataset:\n\n**Gesture-Controlled User Interfaces:** The dataset can be used to build gesture-controlled user interfaces for various devices and applications, such as smartphones, tablets, computers, and IoT devices.\n**Sign Language Translation:** Hand gestures are an essential part of sign language used by the hearing-impaired community. This dataset can be used to develop sign language recognition systems that can translate sign language into text or speech, enabling better communication with non-sign language users.\n\n","160":"The dataset containing reviews of British Airways has been meticulously gathered from the Skytrax website. This dataset is intended for comprehensive data analysis and visualization. By analyzing this data, we aim to uncover insights and trends regarding customer experiences and satisfaction levels with British Airways. The visualization of this data will further enhance the understanding by presenting the findings in a clear and engaging manner, making it easier to identify patterns and areas for improvement.","161":"The [\/kaggle\/input\/online-review-csv\/online_review.csv ](url)file contains customer reviews from Flipkart. It includes the following columns:\n\nreview_id: Unique identifier for each review.\nproduct_id: Unique identifier for each product.\nuser_id: Unique identifier for each user.\nrating: Star rating (1 to 5) given by the user.\ntitle: Summary of the review.\nreview_text: Detailed feedback from the user.\nreview_date: Date the review was submitted.\nverified_purchase: Indicates if the purchase was verified (true\/false).\nhelpful_votes: Number of users who found the review helpful.\nreviewer_name: Name or alias of the reviewer.\nUses\nSentiment Analysis: Understand customer sentiments.\nProduct Improvement: Identify areas for product enhancement.\nMarket Research: Analyze customer preferences.\nRecommendation Systems: Improve recommendation algorithms.\nThis dataset is ideal for practicing data analysis and machine learning techniques.","162":"This dataset captures customer reviews of **British Airways** obtained from the website[ https:\/\/www.airlinequality.com\/airline-reviews\/scoot](url). It consists of raw, uncleaned data directly scraped from the source, providing a rich resource for honing skills in **data cleaning, data visualization, analysis, and potentially predicting customer sentiment**.\n\nThe dataset includes various attributes such as review text, ratings, date of travel, and possibly more contextual information relevant to customer experiences with British Airways. Given its raw state, the dataset presents an opportunity to practice preprocessing techniques to ensure data quality and prepare it for deeper analytical insights.","163":"## Capterra Reviews for Ticket Systems\n\n### Dataset Description\n\nThis dataset contains reviews of various ticket systems from Capterra, providing insights into the features, pros, and cons as experienced by users. The dataset is designed to facilitate analysis and comparison of different ticket systems based on user feedback. Each review includes ratings for various aspects such as ease of use, customer service, features, and overall value for money, as well as the likelihood to recommend the system.\n\n### Content\n\nThe dataset consists of the following columns:\n\n- **ticket_system**: The name of the ticket system being reviewed.\n- **title**: The title of the review.\n- **overall_text**: The overall text of the review.\n- **pros_text**: The positive aspects of the ticket system as mentioned by the reviewer.\n- **cons_text**: The negative aspects of the ticket system as mentioned by the reviewer.\n- **overall_rating**: The overall rating given by the reviewer (out of 5).\n- **ease_of_use**: The rating for ease of use (out of 5).\n- **customer_service**: The rating for customer service (out of 5).\n- **features**: The rating for features (out of 5).\n- **value_for_money**: The rating for value for money (out of 5).\n- **likelihood_to_recommend**: The likelihood to recommend the system (out of 10).\n\nIn addition, the dataset includes columns for ten key features, indicating whether each feature was mentioned positively, negatively, or not at all in the review. The feature columns are:\n\n- **Ticket Creation and Assignment**\n- **Status Tracking and Updates**\n- **Priority and SLA Management**\n- **Reporting and Analytics**\n- **Automated Ticket Routing**\n- **Knowledge Base Integration**\n- **Customer and Agent Portals**\n- **Multi-Channel Support (Email, Chat, Phone)**\n- **Email Notifications and Alerts**\n- **Customizable Workflows**\n\nEach feature column contains:\n- `1` if the feature was mentioned positively,\n- `-1` if the feature was mentioned negatively,\n- `0` if the feature was not mentioned.\n\n### Usage\n\nThis dataset can be used for:\n- **Sentiment Analysis**: Analyzing user sentiments towards different features of ticket systems.\n- **Feature Importance**: Identifying which features are most positively or negatively received.\n- **Comparative Analysis**: Comparing different ticket systems based on user reviews and ratings.\n- **Market Research**: Understanding customer needs and preferences in the ticketing system market.\n\n### Acknowledgements\n\nThe reviews in this dataset are sourced from Capterra. The dataset is intended for educational and research purposes.\n\n### Keywords\n\nTicket systems, Capterra reviews, customer service, sentiment analysis, feature analysis, user feedback, software reviews, helpdesk solutions, comparative analysis, market research\n\n### License\n\nThe dataset is available for public use. If you use this dataset, please credit the original source and this dataset on Kaggle.\n\n### Contact\n\nFor any questions or contributions regarding this dataset, please reach out via Kaggle or through the provided contact information in the dataset repository.\ndata@softoft.de","164":"","165":"this graph was created in PowerBi,Loocker studio and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fea63e695d326e3f57b044725376024c2%2Fgraph1.jpg?generation=1718574239511915&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fc285144b368e9b25b2d576d5bf6d4b81%2Fgraph2.png?generation=1718574245118394&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F80adbea61286526a5910b803a8d4822e%2Fgraph3.png?generation=1718574250267201&alt=media)\n\n\nAliExpress Data is vast collection of information and statistics gathered from the AliExpress online marketplace. It includes details about products, sellers, prices, customer reviews, ratings, and other relevant data points. This data can be analyzed to gain insights into market trends, consumer behavior, and business performance, helping businesses make informed decisions and optimize their operations on the platform.\nExamples of AliExpress data include product listings, customer reviews, seller ratings, and transaction details. AliExpress data is used for various purposes such as market research, competitor analysis, trend identification, and customer behavior analysis. In this page, you\u2019ll find the best data sources for AliExpress data, AliExpress dataset, AliExpress data analysis.","166":"This is a synthetic dataset of an online store selling natural cosmetics. It is closely approximated to real data, with logic maintained in the number of sales during certain periods, the influence of user ratings on products, and even the seasonality of some product sales.","167":"## Introduction:\n\nThis dataset contains historical daily exchange rates for the US Dollar (USD) against three major currencies: Pakistani Rupee (PKR), Indian Rupee (INR), and Chinese Yuan (CNY). The data covers the period from January 2004 to June 2024, providing a comprehensive view of currency fluctuations over time.\n\n## About the Dataset\n\nThis dataset contain daily exchange rate from (2004 - 2024) of Dollar to PKR, Dollar to INR, Dollar to Chinese Yuan and contains 9 columns.\n\n## Columns:\n\n- `Date`: The date of the exchange rate.\n- `Open_pkr, High_pkr, Low_pkr, Close_pkr, Adj Close_pkr`: Exchange rate details from USD to PKR.\n- `Open_inr, High_inr, Low_inr, Close_inr, Adj Close_inr`: Exchange rate details from USD to INR.\n- `Open, High, Low, Close, Adj Close`: Exchange rate details from USD to CNY.\n- `PKR_pct_change`: Daily percentage change in the exchange rate from USD to PKR.\n- `INR_pct_change`: Daily percentage change in the exchange rate from USD to INR.\n- `CNY_pct_change`: Daily percentage change in the exchange rate from USD to CNY.\n\n## Source:\nThe data has been compiled from various reliable financial sources to ensure accuracy and consistency.\n\n## Usage:\nThis dataset is valuable for financial analysts, economists, and researchers interested in studying currency trends, economic events, and their impacts on exchange rates. It can also be used for machine learning projects involving time series analysis, forecasting, and financial modeling.\n\n## Acknowledgements:\nWe acknowledge the data sources for providing the necessary exchange rate information.","168":"This dataset provides detailed information on the longest-running Indian television series in the Hindi language, specifically focusing on non-fiction genres. The data encompasses various attributes, including the length of time each show has been on air, the title, the network it was broadcasted on, the genre, start and end dates, the number of episodes, and additional notes. Below is a summary of the dataset structure:\n\n## Dataset Columns:\n- **Length (years)**: The duration for which the show has been running, expressed in years.\n- **Title**: The name of the television series.\n- **Network**: The TV network(s) that aired the show.\n- **Genre**: The category or type of the show (e.g., Agriculture, Music, Reality, Game show, Talk show, Cooking show, Stunt based Reality show, Cultural).\n- **Start date**: The date when the show first premiered.\n- **End date**: The date when the show ended, if applicable.\n- **No. of episodes**: The total number of episodes aired.\n- **Notes**: Additional information or significant achievements related to the show.","169":"This script, a potent analytical tool, is a potential game-changer for music industry stakeholders such as data analysts, marketing strategists, and business decision-makers. It enables users to predict music release dates and trends with precision, leveraging historical data. The script's modularity ensures it can seamlessly adapt to new datasets and analytical methods. In real-life scenarios, the script's placeholder variables and paths can be effortlessly replaced with data fields and file locations, enhancing its practical value. The script's visualisations, predictive modelling, and time series analysis are pivotal for music streaming data-driven decision-making. ","170":"**Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis.","171":"","172":"## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media)","173":"","174":"This dataset contains around 136k posts scrapped form linkedIn from Dec 2022 to May 2023, specifically about the mass layoffs which have occurred during that time.\n\nThe data was scraped on a regular basis through the time period and many of the entries were deduplicated for better usability. ","175":"","176":"","177":"In 2020, Meta (then Facebook) created a quasi-independent, 20 member oversight board with the power to override the company's decisions around content-moderation. These are their major cases and outcomes. ","178":"This dataset comprises articles from the New York Times, covering the israel-hamas war. It is ideal for Natural Language Processing (NLP) tasks such as text classification, sentiment analysis, and named entity recognition (NER). The dataset includes the following features:\n\ncomprises all articles related to israel hamas war published by the nytimes during the year 2024 until 26\/june.\n\nheadline: The headline of the article.\ntext: The full text of the article.","179":"Dataset Overview\nThis dataset comprises a collection of 324 news headlines from various leading companies, structured over several months in 2024. Each entry in the dataset is associated with a specific company and date, along with the primary news headline of that day and the headline from the previous day.\n\nContent\nCompany Name: Indicates the company to which the news is related.\nDate: The publication date of the news.\nNews Headlines: The headline of the news corresponding to the company and date.\nPrevious Day Headlines: News headline from the day before, for the same company.\nPotential Uses\nThis dataset is ideal for temporal text analysis, news trend monitoring, and sentiment analysis. Researchers and data scientists can explore changes in news coverage over time, analyze the impact of news on stock prices, or use the dataset for training machine learning models for text classification or summarization.\n\nAcknowledgements\nThis data was compiled and curated with the intent to provide a rich source for news headline analysis related to major companies. It is presented here for educational and informational purposes.\n\nInspiration\nSome questions and tasks this dataset could inspire include:\n\nHow do headlines about a company change over time?\nCan the impact of news on a company's public perception be quantified?\nDeveloping models that predict the sentiment conveyed in the headlines.\nComparing news coverage between companies within the same time frame.","180":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","181":"Description for the tmrospacenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024.","182":"Description for the truecrimenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024.","183":"","184":"This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n","185":"# Information Classification Dataset\n\n## Description\n\nThis dataset is designed for classifying information into three categories: fake, spam, and legit. It combines data from multiple sources to provide a diverse and comprehensive collection.\n\n## Source Attribution\n\n1. **YouTube Spam and Ham Data**:\n   - Source: [YouTube Spam Collection](https:\/\/doi.org\/10.24432\/C58885)\n   - Description: Contains spam and legitimate comments from YouTube videos.\n\n2. **Email Spam Dataset**:\n   - Source: [Email Spam Collection](https:\/\/www.kaggle.com\/datasets\/venky73\/spam-mails-dataset)\n   - Description: Collection of spam and legitimate emails.\n\n3. **SMS Spam Dataset**:\n   - Source: [SMS Spam Collection](https:\/\/www.kaggle.com\/datasets\/uciml\/sms-spam-collection-dataset)\n   - Description: Dataset of spam and legitimate SMS messages.\n\n4. **WELFake Dataset**:\n   - Source: [WELFake Fake News Dataset](https:\/\/www.kaggle.com\/datasets\/vcclab\/welfake-dataset)\n   - Description: Contains fake and legitimate news articles.\n\n5. **GossipCop Dataset**:\n   - Source: [GossipCop Fake News Dataset](Retrieved from https:\/\/github.com\/KaiDMML\/FakeNewsNet )\n   - Description: Dataset of fake and legitimate news articles from GossipCop.\n\nThe dataset was carefully cleaned and processed to ensure quality and consistency. Duplicate entries were removed.\n\n## Files\n\n- `data.csv`: The main dataset file.\n- `README.md`: Detailed description of the dataset creation process and source attribution.\n","186":"","187":"","188":"","189":"","190":"","191":"# Sinhala Ada Derana News Articles Dataset (2008-2024)\n\nThis dataset is a comprehensive collection of news articles from [Sinhala Ada Derana](https:\/\/sinhala.adaderana.lk\/), spanning from 2008 to 2024. It includes **over 200,000** articles, offering a rich resource for a variety of applications in both machine learning and non-machine learning domains.\n\n## Dataset Features:\n- **ID:** Unique identifier for each news article from the website.\n- **Title:** The title of the news article.\n- **Description:** The full text of the news article.\n- **Date:** The publication date of the news article.\n- **URL:** The URL of the image associated with the news article.\n\n## Potential Uses:\n1. **Natural Language Processing (NLP):**\n   - Sentiment analysis\n   - Topic modeling\n   - Named entity recognition\n   - Language translation\n   - Text summarization\n\n2. **Machine Learning:**\n   - Embedding generation\n   - News classification\n   - Fake news detection\n\n3. **Non-Machine Learning Applications:**\n   - Historical analysis\n   - Media studies\n   - Sociopolitical research\n\n## Additional Information:\n- **Historical Context:** This dataset provides a detailed view of the historical news landscape in Sri Lanka, capturing the socio-political and cultural narratives over a significant period.\n- **Data Format:** The dataset is structured in a user-friendly format, making it easy to parse and analyze.\n\nThis dataset serves as a valuable resource for researchers, data scientists, and anyone interested in the evolution of news media in Sri Lanka. It provides insights into the past events, public sentiment, and media trends in the country, offering a broad range of possibilities for analysis and application.\n\nFeel free to explore, analyze, and build upon this dataset to uncover interesting patterns and insights.\n","192":"","193":"# Description\n\n**Weibo V2**: Includes **11,329** number of news from the Chinese microblogging social media platform. There are **5,661 fake news** items and **5,668 real news** items. Comparable to version 1 (V1), version 2 (V2) expands the data magnitude on the basis of V1. Meanwhile, V2 provides news **multi-modal data**, including **news posts**, **comment collections**, **images**, **videos** and **voice information**. As a result, V2 provides a better simulation of the real environment of social networks, thus supporting downstream tasks. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n```","194":"# Description\n\n**Weibo V1**: Includes data on 2,067 news items from the microblogging platform in the half of 2023. There are **1,000 fake news** and **1,067 real news**. The dataset consists of comment data on news spreads and contains **user** and **comment information**. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n```","195":"","196":"The internet and social media have led to a major problem\u2014fake news. Fake news is false information presented as real news, often with the goal of tricking or influencing people. It's difficult to identify fake news because it can look very similar to real news.\nThe Fake News detection dataset deals with the problem indirectly by using tabular summary statistics about each news article to attempt to predict whether the article is real or fake. This dataset is in a tabular format and contains features such as word count, sentence length, unique words, average word length, and a label indicating whether the article is fake or real.","197":"","198":"","199":"","200":"# **Fake News detection dataset**\n## \nDataset separated in two files:\n1. Fake.csv (23502 fake news article)\n2. True.csv (21417 true news article)\n\nDataset columns:\n1. Title: title of news article\n2. Text: body text of news article\n3. Subject: subject of news article\n4. Date: publish date of news article","201":"The Fake News Detection Dataset is created to assist researchers, data scientists, and machine learning enthusiasts in tackling the challenge of distinguishing between genuine and false information in today's digital landscape inundated with social media and online channels. With thousands of news items labeled as either \"Fake\" or \"Real,\" this dataset provides a robust foundation for training and testing machine learning models aimed at automatically detecting deceptive content.\n\nEach entry in the dataset contains the full text of a news article alongside its corresponding label, facilitating the development of supervised learning projects. The inclusion of various types of content within the news articles, ranging from factual reporting to potentially misleading information or falsehoods, offers a comprehensive resource for algorithmic training.\n\nThe dataset's structure, with a clear binary classification of news articles as either \"Fake\" or \"Real,\" enables the exploration of diverse machine learning approaches, from traditional methods to cutting-edge deep learning techniques.\n\nBy offering an accessible and practical dataset, the Fake News Detection Dataset aims to stimulate innovation in the ongoing battle against online misinformation. It serves as a catalyst for research and development within the realms of text analysis, natural language processing, and machine learning communities. Whether it's refining feature engineering, experimenting with state-of-the-art transformer models, or creating educational tools to enhance understanding of fake news, this dataset serves as an invaluable starting point for a wide range of impactful projects.","202":"The research on fake news and machine learning focuses on using advanced techniques to detect and classify fake news from real news. Various studies have shown the effectiveness of machine learning models, such as Naive Bayes Classifier, in distinguishing between fake and true news based on language patterns and content analysis\n\nEfforts have been made to automate the detection of fake news due to the limitations of human ability in consistently identifying fake news. Machine learning and natural language processing techniques have been employed to create tools that can analyze language patterns and classify news articles as real or fake, demonstrating the potential of machine learning in this task\n1\n5\n. While some deep learning models have been explored, they may require more data to predict accurately for fake news detection","203":"","204":"","205":"Overview\nIn an era where information spreads rapidly through social media and other digital channels, distinguishing between real and fake news has become increasingly challenging. The Fake News Detection Dataset aims to provide researchers, data scientists, and machine learning enthusiasts with a robust dataset for training models to automatically detect fake news with high accuracy.\n\nDataset Description\nThis dataset comprises thousands of news items labeled as \"Fake\" or \"Real\", providing a rich foundation for developing and testing machine learning models capable of identifying deceptive information. Each entry in the dataset consists of a news text and a corresponding label, offering a straightforward yet powerful resource for supervised learning projects.\n\nData Fields\nText: The full text of the news article. This field includes the body of the article, representing a mix of factual reporting, opinions, and potentially misleading information or falsehoods.\nLabel: A categorical label indicating whether the news article is \"Fake\" or \"Real\". This binary classification makes it suitable for a wide range of machine learning approaches, from traditional models to advanced deep learning techniques.\n\nInspiration\nThe Fake News Detection Dataset is designed to inspire the development of innovative algorithms that can contribute to the fight against misinformation online. By providing a readily accessible and useable dataset, we aim to foster research and development in text analysis, natural language processing, and machine learning communities. Whether you're exploring feature engineering, experimenting with the latest transformer models, or developing educational tools to help understand fake news, this dataset offers a starting point for a myriad of impactful projects.","206":"","207":"","208":"A larger and more generic Word Embedding over Linguistic Features for Fake News Detection (WELFake) dataset of 72,134 news articles with 35,028 real and 37,106 fake news. Merged four popular news datasets (i.e. Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent over-fitting of classifiers and to provide more text data for better ML training.\n\nThe dataset contains four columns: Serial number (starting from 0); Title (about the text news heading); Text (about the news content); and Label (0 = fake and 1 = real).\n\nThere are 78098 data entries in csv file out of which only 72134 entries are accessed as per the data frame. \n\n[Here is the demo project on this dataset!] (https:\/\/youtu.be\/IGOCtD4HFFQ)","209":"","210":"","211":"","212":"","213":"","214":"","215":"The dataset is a comprehensive sales record from Gottlieb-Cruickshank, detailing various transactions that took place in Poland in January 2018. The data includes information on customers, products, and sales teams, with a focus on the pharmaceutical industry. Below is a detailed description of the dataset:\n\n### Dataset Description\n\n**Columns:**\n\n1. **Distributor:** The name of the distributing company, which is consistent across all records as \"Gottlieb-Cruickshank.\"\n2. **Customer Name:** The name of the customer or the purchasing entity.\n3. **City:** The city in Poland where the customer is located.\n4. **Country:** The country of the transaction, consistently listed as \"Poland.\"\n5. **Latitude:** The latitude coordinate of the customer's city.\n6. **Longitude:** The longitude coordinate of the customer's city.\n7. **Channel:** The distribution channel, either \"Hospital\" or \"Pharmacy.\"\n8. **Sub-channel:** Specifies whether the sub-channel is \"Private,\" \"Retail,\" or \"Institution.\"\n9. **Product Name:** The name of the pharmaceutical product sold.\n10. **Product Class:** The classification of the product, such as \"Mood Stabilizers,\" \"Antibiotics,\" or \"Analgesics.\"\n11. **Quantity:** The number of units sold.\n12. **Price:** The price per unit of the product.\n13. **Sales:** The total sales amount, calculated as Quantity * Price.\n14. **Month:** The month of the transaction, which is \"January\" for all records.\n15. **Year:** The year of the transaction, which is \"2018\" for all records.\n16. **Name of Sales Rep:** The name of the sales representative handling the transaction.\n17. **Manager:** The manager overseeing the sales representative.\n18. **Sales Team:** The sales team to which the sales representative belongs, such as \"Delta,\" \"Bravo,\" or \"Alfa.\"\n\n### Example Rows:\n\n1. **Row 1:**\n   - **Customer Name:** Zieme, Doyle and Kunze\n   - **City:** Lublin\n   - **Product Name:** Topipizole\n   - **Product Class:** Mood Stabilizers\n   - **Quantity:** 4\n   - **Price:** 368\n   - **Sales:** 1472\n   - **Sales Rep:** Mary Gerrard\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\n2. **Row 2:**\n   - **Customer Name:** Feest PLC\n   - **City:** \u015awiecie\n   - **Product Name:** Choriotrisin\n   - **Product Class:** Antibiotics\n   - **Quantity:** 7\n   - **Price:** 591\n   - **Sales:** 4137\n   - **Sales Rep:** Jessica Smith\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\nThis dataset can be utilized for various analyses, including sales performance by city, product, and sales teams, as well as geographical distribution of sales within Poland. It provides valuable insights into the pharmaceutical sales strategies and their execution within a specific time frame.","216":"This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes.","217":"# Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613","218":"### Context  \n\nIndonesian foods are well-known for their rich taste. There are many spices used even for daily foods. This dataset may give insight on how to prepare Indonesian food, in many ways. \n\n### Content  \n\nThis dataset contains 14000 recipes divided in 7 categories:  \n- `dataset-ayam.csv` (chicken recipes)  \n- `dataset-kambing.csv` (lamb recipes)   \n- `dataset-sapi.csv` (beef recipes)   \n- `dataset-telur.csv` (egg recipes)  \n- `dataset-tahu.csv` (tofu recipes)  \n- `dataset-ikan.csv` (fish recipes)  \n- `dataset-tempe.csv` (tempe recipes)  \n\nFor each category, there are 5 columns:  \n- Title  \n- Ingredients  \n- Steps  \n- Love  \n- URL\n\n### Acknowledgements\n\nAll the data were taken from Cookpad on 23 Feb 2018.\n\n\n### Inspiration\n\nDare to find out what is the unique recipe? The most strange? Or the common way to cook particular ingredients.   \nCan you create your own recipe based on this dataset?","219":"This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes.","220":"Known for its aromatic spices, bold flavors, and regional specialties, Indian food is a delightful journey for the taste buds. From the tandoori delights of the North to the coconut-infused dishes of the South, each region offers a unique culinary experience. Indian cuisine not only caters to various palates but also holds a significant place in Indian traditions and celebrations.\nThis Dataset is inspired by the Dataset \"Indian Food 101\" By NEHA PRABHAVALKAR.\nIn addition to that, it also contains the image URLs for all the dishes.","221":"# **Context**\nThere's a story behind every dataset and here's your opportunity to share yours.\n\n# **Content**\nJordan cuisine consists of a variety of regional and traditional to the Arabic subcontinent. Given the diversity in soil, climate, culture, ethnic groups, and occupations, these cuisines vary substantially and use locally available spices, herbs, vegetables, and fruits.\n\nThis dataset consists of information about various Jordan dishes, their ingredients, their place of origin, etc.\n\n# **Challenge**\nThe first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions\/image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.\n\n#**About**\nFor more information: http:\/\/arartawil.com\/\n","222":"# Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613","223":"Pd: The banner image was obtained [here](https:\/\/radioambulante.org). Credits to them.\n\n### Context \nPeru is one of best culinary destination in the world. This country has diverse climates and ecological floors, where various crops have been developed. In this way, it has a lot of natural and unique inputs. So, peruvian food is a cuisine of opposites: hot and cold on the same plate. Acidic tastes melding with the starchy. Robust and delicate at the same time. This balance occurs because traditional Peruvian food relies on spices and bold flavors, ranging from the crisp and clean to the heavy and deep. Each flavor counters or tames the other. While many people see Peru as a land of cloud-topped mountains and ruins of ancient civilizations, Peru\u2019s true treasure is its rich culinary heritage. Ingredients and cooking techniques from Africa, Europe, and East Asia come together in a delightful melange that is utterly unique the world over. But what kind of food do Peruvians eat? And, **what restaurants should you visit?** \ud83d\ude0a \n\n![gg](https:\/\/th.bing.com\/th\/id\/R.2ba072e671a2fdb017da8c135fc2bc08?rik=g8HU8568d8RHog&riu=http%3a%2f%2f2.bp.blogspot.com%2f-KhLPAIsagTQ%2fVie_1VlvTUI%2fAAAAAAAAACc%2fzBjaekoIkUM%2fs1600-r%2f2.jpg&ehk=stHuhD5xQj5P5tx3MNqlLctSB72pLLTMm2A7Wi%2b6Msk%3d&risl=&pid=ImgRaw)\n\n### Content\nThis kaggle dataset contains information scraped from GooglePlaces and Tripadvisor using Selenium, Requests, BeautifulSoup and Rvest. More info about the used web-scraping in this [github repository](https:\/\/github.com\/Lazaro-97\/satisfaction-in-restaurants)\nThe content here have a lot (at the moment not all) of restaurants reviews in Lima, Peru between 2010 and 2021. In total exist more of 8791 restaurants and more of 1160666 reviews. With a total of 20 features with a high diversity: geospatial, text, date ,categoric and numeric feafures!\n\nThis has two general sections. The first is the *Restaurants*. This contains general and geospatial information.  The second is the *Reviews*. This contains the interaction between user and restaurant, with this way is possible to see the satisfaction of the client with a service.\nExist a possible third section: the *Users*. This information maybe will be added in two months.\n\nAbout the collection methodology, this is explained below:\n\n-**The sample:** The scraped reviews are the most recent reviews in all possible restaurants in the province of Lima. \n\n-**Set of items:** In one way, the users. In other way: the restaurants.\n\n-**Set of variables:** Exist two general tables. See the information below\n\nThe following diagram and table summarise all.\n\n**Table 1:** Restaurants\n|Variable|Description|\n| --- | ---|\n| Id | Id of the restaurant |\n|Name| Name of the restaurant |\n|Tag| The category of the restaurant |\n|x, y| Geospatial information and exact location of restaurant |\n|District|District where the restaurant is located |\n|Direction|District where the restaurant is located|\n|Stars|Mean Stars of restaurant in all time|\n|N_reviews|Number of reviews of restaurant in all time |\n|Min_Price| Minimum price in the menu of restaurant |\n|Max_Price| Maximum price in the menu of restaurant |\n|Platform| Platform where the information was downloaded |\n\n**Table 2:** Reviews\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review |\n|Id_nick| Id of the user. With this is possible to get the profile link |\n|Date| Date when the review was written |\n|Service| Id of the restaurant. Conection with Table 1 |\n|Review|Content of the review. This describe the satisfaction of the user |\n|Title|Title of the review. Only available in Tripadvisor|\n|Score|Punctuation in the review|\n|Likes|Number of votes in the publication|\n|Platform|Platform where the information was downloaded |\n\nAlso, exist auxiliar information related with the sentiment and emotion. This probabilities was obtained with a Spanish NrcLexicon, however, that results is not ok. Anyway, that is a reference and you can propose a fine tuning here. In adittion, also exist the probability to get a specific star, however, this was obtained with a simple logistic regression. Also i showed the information about Spanish NrcLexicon and Geospatial Borders. The author ands more information you can find [there](https:\/\/github.com\/jboscomendoza\/lexicos-nrc-afinn) and [there](https:\/\/www.geogpsperu.com\/2018\/02\/limite-distrital-politico-shapefile-ign.html).\n\n**Table 3:** Models\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review. Conection with Table 1 |\n|Positive| Probability of review that it will  show positive sentiment|\n|Negative| Probability of review that it will show negative sentiment|\n|Anger| Probability of review that it will show anger emotion|\n|Anticipation|Probability of review that it will show anticipation emotion |\n|Disgust|Probability of review that it will show disgust emotion|\n|Fear|Probability of review that it will show fear emotion|\n|Joy|Probability of review that it will show joy emotion|\n|Sadness|Probability of review that it will show sadness emotion|\n|Surprise|Probability of review that it will show surprise emotion|\n|Stars_1|Probability of review that it will get 1 star|\n|Stars_2|Probability of review that it will get 2 stars|\n|Stars_3|Probability of review that it will get 3 stars|\n|Stars_4|Probability of review that it will get 4 stars|\n|Stars_5|Probability of review that it will get 5 stars|\n\nThe entity relationship diagram! \n\n![erd](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1251020\/2455759\/erd.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210803%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210803T000105Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=53e7abea86abbc9efd45a0a7bdc40e950ba972565fd08ecd354f04e7ccb7ac6f7cc216f6e82d147b55ea3ab32badfb95501549291dfa783968246633b73dad6fb25206696d134b8a2352f7741c3002fa427d12b0320bfaedaeed7a1f8de0658804931e99708899ef95ad44a1bbbafc612f3a2e110ef4442f623003d79d12d8ae572988e8271c038d31d436ee2406e7da07a503548f443d24f65769324034c4ab63efc081a653e86f93f98e36c95198feb925515e487319f07009446d462ee71befeab29e3ef33c7bb48c9bc950974ee34c9fea648f9c3b0d125168035b75d6d6b2de5d09749b1cfce30208c65fe90653713461aa7acd5a1002dbbb4f50ecf44e)\n\n## Usage\n**Text classification:** The main topic in this types of datasets. Vectorize the reviews and define a predictive model. Identify strong and weak points of each restaurant.\n\n**Find patterns:** Compare districts (or restaurants) along the time. What is the common words in an excellent restaurant? Why these restaurants are better? \n\n**Reduction of dimention:** Detect similarities and then, clustering the reviews.\n\n## Acknowledgements\nThanks to Kaggle and its community. In general, thanks to the learners and teachers in machine learning, deep learning, natural language processing and computer vision.\n\n## Inspiration\nNatural language processing is a great tool. One application that I'm interested is detect bullies messages in any social network. I know that exist many notebooks and papers, but I'd like to build a bot that detect **all** possible cases and surely, there exist!\n","224":"The Bahama Breeze Menu Nutrition Data dataset provides detailed nutritional information for various items available at Bahama Breeze, a restaurant known for its Caribbean-inspired cuisine. The dataset is organized into the following columns:\n\n- Item: The name of the food or beverage item offered on the Bahama Breeze menu.\n- Category: The type or classification of the item, such as appetizers, entrees, desserts, or beverages.\n- Calories: The total number of calories contained in one serving of the item.\n- Total_Carbs_g: The total amount of carbohydrates in grams present in one serving of the item.\n- Sodium_mg: The amount of sodium in milligrams found in one serving of the item.\n- Weight_Watchers: The Weight Watchers points value assigned to one serving of the item, useful for individuals following the Weight Watchers diet program.\n\nThis dataset can be used to analyze the nutritional content of Bahama Breeze's offerings, helping customers make informed dietary choices based on their nutritional needs and preferences.","225":"This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes.","226":"## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n","227":"# Natural Language Understanding benchmark\n\nThis repository contains the results of three benchmarks that compare natural language understanding services offering:\n1. **built-in intents** (Apple\u2019s SiriKit, Amazon\u2019s Alexa, Microsoft\u2019s Luis,\nGoogle\u2019s API.ai, and [Snips.ai](https:\/\/snips.ai\/)) on a selection of\nvarious intents. This benchmark was performed in December 2016. Its results\nare described in length in the [following post](https:\/\/medium.com\/snips-ai\/benchmarking-natural-language-understanding-systems-d35be6ce568d).\n2. **custom intent engines** (Google's API.ai, Facebook's Wit, Microsoft's Luis, Amazon's Alexa, and Snips' NLU) for seven chosen intents. This benchmark was performed in June 2017. Its results are described in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/@alicecoucke\/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19).\n3. **extension of Braun et al., 2017** (Google's API.AI, Microsoft's Luis, IBM's Watson, Rasa)\nThis experiment replicates the analysis made by Braun et al., 2017, published in Evaluating Natural Language Understanding Services for Conversational Question Answering Systems as part of SIGDIAL 2017 proceedings. Snips and Rasa are added. Details are available in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/snips-ai\/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a).\n\nThe data is provided for each benchmark and more details about the methods are available in the README file in each folder.\n\n**Any publication based on these datasets must include a full citation to the following paper in which the results were published by the Snips Team:** \n\n[Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system \nfor private-by-design voice interfaces.\" 2018,](https:\/\/arxiv.org\/abs\/1805.10190)\n\naccepted for a spotlight presentation at the [Privacy in Machine Learning and Artificial Intelligence workshop](https:\/\/pimlai.github.io\/pimlai18\/#papers) colocated with ICML 2018.\n\n\n\n *The Snips team has joined Sonos in November 2019. These open datasets remain available and their access is now managed by the Sonos Voice Experience Team. Please email sve-research@sonos.com with any question.*","228":"The dataset roughly include 750 question answering pairs in json format. With three name component (instruction , input and output).Data set was collected via college level intro cs class in China.","229":"### Dataset Description for Filtered Sigma Dolphin Dataset\n\n#### Overview\nThis dataset is a cleaned and filtered version of the Sigma Dolphin dataset (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin), designed to aid in solving maths word problems using AI techniques. This was used as an effort towards taking part in the AI Mathematical Olympiad - Progress Prize 1 (https:\/\/www.kaggle.com\/competitions\/ai-mathematical-olympiad-prize\/overview). The dataset was processed using TF-IDF vectorisation and K-means clustering, specifically targeting questions relevant to the AIME (American Invitational Mathematics Examination) and AMC 12 (American Mathematics Competitions).\n\n#### Context\nThe Sigma Dolphin dataset is a project initiated by Microsoft Research Asia, aimed at building an intelligent system with natural language understanding and reasoning capacities to automatically solve maths word problems written in natural language. This project began in early 2013, and the dataset includes maths word problems from various sources, including community question-answering sites like Yahoo! Answers.\n\n#### Source and Original Dataset Details\n- **Original Dataset:** Sigma Dolphin (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin)\n- **Original Source:** https:\/\/msropendata.com\/datasets\/f0e63bb3-717a-4a53-aa79-da339b0d7992\n- **Project Page:** http:\/\/research.microsoft.com\/en-us\/projects\/dolphin\/\n- **References:**\n  - Shuming Shi, et al. \"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning.\" EMNLP 2015.\n  - Danqing Huang, et al. \"How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation.\" ACL 2016.\n  - JSON: http:\/\/json.org\/\n\n#### Content\nThe filtered dataset includes problems that are relevant for preparing for maths competitions such as AIME and AMC. The data is structured to facilitate the training and evaluation of AI models aimed at solving these types of problems. \n\n#### Datasets:\nThere are several filtered versions of the dataset based on different similarity thresholds (0.3 and 0.5). These thresholds were used to determine the relevance of problems from the original Sigma Dolphin dataset to the AIME and AMC problems.\n\n1. **Number Word Problems Filtered at 0.3 Threshold:**\n   - File: `number_word_test_filtered_0.3_Threshold.csv`\n   - Description: Contains problems filtered with a similarity threshold of 0.3, ensuring moderate relevance to AIME and AMC 12 problems.\n\n2. **Number Word Problems Filtered at 0.5 Threshold:**\n   - File: `number_word_std.test_filtered_0.5_Threshold.csv`\n   - Description: Contains problems filtered with a higher similarity threshold of 0.5, ensuring higher relevance to AIME and AMC 12 problems.\n\n3. **Filtered Number Word Problems 2 at 0.3 Threshold:**\n   - File: `filtered_number_word_problems2_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.3 similarity threshold.\n\n4. **Filtered Number Word Problems 2 at 0.5 Threshold:**\n   - File: `filtered_number_word_problems_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.5 similarity threshold.\n\n#### Why Different Similarity Thresholds?\nDifferent similarity thresholds (0.3 and 0.5) are used to provide flexibility in selecting problems based on their relevance to AIME and AMC problems. A lower threshold (0.3) includes a broader range of problems, ensuring a diverse set of questions, while a higher threshold (0.5) focuses on problems with stronger relevance, offering a more targeted and precise dataset. This allows users to choose the level of specificity that best fits their needs.\n\nFor a detailed explanation of the preprocessing and filtering process, please refer to the [Sigma Dolphin Filtered & Cleaned Notebook](https:\/\/www.kaggle.com\/code\/ryanmutiga\/sigma-dolphin-filtered-cleaned-notebook).\n\n#### Acknowledgements\nWe extend our gratitude to all the original authors of the Sigma Dolphin dataset and the creators of the AIME and AMC problems. This project leverages the work of numerous researchers and datasets to build a comprehensive resource for AI-based problem solving in mathematics.\n\n#### Usage\nThis dataset is intended for research and educational purposes. It can be used to train AI models for natural language processing and problem-solving tasks, specifically targeting maths word problems in competitive environments like AIME and AMC.\n\n#### Licensing\nThis dataset is shared under the Computational Use of Data Agreement v1.0.\n\n---\n\nThis description provides an extensive overview of the dataset, its sources, contents, and usage. If any specific details or additional sections are needed, please let me know!","230":"This dataset, titled \"Financial-QA-10k\", contains 10,000 question-answer pairs derived from company financial reports, specifically the 10-K filings. The questions are designed to cover a wide range of topics relevant to financial analysis, company operations, and strategic insights, making it a valuable resource for researchers, data scientists, and finance professionals. Each entry includes the question, the corresponding answer, the context from which the answer is derived, the company's stock ticker, and the specific filing year. The dataset aims to facilitate the development and evaluation of natural language processing models in the financial domain.\n\nAbout the Dataset\nDataset Structure:\n\n- **Rows**: 7000\n- **Columns**: 5\n- **question**: The financial or operational question asked.\n- **answer**: The specific answer to the question.\n- **context**: The textual context extracted from the 10-K filing, providing additional information.\n- **ticker**: The stock ticker symbol of the company.\n- **filing**: The year of the 10-K filing from which the question and answer are derived.\n\n\n**Sample Data:**\n\nQuestion: What area did NVIDIA initially focus on before expanding into other markets?\nAnswer: NVIDIA initially focused on PC graphics.\nContext: Since our original focus on PC graphics, we have expanded into various markets.\nTicker: NVDA\nFiling: 2023_10K\n\nPotential Uses:\n\n**Natural Language Processing (NLP):** Develop and test NLP models for question answering, context understanding, and information retrieval.\n**Financial Analysis:** Extract and analyze specific financial and operational insights from large volumes of textual data.\n**Educational Purposes:** Serve as a training and testing resource for students and researchers in finance and data science.","231":"Embark on a captivating exploration of Neymar, the revered Brazilian football sensation, with this meticulously curated dataset. Derived from a diverse array of dialogues between users and an interactive chatbot, this dataset provides an immersive journey into Neymar's multifaceted world, sourced directly from the wealth of information available on Wikipedia (https:\/\/en.wikipedia.org\/wiki\/Neymar)  . \n\nDelve into the intricacies of Neymar's illustrious career, from his groundbreaking exploits on the football field to his profound impact beyond it. Each conversation within this dataset offers a nuanced glimpse into Neymar's professional milestones, statistical feats, and personal anecdotes, carefully selected and distilled from the extensive discourse surrounding his life and legacy.\n\nThis dataset is well-suited for Natural Language Processing (NLP) tasks and deep learning applications. With its rich collection of conversational data, it provides an ideal resource for training NLP models, exploring sentiment analysis, question answering systems, and other advanced deep learning techniques.\n\nWhether you're a passionate football aficionado, a data enthusiast, or simply curious about Neymar's remarkable journey, this dataset promises to ignite your imagination and deepen your appreciation for one of football's most iconic figures.\n\nLicense: Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)\n\n","232":"## **Source Dataset:**\n\nThis data is a processed version of [https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa](https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa) dataset. \n\n## **Fields are mapped as below:**\n\n1. data -&gt; questions -&gt; input_text to **question**\n2. data -&gt; answers -&gt; input_text to **answer**\n3. data -&gt; answers -&gt; span_text to **ground_truth**\n4. data -&gt; story to **context**\n5. data -&gt; id to **data_id**\n6. **question_id** is generated sequentially. \n\n\n## **Processed using the notebook:**\n\n[https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth](https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth)\n\n\n## **Dataset Logo Credit:**\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@wesleyphotography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Wesley Tingey<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/yellow-and-white-10-card-FIq7K_wD4jM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n  \n\n","233":"# AstroMCQA Dataset\n\n## Purpose and scope\n\nThe primary purpose of AstroMCQA is for application developers in the domain of space engineering to be able to comparatively assess LLM performances on the specific task of multiple-choice question-answering\n\n## Intended Usage\n\nComparative assessement of differents LLMs, Model evaluation, audit, and model selection. Assessment of different quantization levels, different prompting strategies, and assessing effectiveness of domain adaptation or domain-specific fine-tuning.\n\n## Quickstart\n\n- Explore the dataset here:  https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/viewer\/default\/train\n- Evaluate an LLM (Mistral-7b) on AstroMCQA on collab here:<a target=\"_blank\" href=\"https:\/\/colab.research.google.com\/github\/patrickfleith\/astro-llms-notebooks\/blob\/main\/Evaluate_an_HuggingFace_LLM_on_a_Domain_Specific_Benchmark_Dataset.ipynb\">\n  <img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\">\n<\/a>\n\n## What is AstroMCQA GOOD for?\n\nWhat is AstroMCQA good for?\nThe primary purpose of AstroMCQA is for application developers in the domain of space mission design and operations to be able to address some questions such as: which LLM to use and how does it perform in the different subdomains? It enables to benchmark different models, different size, quantization methods, prompt engineering strategies, effectiveness of fine-tuning on the specific task of multiple-choice question-answering in space engineering.\n\n## What is AstroMCQA NOT GOOD for?\n\nIt is not suitable for training \/ fine-tuning LLM due to the very limited size of the dataset even if it could be combined with other tasks and science dataset for meta-learning.\n\n# DATASET DESCRIPTION\n### Access\n- Manual download from Hugging face hub: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\n- Or with python:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"patrickfleith\/Astro-mcqa\")\n```\n\n### Structure\n200 expert-created Multiple Choice Questions and Answers, one question per row in a comma separated file. Each instance is made of the following field (column):\n- **question**: a string.\n- **propositions**: a list of string. Each item in the list is one choice. At least one of the propositions correctly answer the question, but there can be multiple correct propositions. Even all propositions can be correct.\n- **labels**: list of integer (0\/1). Each element in the labels list correspond to proposition at the same position within the proposition list. A label of 0 means that the proposition is incorrect. A label of 1 means that the proposition is a correct choice to answer the question.\n- **justification**: Optional string. An optional field which may provide a justification of the answer.\n- **answerable**: A boolean, whether the question is answerable or not. At the moment, AstroMCQA only includes answerable questions.\n- **uid**: A unique identifier for the MCQA instance. May be useful for traceability in further processing tasks.\n\n### Metadata\nDataset is version controlled and commits history is available here: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/commits\/main\n \n### Languages\nAll instances in the dataset are in english\n \n### Size\n200 expert-created Multiple Choice Questions and Answers\n \n### Types of Questions\n- Some questions request expected generic knowledge in the field of space science and engineering.\n- Some questions require reasoning capabilities\n- Some questions require mathematical operations since a numerical result is expected (exam-style questions)\n\n### Topics Covered\nDifferent subdomains of space engineering are covered, including propulsion, operations, human spaceflight, space environment and effects, space project lifecycle, communication and link analysis, and more.\n\n# USAGE AND GUIDELINES\n#### License\nAstroMCQA \u00a9 2024 by Patrick Fleith is licensed under Creative Commons Attribution 4.0 International\n\n#### Restrictions\nNo restriction. Please provide the correct attribution following the license terms.\n \n#### Citation\nP. Fleith, AstroMCQA \u2013 Astronautics multiple choice questions and answers benchmark dataset for domain of Space Mission Engineering for LLM Evaluation, (2024). \n \n#### Update Frequency\nMay be updated based on feedbacks. If you want to become a contributor, let me know.\n \n#### Have a feedback or spot an error?\nUse the community discussion tab directly on the huggingface Astro-mcqa dataset page.\n \n#### Contact Information\nReach me here on the community tab or on LinkedIn (Patrick Fleith) with a Note.\n\n#### Current Limitations and future work\n- Only 200 multiple choice questions and answers. This makes it useless for fine-tuning purpose, although it could be integrated as part of a larger pool of datasets compiled for a larger fine-tuning.\n- While being a descent size enabling LLM evaluation, the space engineering expert time is scarce and expensive. On average it takes 8 minutes to create one MCQA example. Having more examples would be much better for robustness.\n- The dataset might be biased toward the very low number of annotators.\n- The dataset might be biased toward European Space Programs.\n- The dataset might not cover all subsystems or subdomain of astronautics although we tried to do our best covering the annotator\u2019s domains of expertise.\n- No peer-reviewing. Ideally we would like to have a Quality Control process to ensure high quality, and correctness of each example in the dataset. Given the limited resources, this is not yet possible. Feel free to come and contribute if you feel that is an issue","234":"EVJVQA, the first multilingual Visual Question Answering dataset with three languages: English, Vietnamese, and Japanese, is released in this task. UIT-EVJVQA includes question-answer pairs created by humans on a set of images taken in Vietnam, with the answer created from the input question and the corresponding image. EVJVQA consists of 33,000+ question-answer pairs for evaluating the mQA models.","235":"Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to fill this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new resource to design and evaluate personalized models related to the two tasks of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with both questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization improves remarkably the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.\n\nMore details can be found here: https:\/\/github.com\/pkasela\/SE-PQA\n\n```\n@inproceedings{10.1145\/3589335.3651445,\nauthor = {Kasela, Pranav and Braga, Marco and Pasi, Gabriella and Perego, Raffaele},\ntitle = {SE-PQA: Personalized Community Question Answering},\nyear = {2024},\nisbn = {9798400701726},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https:\/\/doi.org\/10.1145\/3589335.3651445},\ndoi = {10.1145\/3589335.3651445},\nbooktitle = {Companion Proceedings of the ACM on Web Conference 2024},\npages = {1095\u20131098},\nnumpages = {4},\nkeywords = {personalization, question answering, user model},\nlocation = {},\nseries = {WWW '24}\n}\n```","236":"Dataset Summary\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n\nThese problems take between 2 and 8 steps to solve.\nSolutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ \u2212 \u00d7\u00f7) to reach the final answer.\nA bright middle school student should be able to solve every problem: from the paper, \"Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable.\"\nSolutions are provided in natural language, as opposed to pure math expressions. From the paper: \"We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models\u2019 internal monologues\"","237":"## Discerption :\nThis dataset is a .parquet file prepared on instruction Question Answer dataset in Arabic language on multiple different categories.\n\n## Usage :\nbest used for instruct fine-tuning large language models to perform better on Arabic language.\n\n## License and Usage :  \nThis dataset is a Kaggle version of generated Arabic dataset is created by Yasbok from hugging face. due to issues extracting it directly from \"HF\" using the `Dataset` library, i downloaded it and uploaded it here on kaggle so it can be used easily.\n[Hugging face dataset link](https:\/\/huggingface.co\/datasets\/Yasbok\/Alpaca_arabic_instruct)\n","238":"","239":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8442057%2Ff7c62f9d14b5620b8452f39690a24568%2Ffig1.png?generation=1715577026848120&alt=media)\n\nAbstract: FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). This repository contains an open source sample of 150 annotated examples used in the evaluation and analysis of models assessed in the FinanceBench paper. FinanceBench comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.\n\n**Contact:** To evaluate your models on the full FinanceBench dataset, or if you have questions about this work, you can email us at contact@patronus.ai\n\n**Dataset Overview:** The provided open-source dataset (n=150) consists of the following attributes:\n\n```python\n- financebench_id:  unique question identifier  \n    - question:         question of interest\n    - answer:           gold answer\n    - question_type:    type of the question {domain-relevant, metrics-generated, novel-generated}\n    - doc_name:         name of the relevant financial document to answer the question\n    - doc_link:         url to retrieve the relevant financial document\n    - doc_period:       period of the relevant financial document\n    - evidence_text:    extracted evidence text\n    - page_number       page number(s) of evidence text\n```\n\n**Citation:** If you use our open-source dataset or refer to our result, please use the following citation:\n\n```python\n@misc{islam2023financebench,\n      title={FinanceBench: A New Benchmark for Financial Question Answering}, \n      author={Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},\n      year={2023},\n      eprint={2311.11944},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n","240":"VQA, or Visual Question Answering, is a multifaceted challenge that tasks models with providing coherent responses to natural language questions posed about images. This intricate task demands a deep understanding of both the visual content and the semantic nuances embedded within the questions. By bridging the realms of Computer Vision and Natural Language Processing, VQA addresses a plethora of sub-problems, including object detection, scene classification, and more, rendering it a quintessential benchmark in the realm of AI","241":"**This work has been accepted to ACL 2024.** You can find the pre-print related to this work https:\/\/arxiv.org\/abs\/2309.08902\n\n**Dataset Summary:**\n\n**GenAssocBias** is a dataset that measures **stereotype bias in LLMs**. GenAssocBias consists of 11,940 sentences that measure model preferences across **ageism, beauty, beauty_profession, nationality, and institutional bias.**\n\nSupported Tasks and Leaderboards\n\nmultiple-choice question answering\n\nLanguages\n\nEnglish (en)\n\nDataset Descriptions:\n\nThere are 8 columns in our dataset. The description of each column is given below:\n\nbias_type: This column indicates different types of biases including ageism, beauty, beauty_profession, nationality, and institutional.\n\ntarget_gender: This column indicates the particular gender type. There are three unique gender types namely 'male', 'female', and 'not_specified'.\n\ncontext: This column indicates different sentences. These are the context sentences.\n\nitem_category: This column is either 'positive' or 'negative'. When the attribute or stimulus in the context sentence is positive, we named it as 'positive' and when the attribute or stimulus is negative, then we named it as 'negative'.\n\ntype_category: This column tells us, which direction the data is. There are two different types of direction, namely SAI and ASA.\n\nanti_stereotype: When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is positive among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is negative among the options.\n\nstereotype: This column is the opposite of the 'anti_stereotype' column. When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is negative among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is positive.\n\nunrelated: This column contains the neutral attributes or stimuli.\n\nCitation Information:\n\n@article{kamruzzaman2023investigating, title={Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models}, author={Kamruzzaman, Mahammed and Shovon, Md Minul Islam and Kim, Gene Louis}, journal={arXiv preprint arXiv:2309.08902}, year={2023} }","242":"PDFs of different boardgames rulebooks\n- Root\n- Lords of Waterdeep\n- Troyes","243":"","244":"","245":"","246":"","247":"","248":"This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes.","249":"this graph was created in Loocker studio,PowerBi and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Ff35dd0bdb2f87ea2badf1ea1d57086bc%2Fgraph1.jpg?generation=1718398785729815&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F52689afbe7380af61e578767366ac925%2Fgraph2.jpg?generation=1718398791128270&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F992bd2cc851499b2f8f0bcb34732fab7%2Fgraph3.png?generation=1718398796130065&alt=media)\n\nAbstract\nWe propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.\n\nIntroduction\nOnce viewed as a luxury good, nowadays wine is increasingly enjoyed by a wider range of consumers. Portugal is a top ten wine exporting country, with 3.17% of the market share in 2005 [11]. Exports of its vinho verde wine (from the northwest region) have increased by 36% from 1997 to 2007 [8]. To support its growth, the wine industry is investing in new technologies for both wine making and selling processes. Wine certification and quality assessment are key elements within this context. Certification prevents the illegal adulteration of wines (to safeguard human health) and assures quality for the wine market. Quality evaluation is often part of the certification process and can be used to improve wine making (by identifying the most influential factors) and to stratify wines such as premium brands (useful for setting prices).","250":"","251":"","252":"  Available at: [@Elsevier] http:\/\/dx.doi.org\/10.1016\/j.dss.2009.05.016\n                [Pre-press (pdf)] http:\/\/www3.dsi.uminho.pt\/pcortez\/winequality09.pdf\n                [bib] http:\/\/www3.dsi.uminho.pt\/pcortez\/dss09.bib\n\n1. Title: Wine Quality \n\n2. Sources\n   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n   \n3. Past Usage:\n\n  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n  Modeling wine preferences by data mining from physicochemical properties.\n  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n\n  In the above reference, two datasets were created, using red and white wine samples.\n  The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n  between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n  these datasets under a regression approach. The support vector machine model achieved the\n  best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n  etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n  analysis procedure).\n \n4. Relevant Information:\n\n   The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n   For more details, consult: http:\/\/www.vinhoverde.pt\/en\/ or the reference [Cortez et al., 2009].\n   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\n   These datasets can be viewed as classification or regression tasks.\n   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n   or poor wines. Also, we are not sure if all input variables are relevant. So\n   it could be interesting to test feature selection methods. ","253":"","254":"","255":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","256":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","257":"This dataset contains detailed information on residential property prices in India. It includes features such as the number of bedrooms and bathrooms, living and lot area, number of floors, waterfront presence, house condition and grade, construction and renovation years, and geographic coordinates. Additional features include the number of nearby schools, distance from the airport, and property price. This dataset is ideal for real estate market analysis and predictive modeling, providing insights into factors influencing property prices across different regions in India.","258":"The Bengaluru House Prediction project is a machine learning-based solution designed to help potential homebuyers in Bangalore predict home prices accurately. It addresses a common problem faced by homebuyers who struggle to make informed decisions due to the complexity of factors that influence property prices.\n\nOverall, the Bengaluru House Prediction project is a valuable tool that empowers users with the right information to make informed decisions and demonstrates the potential of machine learning in the real estate industry.","259":"This house costing data is transformed for data modelling and is the updated version of my previous dataset 'Transformed Housing Data'. The updation is the creation of dummy variables of certain categorical features for modelling purpose. There are a total of 31 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","260":"This house costing data is the transformed version of my previous dataset titled 'Raw Housing Cost Data (imputed)'; it's imputed and the variables in it are transformed (for modelling). Same as the earlier data, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","261":"This house costing data has 21609 rows of information about different Houses. It's the imputed version of my previous dataset titled Raw Housing Cost Data. Same as earlier, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","262":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","263":"Dataset includes house sale prices for King County in USA. \nHomes that are sold in the time period: May, 2014 and May, 2015.\n\nColumns:\n    - ida: notation for a house\n    - date: Date house was sold\n    - price: Price is prediction target\n    - bedrooms: Number of Bedrooms\/House\n    - bathrooms: Number of bathrooms\/House\n    - sqft_living: square footage of the home\n    - sqft_lot: square footage of the lot\n    - floors: Total floors (levels) in house\n    - waterfront: House which has a view to a waterfront\n    - view: Has been viewed\n    - condition: How good the condition is ( Overall )\n    - grade: overall grade given to the housing unit, based on King County grading system\n    - sqft_abovesquare: footage of house apart from basement\n    - sqft_basement: square footage of the basement\n    - yr_built: Built Year\n    - yr_renovated: Year when house was renovated\n    - zipcode: zip\n    - lat: Latitude coordinate\n    - long: Longitude coordinate\n    - sqft_living15: Living room area in 2015(implies-- some renovations) \n    - sqft_lot15: lotSize area in 2015(implies-- some renovations)","264":"This dataset contains information related to housing sales, in the form of individual properties. Here's a breakdown of the columns:\n\n| Column Name    | Description                                         |\n|----------------|-----------------------------------------------------|\n| Lot_Frontage   | Linear feet of street connected to the property    |\n| Lot_Area       | Lot size in square feet                             |\n| Bldg_Type      | Type of building|\n| House_Style    | Style of the house      |\n| Overall_Cond   | Overall condition rating of the house               |\n| Year_Built     | Year the house was built                            |\n| Exter_Cond     | Exterior condition rating of the house               |\n| Total_Bsmt_SF  | Total square feet of basement area                  |\n| First_Flr_SF   | First-floor square feet                             |\n| Second_Flr_SF  | Second-floor square feet                            |\n| Full_Bath      | Number of full bathrooms                            |\n| Half_Bath      | Number of half bathrooms                            |\n| Bedroom_AbvGr  | Number of bedrooms above ground                     |\n| Kitchen_AbvGr  | Number of kitchens above ground                     |\n| Fireplaces     | Number of fireplaces                                |\n| Longitude      | Longitude coordinates of the property location      |\n| Latitude       | Latitude coordinates of the property location       |\n| Sale_Price     | Sale price of the property                          |\n\nThe dataset contains 2413 entries and has a mixture of numerical and categorical data. It's likely used for analyzing various factors influencing housing sale prices, such as location, size, condition, and amenities.","265":"House price prediction\nPredicting house prices is a common task in data science and machine learning. Here's a high-level overview of how you might approach it:\n\nData Collection:\nGather a dataset containing features of houses (e.g., size, number of bedrooms, location, amenities) and their corresponding prices. Websites like Zillow, Kaggle, or government housing datasets are good sources.\n\nData Preprocessing:\nClean the data by handling missing values, encoding categorical variables, and scaling numerical features if necessary. This step ensures that the data is in a suitable format for training a model. Feature Selection\/Engineering: Choose relevant features that are likely to influence house prices. You may also create new features based on domain knowledge or data analysis.\n\nModel Selection:\nSelect a regression model suitable for predicting continuous target variables like house prices. Common choices include Linear Regression, Decision Trees, Random Forests, Gradient Boosting, and Neural Networks.\n\nModel Training:\nSplit your dataset into training and testing sets to train and evaluate the performance of your model. You can further split the training set for validation purposes or use cross-validation techniques.\n\nModel Evaluation:\nAssess the performance of your model using appropriate evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or Root Mean Squared Error (RMSE).\n\nHyperparameter Tuning:\nFine-tune your model's hyperparameters to improve its performance. Techniques like grid search or random search can be employed for this purpose.\n\nDeployment:\nOnce satisfied with your model's performance, deploy it to make predictions on new data. This could be as simple as saving the trained model and creating an interface for users to input house features.","266":"Real Estate Prices Dataset\nThis dataset comprises information on 4,600 real estate transactions, providing a detailed snapshot of the housing market in various locations. Each record captures the characteristics of a house, its surroundings, and transaction details from transactions that occurred around May 2, 2014. The dataset includes the following fields:\n\ndate: The date of the transaction.\nprice: The sale price of the property (in USD).\nbedrooms: The number of bedrooms.\nbathrooms: The number of bathrooms, represented in half-baths (e.g., 1.5 indicates one full bath and one half bath).\nsqft_living: The square footage of the home's living area.\nsqft_lot: The square footage of the lot.\nfloors: The number of floors.\nwaterfront: A binary indicator for whether the property is on the waterfront (1) or not (0).\nview: An index from 0 to 4 indicating the quality of the view.\ncondition: An index from 1 to 5 on the condition of the property.\nsqft_above: The square footage of the house apart from the basement.\nsqft_basement: The square footage of the basement.\nyr_built: The year the property was built.\nyr_renovated: The year of the last renovation.\nstreet: The street address of the property.\ncity: The city in which the property is located.\nstatezip: The state and ZIP code.\ncountry: The country of the property.\n\n\nThis dataset can be particularly useful for projects involving real estate market analysis, price prediction models, and economic research related to housing trends. Researchers and enthusiasts can explore aspects such as the impact of property characteristics on price, trends over time, and geographical price variations.","267":"# General Information and License\n- HDB Resale Price data made available by SG government through the Open Data License.\n    1. https:\/\/beta.data.gov.sg\/open-data-license\n    2. https:\/\/beta.data.gov.sg\/collections\/189\/datasets\/d_ebc5ab87086db484f88045b47411ebc5\/view\n- Geodata made available by SG government through the OneMap\n    1. https:\/\/www.onemap.gov.sg\/apidocs\/apidocs\n- I claim NO rights whatsoever to this data. This data has been uploaded here for educational purposes.\n- There is no personally identifiable information in this data.\n\n# Data Availability\n- 1990 - 02-2012 (based on registration date)\n- 03-2012 - 2023-09 (based on approval date)\n\nContains resale prices only because most of Singapore's population lives in (very well-made and maintained!) public housing. \n\nDoes not contain condo\/landed house prices.\n\n# Special Notes\n- Public housing in Singapore is generally sold on a leasehold (typically 99-year) basis.\n- Prices tend to decrease as the remaining lease period decreases (see Bala's Curve).\n\n# Data Representation\nEach row represents 1 transaction.\n\n# Data Dictionary\n\n| Column | Description  |\n| --- | --- |\n| latitude | Latitude coordinate.  |\n| longitude | Longitude coordinate. |\n| postal_code | 6-digit postal code. |\n| address | Street address. |\n| closest_mrt | Name of closest Mass Rapid Transit (MRT) station. |\n| closest_mrt_dist| Distance (metres) to closest MRT station. |\n| cbd_dist | Distance (metres) to Central Business District (1.2830, 103.8513 -- Raffles Place Station) | \n| month| YYYY-MM representation of the transaction date. |\n| town | Name of the neighbourhood in which the flat is located. |\n| flat_type | Generic type of flat, e.g. \"3 ROOM\", \"4 ROOM\". |\n| block | Block number. |\n| street name | Street name. |\n| storey_range | Height of the flat. Specific floors are never given (as of 2024). |\n| floor_area | Floor area in Square Metres. |\n| flat_model | Model\/subtype of flat. Each flat type has certain specifications and configurations. |\n| lease_commence_date | The year that the leasehold contract of the flat began. Most (not all) flats have 99-year leases.|\n| remaining_lease| Remaining lease of the contract in years and months. Not all files have this column. It only starts appearing in the file with data from Jan 2015 to Dec 2016.|\n| resale_price| Target column. This is the ***transacted*** resale price as agreed between buyer and seller. |\n|years remaining| Years remaining on the lease. |\n\n# Update Frequency\n- This dataset may be updated every year or so as new transaction data becomes available.","268":"# **About Dataset**\nThis project is all about taking a closer look at the books available on Amazon. We've collected information on different types of genres, sub-genres, and individual books like their titles, authors, prices, and ratings. By digging into this data, we hope to learn interesting things about what kinds of books are popular, how they're priced, and what people like to read. This can help us understand more about the world of books and what makes them tick on Amazon.\n\n### **Dataset 1: Genre**\n- **Title**: This column contains the main genres of books available on Amazon.\n- **Number of Sub-genres:** Indicates the count of sub-genres associated with each main genre.\n- **URL:** Provides the link to the page on Amazon where books of this genre are listed.\n### **Dataset 2: SubGenre**\n- **Title:** Lists the specific sub-genres within each main genre.\n- **Main Genre:** Indicates the overarching genre to which each sub-genre belongs.\n- **No. of Books:** Shows the count of books categorized under each sub-genre.\n- **URL:** Provides the link to the page on Amazon where books of this sub-genre are listed.\n### **Dataset 3: Books_df**\n- **Title:** The title of the book.\n- **Author:** Name of the author or publication house.\n- **Main Genre:** The main genre the book belongs to.\n- **Sub Genre:** The specific sub-genre of the book.\n- **Type:** Indicates the format of the book, such as paperback, Kindle, audiobook, or hardcover.\n- **Price:** The price of the book.\n- **Rating:** The average rating of the book given by users.\n- **No. of People Rated:** Indicates the count of users who have rated the book.\n- **URLs:** Provides the link to the book's page on Amazon for further details and purchase options.","269":"The dataset provides information about apartments available for sale in Portugal, gathered from Imovirtual. Each entry in the dataset represents an apartment and includes the following attributes:\n\nIndex: A unique identifier for each entry in the dataset.\nName: The apartment type, indicating the number of bedrooms.\nPrice: The selling price of the apartment.\nArea: The area of the apartment in square meters.\nLocation: The district where the apartment is situated.\n\nThe provided data sample encompasses a variety of apartment types, prices, areas, and locations in different districts of Portugal, covering both new and used apartments. These data are valuable for real estate market analysis and price predictions.","270":"This dataset provides detailed information about residential properties, including their prices, area sizes, number of bedrooms, and several other key features. The dataset aims to assist in the analysis and prediction of housing prices based on various property attributes.\n\n","271":"This dataset provides comprehensive information about rental house prices across various locations in India. It includes details such as house type, size, location, city, latitude, longitude, price, currency, number of bathrooms, number of balconies, negotiability of price, price per square foot, verification date, description of the property, security deposit, and status of furnishing (furnished, unfurnished, semi-furnished).\n\n`Note: This is Recently scraped data of April 2024.`\n\n### **Dataset Glossary (Column-Wise)**\n- **House Type**: Type of house (e.g., apartment, villa, duplex).\n- **House Size**: Size of the house in square feet or square meters.\n- **Location**: Specific area or neighborhood where the property is located.\n- **City**: City in India where the property is situated.\n- **Latitude**: Geographic latitude coordinates of the property location.\n- **Longitude**: Geographic longitude coordinates of the property location.\n- **Price**: Rental price of the house.\n- **Currency**: Currency in which the price is denoted (e.g., INR - Indian Rupees).\n- **Number of Bathrooms**: Total number of bathrooms in the house.\n- **Number of Balconies**: Total number of balconies in the house.\n- **Negotiability**: Indicates whether the price is negotiable (Yes\/No).\n- **Price per Square Foot**: Price of the house per square foot.\n- **Verification Date**: Date when the rental information was verified.\n- **Description**: Additional description or details about the property.\n- **Security Deposit**: Amount of security deposit required for renting the property.\n- **Status**: Indicates the furnishing status of the property (furnished, unfurnished, semi-furnished).\n\n### **Usage**\nThis dataset aims to provide valuable insights into the rental housing market in India, enabling analysis of rental trends, comparison of prices across different locations and property types, and understanding the impact of various factors on rental prices. Researchers, analysts, and policymakers can utilize this dataset for a wide range of applications, including real estate market analysis, urban planning, and economic research.\n\n### **Acknowledgement**\nThis Dataset is created from [https:\/\/www.makaan.com\/](https:\/\/www.makaan.com\/). If you want to learn more, you can visit the Website.\n\nCover Photo by: [Playground.ai](https:\/\/playground.com\/post\/a-dynamic-and-lively-shot-of-a-daman--diu-haveli-of-two-flo-clrrmmk6a039gs6019ez0xpjm)","272":"The California Housing dataset contains information about various factors affecting housing prices in the California area. It includes features such as the per capita crime rate, average number of rooms per dwelling, proportion of residential land zoned for lots over 25,000 square feet, and more.\n# Datasource: The data were derived from information collected by the U.S. Census Service concerning housing in the area of California.\n\nColumns:\nLocation: Region\nAddress: Address of the house\nPrice: Price in millions\nCRIMS per capita crime rate by town\nZN proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS proportion of non-retail business acres per town\nCHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nNOX nitric oxides concentration (parts per 10 million)\nRM average number of rooms per dwelling\nAGE proportion of owner-occupied units built prior to 1940\nDIS weighted distances to five Boston employment centres\nRAD index of accessibility to radial highways\nTAX full-value property-tax rate per 10,000usd\nPTRATIO pupil-teacher ratio by town\nB 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT % lower status of the population\n","273":"Welcome to the Gurgaon Real Estate Dataset tailored specifically for training predictive models to forecast house prices in one of India's most dynamic urban centers. This dataset is meticulously curated, merged, and cleaned to facilitate seamless model training and accurate predictions.\n\nEncompassing a diverse array of attributes crucial for predictive modeling, this dataset provides an unparalleled opportunity for data scientists, machine learning enthusiasts, and real estate professionals to develop robust pricing models. Each entry in the dataset represents a unique property, meticulously curated and standardized to ensure consistency and reliability.\n\nKey attributes included in the dataset comprise property type, location, area, number of bedrooms and bathrooms, amenities, parking facilities, and additional features. Additionally, historical transaction data, such as previous sale prices and dates, are provided to enrich the predictive modeling process.\n\nBy leveraging this dataset, users can embark on a journey to:\n\n1. Develop sophisticated machine learning models to accurately predict house prices in Gurgaon.\n2. Identify significant predictors and drivers of house prices, aiding in market analysis and decision-making.\n3. Explore feature engineering techniques to enhance model performance and interpretability.\n4. Evaluate the impact of location, amenities, and other property attributes on pricing dynamics.\n5. Assess model performance through rigorous evaluation metrics and validation techniques.\n\nThis dataset serves as a catalyst for innovation and collaboration within the data science community, empowering practitioners to unlock insights and drive value in the real estate domain. Whether you're a seasoned data scientist or an aspiring enthusiast, this dataset offers an exciting opportunity to delve into the complexities of real estate pricing dynamics and make meaningful contributions to the field.\n\nWe encourage users to explore, analyze, and experiment with this dataset, pushing the boundaries of predictive modeling and advancing our understanding of house price dynamics in Gurgaon. Together, let's harness the power of data to unravel the mysteries of real estate pricing and pave the way for informed decision-making in one of India's most vibrant urban landscapes.","274":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","275":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","276":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","277":"","278":"This dataset contains 284 images of human chest X-ray belonging to 2 classes (Covid-19 Positive and Negative). The dataset has been divided into train and validation splits with 112 and 30 images respectively. The dataset shall be used to train deep learning models such as CNN.\n\n**Dataset Hierarchy**\n\nDataset.zip\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 normal\n\u2502   \u2514\u2500\u2500 infected\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 normal\n    \u2514\u2500\u2500 infected\n\n**Citations** : \n- Covid-19 Positive Patient Chest X-ray images (Source : [ https:\/\/github.com\/ieee8023\/covid-chestxray-dataset\/tree\/master](url))\n- Kaggle Human Lung X-ray Image Dataset (Extracted only \"Normal\") (Source : [https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia](url))","279":"# Overview\nThe Chest X-ray 8 Subset dataset is a curated collection of chest radiographs specifically designed for the development and evaluation of object detection models focused on thoracic diseases. This subset consists of 790 images and 984 annotated bounding boxes, capturing various abnormalities within the lungs and surrounding structures. The annotations are provided in both YOLO and Pascal VOC formats, facilitating their use in a wide range of machine learning frameworks.\n\n# Data Composition\n- Total Images: 790\n- Total Bounding Boxes: 984\n- Image Format: PNG\n- Annotation Formats: YOLO and Pascal VOC\n- All Images are resized to 512x512 pixels.\n\nTest set: 631 images\nVal set: 159 images\n\n# Classes and Labels\nThe dataset includes annotations for 14 different classes of thoracic diseases:\n\n1. Atelectasis,\n2. Cardiomegaly,\n3. Effusion,\n4. Infiltrate,\n5. Nodule,\n6. Mass,\n7. Pneumonia,\n8. Pneumothorax\n\n","280":"","281":"### Content\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Normal\/Covid-19\/Tuberculosis). A total of 7135 x-ray images are present.\n\n### Acknowledgements\nSources:\nhttps:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia\nhttps:\/\/www.kaggle.com\/tawsifurrahman\/tuberculosis-tb-chest-xray-dataset\nhttps:\/\/www.kaggle.com\/prashant268\/chest-xray-covid19-pneumonia\nhttps:\/\/github.com\/ieee8023\/covid-chestxray-dataset\n\n### Inspiration\nApplication of Deep Learning techniques to detect and classify lung diseases from x-ray images","282":"<div><div><div style=\"min-height: 80px\"><div><h1>Content and context<\/h1>\n<p>Tuberculosis is a disease that affects many people in developing countries. While treatment is possible, it requires an accurate diagnosis first. In these countries projects there are in many cases available X-ray machines (through low-cost projects and donations), but often the radiological expertise is missing for accurately assessing the images. An algorithm that could perform this task quickly and cheaply could drastically improve the ability to diagnose and ultimately treat the disease.<\/p>\n<p>In more developed countries, X-ray radiography is often used for screening new arrivals and determining eligibility for a work-permit. The task of manually examining images is time consuming and an algorithm could increase efficiency, improve performance and ultimately reduce cost of this screening. <\/p>\n<p>This dataset contains over 500 x-rays scans with clinical labels collected by radiologists.<\/p>\n<h1>Acknowledgements<\/h1>\n<p>The two datasets were published together in an analysis here: <a target=\"_blank\" href=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/\">https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/<\/a>.<br>\nThe datasets come from Shenzhen and Montgomery respectively.<\/p>\n<h3>China Set - The Shenzhen set - Chest X-ray Database<\/h3>\n<p>The standard digital image database for Tuberculosis is created by the National Library of Medicine, Maryland, USA in collaboration with Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China. The Chest X-rays are from out-patient clinics, and were captured as part of the daily routine using Philips DR Digital Diagnose systems. <br>\nNumber of X-rays: <\/p>\n<ul>\n<li>336 cases with manifestation of tuberculosis, and <\/li>\n<li>326 normal cases.<\/li>\n<\/ul>\n<p>It is requested that publications resulting from the use of this data attribute the source (National Library of Medicine, National Institutes of Health, Bethesda, MD, USA and Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China) and cite the following publications:  <\/p>\n<ul>\n<li>Jaeger S, Karargyris A, Candemir S, Folio L, Siegelman J, Callaghan F, Xue Z, Palaniappan K, Singh RK, Antani S, Thoma G, Wang YX, Lu PX, McDonald CJ.  Automatic tuberculosis screening using chest radiographs. IEEE Trans Med Imaging. 2014 Feb;33(2):233-45. doi: 10.1109\/TMI.2013.2284099. PMID: 24108713<\/li>\n<li>Candemir S, Jaeger S, Palaniappan K, Musco JP, Singh RK, Xue Z, Karargyris A, Antani S, Thoma G, McDonald CJ. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Trans Med Imaging. 2014 Feb;33(2):577-90. doi: 10.1109\/TMI.2013.2290491. PMID: 24239990<\/li>\n<\/ul>\n<h3>Montgomery County X-ray Set<\/h3>\n<p>X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays are abnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities, including effusions and miliary patterns. The data set includes radiology readings available as a text file.<\/p>\n<h1>Ideas<\/h1>\n<ul>\n<li>Experiment with lung segmentation<\/li>\n<li>Build disease classifiers for various conditions<\/li>\n<li>Test models on data across different manufacturers <\/li>\n<li>Build GANs that are able to make the datasets indistinguishable (Adversarial Discriminative Domain Adaptation: <a target=\"_blank\" href=\"https:\/\/arxiv.org\/abs\/1702.05464\">https:\/\/arxiv.org\/abs\/1702.05464<\/a>)<\/li><\/ul><\/div><\/div><\/div><\/div>","283":"Please cite: **Enhancing COVID-19 prediction using transfer learning from Chest X-ray images**\n\nP. -H. Huynh, T. -N. Tran and V. H. Nguyen, \"Enhancing COVID-19 prediction using transfer learning from Chest X-ray images,\" 2021 8th NAFOSTED Conference on Information and Computer Science (NICS), Hanoi, Vietnam, 2021, pp. 398-403, doi: 10.1109\/NICS54270.2021.9701516.\n\n&gt;@INPROCEEDINGS{9701516,\n  author={Huynh, Phuoc-Hai and Tran, Trung-Nguyen and Nguyen, Van Hoa},\n  booktitle={2021 8th NAFOSTED Conference on Information and Computer Science (NICS)}, \n  title={Enhancing COVID-19 prediction using transfer learning from Chest X-ray images}, \n  year={2021},\n  volume={},\n  number={},\n  pages={398-403},\n  keywords={COVID-19;Training;Pandemics;Pulmonary diseases;Computational modeling;Transfer learning;Predictive models;COVID-19;transfer learning;imbalanced dataset;X-Ray images},\n  doi={10.1109\/NICS54270.2021.9701516}}\n\n\n","284":"In this case study, you are hired by a hospital in Toronto as a deep learning consultant and tasked with automating the detection and classification process of pulmonary diseases.\n\nThe team collected extensive X-Ray chest data and asked you to develop a model that could detect and classify diseases in less than a minute.\n\nThey provided a dataset consisting of 133 images and divided into 4 classes:\n\n0 - Covid-19\n1 - Healty X-ray\n2 - Viral Pneumonia X-ray\n3 - Bacterial Pneumonia X-ray\n\nHowever, it's crucial to note that the use of AI in healthcare raises some ethical and social concerns. For instance, there are worries that AI systems may be biased and result in misdiagnoses. Additionally, there are concerns that AI may replace doctors and other healthcare professionals.","285":"","286":"In this dataset we have 3 classes.\n\n1. Pneumonia\n2. TB\n3. Normal\n\n\nA brief idea about the diseases ,\nPneumonia:\nPneumonia is an infection that inflames the air sacs  in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. Pneumonia can range in seriousness from mild to  life-threatening. It is most serious for infants and young children, people older than age 65, and people with weakened immune systems. Common causes  of pneumonia include bacteria, viruses, and fungi. Treatment depends on the type and severity of pneumonia, but typically involves antibiotics for bacterial pneumonia and antiviral medications for viral pneumonia. Vaccines  are available to prevent some types of pneumonia, such as pneumococcal pneumonia and influenza pneumonia.\n\nTuberculosis (TB):\nTuberculosis (TB) is a bacterial infection primarily affecting the lungs but can  also impact other parts of the body. It spreads through the air when an infected person coughs or sneezes, releasing infectious particles. Symptoms include persistent cough, chest pain,  coughing up blood or sputum, weakness, weight loss, lack of appetite,  chills, fever, and night sweats. TB can be latent, without symptoms and not contagious, but it can become active if untreated. Active TB is  contagious and requires several  months of antibiotic treatment. Drug-resistant strains may need stronger medications and longer treatment. Prevention involves screening, early  detection, treatment, and vaccination with the BCG vaccine, though  its effectiveness varies.","287":"This dataset is what I used to train my model. I made this dataset by combining two different datasets which are available for use in Kaggle. So I would like to cite them here. I am also thanking them for this. \n\nThis dataset combines chest X-ray images from two sources for pneumonia detection. The primary dataset, sourced from Kaggle Chest X-Ray Images (Pneumonia) [1], includes 5,863 images (1,583 normal, 4,273 pneumonia). Supplementary data from the COVID-19 Radiography Database [2], contributes 1,345 pneumonia images and 4,053 normal images. Thus making the number of pneumonia and the normal images the same that is 5618 images and in total, the dataset comprises 11,236 images. \n\nThe two sources from which I took the images are trust worthy as you can see in the source dataset pages.\n\n[1]. Chest X-Ray Images (Pneumonia) :\n https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n[2]. COVID-19 Radiography Database\nhttps:\/\/www.kaggle.com\/datasets\/tawsifurrahman\/covid19-radiography-database?rvi=1","288":"","289":"Context\n\n  The whole world is suffering from COVID-19 disease and its proper and timely diagnosis is the need of the hour. So for building an efficient AI-based diagnostic system we have collected Chest ray images from different sources so that we can train our CNN model for automating the whole diagnosis method. Therefore we have collected Chest ray images from different sources and research papers and combined them to create one comprehensive dataset that can be used by research community.\n\n  This dataset is also used in COVID Lite paper which has shown significant results by building novel CNN based solution.\n\nContent\n\n  This dataset consists of a posteroanterior (PA) view of chest X-ray images comprising Normal, Viral, and CVOID-19 affected patients. There are total 1709 CXR images.","290":"This dataset is shared by Lecturer **Mr. Wahyono, Ph. D.** as a base for his Computer Vision course at Universitas Gadjah Mada. This is the provided dataset for individual assignment COVID-19 Classification. \n\nThe source as described in the `Sources.txt` file is this GitHub link: \n\n[https:\/\/github.com\/ieee8023\/covid-chestxray-dataset](url)","291":"","292":"### pneumonia CXR dataset\nThe dataset is organized into 3 folders (train, validation, test) and contains subfolders for each image category (Normal\/COVID-19\/Bacteria\/Viral). There are 6,126 chest X-ray (CXR) images and 4 categories (Normal\/COVID-19\/Bacteria\/Viral).\n- 1,535 Normal\n- 1,708 COVID-19\n- 1,390 Bacteria pneumonia \n- 1,493 Viral pneumonia \nGround-truth lung segmentation masks are provided for the entire dataset. \n\n### References\nIn pneumonia CXR dataset, the X-ray images are collected from the following repositories:\n1. Chest X-Ray Images (Pneumonia): https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n2. COVID-QU-Ex Dataset: https:\/\/www.kaggle.com\/datasets\/cf77495622971312010dd5934ee91f07ccbcfdea8e2f7778977ea8485c1914df\n","293":"**Description**\n\nLung disease encompasses a wide range of conditions that affect the lungs and their ability to function effectively. These conditions can be caused by various factors, including infections, environmental factors, genetic predispositions, and lifestyle choices. Lung diseases can result in symptoms such as coughing, shortness of breath, chest pain, and reduced lung function. Detecting and diagnosing lung diseases is crucial for patient care, as they can have a significant impact on an individual's health and quality of life.\n\n**Global Impact:**\n\nLung diseases have a substantial global impact. According to the World Health Organization (WHO), respiratory diseases, including lung diseases, are responsible for a significant portion of global mortality. In 2016, respiratory diseases were the fourth leading cause of death worldwide, with an estimated 3.0 million deaths attributed to them. Conditions like pneumonia, chronic obstructive pulmonary disease (COPD), and lung cancer contribute to this high mortality rate. Early detection and accurate diagnosis are essential for reducing the burden of lung diseases on public health.\n\n**The Need to Detect Lung Diseases:**\n\n**Detecting lung diseases is vital for several reasons:**\n\nEarly Intervention: Early detection allows for timely medical intervention and treatment, increasing the chances of successful management and recovery.\n\n**Disease Classification:** Differentiating between various lung diseases, such as pneumonia, tuberculosis, and lung cancer, is crucial for appropriate treatment planning.\n\n**Public Health:** Effective disease detection and management can have a positive impact on public health by reducing the overall disease burden.\n\n**Lung X-Ray Image Dataset:**\n\nThe \"Lung X-Ray Image Dataset\" is a comprehensive collection of X-ray images that plays a pivotal role in the detection and diagnosis of lung diseases. This dataset contains a large number of high-quality X-ray images, meticulously collected from diverse sources, including hospitals, clinics, and healthcare institutions.\n\n**Dataset Contents:**\n\nTotal Number of Images: The dataset comprises a total of 3,475 X-ray images.\nClasses within the Dataset:\n\nNormal (1250 Images): These images represent healthy lung conditions, serving as a reference for comparison in diagnostic procedures.\n\nLung Opacity (1125 Images): This class includes X-ray images depicting various degrees of lung abnormalities, providing a diverse set of cases for analysis.\n\nViral Pneumonia (1100 Images): Images in this category are associated with viral pneumonia cases, contributing to the understanding and identification of this specific lung infection.\n\nIn conclusion, the \"Lung X-Ray Image Dataset\" plays a crucial role in the healthcare sector by providing a diverse and well-documented collection of X-ray images that support the detection, classification, and understanding of lung diseases. This resource is instrumental in advancing the field of respiratory medicine and improving patient outcomes.","294":"# Context\n\"A Song of Ice and Fire\" is an epic series of fantasy novels by George R.R. Martin. The series is set in the fictional continents of Westeros and Essos and chronicles the power struggles among noble families as they vie for control of the Iron Throne of the Seven Kingdoms. The series is renowned for its complex characters, intricate political plots, and the interweaving of multiple storylines.\n\nThe first book, \"A Game of Thrones,\" was published in 1996, and the series has since expanded to include five published novels with two more planned. The books have inspired the highly successful HBO television series \"Game of Thrones,\" which has brought the world of Westeros to a global audience.\n\n# Description of the Dataset\nThis dataset contains dialogues from the \"A Song of Ice and Fire\" book series. Each entry corresponds to a line of dialogue, along with metadata about the context in which the dialogue occurs. This dataset is valuable for those studying narrative structure, character interactions, and dialogue patterns in literature.\n\n##Column Descriptions\n- **Release Date:** The date when the book was released.\n- **Book:** The title of the book from which the dialogue is taken.\n- **Chapter Number:** The sequential number of the chapter within the book.\n- **Chapter Name:** The name of the chapter.\n- **Speaker(s):** The character(s) who is\/are speaking the line of dialogue.\n- **Dialogue:** The actual line of dialogue spoken by the character(s).\n- **Addressee(s):** The character(s) to whom the dialogue is directed.\n- **Listener(s):** The character(s) who hear the dialogue, regardless of whether it is directed to them.\n- **Mentioned Character(s):** Characters mentioned in the dialogue but not necessarily present.\n- **Present:** A boolean indicating whether the dialogue is in the present or in the past.\n\n# Acknowledgment\nThis dataset was created using a combination of artificial intelligence and human corrections to ensure accuracy and comprehensiveness. We acknowledge the effort and time invested by both AI and human editors in compiling this dataset.\n\n# Usage\nThe original work belongs to the author, George R.R. Martin. This dataset is provided for educational and research purposes only. No commercial use is allowed or will be conducted with this dataset. Users must adhere to these terms and respect the intellectual property rights of the original author.","295":"## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n","296":"","297":"### Description:\n\n**Percy Jackson Book Series NLP Dataset**\n\nThis dataset contains the raw text from the first five books of the Percy Jackson series by Rick Riordan. It provides a rich corpus for various Natural Language Processing (NLP) tasks and is ideal for researchers, educators, and developers interested in text analysis, sentiment analysis, named entity recognition, and more.\n\n**Contents:**\n- The Lightning Thief\n- The Sea of Monsters\n- The Titan's Curse\n- The Battle of the Labyrinth\n- The Last Olympian\n\n**Key Features:**\n- **Raw Text:** The dataset includes unprocessed text from the books, providing an authentic and comprehensive source for a variety of NLP experiments.\n- **Rich in Entities:** With a plethora of characters, places, and mythical references, this dataset is perfect for named entity recognition and entity linking tasks.\n- **Diverse Sentiments:** The narrative includes a wide range of emotional expressions, making it suitable for sentiment analysis projects.\n- **Complex Sentences:** The books contain varying sentence structures and vocabularies, beneficial for language modeling and text generation.\n\n**Potential Use Cases:**\n- Sentiment Analysis\n- Named Entity Recognition\n- Text Classification\n- Language Modeling\n- Text Generation\n- Tokenization and Lemmatization Practice\n\n**Notes:**\n- **Raw Data:** The text is presented as-is from the books, with no preprocessing performed. Users may need to clean and preprocess the text according to their specific project requirements.\n- **Ethical Use:** Please ensure the dataset is used in compliance with copyright laws and is intended for educational and research purposes only.\n\nUnlock the potential of Rick Riordan's engaging narratives to enhance your NLP models and algorithms!\n","298":"","299":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","300":"**Context:**\nThis dataset was created through web scraping to compile a comprehensive collection of book information, including titles, prices, and star ratings. The purpose of creating this dataset was to analyze and utilize the book data available on the Books to Scrape website, which offers a wide range of books with various attributes that can be beneficial for data analysis and project development.\n**Source:**\nThis dataset was created by web scraping this website (https:\/\/books.toscrape.com). The source code used for web scraping can be found in this GitHub repository: https:\/\/github.com\/NoorFatimaAfzal\/web-scraping\n**Inspiration:**\nThis dataset can be used for a variety of projects, including but not limited to:\n\nData Analysis and Visualization: Analyzing trends in book prices, and ratings. Creating visual representations of the data to identify patterns and insights.\nRecommendation Systems: Building a book recommendation system based on star ratings and other book attributes.\nInventory Management: Developing a system to manage and track book inventory, helping bookstores optimize their stock levels.\nPrice Comparison Tools: Creating tools to compare book prices across different platforms or over time.\nMachine Learning Projects: Using the dataset to train models for predicting book ratings or prices based on other attributes.","301":"The main dataset  on which all the operations are performed are **books.tsv** containing around 406000+ rows. The file contains two columns, **bookurl** and **data** column. Rest of the csv files are extracted files after running the **jupyter** **code**(book0-20.csv, nbook0-20.csv and book_details.csv with pkl files like tags, simi, tfidf.\nWe have built an **UI** on **Streamlit** with a file name **App1.py** and all the subsidary files required for UI are given in dataset section.\n ","302":"This is a database taken from the open library api. It can be used to perform different types of analysis such as: visual analysis; recommendation analysis; machine learning; among others. \n\nThe initial objective of taking this data is to use it in the application I am developing called LitRecs (literature recommendations). Now that I have obtained this data, and given that it is something initial, I have decided to share because it is very difficult to find databases related to books that are complete in information.","303":"","304":"","305":"##Dataset Description:\n* The dataset represents retail transactional data. It contains information about customers, their purchases, products, and transaction details. The data includes various attributes such as customer ID, name, email, phone, address, city, state, zipcode, country, age, gender, income, customer segment, last purchase date, total purchases, amount spent, product category, product brand, product type, feedback, shipping method, payment method, and order status.\n\n##Key Points:\n##Customer Information:\n* Includes customer details like ID, name, email, phone, address, city, state, zipcode, country, age, and gender.\nCustomer segments are categorized into Premium, Regular, and New.\n##Transaction Details:\n* Transaction-specific data such as transaction ID, last purchase date, total purchases, amount spent, total purchase amount, feedback, shipping method, payment method, and order status.\n##Product Information:\n* Contains product-related details such as product category, brand, and type.\nProducts are categorized into electronics, clothing, grocery, books, and home decor.\n##Geographic Information:\n* Contains location details including city, state, and country.\nAvailable for various countries including USA, UK, Canada, Australia, and Germany.\n##Temporal Information:\n* Last purchase date is provided along with separate columns for year, month, date, and time.\nAllows analysis based on temporal patterns and trends.\n##Data Quality:\n* Some rows contain null values, and others are duplicates, which may need to be handled during data preprocessing.\nNull values are randomly distributed across rows.\nDuplicate rows are available at different parts of the dataset.\n##Potential Analysis:\n* Customer segmentation analysis based on demographics, purchase behavior, and feedback.\nSales trend analysis over time to identify peak seasons or trends.\nProduct performance analysis to determine popular categories, brands, or types.\nGeographic analysis to understand regional preferences and trends.\nPayment and shipping method analysis to optimize services.\nCustomer satisfaction analysis based on feedback and order status.\n##Data Preprocessing:\n* Handling null values and duplicates.\nParsing and formatting temporal data.\nEncoding categorical variables.\nScaling numerical variables if required.\nSplitting data into training and testing sets for modeling.","306":"This describes creating an inscription image database for the ancient Marathi inscription image enhancement system. The database was created from inscription images of carved ancient Marathi text. Thus, the database provides the basis for developing practical Marathi inscription enhancement systems. It provides a standard benchmark for comparing different algorithms for Ancient Marathi Inscription Image text and helps in the research and development of ancient inscription image enhancement and recognition systems. The database contains more than 1000 sample inscription images with different scales. It is consisting of more than 10,000 characters from ancient times. Ancient Marathi Inscription Database servers like MNIST are a dataset for handwritten inscription image enhancement and recognition for Marathi carved characters. Augmentation techniques play a pivotal role in enhancing image processing models' performance and generalization capabilities. \nData Collection\nData collection is a significant step in constructing the database. Various methods are used to collect data from different sources:\n1. Utilizing research papers, archaeological books, and relevant materials.\n2. Consulting with archaeological experts.\n3. Conducting field visits and making observations.\nResearch papers, Archaeology related Books and Material:\n Both quantitative and qualitative data are collected from various archaeological books, including \"Kumbhar Anand Sanshodhan Tarang, Navbharat Prakashan, Mumbai\", \"Madhugin Itihaasaachi Sadhne Bharat Itihas Sanshodhak Mandal Khand 2\", \"Prachin Marathi Koriv Lekh, Pune Vidyapeeth Prakashan\", \"Punashya Pandharpur Shilalekh Mahathi Sahitya Patrika Ank\", and \"Maharashtra va Goa Shilalekh: Tamrapatachi Varnanatmak Suchi Mumbai.\"\nFrom these books, the following activities are conducted:\n1. Collection of Marathi inscription images.\n2. Compilation of details about Marathi inscriptions.\n3. Retrieval of information providing various ancient Marathi inscription sites.\n4. Gathering information on the location and specifics of ancient Marathi inscriptions, among other relevant details.\n Archaeology Experts:\nEngaging in communication with experts in the field of archaeology is a crucial approach to gathering data. These experts possess deep knowledge of ancient inscriptions and various ancient languages. Discussions with these experts help collect images and analyze image data. They also consulted with various archaeology Professors to find inscription reading methods, ancient Marathi characters, writing styles, inscription site locations, and relevant literature. During these discussions with experts, variations in each character were identified, which were documented for future research work. \nField Visits and Observations:\nObtaining reliable evidence through field visits and observation is very useful. The authors conducted field visits to Tuljapur, Pandharpur, Nashik, and other locations in the Solapur district, capturing original photos of relevant inscriptions. Additionally, they engaged with various tourists to gather data.  A DSLR camera was utilized to capture photos of inscriptions. The authors noted that identifying suitable inscriptions was challenging throughout these visits.\nInscription image selection:\n            In the preliminary investigation, an experimental study involved collecting and analyzing three distinct types of inscription images. These types include stone, document, and metal plate inscriptions. The inscription font, style, and size are not standard. However, a document uses standard font, style, and size. In stone inscriptions, characters are carved by different individuals, leading to font, style, and character variations. Binarization of documents proves more suitable for enhancing document images. Initially, enhancement based on binarization is applied to all three types of inscriptions, and various metrics are used to check accuracy. The findings reveal that the accuracy of stone inscription images is lower than the other types. For this research, stone inscription images are specifically chosen.\nImages are captured by using a digital camera NIKON D7200 (Focal Length -140 mm, Horizontal and vertical resolution -300*300 dpi, Bit depth-24, Max Aperture- 5, Metering mode- Pattern, Flash Mode \u2013 No flash)\n","307":"Texts related to Ayurveda in English scraped from 21 Books and 2000+ Articles from the internet.\n\nCan be used for finetuning or RAG applications.\n\nThe dataset contains two directories:\nayurveda_books contains book texts and pdfs both\nayurveda_texts contains only texts","308":"# Project Geutenberg Books\n\n1. The Life And Adventures of Robinson Crusoe\n1. Frankenstein, Or The Modern Prometheus\n\nI wanted to begin work towards literature as data. Thus, I shall begin adding books as rapidly as possible to this dataset.\n\n\n**Original Credit and License:**\n```\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n```","309":"","310":"The vast growth of social media platforms, online news outlets, and digital communication has increased user-generated content exponentially in recent years. This unprecedented surge in online discourse has sparked an urgent need to develop automated tools and techniques to effectively analyze the opinions and attitudes expressed within these expansive streams of text. Stance detection, a critical task within the field of Natural Language Processing (NLP), aims to identify the position or perspective of a writer towards a specific topic or entity by analyzing their written text and\/or social media activity, such as preferences and connections. The applications of stance detection are diverse and encompass domains such as politics, marketing, and social media analysis.\n\n## **Classes**\nThe possible stance labels are:\n\n**FAVOR** means that we can infer from the post that the author supports the target (e.g., explicitly supporting the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is in favor of the target).\n\n**AGAINST** means that we can infer from the tweet that the author is against the target (e.g., explicitly opposing the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is against the target).\n\n**NONE** means that the tweet provides no hint as to the author's stance toward the target (e.g., there is no evidence in the tweet to judge the author's stance, such as inquiries, or news that does not express any positive or negative position).\n\n# **Dataset**\nMawqif comprises 4,121 entries distributed across \"COVID-19 vaccine\" (1,373 entries), \"digital transformation\" (1,348 entries), and \"women empowerment\" (1,400 entries). \n\nIt is structured as a multi-label dataset with labels including  stance (Favor, Against, None), sentiment (Positive, Negative, Neutral), and sarcasm (Sarcastic and Non-sarcastic).","311":"","312":"","313":"","314":"# YouTube Comments Sentiment Analysis Dataset\n\n###Description\nThis dataset comprises a collection of YouTube comments extracted from videos of a specific channels, \"BidonWaraq\", \"mmr_sa1\", \"POWR-Esports\" and \"thmanyahPodcasts\". It provides a rich source of data for text analysis, sentiment analysis, sarcasm detection, and speech act recognition tasks. Each comment is annotated with several features, making it suitable for a variety of natural language processing (NLP) applications.\n\n### Dataset Content\n1. **title:** The title of the YouTube video.\n2. **comment:** The text of the comment posted on the YouTube video.\n3. **video_id:** The unique identifier for the YouTube video.\n4. **channel_id:** The unique identifier for the YouTube channel.\n5. **sentiment:** The sentiment of the comment (e.g., Positive, Neutral, Negative).\n6. **sarcasm:** A boolean indicating whether the comment is sarcastic.\n7. **speech_act:** The type of speech act (e.g., Assertion, Expression).\n8. **dangerous:** A boolean indicating whether the comment contains dangerous content.\n9. **sentiment_reasoning:** Explanation for the sentiment annotation.\n10. **sarcasm_reasoning:** Explanation for the sarcasm annotation.\n11. **speech_act_reasoning:** Explanation for the speech act annotation.\n12. **channel_name:** The name of the YouTube channel.\n\n### Potential Uses\nThis dataset can be used for:\n\nSentiment analysis to understand the general sentiment of comments towards the video content.\nSarcasm detection to identify sarcastic remarks within the comments.\nSpeech act recognition to categorize comments according to their communicative intent.\nDangerous content detection to filter out or study comments that may contain harmful content.\n\n### Challenges\nThe dataset poses several challenges including but not limited to:\n\nDealing with multilingual text, as comments might be in different languages.\nUnderstanding context for accurate sentiment and sarcasm detection.\nManaging imbalanced classes, especially for less common annotations like sarcasm or dangerous content.\n\n### Acknowledgments\nThis dataset has been collected and annotated by **Khalaya**, aiming to facilitate research and development in natural language processing and machine learning fields.","315":"Titre du Projet&nbsp;: La d\u00e9tection du sarcasme en arabe\nObjectifs du Projet :\nLa d\u00e9tection du sarcasme est le processus visant \u00e0 d\u00e9terminer si un morceau de texte est sarcastique ou non. Le sarcasme repr\u00e9sente l'un des principaux d\u00e9fis pour les syst\u00e8mes d'analyse de sentiment. La raison en est qu'une phrase sarcastique porte g\u00e9n\u00e9ralement un sentiment implicite n\u00e9gatif, bien qu'il soit exprim\u00e9 \u00e0 l'aide d'expressions positives. Cette contradiction entre le sentiment apparent et celui intentionnel cr\u00e9e un d\u00e9fi complexe pour les syst\u00e8mes d'analyse de sentiment.\nLa d\u00e9tection du sarcasme a attir\u00e9 l'attention dans d'autres langues, mais l'arabe est encore en retard \u00e0 cet \u00e9gard. Il y a eu peu d'efforts pour la d\u00e9tection du sarcasme en arabe, tels que les travaux de (Karoui et al., 2017 ; Ghanem et al., 2020) et la t\u00e2che partag\u00e9e organis\u00e9e par (Ghanem et al., 2019). Des efforts r\u00e9cents ont \u00e9t\u00e9 d\u00e9ploy\u00e9s pour construire des ensembles de donn\u00e9es standard pour cette t\u00e2che, tels que (Abbes et al., 2020 ; Abu Farha et Magdy, 2020). La t\u00e2che partag\u00e9e sur la d\u00e9tection du sarcasme et du sentiment en arabe se tiendra avec WANLP@EACL2021. Cette t\u00e2che partag\u00e9e se concentrera sur l'analyse des tweets et l'identification de leur sentiment ainsi que la d\u00e9termination de leur caract\u00e8re sarcastique ou non.\nM\u00e9thodologie&nbsp;:\n1. Collecte de Donn\u00e9es : Collecter des donn\u00e9es \u00e0 partir de diff\u00e9rentes plateformes de m\u00e9dias sociaux, en se concentrant sur des sujets sp\u00e9cifiques pertinents.\n2. Pr\u00e9traitement des Donn\u00e9es : Nettoyer et pr\u00e9traiter les donn\u00e9es textuelles pour les rendre compatibles avec les mod\u00e8les de Machine Learning.\n3. Construction d'un Mod\u00e8le de Machine Learning : Mettre en \u0153uvre un mod\u00e8le de Machine Learning adapt\u00e9 \u00e0 l'analyse de sentiments, tel qu'un mod\u00e8le de classification.\n4. Entra\u00eenement du Mod\u00e8le : Entra\u00eener le mod\u00e8le sur un ensemble de donn\u00e9es annot\u00e9 pour la classification des sentiments.\n5. \u00c9valuation du Mod\u00e8le : \u00c9valuer les performances du mod\u00e8le sur un ensemble de test et ajuster les hyper param\u00e8tres si n\u00e9cessaire.\nLivrables Attendus :\n1. Rapport Technique : Document d\u00e9taill\u00e9 d\u00e9crivant chaque \u00e9tape du projet, y compris les choix de conception et les r\u00e9sultats obtenus.\n2. Code Source : Ensemble de scripts, modules, et fichiers n\u00e9cessaires \u00e0 la reproduction du projet.\nCalendrier Pr\u00e9visionnel :\nSemaines 0 : T\u00e9l\u00e9chargement des donn\u00e9es.\n Semaines 1-2 : Uploader les donn\u00e9es T\u00e9l\u00e9charg\u00e9es puis effectuer les pr\u00e9traitements n\u00e9cessaires suivant l\u2019application\n2. Semaines 3-4 : Construction du mod\u00e8le et entra\u00eenement initial.\n3. Semaines 3-4 : \u00c9valuation du mod\u00e8le et ajustements.\n4. Semaines 5-6 : D\u00e9veloppement de l'interface utilisateur.\n5. Semaines 9-10 : Finalisation, r\u00e9daction du rapport, et le d\u00e9p\u00f4t de rapport.\nCrit\u00e8res d'\u00c9valuation :\nLe projet sera \u00e9valu\u00e9 en fonction de la qualit\u00e9 du mod\u00e8le de Machine Learning, de la pr\u00e9cision de l'analyse de sentiments, de la convivialit\u00e9 de l'interface utilisateur, et de la clart\u00e9 du rapport final.\nBaseline_score=33.71\n","316":"","317":"","318":"","319":"ArSarcasm-v2 is provided in a CSV format, we provide the same split that was used for the shared task. The training set contains 12,548 tweets, while the test set contains 3,000 tweets.\n\nThe dataset contains the following fields:\n\n- tweet: the original tweet text.\n- sarcasm: boolean that indicates whether a tweet is sarcastic or not.\n- sentiment: the sentiment of the tweet (positive, negative, neutral).\n- dialect: the dialect used in the tweet, we used the 5 main regions in the Arab world, follows the labels and their - meanings:\n- msa: modern standard Arabic.\n- egypt: the dialect of Egypt and Sudan.\n- levant: the Levantine dialect including Palestine, Jordan, Syria and Lebanon.\n- gulf: the Gulf countries including Saudi Arabia, UAE, Qatar, Bahrain, Yemen, Oman, Iraq and Kuwait.\n- magreb: the North African Arab countries including Algeria, Libya, Tunisia and Morocco.\n\nSource: https:\/\/github.com\/iabufarha\/ArSarcasm-v2\n\n@inproceedings{abufarha-etal-2021-arsarcasm-v2,\ntitle = \"Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic\",\n    author = \"Abu Farha, Ibrahim  and\n    Zaghouani, Wajdi  and\n    Magdy, Walid\",\n    booktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\n    month = april,\n    year = \"2021\",\n    }\n","320":"","321":"This dataset is part of the research \"Towards Computational Sarcastic Tweets identification: An open-Source Dataset and Developmental Framework\"\n\nIt contains 19955 rows and columns (features), that are, 'urdu_tweets' & 'is_sarcastic' (labels with 0 and 1, where 1 denotes sarcastic tweet ). This dataset can be used for sentiment analysis, specifically for sarcasm detection in urdu tweets.  ","322":"# Table of Contents\n\n[Objective](#1)\n\n[Title](#2)\n\n[Description](#3)\n\n[Team Members](#4)\n\n[Websites](#5)\n\n[Pandas Read](#6)\n\n[Notes](#7)\n\n[Egyptian Datasets Description](#8)\n\n- [Format](#9)\n\n- [1.Arabic Online Commentary (AOC)](#10)\n\n- [2.Arabic Egyptian Tweets](#11)\n\n- [3.TaghreedT](#12)\n\n- [4.Topic Extraction Data](#13)\n\n- [5.Habibi Lyrics Corpus](#14)\n\n- [6.Arabic Political Tweets](#15)\n\n- [7.ArabicReddit](#16)\n\n- [8.ar_arz_wiki_corpus](#17)\n\n- [9.QCRI](#18)\n\n- [10.SADID Benchmark Dataset](#19)\n\n- [11.DART](#20)\n\n- [12.Callhome Corpus](#21)\n\n[Other 40+ Arabic Datasets Links](#22)\n\n- [Contact](#23)\n\n- [Papers](#24)\n\n- [License](#25)\n\n# <a name=\"1\">Objective<\/a>\n\nThe datasets were part of our AI & Data Science Master's graduation project sponsored by Microsoft.\n\nCheck all the projects here: [Link](https:\/\/github.com\/Mostafanofal453)\n\n# <a name=\"2\">Title<\/a>\n\nAutomating novel terms and usage detection in Egyptian Arabic dialect\n\n# <a name=\"3\">Description<\/a>\n\nThe project aims at identifying unfamiliar terms relatively new in the Egyptian Arabic dialect. In addition to words that their meaning was changed over time with another context or misspelled to enhance the translation corpus.\n\n# <a name=\"4\">Team Members<\/a>\n\n[Khaled Elsaka](https:\/\/www.linkedin.com\/in\/khaled-el-saka-962700161\/)\n\n[Nada Montasser](https:\/\/www.linkedin.com\/in\/nada-montasser-014b1616b\/)\n\n[Shaimaa Mamdouh](https:\/\/www.linkedin.com\/in\/shaimaa-mamdouh-2616a9159\/)\n\n[Aya Reda](https:\/\/www.linkedin.com\/in\/aya-reda-5202b2191\/)\n\n[Mostafa Nofal](https:\/\/www.linkedin.com\/in\/mostafa-nofal-772625194\/)\n\n---\n\n# <a name=\"5\">Websites<\/a>\n\n We searched for the datasets in the related work papers and the most well-known websites like:\n\n\n* Kaggle\n\n* Github\n\n* Huggingface\n\n* Google Datasets\n\n* ScienceDirect\n\n* ResearchGate\n\n* IEEE Xplore\n\nThe datasets had different formats like XML, JSON, CSV and text so we unified them all to excel files format.\n\n\n\nWe also worked on labeling the datasets with the year label and filtered them to combine our final output.\n\n---\n\n# <a name=\"6\">Pandas Read<\/a>\n\n`import pandas as pd`\n\n`data = pd.read_excel(\"Name.xlsx\")`\n\n# <a name=\"7\">Notes<\/a>\n\n\u2022 All Datasets have 3 columns (Text, Year, Source)\n\n\u2022 Maximum Rows for excel file is 1,048,576 rows\n\n\u2022 If Arabic is encoded in excel:\n           - Create new excel file\n           \n           - In the Data tab, click on (From Text\/CSV) button\n           \n           - Browse and select the excel file\n           \n           - Change the File_origin to \"Unicode (UTF-8)\"\n           \n           - Save\n\n---\n\n# <a name=\"8\">Egyptian Datasets Description<\/a>\n\n## <a name=\"9\">Format<\/a>\n\nEvery dataset has the following format\n\n\u2022 Source Name\n\n\u2022 Year\n\n\u2022 Information\n\n\u2022 Rows Number\n\n\u2022 File Name\n\n---\n\n## <a name=\"10\">1.Arabic Online Commentary (AOC)<\/a>\n\n2010\n\nOnline version of Arabic newspaper: Al-Youm Al-Sabe\u2019.\n\n688,550 rows AOC_youm7_articles (ar)\n\n1,048,576 rows AOC_youm7_comments\n\n333,225 rows RestOf_AOC_youm7_comments\n\n564,854 rows AOC_subtitles_authors\n\n---\n\n## <a name=\"11\">2.Arabic Egyptian Tweets<\/a>\n\n2019\n\nEgyptian tweets cover a blend of different general topics (sentiment analysis)\n\n40,000 rows\n\nEgyptian Tweets\n\n---\n\n## <a name=\"12\">3.TaghreedT<\/a>\n\n2021\n\nEgyptian Dialect Corpus (EDC) from Facebook\n\n13,740 rows\n\nTaghreedT\n\n---\n\n## <a name=\"13\">4.Topic Extraction Data<\/a>\n\n2019\n\nEgyptian dialect tweets used for topic extraction and topic modelling research\n\n2,256 rows TE_News\n\n2,052 rows TE_Sports\n\n2,358 rows TE_Telecom\n\n2,065 rows TE_Tweets\n\n---\n\n## <a name=\"14\">5.Habibi Lyrics Corpus<\/a>\n\n2019\n\nArabic Song Lyrics corpus\n\n139,162 rows\n\nHabibi\n\n---\n\n## <a name=\"15\">6.Arabic Political Tweets<\/a>\n\n2019\n\nTwitter hashtag\n\n431,452 rows\n\nPolitical Tweets\n\n---\n\n## <a name=\"16\">7.ArabicReddit<\/a>\n\n2021\n\nArabic dataset of Reddit titles and comments from Arab and Egypt subreddits\n\n10,129 rows\n\nReddit\n\n---\n\n## <a name=\"17\">8.ar_arz_wiki_corpus<\/a>\n\n2017\n\nArabic (Modern Standard) and Egyptian Arabic dialect comparable documents from Wikipedia\n\n9126 rows\n\nArabic_Egyptian_Wikipedia (ar)\n\n---\n\n## <a name=\"18\">9.QCRI<\/a>\n\n2018\n\nManually segmented and POS tagged tweets\n\n350 rows\n\nQCRI\n\n---\n\n## <a name=\"19\">10.SADID Benchmark Dataset<\/a>\n\n2020\n\nSADID Evaluation Datasets for Low-Resource Spoken Language Machine Translation of Arabic Dialects\n\n8,989 rows\n\nSADID\n\n---\n\n## <a name=\"20\">11.DART<\/a>\n\n2018\n\nA Large Dataset of Dialectal Arabic Tweets (c) 2018 Qatar University\n\n5,889 rows\n\nDART\n\n---\n\n## <a name=\"21\">12.Callhome Corpus<\/a>\n\n2014\n\nEgyptian Arabic Speech Translation Corpus\n\n9,637 rows\n\nCallhome\n\n---\n\n# <a name=\"22\">Other 40+ Arabic Datasets Links<\/a>\n\n## <a name=\"23\">Contact<\/a>\n\nTEAD:\n\nTwo million  Egyptian tweets for sentiment analysis\n\nhttps:\/\/github.com\/HSMAabdellaoui\/TEAD\n\n\nQADI\n\n67,783 egyptian tweets\n\nhttps:\/\/alt.qcri.org\/resources\/qadi\n\n\n2016\n\n5963 words\n\nEgyptian Arabic and Modern Standard Arabic sentiment words and their polarity\n\nhttps:\/\/github.com\/NileTMRG\/NileULex\n\n\nEgyptian-Dialect-Gender-Annotated-Dataset\n\nhttps:\/\/github.com\/shery91\/Egyptian-Dialect-Gender-Annotated-Dataset\n\n\n2020\n\nar_cov19\n\n1M tweets of COVID-19 pandemic include both retweets and conversational threads\n\nhttps:\/\/huggingface.co\/datasets\/ar_cov19\n\n\nArabic Poetry\n\n 55K Arabic poem from different Categories with poets from different countries and era\n\nhttps:\/\/www.kaggle.com\/datasets\/ahmedabelal\/arabic-poetry\n\n\n2016\n\n93,700 hotel reviews in Modern Standard Arabic as well as dialectal Arabic from Booking.com\n\nhttps:\/\/huggingface.co\/datasets\/hard\n\n\n2015\n\n8364 restaurant reviews from qaym.com in Arabic for sentiment analysis\n\nhttps:\/\/huggingface.co\/datasets\/ar_res_reviews\n\n\n2019\n\nArabic Flood Twitter Dataset\n\nhttps:\/\/github.com\/alaa-a-a\/Arabic-Twitter-Corpus-for-Flood-Detection\n\n\n2021\n\nArabizi transliteration\n\nhttps:\/\/github.com\/bashartalafha\/Arabizi-Transliteration\n\n\n2018\n\n15,050 labelled YouTube comments in Arabic\n\nAnti-Social Behaviour in Online Communication\n\nhttps:\/\/onedrive.live.com\/?authkey=!ACDXj_ZNcZPqzy0&id=6EF6951FBF8217F9!105&cid=6EF6951FBF8217F9\n\n\narabic-hatespeech-data\n\nhttps:\/\/github.com\/motazsaad\/arabic-hatespeech-data\/blob\/master\/OSACT4\/README.md\n\n\nArabic Offensive Comments dataset from Multiple Social Media Platforms\n\nhttps:\/\/github.com\/shammur\/Arabic-Offensive-Multi-Platform-SocialMedia-Comment-Dataset\n\n\n2017\n\n75 million of fully vocalized words mainly 97 books from classical and modern Arabic language\n\nhttps:\/\/huggingface.co\/datasets\/tashkeela\n\n\nNADiA:\n\nNews Articles Dataset in Arabic for Multi-Label Text Categorization\nSkyNewsArabia will be referred to as NADiA1, while the latter would be NADiA2. NADiA1 is a big dataset containing 37,445 files, while NADiA2 is a huge dataset that contains 678,563 files. However, after filtering and cleaning we reduced the numbers to 35,416 and 451,230 for NADiA 1 and 2, respectively.\nNews, North Africa, Levant, Middle East, The Americas, Research, Finance & Economy, War & Terrorism, Gulf, Europe, Political Figures, Iran, Technology, Russia, Sports, Tennis, Football, English League, Arabian Sports, Spanish League, Health, East Asia, Environment, Other Countries\n\nhttps:\/\/data.mendeley.com\/datasets\/hhrb7phdyx\/1\n\n\nArabic dialect dictionary  (Django)\nArabic to English or English to Arabic definitions and see results in Levantine, Gulf, or Egyptian\nhttps:\/\/github.com\/moraesc\/arabic-dialect-dict\n\n\n2018\n\nArabic POS Dialect\n\n350 manually segmented and POS tagged tweets for each of four dialects: Egyptian, Levantine, Gulf, and Maghrebi\n\nhttps:\/\/huggingface.co\/datasets\/arabic_pos_dialect\n\n\n2017\n\n10,547 tweets, 1,682 (16%) of which are sarcastic\nArabic sentiment analysis datasets (SemEval 2017 and ASTD) and adds sarcasm and dialect labels to them\n\n{ \"Name\": \"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"2,383\", \"Unit\": \"sentences\" }  and others\n\nhttps:\/\/huggingface.co\/datasets\/ar_sarcasm\n\n\n2021\n\n50K posts\n\nSentiment Analysis for social media posts in Arabic dialect\n\n{ \"Name\":\"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"7,519\", \"Unit\": \"sentences\" }\n\nhttps:\/\/msda.um6p.ma\/msda_datasets\n\n\n2018\n\n { \"Name\": \"EGY\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"4,061\", \"Unit\": \"sentences\" }\n\nhttps:\/\/www.lancaster.ac.uk\/staff\/elhaj\/corpora.html\n\n\n2020 (Register)\n\n{ \"Name\": \"Egypt\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"6,635\", \"Unit\": \"sentences\" }\n\nhttps:\/\/sites.google.com\/view\/nadi-shared-task\n\n\n2015\n\nASTD: Arabic Sentiment Tweets Dataset\n\n10k Arabic sentiment tweets classified into four classes subjective positive, subjective negative, subjective mixed, and objective \n\nhttps:\/\/github.com\/mahmoudnabil\/ASTD\n\n\n2013\n\nA Large Scale Arabic Book Reviews Dataset\n\n63,000 book reviews in Arabic\n\nhttps:\/\/huggingface.co\/datasets\/labr\n\n\n2017\n\nEmotional-Tone\n\n10065 tweets in Arabic for Emotion detection in Arabic text\n\nhttps:\/\/huggingface.co\/datasets\/emotone_ar\n\n\n2016\n\nBRAD: Books Reviews in Arabic Dataset\n\n510,600 book reviews in Arabic language\n\nhttps:\/\/github.com\/elnagara\/BRAD-Arabic-Dataset\n\n\nDialectal system \n\nhttps:\/\/mt.qcri.org\/api\n\nhttps:\/\/alt.qcri.org\/resources1\/mt\/arabench\n\n2020\n\nCOVID-19-Arabic-Tweets-Dataset\n\n6 milllion\n\nhttps:\/\/github.com\/SarahAlqurashi\/COVID-19-Arabic-Tweets-Dataset\n\n\n2014\n\neducational video subtitles\n\nMachine learning translation\n\nEnglish and arabic\n\nhttps:\/\/huggingface.co\/datasets\/qed_amara\n\n\n2020\n\nQCRI Parallel Tweets\n\nhttps:\/\/huggingface.co\/datasets\/tweets_ar_en_parallel\n\n\n2015\n\nArabic News Article Classification\n\n14 million words with 15,891,729 tokens contained in 57,827 different articles\n\nhttps:\/\/github.com\/saidziani\/Arabic-News-Article-Classification\n\n\n2015\n\nLarge Multi-Domain Resources for Arabic Sentiment Analysis\n\nhttps:\/\/github.com\/hadyelsahar\/large-arabic-sentiment-analysis-resouces\n\n## <a name=\"24\">Papers <\/a>\n\nFreely Available Arabic Corpora: A Scoping Review  (various)\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666990022000015#bib0039\n\n\ncombination of subsets of five corpora: DART, SHAMI, TSAC, PADIC and AOC\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340921010519\n\nNileULex: A Phrase and Word Level Sentiment Lexicon for Egyptian and Modern Standard Arabic\n\nhttps:\/\/aclanthology.org\/L16-1463.pdf\n\nDART: A Large Dataset of Dialectal Arabic Tweets\nL18-1579.pdf\n\nhttps:\/\/aclanthology.org\/L18-1579.pdf\n\n\nColloquial Arabic Tweets: Collection, Automatic Annotation, and Classification \n\nhttps:\/\/ieeexplore.ieee.org\/abstract\/document\/9310507\n\n\nA Multidialectal Parallel Corpus of Arabic\n\nhttps:\/\/aclanthology.org\/L14-1435\/#:~:text=The%20daily%20spoken%20variety%20of,within%20other%20Arabic%20speaking%20communities\n\n\nAutomatic Building of Arabic Multi Dialect Text Corpora by Bootstrapping Dialect Words\n\nhttps:\/\/www.researchgate.net\/publication\/261489194_Automatic_building_of_Arabic_multi_dialect_text_corpora_by_bootstrapping_dialect_words\n\n## <a name=\"25\">License<\/a>\n\n2021\n\n10,828 Arabic tweets annotated with 10 different labels\nmulti-label Arabic COVID-19 fake news and hate speech detection dataset\n\nhttps:\/\/github.com\/MohamedHadjAmeur\/AraCOVID19-MFH\n\n\n2016\n\nArabic Web16\n\n150,211,934 Arabic Web pages with high coverage of dialectal Arabic\n\nhttps:\/\/sites.google.com\/view\/arabicweb16\/\n\n\nCamel Resources\n\nhttps:\/\/docs.google.com\/forms\/d\/e1FAIpQLSfQqhxslVSkBN5ScQ2bvvM0xUVCUnjXxtvkAjupvxm3SSeZGw\/viewform\n\nhttps:\/\/camel.abudhabi.nyu.edu\/madar-parallel-corpus\/\n\n\nBOLT Egyptian Arabic PropBank and Sense -- Discussion Forum, SMS\/Chat, and Conversational Telephone Speech\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/YS81IR\n\n\nBOLT Egyptian-English Word Alignment -- Discussion Forum Training\n400,448 words of Egyptian Arabic and English parallel text enhanced with linguistic tags to indicate word relations\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/AR1QCS\n\n\nBOLT Egyptian Arabic SMS\/Chat Parallel Training Data\n723,000 tokens of Egyptian Arabic SMS\/Chat data collected for the DARPA BOLT program along with their corresponding English translations.\n\nhttps:\/\/datasetsearch.research.google.com\/search?src=0&query=egyptian%20dialect&docid=L2cvMTFwNjZfMXRieg%3D%3D\n","323":"This is MUStARD (Multimodal Sarcasm Detection Dataset).\nThese are sarcasm data taken from various famous TV Series.\n\nYou can find paper attached with dataset here:\nhttps:\/\/github.com\/soujanyaporia\/MUStARD","324":"This dataset contains a number of tweets classified into sarcastic and further into sub-categories of sarcastic.\n\nIt is very helpful for learning purposes if you are new to NLP you can use this dataset to train and test your models.","325":"### Context\n\nWe present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https:\/\/www.politifact.com\/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n\nCover photo credits: https:\/\/blog.condati.com\/5-claims-retail-marketing-analytics-fact-check\n\n### Content\nEach record consists of 8 attributes:\n\n* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n* ```statement_originator```: the person who made the statement being fact checked\n* ```statement```:  statement being fact checked\n* ```statement_date```:  the date when statement being fact checked was made\n* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n* ```factchecker```: name of the person who fact checked the claim\n* ```factcheck_date```:  date when the fact checked article was published\n* ```factcheck_analysis_link```:  link to the fact checked analysis article\n\nThe cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https:\/\/gist.github.com\/rishabhmisra\/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n\n### Citation\n\nIf you're using this dataset for your work, please cite the following articles:\n\nCitation in text format:\n```\n1. Misra, Rishabh and Jigyasa Grover. \"Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140\/RG.2.2.29923.22566 (2022).\n```\n\nCitation in BibTex format:\n```\n@incollection{misra2022not,\n  title={Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n  author={Misra, Rishabh and Grover, Jigyasa},\n  booktitle={Deep Learning for Social Media Data Analytics},\n  pages={213--235},\n  year={2022},\n  publisher={Springer}\n}\n@dataset{misra2022politifact,\n  author = {Misra, Rishabh},\n  year = {2022},\n  month = {09},\n  pages = {},\n  title = {Politifact Fact Check Dataset},\n  doi = {10.13140\/RG.2.2.29923.22566}\n}\n```\n\nPlease link to [rishabhmisra.github.io\/publications](https:\/\/rishabhmisra.github.io\/publications\/) as the source of this dataset. Thanks!\n\n### Acknowledgements\n\nThis dataset was collected from [PolitiFact](https:\/\/www.politifact.com\/). \n\n### Inspiration\n\n* Can you categorize facts as true or false?  \n\n* Does the sources of false facts have a temporal pattern?\n\n* Is there a linguistic pattern in false facts?\n\n### Want to contribute your own datasets?\n\nIf you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https:\/\/www.amazon.com\/dp\/B08RN47C5T).\n\n### Other datasets\nPlease also checkout the following datasets collected by me:\n\n* [News Headlines Dataset For Sarcasm Detection](https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection)\n\n* [News Category Dataset](https:\/\/www.kaggle.com\/rmisra\/news-category-dataset)\n\n* [Clothing Fit Dataset for Size Recommendation](https:\/\/www.kaggle.com\/rmisra\/clothing-fit-dataset-for-size-recommendation)\n\n* [IMDB Spoiler Dataset](https:\/\/www.kaggle.com\/rmisra\/imdb-spoiler-dataset)\n","326":"## Data Article: [BanglaSarc: A Dataset for Sarcasm Detection](https:\/\/arxiv.org\/abs\/2209.13461)\n## Implementation Article: [Interpretable Bangla Sarcasm Detection using BERT and Explainable AI](https:\/\/arxiv.org\/abs\/2303.12772)\n\n## Implementation: [Sarcasm Detection](https:\/\/github.com\/Sakibapon\/SarcasmDetection)\n\n#### **Citation** \n@article{apon2022banglasarc,\n  title={BanglaSarc: A Dataset for Sarcasm Detection},\n  author={Apon, Tasnim Sakib and Anan, Ramisa and Modhu, Elizabeth Antora and Suter, Arjun and Sneha, Ifrit Jamal and Alam, MD},\n  journal={arXiv preprint arXiv:2209.13461},\n  year={2022}\n}","327":"","328":"","329":"","330":"Three classes of lesions:\n\n- Malignant Melanoma: Melanoma, also known as malignant melanoma, is the most common type of skin cancer and arises from pigment-producing cells known as melanocytes. Melanomas typically appear on the skin and rarely in other locations such as the mouth, intestines, or eye.\n\n- Seborrheic Keratosis: Seborrheic keratosis is a non-cancerous (benign) skin tumor that originates in the cells of the outer layer of the skin (keratinocytes), making it a non-melanocytic lesion.\n\n- Benign Nevus (Mole): A benign skin tumor originating from melanocytes (melanocytic).\n\n# Dataset Description\nThe dataset has been obtained from the 'International Skin Imaging Collaboration' (ISIC) archive. It contains 2750 images divided into 3 sets:\n\nTraining Set: 2000 images\n\nValidation Set: 150 images\n\nTest Set: 600 images\n\nFor each clinical case, we have two images available:\n\nDermoscopic image of the lesion (in the 'images' folder).\n\nBinary mask with segmentation between lesion (mole) and skin (in the 'masks' folder).\n\nAdditionally, there is a CSV file for each dataset (training, validation, and test), where each row corresponds to a clinical case, defined with two fields separated by commas:\n\nThe numeric id of the lesion: which allows defining the paths to the files containing the image and the mask.\n\nThe label of the lesion: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).","331":"# Multi-Races Human Body Semantic Segmentation Data\n\n\n## Description\n602 People \u20133,010 Images Multi-Races Human Body Semantic Segmentation Data,The data diversity includes headphones, body, background,and glasses.In terms of annotation, we adpoted segmentation annotations on headphones, body, background and glasses.The data can be used for tasks such as human body segmentation and the behavior detection of Video conference.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1181?source=Kaggle\n\n## Data size\n602 people, 5 images for each person\n## Collection environment\nOffice, coffee shop, supermarket, apartment\n## Race distribution\n151 Asian people, 151 black people, 150 Caucasians people, 150  brown people ,ranging from teenager to middle-aged people, (Aged between 16 and 60)\n## Gender distribution\n301 males, 301 females\n## Data diversity\ndifferent poses, different ages, different races, different collection backgrounds\n## Device\ncomputer, cellphone\n## Collecting angles\neye-level angle\n## Data format\nthe image data format is .jpg, the annotation file (mask) format is .png\n## Annotation content\nsegmentation annotation of headphones, body, background, glasses\n## Accuracy\nbased on the accuracy of the actions, the accuracy  is more than 97%; Accuracy of semantic segmentation annotation: for each object, the mask edge location errors in x and y directions are less than 5 pixels, and the category label was correctly labeled, which were considered as a qualified annotation; Annotation accuracy: each object is regarded as the unit, annotation accuracy is more than 97%\n# Licensing Information\nCommercial License\n","332":"","333":"CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 .","334":"Overview\nThis dataset is designed for the task of detecting and counting empty and occupied parking spots in a parking area. It includes images, a mask image, a video, and a utility file (util.py) that provides functions to process these images and video frames.\n\nContents\nImages: The dataset includes images of a parking area under different conditions (e.g., various times of the day). Each image captures the entire parking area and shows the status of each parking spot (empty or occupied).\n\nMask Image: A mask image is provided to identify and locate the parking spots within the parking area. This mask is a binary image where the white regions correspond to the parking spots.\n\nVideo: A video file is included to facilitate real-time detection of parking spot occupancy. The video captures the parking area over a period, showing changes in parking spot status.\n\nUtility File: The util.py file contains helper functions:\n\nempty_or_not: Determines if a given parking spot is empty or occupied.\nget_parking_spots_bboxes: Extracts bounding boxes for each parking spot based on the mask image.\nFiles\nImages Directory:\n\nContains images of the parking area.\nExamples:\nimage_001.jpg\nimage_002.jpg\n...\nMask Image:\n\nmask_1920_1080.png: A binary mask image of size 1920x1080 pixels. The white regions indicate the parking spots.\nVideo File:\n\nparking_1920_1080_loop.mp4: A video file capturing the parking area over a period. This video is used for detecting changes in the occupancy status of parking spots in real-time.\nModel File:\n\nmodel.p: A pre-trained machine learning model used by the empty_or_not function to classify parking spots as empty or occupied.\nUtility File:\n\nutil.py: Contains the utility functions for processing the images and detecting parking spots.\nUsage\nThis dataset can be used for training and testing machine learning models aimed at automating the detection of empty and occupied parking spots. The provided utility functions and pre-trained model facilitate quick experimentation and validation of results.\n\nExample Workflow\nLoad Images: Read the images and the mask image.\nExtract Parking Spots: Use the mask image and get_parking_spots_bboxes function to locate and extract parking spots from each image.\nClassify Spots: Use the empty_or_not function to classify each parking spot as empty or occupied.\nCount Available Spots: Count the number of empty parking spots and display the result.\n\nPotential Applications\nAutomated parking management systems.\nSmart city infrastructure to monitor parking lot occupancy.\nReal-time parking guidance systems for drivers.\n\nInstructions for Kaggle\nTo run the provided code on Kaggle, ensure that all necessary files (images, mask, model, and utility file) are correctly uploaded and paths are set appropriately. Use the provided functions and example code snippets to implement the parking spot detection and counting logic.\n\nBy using this dataset, researchers and developers can develop and test sophisticated parking lot detection algorithms, contributing to more efficient and automated parking management solutions.","335":"# Silicone Mask Biometric Attack Dataset\n\n## This is a demo version, full dataset is coming soon. Share with us your feedback and recieve additional samples for free!\ud83d\ude0a\n\n##Introduction\nThe Silicone Mask Attack Dataset is designed to address security challenges in liveness detection systems through 3D silicone mask attacks. These presentation attacks are key for testing Passive Liveness Detection systems crucial for iBeta Level 2 certification. This dataset significantly enhances the capabilities of liveness detection models \n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2Fe70f55bdfc175f2d96b3dfcfabe4eb11%2FIllustration.jpg?generation=1715795985085319&alt=media)\n\n##Why Silicone Mask Data?\nThis dataset is crucial for companies preparing to comply with iBeta Level 2 certification which requires anti-spoofing technologies. In today's digital security landscape, the Silicone Mask Dataset serves as a critical resource for training Machine Learning (ML) models and advanced biometric techniques to detect spoofing attempts. \n\n##Dataset Features\n\u2022\t**Variety of Masks:** Encompasses 8 unique silicone masks (male and female, Caucasian ans Asian ethnicity)\n\u2022\t**Video Collection:** There are roughly 7,000 videos that showcase detailed spoofing detection scenarios.\n\u2022\t**Helpful for Active liveness:** Head movements and blinking are included in the dataset\n\u2022\t**5 Different shooting angles:** Front Selfie and far\/ close back camera, two side shots available\u200b\n\u2022\t**Capture Devices:** Three different recording devices in selfie mode to mirror real-life conditions (Modern iPhone, Xiaomi and Samsung)\n\u2022\t **Environmental Conditions:** Captures videos across diverse lighting and background settings to ensure robustness.\n\n\u2022 **Additional Flexibility:** We can recreate this dataset using both RGB and USB camera inputs to accommodate various research needs.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n##Technical Specifications\n\u2022\t**File Format:** Videos are formatted to be compatible with mainstream ML frameworks.\n\u2022\t**Resolution and Frame Rate:** Tailored for high-resolution and optimal frame rates to capture quick mask placements.\n\n##Best Uses\nThis dataset is ideal for entities striving to meet or exceed iBeta Level 2 certification. By integrating this dataset, organizations can greatly enhance the training effectiveness of anti-spoofing algorithms, ensuring a robust and accurate performance in practical scenarios.\n\n##Conclusion\nWith its comprehensive features and simulation of real attack scenarios, the Silicone Mask Biometric Attack Dataset is indispensable for anyone involved in developing and certifying facial recognition and liveness detection technologies. Utilize this dataset to strengthen your systems against the most deceptive digital security threats.\n\n","336":"","337":"## 3D Mannequin Face Dataset for Liveness Detection (1K+ pictures)\n\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nOur 3D Mannequin Anti-Spoofing Dataset provides a comprehensive collection of mannequin images, optimized for enhancing liveness detection models in face anti-spoofing. Utilizing retail mannequins, this dataset simulates 3D faces, presenting a realistic challenge for spoofing scenarios. By incorporating 3D textures, it significantly improves the capability of anti-spoofing algorithms\n\n## Some Liveness detection SDK do not recognize this attack - here is an example\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2F484daafbdd3b29fa87822fb7d54405c1%2FTest%20example.png?generation=1715709935878279&alt=media)\n\n**Why Using Mannequin Data?**\n\nIntegrating 3D mannequin images into training scenarios boosts the effectiveness of anti-spoofing models. By training with varied 3D representations of masks, algorithms enhance their ability to distinguish between genuine and spoofed faces. This training is crucial for increasing the security and reliability of biometric systems.\n\n**Dataset Description:**\n- Scope: Features over 100 mannequins from retail environments.\n- Diversity: Includes female, male, and children mannequins, some sporting natural hair.\n- Image Capture: Utilizes both selfie and frontal camera perspectives.\n- Variations: Encompasses accessories such as glasses, sunglasses, scarves, and hats.\n- Lighting Conditions: Offers a range of lighting scenarios for well-rounded algorithm training.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n**Best Used For Anti-Spoofing Training:** The dataset\u2019s 3D characteristics elevate the training efficiency of anti-spoofing algorithms, ensuring a more robust learning experience for detection models.\n\n\nKeywords: 3D Mannequin Face Dataset, Liveness Detection Models, Anti-Spoofing, Comprehensive Dataset, Realistic Features, Detection of Genuine Faces, Sophisticated Spoofing Scenarios, Diverse Lighting, Retail Mannequins, Variability in Facial Accessories, Frontal Camera Usage, Enhanced Anti-Spoofing Algorithms, Security in Biometric Systems, Comprehensive Exposure, Facial Recognition Training, Mannequin-Based Anti-Spoofing, 3D Mask Simulation.\n","338":"## Source:\n\n- Derived off the original \"[Construction Site Safety Image Dataset Roboflow](https:\/\/www.kaggle.com\/datasets\/snehilsanyal\/construction-site-safety-image-dataset-roboflow\/data)\" dataset.\n\n## Modifications:\n\n- The original dataset had 10 different classes namely `{0: 'Hardhat', 1: 'Mask', 2: 'NO-Hardhat', 3: 'NO-Mask', 4: 'NO-Safety Vest', 5: 'Person', 6: 'Safety Cone', 7: 'Safety Vest', 8: 'machinery', 9: 'vehicle'}` for object classification after detection.\n- This modified dataset has a total of 6 classes namely `{0: 'Hardhat', 1: 'NO-Hardhat', 2: 'NO-Safety Vest', 3: 'Person', 4: 'Safety Vest', 5: 'NOT-Person'}`.\n- It combines the classes `{6: 'Safety Cone', 8: 'machinery', 9: 'vehicle'}` into a single class named 'NOT-Person' and completely removes the classes `{1: 'Mask', 3: 'NO-Mask'}`.","339":"","340":"It provides a brief overview of the recent developments in tumor classification approaches, tumor shape and size detection techniques. The aim is to generate imposed images for future classification by fusing the tumor mask with image pixels.\n\n**Refer:** M. Rana and M. Bhushan, \"Effective Tumor Diagnosis based on Shape and Size of Tumor,\" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-6, doi: [10.1109\/ICCCNT56998.2023.10308022](url).","341":"## Cutout Photo Print attack dataset (1,5K individuals+) for Presentation Attack Detection level 1 (PAD)\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nThis dataset focuses on cutout photo print attacks which might be used by iBeta and NIST FATE to assess liveness detection algorithms. This dataset is tailored for training AI models to identify a variation of cutout 2D print attack. \n\n\n**Dataset Description**\n- 1,500+ Participants: Engaged in the project\n- Diverse Representation: Balanced mix of genders and ethnicities\n- 1,500+ Cutout Mask Attacks on the participants\n\n**Photo Print attack description**\n- Each attack comprises of 10-15 sec. video \n- High-quality cutout photos with realistic colors\n- Paper attacks conducted on flat photos with a straight view on the camera (not bent or skewed)\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n## Potential Use Cases:\n\nLiveness detection: This dataset is ideal for training and evaluating liveness detection models, enabling researchers to distinguish between selfies and photo cutout print mask with high accuracy\n\nKeywords: \nCutout Print photo attack dataset, Antispoofing for AI, Liveness Detection dataset for AI, Spoof Detection dataset, Facial Recognition dataset, Biometric Authentication dataset, AI Dataset, PAD Attack Dataset, Anti-Spoofing Technology, Facial Biometrics, Machine Learning Dataset, Deep Learning\n\n","342":"CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 .","343":"**Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing)","344":"This is a collection of 1698 images featuring medical face masks. The images in this dataset could depict a variety of scenarios, such as individuals wearing masks, masks on different surfaces, or masks in various environments. This dataset could be useful for projects related to public health, medical studies, or machine learning models aimed at mask detection. Please note that the actual content of the images may vary, and a thorough review of the dataset is recommended to understand its specifics and potential applications.","345":"**Dataset Overview:**\nWelcome to the Retina Blood Vessel Segmentation dataset, a valuable resource for advancing the field of medical image analysis and enhancing the diagnosis of retinal vascular diseases. This dataset contains a comprehensive collection of retinal fundus images, meticulously annotated for blood vessel segmentation. Accurate segmentation of blood vessels is a critical task in ophthalmology as it aids in the early detection and management of various retinal pathologies, such as diabetic retinopathy and macular degeneration.\n\n**Content:**\nThe dataset comprises a total of X high-resolution retinal fundus images captured using state-of-the-art imaging equipment. Each image comes with corresponding pixel-level ground truth annotations indicating the exact location of blood vessels. These annotations facilitate the development and evaluation of advanced segmentation algorithms.\n\n**Key Features:**\n\nImage Size: The images in the dataset are of varying dimensions, ranging from XXX pixels to XXX pixels, mimicking the real-world diversity of retinal images.\nAnnotations: For each image, corresponding pixel-wise annotations in a binary mask format are provided. Blood vessel pixels are marked as 1, while background pixels are labeled as 0.\nPathological Variation: The dataset encompasses a spectrum of retinal conditions, including varying vessel widths, branching patterns, and presence of anomalies, making it suitable for evaluating the robustness of segmentation models.\nUse Cases:\nResearchers and practitioners in the fields of medical image analysis, computer vision, and artificial intelligence will find this dataset invaluable for several applications:\n\nAlgorithm Development: Use the dataset to train and test innovative segmentation algorithms, leveraging the precise annotations to achieve accurate and reliable results.\nDisease Detection: Create models that can assist in the early detection of retinal pathologies, contributing to timely medical interventions.\nEducation: The dataset can be used for educational purposes to help students and professionals understand the complexities of retinal blood vessel structure.\nEvaluation Metrics:\nPerformance evaluation will primarily involve measuring the segmentation accuracy against the ground truth annotations. Common metrics such as Intersection over Union (IoU), Dice Coefficient, and pixel-wise accuracy can be employed to quantify the model's performance.","346":"# Brain segmentation with mateuszbuda brain segmentation pytorch unet\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n# Introduction\n\nThe RSNA-MICCAI Brain Tumor Radiogenomic Classification Contest is a multi-class classification problem, giving MRIs based on radiomic features, where the goal is to predict the presence of MGMT promoter methylation.\n\nProject: [RSNA-MICCAI Brain Tumor Classification Project](https:\/\/www.kaggle.com\/code\/yannicksteph\/rsna-miccai-brain-tumor-classification)\n\nThere are three classes: \n- LGG (low-grade glioma)\n- HGG (high-grade glioma) \n- WT (hemangioblastoma)\n\nThe dataset we will be working with consists of MRI datasets provided by the Radiological Society of North America (RSNA\u00ae) and the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society). The images are provided in DICOM format and are accompanied by a CSV file containing radiomic features extracted from the images.\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n#### Why\nThe **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was utilized to obtain data for our brain dataset. This library is based on the Unet neural network architecture and specifically designed for brain segmentation from medical images.\n\nBy leveraging the capabilities of this library, accurate brain segmentation was achieved on the images within our dataset. Segmentation is a critical task in medical imaging as it enables the extraction of precise information about different regions or classes, in this case, brain structures.\n\nThe selection of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was based on its exceptional performance and user-friendly nature. It provides an efficient implementation of the Unet architecture, renowned for its success in biomedical image segmentation. Consequently, our project was able to deliver reliable and accurate results for brain segmentation.\n\nIn conclusion, the utilization of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library played a pivotal role in acquiring accurate brain segmentation data for our dataset. By leveraging this library, we efficiently segmented medical images and extracted valuable information to further our project's objectives.\n\nSource: [mateuszbuda_brain-segmentation-pytorch_unet on PyTorch Hub](https:\/\/pytorch.org\/hub\/mateuszbuda_brain-segmentation-pytorch_unet\/)\n\n#### Library\n\nTo achieve tumor segmentation, the U-Net for Brain MRI model will be employed.\n\nU-Net for Brain MRI is a convolutional neural network model specifically designed for segmenting brain MRI images. It features a U-shaped architecture with branch connections, comprising four levels of blocks. Each block consists of two convolution layers with batch normalization, ReLU activation function, and an encoding part with a max pooling layer, while the decoding part utilizes up-convolution. The number of convolution filters varies across the model's levels, ranging from 32 to 256.\n\nTo utilize the model, an input brain MRI image with three channels corresponding to pre-contrast, FLAIR, and post-contrast sequences should be provided. The image should be scaled to a size of 256x256 pixels and normalized using the z-score method per volume.\n\nThe pre-trained U-Net model produces a single-channel probability map indicating anomalous regions in the input image. By applying an appropriate threshold, this probability map can be converted into a binary segmentation mask.\n\nIn summary, U-Net for Brain MRI is a pre-trained model capable of automatically segmenting abnormalities in brain MRI images. Its application extends to various medical imaging tasks, including brain tumor detection and analysis.\n\n\nTo perform shape analysis and extract relevant features, the **2D and 3D** class will be utilized.\n\nData from Source: [rsna-miccai-brain-tumor-radiogenomic-classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification)\n","347":"This dataset is the processed version of the original dataset available at [Computed Tomography Images for Intracranial Hemorrhage Detection and Segmentation](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/). The `split_raw_data.py` provided by the original dataset author has been modified for updated libraries and Python 3.11. `ct_ich.yml` is also updated to reflect the same to get the environment where the data was processed.\n\nThe data present in `image` and `label` folders are in 512 x 512 grayscale PNG files. There are 2814 pairs of image and segmentation mask respectively.\n\nOriginal DOI for this data: https:\/\/doi.org\/10.13026\/4nae-zg36\n\nLicense: The PhysioNet Restricted Health Data License\nVersion 1.5.0\nMake sure to sign in to your Physionet account and agree to the the terms and conditions of the [original dataset](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/) to use this medical dataset.","348":"This Dataset contains the ***JPG*** images of ***Breast Cancer*** taken from the ***CBIS-DDSM***.\n\n![Breast Cancer Images](https:\/\/i.imgur.com\/rz4rtQI.png)\n\n## Descripton\nThis dataset contains ***JPG*** format images (2.49 GB) of the original ***CBIS-DDSM dataset*** (163 GB) which are in ***DICOM*** format and by maintaining the same resolution of the images as it was in the original dataset.\n\nThe original dataset was split into train and test by having two cases one is ***Mass*** and another is ***Calcification(Calc)*** i.e. ***calc_case_description_test_set.csv, calc_case_description_train_set.csv,  mass_case_description_test_set.csv, mass_case_description_train_set.csv, and metadata.csv*** but in here this dataset is made by *converting the images from DICOM to JPG format, removing the unnecessary columns by Data Cleaning and concatenating both the Mass and Calcification(Calc) cases train test into one* i.e. ***calc_case(with_jpg_img).csv, mass_case(with_jpg_img).csv, and metadata(with_jpg_img).csv***.\n\n<br>\n| Collection | |\n| --- | --- |\n| Number of Studies | 6775 |\n| Number of Series | 6775 |\n| Number of Participants | 1,566(NB) |\n| Number of Images | 10239 |\n| Modalities | MG |\n| Image Size (GB) | 6(.jpg) |\n\n\n<br>\n\n**NB**: *The image data for this collection is structured such that each participant has multiple patient IDs. For example, pat_id 00038 has 10 separate patient IDs which provide information about the scans within the IDs (e.g. Calc-Test_P_00038_LEFT_CC, Calc-Test_P_00038_RIGHT_CC_1) This makes it appear as though there are 6,671 participants according to the DICOM metadata, but there are only 1,566 actual participants in the cohort.*\n<br>\n\n##File Description\n###1. JPG image folder file structure\n![Cancer Image file structure](https:\/\/i.imgur.com\/KtBMlVm.png)\n**File naming:** \n- Folder name: `Subject ID &gt; Study UID &gt; Series UID`\n- File name:  `Series Description &gt; img_0 &gt; 1.jpg`\n\n<br>\n###2. CSV files description\n| CSV File | Description |\n| --- | --- | \n| calc_case(with_jpg_img).csv | This file contains the ***Calcification cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| mass_case(with_jpg_img).csv | This file contains the ***Mass cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| metadata(with_jpg_img).csv | This file contains both of the ***Mass and Calcification(Calc) patients*** with their *Series UID, Subject ID, Study UID, Series Description, Modality, SOP Class Name, SOP Class UID, Number of Images, jpg_folder_path* |\n\n<br><br>\n\n##Summary\nThe ***CBIS-DDSM (Curated Breast Imaging Subset of DDSM)*** is an updated and standardized version of the  ***Digital Database for Screening Mammography (DDSM)***.  The DDSM is a database of ***2,620 scanned film mammography studies***. It contains ***normal, benign, and malignant*** cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer.  The images have been decompressed and converted to DICOM format.  Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included.  A manuscript describing how to use this dataset in detail is available at https:\/\/www.nature.com\/articles\/sdata2017177.\n\nPublished research results from work in developing decision support systems in mammography are difficult to replicate due to the lack of a standard evaluation data set; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. Few well-curated public datasets have been provided for the mammography community. These include the DDSM, the Mammographic Imaging Analysis Society (MIAS) database, and the Image Retrieval in Medical Applications (IRMA) project. Although these public data sets are useful, they are limited in terms of data set size and accessibility.\n\nFor example, most researchers using the DDSM do not leverage all its images for a variety of historical reasons. When the database was released in 1997, computational resources to process hundreds or thousands of images were not widely available. Additionally, the DDSM images are saved in non-standard compression files that require the use of decompression code that has not been updated or maintained for modern computers. Finally, the ROI annotations for the abnormalities in the DDSM were provided to indicate a general position of lesions, but not a precise segmentation for them. Therefore, many researchers must implement segmentation algorithms for accurate feature extraction. This causes an inability to directly compare the performance of methods or to replicate prior results. The CBIS-DDSM collection addresses that challenge by publicly releasing a curated and standardized version of the DDSM for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography.\n<br><br>\n\n##Source of the Original dataset\nhttps:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n<br><br>\n\n##Citations & Data Usage Policy \nUsers must abide by the TCIA Data Usage Policy and Restrictions. Attribution should include references to the following citations:\n\n<br>\n###CBIS-DDSM Citation\n&gt;Sawyer-Lee, R., Gimenez, F., Hoogi, A., & Rubin, D. (2016). Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) (Version 1) [Data set]. The Cancer Imaging Archive. https:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n\n<br>\n###Publication Citation\n&gt;Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M., & Rubin, D. L. (2017). A curated mammography data set for use in computer-aided detection and diagnosis research. In Scientific Data (Vol. 4, Issue 1). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1038\/sdata.2017.177\n\n<br>\n###TCIA Citation\n&gt;Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., & Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. In Journal of Digital Imaging (Vol. 26, Issue 6, pp. 1045\u20131057). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1007\/s10278-013-9622-7\n\n\n<br><br>\nNOTE: ***The CBIS-DDSM dataset lack DICOM image for cropped and ROI images, so there are some cropped and ROI images that may not be found corresponding to their given path, so it is recommended to use the full mammography images from this dataset (only)***","349":"# The Portrait and 26 Photos (`272` people), faces dataset\n\nEach set includes 27 photos of people. Each person provided two types of photos: one photo in profile (portrait_1), and 26 photos from their life (photo_1, photo_2, ..., photo_26).\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 7,300+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, \u2026, photo_26_extension, portrait_1_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, \u2026, photo_26_resolution, portrait_1_resolution** - photo resolution in the dataset\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test) to discuss your requirements, learn about the price and buy the dataset**\n\n## The Portrait\nThe portrait photo is a photo that shows a person in profile. Mandatory conditions for the photo are:\n- The person is pictured alone;\n- Shoulder-length photo;\n- No sunglasses or medical mask on the face;\n- The face is calm, with no smiling or gesturing.\n\n## 26 Photos\nThe rest of the photos are completely different, with one exception being that they show a person from The Portrait. There may be different people in it, taken at different times of life and in different locations. The person may be laughing, wearing a mask, and surrounded by friends.\n \n*keywords: biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, supervised learning dataset, person re-identification, person re-identification dataset, person re-Identification by photo, person re-ID dataset*","350":"# Paper\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html\n\n# Abstract\nThe previous image synthesis research for surgical vision had limited results for real-world applications with simple simulators, including only a few organs and surgical tools and outdated segmentation models to evaluate the quality of the image. Furthermore, none of the research released complete datasets to the public enabling open research. Therefore, we release a new dataset to encourage further study and provide novel methods with extensive experiments for surgical scene segmentation using semantic image synthesis with a more complex virtual surgery environment. First, we created three cross-validation sets of real image data considering demographic and clinical information from 40 cases of real surgical videos of gastrectomy with the da Vinci Surgical System (dVSS). Second, we created a virtual surgery environment in the Unity engine with \ufb01ve organs from real patient CT data and 22 the da Vinci surgical instruments from actual measurements. Third, We converted this environment photo-realistically with representative semantic image synthesis models, SEAN and SPADE. Lastly, we evaluated it with various state-of-the-art instance and semantic segmentation models. We succeeded in highly improving our segmentation models with the help of synthetic training data. More methods, statistics, and visualizations on https:\/\/sisvse.github.io\/.\n\n# The contribution of our work\n- We release the \ufb01rst large-scale instance and semantic segmentation dataset, including both real and synthetic data that can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS\n- We systematically analyzed surgical scene segmentation using semantic image synthesis with state-of-the-art models with ten combinations of real and synthetic data.\n- We found exciting results that synthetic data improved low-performance classes and was very e\ufb00ective for Mask AP improvement while improving the segmentation models overall.\n\n# Data generation\nWe collected 40 cases of real surgical videos of distal gastrectomy for gastric cancer with the da Vinci Surgical System (dVSS), approved by an institutional review board at the medical institution. In order to evaluate generalization performance, we created three cross-validation datasets considering demographic and clinical variations such as gender, age, BMI, operation time, and patient bleeding. Each cross-validation set consists of 30 cases for train\/validation and 10 cases for test data. You can find the overall statistics and demographic and clinical information details [in the paper](https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html).\n\n# Object categories\nWe list \ufb01ve organs (Gallbladder, Liver, Pancreas, Spleen, and Stomach) and 13 surgical instruments that commonly appear from surgeries (Hamonic Ace; HA, Stapler, Cadiere Forceps; CF, Maryland Bipolar Forceps; MBF, Medium-large Clip Applier; MCA, Small Sclip Applier; SCA, Curved Atraumatic Graspers; CAG, Suction, Drain Tube; DT, Endotip, Needle, Specimenbag, Gauze). We classify some rare organs and instruments as \u201cother tissues\u201d and \u201cother instruments\u201d classes. The surgical instruments consist of robotic and laparoscopic instruments and auxiliary tools mainly used for robotic subtotal gastrectomy. In addition, we divide some surgical instruments according to their head, H, wrist; W, and body; B structures, which leads to 24 classes for instruments in total.\n\n# Virtual Surgery Environment and Synthetic Data\nAbdominal computed tomography (CT) DICOM data of a patient and actual measurements of each surgical instrument are used to build a virtual surgery environment. We aim to generate meaningful synthetic data from a sample patient. We annotated \ufb01ve organs listed for real data and reconstructed 3D models by using VTK. In addition, we precisely measured the actual size of each instrument commonly used for laparoscopic and robotic surgery with dVSS. We built 3D models with commercial software such as 3DMax, Zbrush, and Substance Painter. After that, we integrated 3D organ and instrument models into the unity environment for virtual surgery. A user can control a camera and two surgical instruments like actual robotic surgery through a keyboard and mouse in this environment. To reproduce the same camera viewpoint as dVSS, we set the exact parameters of an endoscope used in the surgery. While the user simulates a surgery, a snapshot function projects a 3D scene into a 2D image. According to the projected 2D image, the environment automatically generates corresponding segmentation masks.\n\n# Qualified annotations\nSeven annotators trained for surgical tools and organs annotated six organs and 14 surgical instruments divided into 24 instruments according to head, wrist, and body structures with a web-based computer vision annotation tool (CVAT). We call this real data (R). After that, three medical professionals with clinical experience inspected the annotations to ensure their quality. The three medical professionals also manually simulated virtual surgeries to generate virtual surgical scenes. We call this manual synthetic data (MS). On the other hand, we also use an automatic data generation method called domain randomization, a technique to put objects randomly in a scene to cover the variability in real-world data. We call this domain randomized synthetic data (DRS).\n\nIf the dataset is helpful for your research, [please cite the research](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-031-16449-1_53).\n\n#Contact\n- Email to yjh2020@hutom.io , mkchoi@hutom.io\n- Discussion on https:\/\/www.kaggle.com\/datasets\/yjh4374\/sisvse-dataset\n- Issues on https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n\n# Links\n- https:\/\/sisvse.github.io\n- https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html","351":"The dataset introduced in the paper [\"MFVT Dataset for Real-time Masked Facial Detection\"](https:\/\/drive.google.com\/file\/d\/1ZDmdQHI9KFFFDYbhKxun_44QBhsRDNt5\/view)\n\nNowadays, there has been quite a lot of research done on the Masked Facial Detection problem, including the introduction of new datasets and improvements to existing baseline models. However, many of the datasets that have been introduced have some limitations.\n- Some datasets, such as **MAFA**, **Moxa3K**, and **AIZOOTech**, only consider whether a person is wearing a mask or not. The detail that a person is wearing a mask in a proper way as recommended by medical experts has been ignored.\n- For that point, there are also some datasets that take it into account as well, such as **PWMFD** and **FMLD**. However, the way the authors annotate those datasets has some unreasonableness about the material of the mask and the items used alternatively for masks (scarfs, hijabs for example). Furthermore, the bounding boxes of the objects in those datasets are drawn quite carelessly.\n\nTherefore, we introduce the MFVT dataset in order to address those shortcomings of the datasets used for the Masked Facial Detection problem. To build the MFVT dataset, we selected images from previously published datasets and carefully re-annotated the entire dataset with our mask-wearing conventions.\n\nThe MFVT dataset has total of 13507 images. The overall statistics for this dataset are given in the below table:\n<img src=\"https:\/\/i.imgur.com\/V5c1Ipb.jpeg\">\n\nWe provide the COCO annotation format version of this dataset. For the meaning of class indexes, **0**, **1** and **2** correspond to **OK**, **NONE** and **WRONG**.","352":"The dataset contains the Optical Coherence Tomography (OCT) images and their masks of Cystoid Macular Edema (CME) ocular disease for image segmentation purposes.\n\nAcquiring OCT data from Diabetic Macular Edema (DME) patients is difficult due to the lack of an open database from any ophthalmic hospital. Therefore, we obtained a database of retinal images available at [Retinal OCT Images](https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/kermany2018) in which we took a dataset of only DME subjects. From this dataset, only 1000 images were chosen by the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, that are trained to identify Cystoid Macular Edema (CME) and its progression and provide a confirmatory diagnosis of CME. Whereas 200 OCT images are taken with the consent of patients from the Institute of Ophthalmology, Jamshoro for the testing set. These OCT images were explicitly selected to contain several CME regions and a wide variety in their shapes and sizes which is utilized for both training and test sets to get better results.\n\nIn OCT images, the approach of semantic segmentation is that each pixel of an image is labeled with a corresponding clinical class representing a particular disease. The manual binary image masking was performed accordingly using the Image Segmentation application in MATLAB software offers a variety of tools (flood fill\/draw freehand) for manually selecting an image region of interest (ROI). This was necessary for the training of our model to identify and isolate the CME and to validate the accuracy of digitally picked ROIs (CME). The pixels whose value is \u201c1\u201d in the mask image indicate the foreground, that is CME, and the pixels whose value is \u201c0\u201d in the mask image indicate the background.\nThese are the actual ground truth label image and for the given input DME image, the model must predict the labels.\n\nThis dataset has been arranged and categorized under the supervision of the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, Pakistan, that are trained to identify CME and its progression and provide a confirmatory diagnosis of CME.\n\n\n## **Cite Our Work as:** <br>\n#### **Research Paper**\n* Ahmed Z, Panhwar SQ, Baqai A, Umrani FA, Ahmed M, Khan A. Deep learning based automated detection of intraretinal cystoid fluid. Int J Imaging Syst Technol. 2021;1-16. https:\/\/doi.org\/10.1002\/ima.22662 \n\n#### **Dataset**\n* Zeeshan Ahmed, Munawar Ahmed, Attiya Baqai, & Fahim Aziz Umrani. (2022). <i>Intraretinal Cystoid Fluid<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DS\/2277068","353":"","354":"# Face Mask Detection - Faces Dataset\nDataset includes  376 000+ images, 4 types of mask worn on 94 000 unique faces. All images were collected by **[TrainingData.pro](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)**\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 376 000+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, photo_3_extension, photo_4_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, photo_3_extension, photo_4_resolution** - photo resolution in the dataset\n\n# Content\n\nFile with the extension **.csv** includes variables:\n- ID - image id\n- TYPE - image type\n- USER_ID - user id\n- GENDER - gender of a person\n- AGE - person's age\n- name - file name\n- size_mb - image size\n\n## Types of images:\n- **TYPE 1** - There is no mask on the face. \n- **TYPE 2**  - The mask is on, but does not cover the nose or mouth.\n- **TYPE 3** - The mask covers the mouth, but does not cover the nose.\n- **TYPE 4** - The mask is worn correctly, covers the nose and mouth.\n\n![](https:\/\/sun9-10.userapi.com\/impg\/qn0W_s_C3xVYUc_5_IUNEJ6a3xQexHj8GSLlHg\/breQf6Qthzo.jpg?size=2560x988&quality=96&sign=1d633a32909adb9c95eeb5e781e17490&type=album)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1) to learn about the price and buy the dataset**\n\n# **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** provides high-quality data annotation tailored to your needs.\n\n*keywords: facial mask detection, face masks detection, face masks classification, face masks recognition, covid-19, re-identification, public safety, health, automatic face mask detection, biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, object detection dataset, deep learning datasets, computer vision datset, human images dataset, human faces dataset*","355":"# Kvasir-Instrument\nGastrointestinal (GI) tract pathologies are screened, biopsied, and resected (if needed) periodically using surgical tools. However, these biopsied and\/or resected areas are not tracked due to which the video analysis for assessing disease burden or the amount of pathology resection remains unknown. To tackle such issues, we have released the novel \u201cKvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy\u201d dataset, which consists of 590 annotated frames comprising of GI procedure tools such as snares, balloons, biopsy forceps, etc. By adding segmentation masks and bounding boxes information to this dataset, we enable computer vision and GI endoscopy researchers to contribute to the field of automated tool segmentation.\n\n# Dataset Details\nThe Kvasir-Instrument dataset (size 170 MB) contains 590 endoscopic tool images and their ground truth mask. The resolution of the image in the dataset varies from 720x576 to 1280x1024. The image file is encoded using jpeg compression. To the best of our knowledge, this is the first attempt to provide the GI tract organ tools dataset. The open-access dataset can be easily downloaded for research and educational purposes. To facilitate the training and testing on the same dataset, we also provide a train-test split so that researchers can build the methods and improve the results using the same dataset. The bounding box information (box coordinates (x, y, width, height)) for the corresponding images are stored in a JSON file. The dataset is designed to push the state-of-the-art solution for the automatic tool segmentation in gastrointestinal endoscopy.\n\n# Applications of the Dataset\nThe Kvasir-Instrument is intended to be used for researching and developing new algorithms for image segmentation, detection, and localization tasks. We have provided a separate file for training and validation which will assist in the development of approaches across the same dataset.\n\n# Annotation Protocol\nWe took a three-step strategy during annotation. First, the selected dataset samples were labeled by two experienced research assistants. These annotations were sent to the expert gastroenterologist for the verification. Finally, the suggested changes were incorporated, and the images were validated for those samples.\n\n# Suggested Metrics for Segmentation\nWe suggest calculating the dice similarity coefficient (DSC) and Jaccard index or Intersection over Union (IoU) for the segmentation task. The other standard metrics for medical image segmentation are precision, recall, and overall accuracy. We also suggest calculating these metrics.\n# \nSuggested Metrics for Detection and Localization\nFor the detection and localization task, we suggest calculating the average precision at different IoU thresholds. Usually, AP at IoU threshold of 50 is taken for evaluation of this dataset. We also recommend calculating overall IoU.\n\n\nFor more information:\nhttps:\/\/datasets.simula.no\/kvasir-instrument\/","356":"The \u201cMedico automatic polyp segmentation challenge\u201d aims to develop computer-aided diagnosis systems for automatic polyp segmentation to detect all types of polyps (for example, irregular polyp, smaller or flat polyps) with high efficiency and accuracy. The main goal of the challenge is to benchmark semantic segmentation algorithms on a publicly available dataset, emphasizing robustness, speed, and generalization.\n\nParticipants will get access to a dataset consisting of 1,000 segmented polyp images from the gastrointestinal tract and a separate testing dataset. The challenge consists of two mandatory tasks, each focused on a different requirement for efficient polyp detection. We hope that this task encourages multimedia researchers to apply their vast knowledge to the medical field and make an impact that may affect real lives.\n\n# Data\nThe dataset contains 1,000 polyp images and their corresponding ground truth mask. The datasets were collected from real routine clinical examinations at Vestre Viken Health Trust (VV) in Norway by expert gastroenterologists. The VV is the collaboration of the four hospitals that provide healthcare service to 470,000 peoples. The resolution of images varies from 332\u2715487 to 1920\u27151072 pixels. Some of the images contain green thumbnail in the lower-left corner of the images showing the position marking from the ScopeGuide (Olympus). The training dataset can be downloaded from https:\/\/datasets.simula.no\/kvasir-seg\/.\n\nThe test dataset is now released. It can be downloaded from https:\/\/drive.google.com\/file\/d\/1uP2W2g0iCCS3T6Cf7TPmNdSX4gayOrv2\/view?usp=sharing.","357":"Dataset Details\nLabeled Images. In total, the dataset contains 10,662 labeled images stored using the JPEG format. The images can be found in the images folder. The classes, which each of the images belongto, correspond to the folder they are stored in (e.g., the \"polyp\" folder contains all polyp images, the \"barretts\" folder contains all images of Barrett\u2019s esophagus, etc.). The number of images per class are not balanced, which is a general challenge in the medical field due to the fact that some findings occur more often than others. This adds an additional challenge for researchers, since methods applied to the data should also be able to learn from a small amount of training data. The labeled images represent 23 different classes of findings.\n\nUnlabeled Images. In total, the dataset contains 99,417 unlabeled images. The unlabeled images can be found in the unlabeled folder which is a subfolder in the image folder, together with the other labeled image folders. In addition to the unlabeled image files, we also provide the extracted global features and cluster assignments in the Hyper-Kvasir Github repository as Attribute-Relation File Format (ARFF) files. ARFF files can be opened and processed using, for example, the WEKA machine learning library, or they can easily be converted into comma-separated values (CSV) files.\n\nSegmented Images. We provide the original image, a segmentation mask and a bounding box for 1,000 images from the polyp class. In the mask, the pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain polyp pixels. The bounding box is defined as the outermost pixels of the found polyp. For this segmentation set, we have two folders, one for images and one for masks, each containing 1,000 JPEG-compressed images. The bounding boxes for the corresponding images are stored in a JavaScript Object Notation (JSON) file. The image and its corresponding mask have the same filename. The images and files are stored in the segmented images folder. It is important to point out that the segmented images have duplicates in the images folder of polyps since the images were taken from there.\n\nAnnotated Videos. The dataset contains a total of 373 videos containing different findings and landmarks. This corresponds to approximately 11.62 hours of videos and 1,059,519 video frames that can be converted to images if needed. Each video has been manually assessed by a medical professional working in the field of gastroenterology and resulted in a total of 171 annotated findings.\n\nDevelopment Dataset\nThe dataset can be split into four distinct parts; Labeled image data, unlabeled image data, segmented image data, and annotated video data. Each part is further described below. In total, the dataset contains 110,079 images and 373 videos where it captures anatomical landmarks and pathological and normal findings. The results is more than 1.1 million images and video frames all together.\n\nTest Dataset\nThe dataset is split into two distinct parts; the classification dataset and segmentation dataset. The classification dataset should be used to perform the detection and speed tasks, while the segmentation part should be used for the segmentation task.\n\nMore details can be found here:\nhttps:\/\/endotect.com\/\n","358":"# Kvasir-SEG information:\n\nThe Kvasir-SEG dataset (size 46.2 MB) contains 1000 polyp images and their corresponding ground truth from the Kvasir Dataset v2. The images' resolution in Kvasir-SEG varies from 332x487 to 1920x1072 pixels. The images and its corresponding masks are stored in two separate folders with the same filename. The image files are encoded using JPEG compression, facilitating online browsing. The open-access dataset can be easily downloaded for research and educational purposes.\n\n# Applications of the Dataset\nThe Kvasir-SEG dataset is intended to be used for researching and developing new and improved methods for segmentation, detection, localization, and classification of polyps. Multiple datasets are prerequisites for comparing computer vision-based algorithms, and this dataset is useful both as a training dataset or as a validation dataset. These datasets can assist the development of state-of-the-art solutions for images captured by colonoscopes from different manufacturers. Further research in this field has the potential to help reduce the polyp miss rate and thus improve examination quality. The Kvasir-SEG dataset is also suitable for general segmentation and bounding box detection research. In this context, the datasets can accompany several other datasets from a wide range of fields, both medical and otherwise.\n\n# Ground Truth Extraction\nWe uploaded the entire Kvasir polyp class to Labelbox and created all the segmentations using this application. The Labelbox is a tool used for labeling the region of interest (ROI) in image frames, i.e., the polyp regions for our case. We manually annotated and labeled all of the 1000 images with the help of medical experts. After annotation, we exported the files to generate masks for each annotation. The exported JSON file contained all the information about the image and the coordinate points for generating the mask. To create a mask, we used ROI coordinates to draw contours on an empty black image and fill the contours with white color. The generated masks are a 1-bit color depth images. The pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain positive pixels. Some of the original images contain the image of the endoscope position marking probe, ScopeGuide TM, Olympus Tokyo Japan, located in one of the bottom corners, seen as a small green box. As this information is superfluous for the segmentation task, we have replaced these with black boxes in the Kvasir-SEG dataset.\n\n# Suggested Metrics\nThere are different metrics for evaluating the performance of the architectures on the image segmentation dataset. For medical image segmentation task, the most commonly used ones are Dice coefficient and Intersection over Union (IOU). Based on related work in this field, we have used these metrics for the evaluation of the algorithms. In future work, we encourage the use of these metrics for evaluating the performance of the model. In the future, it might be even better to include as many as possible metrics for the fair comparison of the models.\n\nThe bounding box (coordinate points) for the corresponding images are stored in a JSON file. This dataset is designed to push the state-of-the-art solution for the polyp detection task. Some examples of the dataset.","359":"### Context\n\nHumans in the Loop is publishing an open access dataset annotated as a contribution to the worldwide fight against COVID-19. \n\n### Content\n\nThe dataset consists of 6k images acquired from the public domain with an extreme attention to diversity, featuring people of all ethnicities, ages, and regions. In addition, the dataset covers 20 classes of different accessories as well as a classification of faces with a mask, without a mask, or with an incorrectly worn mask (class list is available in meta.json).\n\n### Acknowledgements\n\nThe images were collected and annotated by the refugee workforce of Humans in the Loop in Bulgaria. The platform used for annotation is Supervise.ly.","360":"### COVID-QU-Ex Dataset\nThe researchers of Qatar University have compiled the COVID-QU-Ex dataset, which consists of 33,920 chest X-ray (CXR) images including:\n-\t11,956 COVID-19\n-\t11,263 Non-COVID infections (Viral or Bacterial Pneumonia)\n-\t10,701 Normal\nGround-truth lung segmentation masks are provided for the entire dataset. This is the largest ever created lung mask dataset. \n\n**If you use  COVID-QU-Ex Dataset in your research, please consider to cite the publications\/dataset below:**\n[1] A. M. Tahir, M. E. H. Chowdhury, A. Khandakar, Y. Qiblawey, U. Khurshid, S. Kiranyaz, N. Ibtehaz, M. S. Rahman, S. Al-Madeed, S. Mahmud, M. Ezeddin, K. Hameed, and T. Hamid, \u201cCOVID-19 Infection Localization and Severity Grading from Chest X-ray Images\u201d, Computers in Biology and Medicine, vol. 139, p. 105002, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.105002.\n[2] Anas M. Tahir, Muhammad E. H. Chowdhury, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Serkan Kiranyaz, Uzair Khurshid, Nabil Ibtehaz, Sakib Mahmud, and Maymouna Ezeddin, \u201cCOVID-QU-Ex .\u201d Kaggle, 2021, https:\/\/doi.org\/10.34740\/kaggle\/dsv\/3122958.\n[3] T. Rahman, A. Khandakar, Y. Qiblawey A. Tahir S. Kiranyaz, S. Abul Kashem, M. Islam, S. Al Maadeed, S. Zughaier, M. Khan, M. Chowdhury, \"Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-rays Images,\" Computers in Biology and Medicine, p. 104319, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.104319.\n[4] A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. H. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj, \"Covid-19 infection map generation and detection from chest X-ray images,\" Health Inf Sci Syst 9, 15 (2021), https:\/\/doi.org\/10.1007\/s13755-021-00146-8.\n[5] M. E. H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir, Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. A. Emadi, M. B. I. Reaz, M. T. Islam, \"Can AI Help in Screening Viral and COVID-19 Pneumonia?,\" IEEE Access, vol. 8, pp. 132665-132676, 2020, https:\/\/doi.org\/10.1109\/ACCESS.2020.3010287.\n\nTo the best of our knowledge, this is the first study that utilizes both lung and infection segmentation to detect, localize and quantify COVID-19 infection from X-ray images. Therefore, it can assist the medical doctors to better diagnose the severity of COVID-19 pneumonia and follow up the progression of the disease easily.\n\nThe experiments were conducted on two CXR sets, where each set is divided into train, validation and test sets: \n1)\tLung Segmentation Data\n        Entire COVID-QU-Ex dataset (33,920 CXR images with corresponding ground-truth lung masks)\n2)\tCOVID-19 Infection Segmentation Data\n         A subset of COVID-QU-Ex dataset (1,456 Normal and 1,457 Non-COVID-19 CXRs with corresponding lung mask, plus 2,913 COVID-19 CXRs with \n         corresponding lung mask from COVID-QU-Ex dataset and corresponding infections masks from QaTaCov19 dataset).\n### References\nIn COVID-QU-Ex, the X-ray images are collected from the following repositories and studies:\n\u2022\tCOVID-19 Samples: [1- 7].\n\u2022\tNon-COVID Samples: [8- 10].\n\u2022\tNormal Samples: [8- 10].\n\n[1] QaTa-COV19 Database. https:\/\/www.kaggle.com\/aysendegerli\/qatacov19-dataset. Accessed 14 March 2021.\n[2] Covid-19-image-repository. Available: https:\/\/github.com\/ml-workgroup\/covid-19-image-repository\/tree\/master\/png. Accessed 14 March 2021.\n[3] Eurorad. Available: https:\/\/www.eurorad.org\/. Accessed 14 March 2021.\n[4] Covid-chestxray-dataset. Available: https:\/\/github.com\/ieee8023\/covid-chestxray-dataset. Accessed 14 March 2021.\n[5] COVID-19 DATABASE. Available: https:\/\/www.sirm.org\/category\/senza-categoria\/covid-19\/. Accessed 14 March 2021.\n[6] Kaggle. (2020). COVID-19 Radiography Database. Available: https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database. Accessed 14 March 2021.\n[7] GitHub. (2020). COVID-CXNet. Available: https:\/\/github.com\/armiro\/COVID-CXNet. Accessed 14 March 2021.\n[8] RSNA Pneumonia Detection Challenge. Available: https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge\/data. Accessed 14 March 2021.\n[9] Chest X-Ray Images (Pneumonia). Available: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia. Accessed 14 March 2021.\n[10] Medical Imaging Databank of the Valencia Region. PadChest: A large chest x-ray image dataset with multi-label annotated reports. Available: https:\/\/bimcv.cipf.es\/bimcv-projects\/padchest\/. Accessed 14 March 2021.","361":"Please read our paper:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\n### Abstract\n\nDeep learning has shown tremendous potential in many real-life applications in different domains. One of these potentials is object detection. Recent object detection which is based on deep learning models has achieved promising results concerning the finding of an object in images. The objective of this paper is to annotate and localize the medical face mask objects in real-life images. Wearing a medical face mask in public areas, protect people from COVID-19 transmission among them. The proposed model consists of two components. The first component is designed for the feature extraction process based on the ResNet-50 deep transfer learning model. While the second component is designed for the detection of medical face masks based on YOLO v2. Two medical face masks datasets have been combined in one dataset to be investigated through this research. To improve the object detection process, mean IoU has been used to estimate the best number of anchor boxes. The achieved results concluded that the adam optimizer achieved the highest average precision percentage of 81% as a detector. Finally, a comparative result with related work has been presented at the end of the research. The proposed detector achieved higher accuracy and precision than the related work.\n\n\n### Context\n\nThis Dataset conducted its experiments based on two public medical face mask datasets. The first dataset is Medical Masks Dataset (MMD)published by Mikolaj Witkowski (https:\/\/www.kaggle.com\/vtech6\/me dical-masks-dataset). The MMD dataset consists of 682 pictures with over 3k medical masked faces wearing masks. The second public masked face dataset is a Face Mask Dataset (FMD)in (https:\/\/www.kaggle.com\/andrewmvd\/face-mask-detection). The FMD dataset consists of 853 images. We created a new dataset by combining MMD and FMD. The merged dataset contains 1415 images by removing bad quality images and redundancy\n\n\n### Acknowledgements\n\nCite our papers:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\nLoey, M., Manogaran, G. & Khalifa, N.E.M. A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images. Neural Comput & Applic (2020). https:\/\/doi.org\/10.1007\/s00521-020-05437-x\nLoey, Mohamed; Smarandache, Florentin; M. Khalifa, Nour E. 2020. \"Within the Lack of Chest COVID-19 X-ray Dataset: A Novel Detection Model Based on GAN and Deep Transfer Learning\" Symmetry 12, no. 4: 651. https:\/\/doi.org\/10.3390\/sym12040651\nKhalifa, N.E.M., Smarandache, F., Manogaran, G. et al. A Study of the Neutrosophic Set Significance on Deep Transfer Learning Models: an Experimental Case on a Limited COVID-19 Chest X-ray Dataset. Cogn Comput (2021). https:\/\/doi.org\/10.1007\/s12559-020-09802-9\n\n### Inspiration\n\nCreating the proposed database presents more challenges\nBenha University\nhttp:\/\/bu.edu.eg\/staff\/mloey\nhttps:\/\/mloey.github.io\/\nhttps:\/\/orcid.org\/0000-0002-3849-4566\nArabic Handwritten Characters Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahcd1\nArabic Handwritten Digits Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahdd1","362":"Welcome to the \"Playing Cards Dataset\" designed for YOLO Object Detection! This dataset contains high-quality images of playing cards, annotated and ready for training object detection models using the YOLO (You Only Look Once) framework. Whether you're developing an AI for card game analysis, magic tricks detection, or any other application involving playing cards, this dataset will provide the necessary data to build and fine-tune your models.\n\nLabels was generated in source (1):\n`['10', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'J', 'K', 'Q']\n['Clubs', 'Diamonds', 'Hearts', 'Spades']\n['10_Clubs', '10_Diamonds', '10_Hearts', '10_Spades', '2_Clubs', '2_Diamonds', '2_Hearts', '2_Spades', '3_Clubs', '3_Diamonds', '3_Hearts', '3_Spades', '4_Clubs', '4_Diamonds', '4_Hearts', '4_Spades', '5_Clubs', '5_Diamonds', '5_Hearts', '5_Spades', '6_Clubs', '6_Diamonds', '6_Hearts', '6_Spades', '7_Clubs', '7_Diamonds', '7_Hearts', '7_Spades', '8_Clubs', '8_Diamonds', '8_Hearts', '8_Spades', '9_Clubs', '9_Diamonds', '9_Hearts', '9_Spades', 'A_Clubs', 'A_Diamonds', 'A_Hearts', 'A_Spades', 'J_Clubs', 'J_Diamonds', 'J_Hearts', 'J_Spades', 'K_Clubs', 'K_Diamonds', 'K_Hearts', 'K_Spades', 'Q_Clubs', 'Q_Diamonds', 'Q_Hearts', 'Q_Spades'] 52`\n\nFeel free to leave comments on things I should fix or improve","363":"## **Overview**\n\nThe success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset\u2019s average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset\u2019s effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union(IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8436749%2F67e47a3479c81c63a09ab4e3deed6f9c%2FDataset.jpg?generation=1716177287419838&alt=media)\n\n\n**Dataset Details**\nTotal Images: 17,326\nInstances: 81,542\nBBox Per Image - 4.7036\nVehicle Categories: 1. Bicycle 2. Bus 3. Bhotbhoti 4. Car 5. CNG 6. Easybike 7. Leguna 8. Motorbike 9. MPV 10. Pedestrian 11. Pickup 12. PowerTiller 13. Rickshaw 14. ShoppingVan 15. Truck 16. Van 17. Wheelbarrow\n\n## Model Testing Results\nThe dataset has been rigorously tested with YOLO v5-v8 models. The mean Average Precision at 50% Intersection over Union (IoU) is an impressive 84.8%.\n\n| Model        | Dataset          | mAP0.5     | mAP 0.5:0.95   | Precision  | Recall     | Weight                                                                                                                                                                   |\n| --------     | ---------------- | ---------- | -------------- | ---------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **YOLOv5**   | CARL-D           | 0.437      | 0.328          | 0.633      | 0.423      | |   \n|              | DhakaAI          | 0.416      | 0.255          | 0.640      | 0.393      | |\n|              | P2 Dhaka         | 0.655      | 0.400          | 0.804      | 0.581      | |\n|              | PoribohonBD      | 0.981      | 0.743          | 0.939      | 0.948      | |\n|              | **BNVD**         | **0.826**  | **0.609**      | **0.836**  | **0.762**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V5\/weights\/best.pt)      |\n| **YOLOv6**   | CARL-D           | 0.479      | 0.372          | 0.58       | 0.453      | |\n|              | DhakaAI          | 0.420      | 0.262          | 0.311      | 0.548      | |\n|              | P2 Dhaka         | 0.775      | 0.494          | 0.762      | 0.71       | |\n|              | PoribohonBD      | 0.899      | 0.648          | 0.899      | 0.81       | |\n|              | **BNVD**         | **0.837**  | **0.624**      | **0.805**  | **0.76**   | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V6\/weights\/best_ckpt.pt) |\n| **YOLOv7**   | CARL-D           | 0.478      | 0.369          | 0.619      | 0.459      | |\n|              | DhakaAI          | 0.464      | 0.284          | 0.692      | 0.438      | |\n|              | P2 Dhaka         | 0.743      | 0.462          | 0.816      | 0.688      | |\n|              | PoribohonBD      | 0.907      | 0.656          | 0.914      | 0.841      | |\n|              | **BNVD**         | **0.841**  | **0.623**      | **0.83**  | **0.779**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V7\/weights\/best.pt)      |\n| **YOLOv8**   | CARL-D           | 0.478      | 0.359          | 0.602      | 0.446      | |\n|              | DhakaAI          | 0.435      | 0.276          | 0.694      | 0.446      | |\n|              | P2 Dhaka         | 0.69       | 0.449          | 0.798      | 0.604      | | \n|              | PoribohonBD      | 0.889      | 0.658          | 0.898      | 0.823      | |\n|              | **BNVD**         | **0.848**  | **0.643**      | **0.841**  | **0.774**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V8\/weights\/yolov8_ods_new_100e_best.pt)      |\n\n\n\n## Checkpoints\nPre-trained model weight files can be found in the \"Checkpoints\" folder of this repository.\n\n## License\nThis dataset is released under the [insert license type] license.\n\n## Citation\nIf you use this dataset in your work, please consider citing:\n\n[Insert citation information here]\n\n## Contributors\nWe would like to thank the following contributors for their valuable contributions to the development of this dataset.\n\n- Bipin Saha (bipinsaha.bd@gmail.com)\n- Md. Johirul Islam* (johirul@phy.ruet.ac.bd)\n- Shaikh Khaled Mostaque* (misha@ru.ac.bd)\n- Aditya Bhowmik (bhowmik.aditya0@gmail.com)\n- Tapodhir Karmakar Taton (tapodhirtaton@gmail.com)\n- Md Nakib Hayat Chowdhury (nakib2025@gmail.com)\n- Mamun Ibne Bin Reaz* (mamun.reaz@iub.edu.bd)","364":"","365":"Modified Version of Indian Driving Dataset to work for SSD and YOLO. \n\nWhat is Indian Driving Dataset?\n\nWhile several datasets for autonomous navigation have become available in recent years, they have tended to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in object or background appearance and strong adherence to traffic rules. We propose a novel dataset for road scene understanding in unstructured environments where the above assumptions are largely not satisfied. It consists of 10,000 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. The label set is expanded in comparison to popular benchmarks such as Cityscapes, to account for new classes.\n\nThe dataset consists of images obtained from a front facing camera attached to a car. The car was driven around Hyderabad, Bangalore cities and their outskirts. The images are mostly of 1080p resolution, but there is also some images with 720p and other resolutions.","366":"## Dataset for the paper entitled \"Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models\" (CVPR 2024) \n\nThe Natural Denoising Diffusion Attack (NDDA) dataset is designed to systematically evaluate the natural attack capability in diffusion models. The latest NDDA dataset consists of the following 15 classes: stop sign, car, dog, hot dog, traffic light, zebra, fire hydrant, frog, horse, bird, boat, air plane, bicycle, cat, and carrot with 6 diffusion models (Dall-E 2, Dall-E 3, Stable Diffusion 2, Deepfloyd IF, Stable Diffusion 1.5, MidJourney, and Google Duet). \n\nThe NDSS dataset is organized into `diffusion parent folder` that separate each diffusion model's set of images, which in turn contains multiple folders for each of the 15 object classes from COCO. Each object class folder then contains multiple subfolders that hold the NDD attack images, and these subfolders' names are the text prompts used to generate the set of NDD attack images. For example, if images were generated using Stable Diffusion 2 with the text prompt, `blue dog`, the path to this prompt subfolder would be `diffusion_output\\_diffusion\\_2\/blue\\_dog`. \n\nMore details are on our website: https:\/\/sites.google.com\/view\/cav-sec\/ndd-attack?authuser=0\n\n\n```bibtex\n@InProceedings{sato2022towards,\n  author    = {Takami Sato and Qi Alfred Chen},\n  title     = {{Towards Driving-Oriented Metric for Lane Detection Models}},\n  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2022}\n}\n```\n","367":"Thermal Object Detection for Autonomous Vehicles\nThis dataset provides real-world thermal imagery for training and evaluating object detection algorithms in autonomous vehicles. It  focuses on identifying moving objects crucial for self-driving cars, including:\n\nCars\nBicycles\nPeople\nDogs\nData Summary:\n\nSize: Over 12,000 thermal images\nFocus: Primarily annotated for cars\nApplications:\nTraining object detection models for autonomous vehicles\nResearch on thermal image analysis for self-driving applications\nIf you're working on self-driving car perception systems, this dataset can be valuable for:\n\nDeveloping and refining thermal object detection algorithms\nEvaluating the performance of object detection models in low-light or adverse weather conditions","368":"This meticulously curated dataset is a comprehensive collection of annotated images featuring Egyptian car license plates, tailored specifically for object detection tasks. With a focus on facilitating advanced computer vision research and development, this dataset is an invaluable resource for training, testing, and benchmarking state-of-the-art algorithms in license plate detection.","369":"This dataset is created from the data available at https:\/\/www.kaggle.com\/datasets\/javiersanchezsoriano\/roundabout-aerial-images-for-vehicle-detection?select=roundabouts.csv .\n\n\nclasses:\n0-car\n1-cycle\n2-bus\n3-truck\n4-van\n\n\n\n","370":"#Carla Object Detection Dataset\n\n##Description:\n\nThe Carla Object Detection Dataset is a labeled dataset designed for object detection tasks within the Carla Simulator environment. The images were captured using the autopilot mode in Carla Simulator across various environments, including Town01, Town02, Town03, Town04, and Town05. Frames were saved at regular intervals during simulation, providing diverse scenes for training and evaluation.\n\n##Labels:\n\nEach image in the dataset is accompanied by annotation files in three different formats:\n\nPascal VOC Format: XML files containing labels are stored in the labels folder.\nYOLO Format: YOLO-formatted label files can be found in the labels_yolo_format folder.\nMS COCO Format: JSON files with annotations in MS COCO format are located in the annotations folder.\nAvailable Classes:\n\nThe dataset includes the following object classes:\n\nVehicle (Car, Truck)\nBike\nMotorbike\nTraffic Light\nTraffic Sign","371":"This dataset contains car images for object detection task. The dataset is split into two folders, namely train and test. However, for training purposes, it should be split into three sets necessary for Machine Learning and Deep Learning tasks, namely train, validation, and test splits. The structure of the data is as follows:\n\n- ROOT\n   - train:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - test:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - meta_deta.csv.\n\nFor the object detection task, the bounding box coordinates can be obtained using meta_deta.csv file .","372":"Diverse License Plate Detection - A combination of images from multiple LPD datasets along with negative samples to make the dataset training-ready.\nThe images are from a large number of datasets from Kaggle as well as general internet sources, as will be listed below.\n\nPurpose:\nTraining a YOLOv8 object detection model for license-plate detection.\nSee https:\/\/github.com\/ultralytics\/ultralytics for pre-trained models.\n\nData:\nThere are about 3100 images of cars with tagged license plates, on the conditions that the plate has at least one recognizable character and is not seen through the glass of another car (or reflection for that matter).\nThere are also about 1500 images which either do not contain cars at all, or contain cars but in such an angle or resolution that their license plate is not detectable. This is to address false positives which I have encountered when using some of the open-source models found on huggingface (ones that were only trained on images with at least one license-plate in them)\n*Update 24\/03\/2024: Dataset contains 5500 images, see Updates for specifications.*\n\nUpdates:\n**Update 24\/03\/2024:**\n- After using a different open-source model from huggingface trained on a dataset from roboflow on yolo5 (dataset in list here) we encountered two items of note.\n  - A. The system yielded much better results when using a Yolo8 model to first identify vehicles, and then detect plates on them\n  - B. The dataset on which the yolov5 model was trained on contained almost no negative examples, and contained a lot of duplications.\n- Added some images from a potholes dataset (listed in Sources) to help with disassociate roads with plates, and also get a few more car images.\n- Added images from the roboflow dataset which included images from a parking garage, allowing for a different angle.\n- Added images from a parking sign dataset, few with car images but mostly to disassociate street and road signs from plates found on actual cars.\n- Trained a Yolo8 model on the dataset, but did not run in production and was only really for testing. However the class loss decreased at a much faster rate than when trained with a much larger dataset consisting of very similar (quality, size, locality) positive images, even by epoch 10.\n- *Current plan moving forward: Run transfer learning on Yolo9 with this dataset for somewhere between 100 and 200 epochs (yay for Programmable Gradient Information) and post some of the model results here.*\n\n**Update 25\/03\/2024** Training update\nSplit dataset into Train, Validation, Test by ratio of 80, 10, 10.\nTrained using the ultralytics library with imgsz=640, batch=16, dropout=0.2 for 150 epochs\nValidation plateaued after about 110 epochs. Out of 100 test photos from real data there were about 5 false positives and 5 false negatives when using this model only, and 2 false positives + 1 false negative when first processing the image through YOLOv8x and filtering for [car, truck, motorcycle, bus]. Currently runs in production on Torchserve.\nFalse positives: Rectangles with lines (like grates)\nFalse negatives: Very low quality images, or images in which the plate was already very small and so disappeared when image was resized to 640.\n\nDiversity:\nThe datasets sampled for this \"diverse\" one contain plates from a variety of countries, but mainly focusing on the ones from this site which are:\n- European (+Russian)\n- American\n- Middle Eastern\n- Indian\n\nSources: (In no particular order)\n- https:\/\/www.kaggle.com\/datasets\/amriteshtiwari20\/truck-licenseplate-dataset\n- https:\/\/www.kaggle.com\/datasets\/gaelcohen\/license-plate-israel\n- https:\/\/www.kaggle.com\/datasets\/kedarsai\/indian-license-plates-with-labels\n- https:\/\/www.kaggle.com\/datasets\/mohamedalitrabelsi\/tunisiania-license-plate-detection\n- https:\/\/www.kaggle.com\/datasets\/mrabduqayum\/license-plate-detection-yolov5\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert\n- https:\/\/www.kaggle.com\/datasets\/balraj98\/stanford-background-dataset\n- https:\/\/www.kaggle.com\/datasets\/mikhailma\/house-rooms-streets-image-dataset\n- https:\/\/www.kaggle.com\/datasets\/paulchambaz\/google-street-view\n- https:\/\/www.kaggle.com\/datasets\/pcmill\/license-plates-on-vehicles\n- https:\/\/www.kaggle.com\/datasets\/psvishnu\/pennfudan-database-for-pedestrian-detection-zip\n- https:\/\/www.kaggle.com\/datasets\/rayanechibani\/dataset\n- https:\/\/www.kaggle.com\/datasets\/yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel\n- https:\/\/www.kaggle.com\/datasets\/amirhoseinahmadnejad\/car-license-plate-detection-iran\n- https:\/\/www.kaggle.com\/datasets\/aritrag\/license\n- https:\/\/www.kaggle.com\/datasets\/marshah\/license-plate-persian-coco\n- https:\/\/www.kaggle.com\/datasets\/narimanjabbar\/dataset-iraq-license-plate\n- https:\/\/www.kaggle.com\/datasets\/nuralitileuov\/car-license-plate\n- https:\/\/www.kaggle.com\/datasets\/saisirishan\/indian-vehicle-dataset\n- https:\/\/www.kaggle.com\/datasets\/sushankghimire\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/trainingdatapro\/license-plates-1-209-438-ocr-plates\n- https:\/\/www.kaggle.com\/datasets\/truongthanh081203\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/andrewmvd\/pothole-detection\n- https:\/\/universe.roboflow.com\/augmented-startups\/vehicle-registration-plates-trudk\/dataset\/1\n- https:\/\/www.kaggle.com\/datasets\/mathurinache\/san-francisco-parking-sign-detection\n- Internet\n\nIt should be noted, with the size of some of these datasets, not all of the images in the datasets were used, depending on the size and variety in each dataset.\n\nLicense:\nAs specified by the datasets (those that did specify a License and did not specify \"Unknown\")\n- amriteshtiwari20\/truck-licenseplate-dataset: MIT\n- kedarsai\/indian-license-plates-with-labels: CC0: Public Domain\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert: CC BY-NC-SA 4.0\n- balraj98\/stanford-background-dataset: The dataset is derived from Stanford DAGS Lab's Stanford Background Dataset from their Scene Understanding Datasets, see the Acknowledgements section for credits\n- mikhailma\/house-rooms-streets-image-dataset: CC0: Public Domain\n- paulchambaz\/google-street-view: GNU Lesser General Public License\n- pcmill\/license-plates-on-vehicles: CC0 Public Domain\n- psvishnu\/pennfudan-database-for-pedestrian-detection-zip: Acknowledgements\n- yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel: CC0 Public Domain\n- amirhoseinahmadnejad\/car-license-plate-detection-iran: Database Contents License (DbCL) v1.0\n- aritrag\/license: CC0 Public Domain\n- marshah\/license-plate-persian-coco: Attribution-NonCommercial 4.0 International\n- trainingdatapro\/license-plates-1-209-438-ocr-plates: Attribution-NonCommercial 4.0 International\n- truongthanh081203\/license-plate: Apache 2.0\n\nNotes on Licenses:\nIf your dataset is included in this list and has a license that could be changed (like CC BY-NC-SA 4.0) please contact me if you'd like your photos removed from the dataset.\n\nAcknowledgements:\nS. Gould, R. Fulton, D. Koller. Decomposing a Scene into Geometric and Semantically Consistent Regions. Proceedings International Conference on Computer Vision (ICCV), 2009.\nhttps:\/\/www.cis.upenn.edu\/~jshi\/ped_html\/","373":"YOLO (You Only Look Once) is a popular object detection algorithm that processes images in a single pass through a neural network to simultaneously predict bounding boxes and class probabilities for objects within those boxes. YOLOv3 is one of the versions of this algorithm, known for its balance between speed and accuracy.\n\nFor training and testing YOLOv3, you can use a variety of datasets depending on your application. Some commonly used datasets for object detection tasks, including those suitable for YOLOv3, are:\n\nCOCO (Common Objects in Context): This dataset is widely used for object detection, segmentation, and captioning tasks. It contains over 200,000 labeled images with 80 object categories.\n\nPascal VOC (Visual Object Classes): This dataset consists of images annotated with object bounding boxes and class labels. It includes 20 object categories such as person, car, dog, etc.\n\nOpen Images Dataset: This is a large-scale dataset with millions of images annotated with object bounding boxes and class labels. It covers a wide range of object categories.\n\nKITTI Vision Benchmark Suite: Primarily used for autonomous driving research, this dataset contains images captured from a moving vehicle with annotations for object detection, tracking, and scene understanding.\n\nImageNet: Although primarily known for image classification, ImageNet also contains bounding box annotations for object detection tasks. It consists of millions of images across thousands of categories.\n\nThese datasets provide a diverse range of objects in various contexts, making them suitable for training and evaluating object detection models like YOLOv3.","374":"\nThe \"26 Class Object Detection Dataset\" comprises a comprehensive collection of images annotated with objects belonging to 26 distinct classes. Each class represents a common urban or outdoor element encountered in various scenarios. The dataset includes the following classes:\n\nBench\nBicycle\nBranch\nBus\nBushes\nCar\nCrosswalk\nDoor\nElevator\nFire Hydrant\nGreen Light\nGun\nMotorcycle\nPerson\nPothole\nRat\nRed Light\nScooter\nStairs\nStop Sign\nTraffic Cone\nTrain\nTree\nTruck\nUmbrella\nYellow Light\nThese classes encompass a wide range of objects commonly encountered in urban and outdoor environments, including transportation vehicles, traffic signs, pedestrian-related elements, and natural features. The dataset serves as a valuable resource for training and evaluating object detection models, particularly those focused on urban scene understanding and safety applications.","375":"## RSUD20K\n\n**Concordia University**\n\nHasib Zunair, Shakib Khan, A. Ben Hamza\n\n[[`Paper`](https:\/\/arxiv.org\/abs\/2401.07322)] [[`Dataset`](https:\/\/www.kaggle.com\/datasets\/hasibzunair\/rsud20k-bangladesh-road-scene-understanding)]\n\n**Here's a collage of outputs of RSUD20K trained model in action! Play in 4K for best results.**\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/pdRXa10SrAc\/0.jpg)](https:\/\/www.youtube.com\/watch?v=pdRXa10SrAc)\n\nThis is official code for our **paper under review at ICIP 2024**:<br>\n[RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving](https:\/\/arxiv.org\/abs\/2401.07322)\n<br>\n\nRSUD20K is a new object detection dataset for road scene understanding, comprised of over **20K** high-resolution images from the driving perspective on Bangladesh roads, and includes **130K** bounding box annotations for **13** objects. RSUD20K consists of the following classes for multi-class object detection:\n\n```bash\n# classes.txt\nperson\nrickshaw\nrickshaw van\nauto rickshaw\ntruck\npickup truck\nprivate car\nmotorcycle\nbicycle\nbus\nmicro bus\ncovered van\nhuman hauler\n```\n\nFor details on format, see [here](https:\/\/github.com\/meituan\/YOLOv6\/blob\/main\/docs\/Train_custom_data.md#1-prepare-your-own-dataset).\n\n**Dataset statistics**:\n\n| Name  | Number of images\/label pairs |\n| ------------- | ------------- |\n| `train`  | 3985 |\n| `val`  | 1004 |\n| `test`  | 649 |\n| `pseudo` (used for training)  | 14696 |\n\n\nAll code and pre-trained models for reproducibility are available in [GitHub](https:\/\/github.com\/hasibzunair\/RSUD20K).\n","376":"The dataset comprises a diverse collection of images, focusing specifically on Nepali motor vehicles, including motorcycles and scooters, along with annotations to facilitate object detection tasks. The dataset is designed to support computer vision and machine learning applications that involve the identification and localization of vehicles, contributing to the development and training of object detection models.\n\nKey features of the dataset include a comprehensive representation of various motorcycle and scooter models commonly found in Nepal, reflecting the unique characteristics and specifications of vehicles used in the region. Additionally, the dataset includes images of cars, providing a broader scope for understanding and detecting different types of motor vehicles on the road.\n\nEach image in the dataset is accompanied by annotations, which typically include information about the bounding boxes around the vehicles and associated class labels. This annotation data is essential for training object detection algorithms, allowing the models to learn and generalize patterns for accurate detection and localization of bikes and cars within images.\n\nResearchers, developers, and data scientists can leverage this dataset to enhance the capabilities of their object detection models, particularly those focused on recognizing and analyzing Nepali motor vehicles. The dataset's specificity to the Nepali context makes it valuable for applications related to traffic monitoring, vehicle counting, and overall transportation-related analysis in the context of Nepal.","377":"The \"Image Obstacles in Public Spaces\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. The description of this dataset includes several key points:\n\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (JPG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces. It is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset.","378":"The \"Obstacles in Public Spaces for Dist-YOLO\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. This dataset has been curated and annotated with the aim of supporting the development of the Dist-YOLO (You Only Look Once) model for object detection.\nThe description of this dataset includes several key points:\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (e.g., JPG or PNG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing Object Detection models, especially models like Dist-YOLO, in recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces.\nIt is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset.","379":"This dataset contains car images with disabled people signs. The dataset includes 1241 images.\nCars are annotated in YOLOv8 format.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 416x416 (Stretch)\n\nThe following augmentation was applied to create 2 versions of each source image:\n\nThe following transformations were applied to the bounding boxes of each image:\n* 50% probability of horizontal flip\n* Random rotation of between -15 and +15 degrees","380":"CLASSES =10 \nbike, \nmotobike,\n person,\n traffic_light_green, \ntraffic_light_orange, \ntraffic_light_red, \ntraffic_sign_30,\n traffic_sign_60, \ntraffic_sign_90,\n vehicle\n\nCarla Self-Driving Car Simulator\n","381":"This data was taken from VOC 2012 competition dataset. I have extracted the data from the .xml files into .txt files in the correct directories as required for yolo. There are 20 classes \n--&gt; labels_dictionary = {'person':0, 'car':1, 'chair':2, 'bottle':3, 'pottedplant':4, 'bird':5, 'dog':6,\n'sofa':7, 'bicycle':8, 'horse':9, 'boat':10, 'motorbike':11, 'cat':12, 'tvmonitor':13,\n'cow':14, 'sheep':15, 'aeroplane':16, 'train':17, 'diningtable':18, 'bus':19}. \n","382":"Dataset Name: Celeb-DF Faces Dataset\n\nDescription:\nThe Celeb-DF Faces Dataset is a curated collection of facial images extracted from the Celeb-DF dataset. This dataset focuses on providing a comprehensive set of facial images for research and analysis in the field of deepfake detection and facial image analysis. The images are categorized into two classes: \"Fake\" and \"Real,\" based on the source of the videos.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\n\nSource Folders:\n\nceleb-df-v2\/Celeb-real: Contains authentic facial videos.\nceleb-df-v2\/Celeb-synthesis: Contains synthesized (fake) facial videos.\nceleb-df-v2\/YouTube-real: Contains additional authentic facial videos from YouTube.\nOutput Folder:\n\nceleb_faces_224\/: Contains the extracted and resized facial images.\nMetadata File:\n\nmetadata_celebs.csv: A CSV file storing metadata information for each extracted image with the following columns:\nName: The filename of the extracted image.\nLabel: The label indicating whether the image is \"Fake\" or \"Real.\"\nCreation Process:\n\nVideo Frame Extraction:\n\nThe first frame from each video in the source folders is extracted.\nImage Resizing:\n\nThe extracted frames are resized to 224x224 pixels to ensure uniformity and compatibility with common machine learning models.\nImage Storage:\n\nThe resized images are saved in the celeb_faces_224\/ folder with filenames corresponding to the original video names.\nMetadata Compilation:\n\nA metadata CSV file (metadata_celebs.csv) is created to store the filenames and labels of the images, indicating whether they are from \"Fake\" or \"Real\" videos.\nIntended Use:\nThe dataset is ideal for tasks such as:\n\nDeepfake detection and analysis\nTraining and evaluation of machine learning models for facial image classification\nImage forensics research and development\nNote: This dataset is derived from the Celeb-DF dataset and is intended for research and educational purposes only.","383":"Dataset Name: FaceForensics++ Faces_224\nDescription:\nThe FaceForensics++ Faces Dataset is a collection of images extracted from the FaceForensics++ dataset, focusing specifically on facial imagery. It comprises two main categories: \"Fake\" and \"Real,\" representing manipulated and authentic facial images, respectively.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\nFolders:\nFF++_Faces_224\/\nContains the extracted and resized images from the FaceForensics++ dataset.\nmetadata_FF++.csv\nA CSV file storing metadata information for each image, including filename (Name) and label (Label) indicating whether the image is \"Fake\" or \"Real.\"\nIntended Use:\nThe dataset is suitable for tasks such as:\n\nFacial image analysis\nDeep learning-based manipulation detection\nImage forensics research and development\nNote: This dataset is derived from the FaceForensics++ dataset and is intended for research and educational purposes only.\n\n","384":"**Contacts Dataset**\nThe Contacts dataset simulates a list of individual contacts with detailed personal and contact information, useful for representing a customer database:\n\nID: A unique identifier for each contact, generated as a UUID.\nEmail: A randomly generated realistic email address.\nCountry: Randomly chosen from a list of six countries (France, US, UK, Germany, Portugal, China), represented by their respective codes.\nCity: A random city, ideally from the chosen country, although this script uses a random city generator without specific country linkage for simplicity.\nPhone: A randomly generated phone number, which should ideally include the country prefix (not implemented in this simple version).\nFirstname: A randomly generated first name.\nBirthdate: A randomly generated birth date for each contact, ranging from 1930 to 2008, making the age of contacts between 15 and 93.\nPostal Code: A postal code corresponding to the randomly chosen city.\nAcquisition Source: The source through which the contact was acquired, chosen randomly from options like Facebook ad, Google ad, promotional email, or a birthday mail campaign.\nCreated At: The timestamp when the contact record was created, ranging from 2019 to the present.\nUpdated At: The timestamp of the last update to the contact record, which is any time between the creation date and the current date.\n\n\n\n**Products Dataset**\nThe Products dataset simulates an inventory of items that might be sold by a company:\n\nID: A unique identifier for each product, generated as a UUID.\nSKU: A unique Stock Keeping Unit code for each product.\nCategories: A list of categories assigned to each product, chosen from predefined options like Electronics, Clothing, Home, Toys, Books. Each product can belong to multiple categories.\nPrice: A randomly generated price for each product, ranging from $10 to $500.\nName: A randomly generated name for the product.\nDescription: A short description for the product, generated randomly.\nParent ID: The ID of a parent product if the current product is a variant; this is left blank in the script for simplicity.\nURL: A fake URL simulating a product page on an e-commerce site.\nImage URL: A fake URL for the product\u2019s image.\nBrand: The brand of the product, either randomly selected from a list or generated.\nCreated At: The timestamp when the product record was created, within the last two years.\nModified At: The timestamp of the last update to the product record.\n\n\n**Stores Dataset**\nThe Stores dataset represents physical or conceptual locations where the company operates:\n\nID: A unique identifier for each store, generated as a UUID.\nName: The name of the store, generated by appending \"Store\" to a randomly generated company name.\nCity: The city where the store is located, chosen randomly.\nCountry: The country where the store is located, chosen randomly.\nPostal Code: A postal code for the store, generated randomly.\nCreated At: The timestamp when the store record was created, within the last two years.\nModified At: The timestamp of the last update to the store record.\n\n\nThese datasets can be used individually or combined to simulate real-world business applications like customer relationship management, inventory tracking, and retail operations. If you need these datasets linked (e.g., linking products to stores, or contacts to purchases), additional scripting would be needed to establish these relationships within the data.","385":"# Biometric Attack Dataset, Asian People\n\n# The similar dataset that includes all ethnicities - [Anti Spoofing Real Dataset](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)\n\nThe dataset for face anti spoofing and face recognition includes images and videos of asian people. **30,600**+ photos & video of **15,300** people from **32** countries. All people presented in the dataset are  **South Asian, East Asian or Middle Asian**.  The dataset helps in enchancing the performance of the model by providing wider range of data for a specific ethnic group.\n\nThe videos were gathered by capturing faces of genuine individuals presenting spoofs, using facial presentations. Our dataset proposes a novel approach that learns and detects spoofing techniques, extracting features from the genuine facial images to prevent the capturing of such information by fake users. \n\nThe dataset contains images and videos of real humans with various **resolutions, views, and colors**, making it a comprehensive resource for researchers working on anti-spoofing technologies.\n\n### People in the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff545aa561432738d251c09f09e1f5e92%2FFrame%20104.png?generation=1713356643038606&alt=media)\n\n### Types of files in the dataset:\n- **photo** - selfie of the person \n- **video** - real video of the person \n\nOur dataset also explores the use of neural architectures, such as deep neural networks, to facilitate the identification of distinguishing patterns and textures in different regions of the face, increasing the accuracy and generalizability of the anti-spoofing models. \n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 30,600 files, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **video_extension** - video extensions in the dataset\n- **video_resolution** - video resolution in the dataset\n- **video_duration** - video duration in the dataset\n- **video_fps** - frames per second for video in the dataset\n- **photo_extension** - photo extensions in the dataset\n- **photo_resolution** - photo resolution in the dataset\n\n### Statistics for the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2F6de78d350a9213d8437f766b085d4551%2Fasian_video_liveness.png?generation=1713356627116331&alt=media)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection) to learn about the price and buy the dataset**\n\n# Content\nThe dataset consists of:\n- **files** - includes 10 folders corresponding to each person and including 1 image and 1 video,\n-  **.csv file** -  contains information about the files and people in the dataset\n\n### File with the extension .csv\n- **id**: id of the person,\n- **selfie_link**: link to access the photo,\n- **video_link**: link to access the video,\n- **age**: age of the person,\n- **country**: country of the person,\n- **gender**: gender of the person,\n- **video_extension**: video extension,\n- **video_resolution**: video resolution,\n- **video_duration**: video duration,\n- **video_fps**: frames per second for video,\n- **photo_extension**: photo extension,\n- **photo_resolution**: photo resolution\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** provides high-quality data annotation tailored to your needs\n\n*keywords: liveness detection systems, liveness detection dataset, biometric dataset, biometric data dataset, biometric system attacks, anti-spoofing dataset, face liveness detection, deep learning dataset, face spoofing database, face anti-spoofing, ibeta dataset, face anti spoofing, large-scale face anti spoofing, rich annotations anti spoofing dataset, asian people, asian classification, asian image dataset*","386":"","387":"Dataset Name: **Banknote Authentication Dataset**\n\n**Description:**\n\nThis dataset contains a collection of features extracted from images of genuine and counterfeit banknotes. It's commonly used to train and evaluate machine learning models for automated banknote authentication, aiming to distinguish real banknotes from forgeries.\n\n**Features:**\n\nEach data point represents a single banknote and includes the following features:\nVariance of Wavelet Transformed image (continuous): Measures image texture variation.\nSkewness of Wavelet Transformed image (continuous): Quantifies image asymmetry.\nCurtosis of Wavelet Transformed image (continuous): Captures image tailedness.\nEntropy of image (continuous): Reflects image randomness or information content.\nClass (categorical): Indicates whether the banknote is genuine (1) or counterfeit (0).\nNumber of Instances:\n\nThe dataset typically contains several hundred banknote images, with approximately equal proportions of genuine and counterfeit examples.\nSource:\n\nThe dataset was originally collected by researchers at the University of Applied Sciences, Ostwestfalen-Lippe, Germany.\nApplications:\n\nDevelop and evaluate machine learning models for banknote authentication.\nCompare the performance of different classification algorithms in this domain.\nExplore feature engineering techniques to improve model accuracy.\nInvestigate the effectiveness of various feature selection methods for identifying the most informative features for authentication.\nAdditional Notes:\n\nThe dataset is often used as a benchmark for classification tasks due to its balanced class distribution and relatively simple feature set.\nIt's essential to consider data preprocessing techniques (e.g., normalization, handling missing values) before model training.\nModel evaluation should involve metrics suitable for imbalanced classes if the distribution of genuine and counterfeit notes is skewed.","388":"This datasets(205k images) will be used for GAN models training.\n\nGenerative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.\n\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n\nGANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.","389":"# GENERATED Vietnamese Passports Dataset\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's *robustness and generalization*.\n\nThe dataset contains GENERATED Vietnamese passports, which are replicas of official passports but with randomly generated details, such as *name, date of birth etc*. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to buy the dataset\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to *develop and refine models using simulated passport data without risking privacy leaks*.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff2778d432611db436f18b9284daec240%2F666.png?generation=1691945421899877&alt=media)\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset\n\n# Passports might be generated in accordance with your requirements.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems*","390":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3032492%2Ff4201da2a12cae17fed7a8f5a242c78e%2F2023-07-12%20%208.33.10.png?generation=1689161628737196&alt=media)\n\n2023 Fake or Real: AI-generated Image Discrimination Competition dataset is now available on Kaggle!\n---\n\n\nHello, Kagglers\ud83d\udd90\ufe0f\n\nWe are excited to announce the release of the dataset for the 2023 Fake or Real: AI-generated Image Discrimination Competition. The competition was held on AI CONNECT(https:\/\/aiconnect.kr\/) from June 26th to July 6th, 2023, with 768 participants.\n\nIf you're interested in evaluating the performance of your model on the test dataset, we encourage you to visit the [competition page](https:\/\/aiconnect.kr\/competition\/detail\/227\/task\/295\/taskInfo) on AI CONNECT and submit your results. Please note that it supports only Korean yet. Of course we data scientists can always use Chrome translate, and\/or even better translation models\ud83e\udd73. Plus, multilingual service will be provided in the (hopefully near) future, so please stay tuned!\n\n\n# Background\nAs the advancement of generative AI technology has enabled the easy creation of indistinguishable fake information from genuine content, concerns regarding its misuse have surfaced. Image generation AI, in particular, has raised significant alarm due to its potential risks such as identity theft, revenge porn, and political manipulation. In response, it has become imperative to develop technologies that can effectively discern between real and AI-generated fake images.\n\nThe training dataset consists of diffusiondb (https:\/\/huggingface.co\/datasets\/poloclub\/diffusiondb) and Flickr images, with the inclusion of some low-quality fake images. For the test dataset, we took measures to construct it in a manner that closely resembles real-world scenarios involving image misuse. We utilized multiple generative AI models, fine-tuned on diverse photorealistic datasets, and applied negative prompt keywords like 'cartoon' and 'too many fingers' to generate realistic images.\n\nWe hope this dataset encourages the development of robust solutions and stimulates discussions on tackling the challenges associated with AI-generated fake images. \n\nBest Regards,\nAI CONNECT\n\n","391":"# GENERATED USA Passports Dataset\n\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's robustness and generalization.\n\nData augmentation techniques involve applying various transformations to existing data samples to create new ones. These transformations include: *random rotations, translations, scaling, flips, and more*. Augmentation helps in increasing the dataset size, introducing natural variations, and improving model performance by making it more invariant to specific transformations.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to buy the dataset\n\nThe dataset contains **GENERATED** USA passports, which are replicas of official passports but with randomly generated details, such as name, date of birth etc. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to develop and refine models using simulated passport data without risking privacy leaks.\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F618942%2F30c6650541e63733f9ea0fcdc3bfc2cb%2FMacBook%20Air%20-%201%20(2).png?generation=1688719414649908&alt=media)\n\n# Content\n\n### Folders\n- **original**: includes original generated images of USA passports\n- **augmentation**: contains subfolders, corresponding to the original photos and including 3 black and white generated passport scans with different photo editing.\n\nThe augmentated photos are presented with random rotations, noise and brightness. Augmentation varies depending on the amount of noise and blur in the passport images, from slight (**us_pass_augmentated_1**) to significant (**us_pass_augmentated_3**).\n\n### File with the extension .csv\n\nincludes the following information for each media file:\n\n- **original**: link to access the image of the generated passport,\n- **us_pass_augmentated_1**: link to the first augmentated image, \n- **us_pass_augmentated_2**: link to the second augmentated image, \n- **us_pass_augmentated_3**: link to the third augmentated image\n\n# USA Passeport Photos might be generated in accordance with your requirements.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems, augmentation dataset, data augmentation, augmented images, image augmentation*","392":" ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F8cdd2e3fbfb1a3e97a9ff5733068564c%2Fvending%20machine.jpg?generation=1683391775041836&alt=media)\nVending machines and money changers deal with taking in cash form a user, identifying the denomination of the cash, and detect cash's validity ( i.e is it a real or a counterfeit (fake)). Coins seem to be a major type of cash that are exchanged in these machines. The current methods of identifying the denomination of coins by the these machines commonly involves measuring the weight, size, and thickness of the coin using techniques such as light sensing, and detecting weather the coin is real or fake involves obtaining the composition of the coin to detect if it is metallic or not using techniques such as electromagnetic field transmission. These methods do work, except there are two problems:\n1. In many countries (like India), the versions of coins keep changing overtime and consequently the dimensions and compositions of the coins change too making it difficult to identify and detect coins using the afore mentioned methods.\n2. Fake coins can be made of metallic plates that resemble the size and composition of real coins, when such plates are inserted into the machine, they might get falsely detect as valid coins. \n\nOne potential way to solve these problems would be to use Machine Learning to identify the coin denomination and detect it's validity. But how? An ML model can be trained on a dataset of images of both real and fake coins of various denominations. If the images were captured with a flash, the amount of light reflected off of the surface of metallic coins would be higher compared to that of non-metallic fake coins. Also the higher reflection from metallic surface would produce regions of bright spots only in images of metallic coins.\n\nThe figure below shows some images captured with flash, taken from the dataset, showing some differences present between the images of real and fake coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F5bf4e9119046d51e7c9258eada047ae0%2FRealVsFakeCoins.jpg?generation=1683396232330130&alt=media)\n(a) & (b) are images of a real coin, showing reflected light as bright spots on the coin.\n(c) is the image of a fake non-metallic coin, not showing any apparent bright spots of light.\n(d) is fake too but is a metallic plate and thus showing spots of brightness, but lacks the presence of imprints of denomination, the rupee symbol and artistry.\n(e) is fake, made by printing the image of a real coin on a piece of paper and pasted on a plastic plate, and thus the image contains both the spots of brightness and the information and artistry similar to that of a real coin but lacks the lustrous appearance of a real coin. Also the paper surface appears rougher compared to a real coin.\n\nKeeping in mind these differences, this dataset was created with 1750 images of various real and fake coins captured with flash and the images are classified into:\n1. The front face of real\/valid Indian coins consisting the imprint of common denominations-\nRs 1, Rs 2, and Rs 5.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F3cbee0087a974d872ff3684709e9ad2d%2Freal_coins_front.png?generation=1683302664681760&alt=media)\nThe images of front face of 1 rupee coins are contained in the folder named '1_rupee', images of 2 rupees in folder named '2_rupee' and 5 rupees in '5_rupee'.\n\n2. The back or reverse face of real\/valid coins that do not show their denomination values.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F23601ab0ffde0cf1465058ea94e03f80%2Freal_coins_back.png?generation=1683302796939179&alt=media)\nThese images are contained in the folder named 'reverse'\n\n3. Fake\/Invalid coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fb4adf4b82266250237fcf61722732aa5%2Ffake_coins.png?generation=1683302878176987&alt=media)\nThese images are contained in the folder named 'invalid'\n\n\nThe images were originally captured with a 12MP camera with a resolution of around 2000x4000 pixels and were processed in OpenCV to detect the region of the coin within the image, crop it along the region and resize it to a resolution of 256x256 pixels. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fa9b0997b7704c2d679eb12db880e5f5f%2FImageProcessing.png?generation=1683394637379764&alt=media)\nThis preprocessing was done to make it easier for ML models to learn important features. The code for processing and the steps involved in the processing are described in detail in this Github repo.\n\nThus a model trained on this dataset potentially can:\n1. Classify coin denominations based on the features present in the coin images such as the denomination value and designs imprinted on the face of the coin.\n2. Further classify the coin as valid(real) or invalid(fake) based on the features such as presence or absence of information and artistry, the amount of light present, the region of bright spots present or absent in the image, among other factors.\n\n&gt;I have created a notebook in which I have built a ConvNet model to classify the coins in this dataset, you can check it out [here](https:\/\/www.kaggle.com\/code\/mssomanna\/coin-classification-using-cnn\/).\n\n&gt;I have also hosted a project on GitHub with the same dataset, notebook and other relevant items, you can check it out [here](https:\/\/github.com\/ms-somanna\/Coin-Denomination-and-Validity-Detection-using-Neural-Network-and-Coin-Images).","393":"Simplified version of Handwritten Signature datasets where it has 5 classes (PersonA...PersonE) splited into training\/ testing folders. and a `CSV files` folder containing whether its real or forged for each class in training \/ test.\nThe goal here is to train 2 models (1 for identifying the person sig. and 2 for validating that it is a real one).\nThis dataset is part of the Computer Vision course by FCIS-ASU divided into 2 main tasks :\n&gt; 1. Signature Identification : Building a classical vision or DL techniques to identify the person of this signature.\n&gt; 2. Signature Verification : Build a 5 different models to identify for each class if it is a real or fake OR build a 1 siamese network that verifies the signature of the person whether it is a real one or fake one.","394":"# CIFAKE: Real and AI-Generated Synthetic Images\nThe quality of AI-generated images has rapidly increased, leading to concerns of authenticity and trustworthiness.\n\nCIFAKE is a dataset that contains 60,000 synthetically-generated images and 60,000 real images (collected from CIFAR-10). Can computer vision techniques be used to detect when an image is real or has been generated by AI?\n\nFurther information on this dataset can be found here: [Bird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290)\n\n## Dataset details\nThe dataset contains two classes - REAL and FAKE. \n\nFor REAL, we collected the images from Krizhevsky & Hinton's [CIFAR-10 dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)\n\nFor the FAKE images, we generated the equivalent of CIFAR-10 with Stable Diffusion version 1.4\n\nThere are 100,000 images for training (50k per class) and 20,000 for testing (10k per class)\n\n## Papers with Code\nThe dataset and all studies using it are linked using [Papers with Code](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n[https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n\n\n## References\nIf you use this dataset, you **must** cite the following sources\n\n[Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images.](https:\/\/www.cs.toronto.edu\/~kriz\/learning-features-2009-TR.pdfl)\n\nBird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.\n\nReal images are from Krizhevsky & Hinton (2009), fake images are from Bird & Lotfi (2024). The Bird & Lotfi study is available [here](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290).\n\n## Notes\n\nThe updates to the dataset on the 28th of March 2023 did not change anything; the file formats \".jpeg\" were renamed \".jpg\" and the root folder was uploaded to meet Kaggle's usability requirements.\n\n## License\nThis dataset is published under the [same MIT license as CIFAR-10](https:\/\/github.com\/wichtounet\/cifar-10\/blob\/master\/LICENSE):\n\n*Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:*\n\n*The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.*\n\n*THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.*","395":"A Dataset comprised of two parts, images generated by AI image generation models such as DALL-E and Midjourney, and real images known to be made by humans. The majority of AI generated images are artistic works of some type and not photorealistic because it was found that having more artistic works than photos in the human generated set yielded better test results. One major issue found when trying to train classifiers on this set is while a test accuracy as high as 94% was achieved, if the image (regardless of source AI or human) contained noise such as a film grain or fur there was a higher error rate and the image was more likely to be mislabeled as AI generated. My theory is because diffusion image generation models (DALL-E etc.) start with random noise and turn it into an image based on the prompt, so the classifier could be using the noise of the image as a way to detect Ai generated art and by adding noise the model is getting confused. One possible solution to this is using image denoising on the image or edge detection however I have yet to test either. \n\n\n## Benefits over other datasets\n\nThe benefit of this dataset compared to other artificially generated image datasets (such as CIFAKE) is that all images are in there original size and aspect ratio.\n\n","396":"Problem statement\n\nIn verification services related to face recognition (such as eKYC and face access control), the key question is whether the input face video is real (from a live person present at the point of capture), or fake (from a spoof artifact or lifeless body). Liveness detection is the AI problem to answer that question.\n\nIn this challenge, participants will build a liveness detection model to classify if a given facial video is real or spoofed.\n\n- Input: a video of selfie\/portrait face with a length of 1-5 seconds (you can use any frames you like).\n\n- Output: Liveness score in [0...1] (0 = Fake, 1 = Real).\n\nExample:\n\n- Input: VideoID.mp4\n\n- Output: Predict.csv\n\nfname, liveness_score\nVideoID.mp4, 0.10372\n...\t...\n...\t...","397":"Dataset is consist of 3 different kinds of gemstones\n1. Ruby\n2. Turquoise\n3. Emerald\n\nThe purpose to collect the dataset was to make a model with Convolutional Neural Network Algorithm to identify real or fake gemstones.\nThe Dataset is split into three folders\n1. Train for training the model \n2. Test for Testing the model\n3. Validation for Validating the model\nEach Folders have six subfolders:\n1. Emerald ~ 507 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n2. Fake Emerald ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n3. Ruby ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n4. Fake Ruby ~ 536 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n5. Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n6. Fake Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n\nTotal of 6043 Images are in this dataset. It can be used for Classification Problem (Binary and Multiple). I didn't used any prebuild model I suggest you to use pre-trained model with it to increase the accuracy. The highest Accuracy I achieved is 95% and Best of luck for you...","398":"**About Dataset**\nThis dataset contains real and fake images of human faces.\nReal and Fake Face Detection\nFake Face Photos by Photoshop Experts\nIntroduction\nWhen using social networks, have you ever encountered a 'fake identity'?\nAnyone can create a fake profile image using image editing tools, or even using deep learning based generators.\nIf you are interested in making the world wide web a better place by recognizing such fake faces, you should check this dataset.","399":"### Context\n\nData collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.\n\nWith high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. \n\nAs an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.\n\n### Content\n\nThe dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.\n\nThe data has been generated using the Python scripts and .blend file present in [this repository](https:\/\/github.com\/TheFamousRat\/ChessR). The chess board and pieces models that have been used for those renders are not provided with the code.\n\nData characteristics :\n\n- Images : 1280x1280 JPEG images representing pictures of chess game boards.\n- Annotations : JSON files containing two variables : \n    - \"config\", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.\n    - \"corners\", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.\n- config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : \n    - \"cellsCoordinates\", a dictionary associating a cell name to its coordinates on the board. We have for example {\"A1\" : [0,0], \"A2\" : [1,0], ...}\n    - \"piecesTypes\", a list of strings containing the types of pieces present in the renders.\n\nNo distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.\nA proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.\n\n### Acknowledgements\n\nI would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. \n\n### Inspiration\n\nTwo interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : \n\nErroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt (2021) *Fake It Till You Make It: Face analysis in the wild using synthetic data alone* https:\/\/arxiv.org\/abs\/2109.15102\nSalehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) *PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision* https:\/\/arxiv.org\/abs\/2112.09290","400":"This dataset contains manipulated images and real images. The manipulated images are the faces which are created by various means. The source for this dataset was [https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd](https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd)\nthis dataset was processed as our will to get maximum outcome out of these images. Each image is a 256 X 256 jpg image of human face either real or fake","401":"### Context\n\nData on the Israeli Knesset from the first Knesset to the 24th. \n\n\n\n### Content\n\nFor Knesset 1-24 this contains:\n- Names of all politicians\n- The Party they are affiliated with\n- The number(s) of parliaments each politician was active in \n- Gender\n- Place of Birth\n- Place of Death\n- image link\n- date of birth\n- primary language \netc.\n\n\nSource:\nhttps:\/\/main.knesset.gov.il\/mk\/Pages\/current.aspx?pg=mklist\nWikipedia\nWikidata\n\n### Inspiration\nDuring 2019-2020 isreal was stuck in a political deadlock with election following election. This contribution was done in the hope to allow easier access to political data which will enable verifying news as fake or real news. ","402":"This dataset is extracted from https:\/\/www.indiansuperleague.com on 6\/25\/2024 at 5:15:00 PM  It contains data of Indian players under the age of 25 for the ISL 2023-2024 season. This dataset aims to find players who can play for India in the upcoming years to qualify for the World Cup.","403":"This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures","404":"This dataset contains detailed statistics on soccer players for the 2023-2024 season, sourced from FBRef. FBRef is a renowned source for soccer statistics and analysis, providing comprehensive and accurate data on players and teams from various leagues and international competitions. The data was extracted using web scraping techniques. \nFor more information, please refer to the GitHub [repository](https:\/\/github.com\/dyomed93\/Scraping-Analysis-FbRef).","405":"#**Description**\nThis dataset contains detailed information on players participating in the Super Lig for the 2023-2024 season. It includes players' numbers, teams, positions, preferred foot, attacking (ATT), technique (TEC), tactical (TAC), defense (DEF), creativity (CRE) skill ratings, market values, and nationalities. With a total of 630 rows, this dataset is a valuable resource for analysis and research purposes.\n\n#**Subtitles**\n**Number:**Player's jersey number\n**Team:** The team the player is playing for\n**Position:** The position the player is playing (e.g., forward, midfielder, defender)\n**Preferred Foot:** The player's dominant foot (right, left)\n**ATT:** Attacking skill rating\n**TEC:** Technique skill rating\n**TAC:** Tactical skill rating\n**DEF:** Defense skill rating\n**CRE:** Creativity skill rating\n**Market Value:**The market value of the player (in million \u20ac)\n**Nationality:** The player's nationality\n\n#**File Information**\n**File Name:** Super Lig 2023-2024.csv\n**File Size:** 39.68KB\n**Format:** CSV (Comma-Separated Values)\n\n#**Author**\nThis dataset has been compiled to provide detailed information on players in the Super Lig for the 2023-2024 season for analysis and research purposes.","406":"**Comprehensive Football Player Statistics: 2023-2024 Season**\nThis dataset contains detailed player statistics from top football leagues for the 2023-2024 season. Sourced from FBref, the dataset includes a wide range of metrics covering various aspects of player performance, such as defense, goalkeeping, passing, and shooting.\n\n**Key Features**\nDetailed Player Metrics: Statistics for individual players across multiple performance areas.\nStructured Data: Organized into tables focusing on different aspects of the game for easy analysis.\nTop Leagues: Includes data from prominent leagues that provide comprehensive detailed stats.\n\nGithub Repository link of the project : https:\/\/github.com\/GuechtouliAnis\/Football-Data-Scraping\n\nBy: Guechtouli Anis","407":"Dataset of all the players that are in the squad of the teams participating in the UEFA EURO 2024. Contains info about clubs, age, height, market value etc. which can be very good for EDA and Data Visualizations.","408":"All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions","409":"**Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n","410":"Context-\nThe data was created from the IPL auction stats displaying funds and players\nSource-\nThe data was scrapped from https:\/\/www.iplt20.com\/auction\/2021 \nAbout-\nEight franchises buy 57 players at IPL 2021 Player Auction The VIVO IPL 2021 Player Auction concluded in Chennai with 57 players filling up the 61 available slots. The eight franchises went for some of the finest talent available, giving them a chance to unleash their potential and make an impact in the upcoming edition of the tournament.","411":"This dataset contains detailed information on 3,060 unique football\/soccer players. Each entry provides a comprehensive profile of a player, including personal details, overall performance ratings, specific skills, and various attributes. The dataset is structured to offer insights into player capabilities, potential, and market value. \n\nBelow is a detailed description of the columns included in the dataset:\n\n**ID:** Unique identifier for each player.\n**Age:** Age of the player in years.\n**Height:** Height of the player in centimeters.\n**Weight:** Weight of the player in kilograms.\n**Preferred foot:** The preferred foot of the player (e.g., Left, Right).\n**Overall rating:** The current overall rating of the player, reflecting their performance level.\n**Potential:** The potential rating indicating the highest possible overall rating a player can achieve.\n**Best overall:** The highest overall rating achieved in any position by the player.\n**Best position:** The position where the player performs best.\n**Growth:** The difference between the player's current overall rating and their potential.\n**Value:** The market value of the player, usually in currency (e.g., Euros).\n**Wage:** The weekly wage of the player.\n**Release clause:** The release clause value in the player's contract.\n**Total attacking:** The total score of all attacking attributes combined.\n**Crossing:** Ability to accurately deliver the ball from wide areas.\n**Finishing:** Ability to score goals in one-on-one situations.\n**Heading accuracy:** Accuracy of headers when trying to score or pass the ball.\n**Short passing:** Precision and accuracy of short-distance passes.\n**Volleys:** Technique and power in volley shots.\n**Total skill:** The combined score of all skill-related attributes.\n**Dribbling:** Ability to maintain control of the ball while maneuvering around opponents.\n**Curve:** Ability to curve the ball, typically used in shots and passes.\n**FK Accuracy:** Accuracy in taking free kicks.\n**Long passing:** Precision and accuracy of long-distance passes.\n**Ball control:** Ability to control and manipulate the ball effectively.\n**Total movement:** The combined score of all movement-related attributes.\n**Acceleration:** Quickness of reaching top speed.\n**Sprint speed:** Top speed a player can achieve.\n**Agility:** Ability to quickly change direction and position.\n**Reactions:** Quickness of responding to situations during the game.\n**Balance:** Stability and coordination while moving or under pressure.\n**Total power:** The combined score of all power-related attributes.\n**Shot power:** Strength and power in shots on goal.\n**Jumping:** Ability to jump vertically.\n**Stamina:** Endurance and ability to sustain performance over time.\n**Strength:** Physical strength and ability to win physical duels.\n**Long shots:** Accuracy and power in long-distance shots.\n**Total mentality:** The combined score of all mentality-related attributes.\n**Aggression:** Level of intensity and physicality in play.\n**Interceptions:** Ability to intercept passes and disrupt the opposition's play.\n**Att. Position:** Positioning intelligence when attacking.\n**Vision:** Ability to see and execute key passes and plays.\n**Penalties:** Ability to score from penalty kicks.\n**Composure:** Calmness and performance under pressure.\n**Total defending:** The combined score of all defending-related attributes.\n**Defensive awareness:** Understanding and anticipation of defensive responsibilities.\n**Standing tackle:** Effectiveness in making standing tackles.\n**Sliding tackle:** Effectiveness in making sliding tackles.\n**Total goalkeeping:** The combined score of all goalkeeping attributes.\n**GK Diving:** Ability to dive and save shots.\n**GK Handling:** Ability to catch and control the ball.\n**GK Kicking:** Accuracy and power of goal kicks.\n**GK Positioning:** Positioning intelligence in goalkeeping situations.\n**GK Reflexes:** Quickness of reflexes to make saves.\n**Total stats:** The overall total of all individual attribute scores.\n**Base stats:** The base statistics without modifiers or bonuses.\n**International reputation:** Reputation of the player on the international stage (e.g., 1 to 5 stars).\n**Pace \/ Diving:** Combined score of Pace attributes for outfield players or Diving for goalkeepers.\n**Shooting \/ Handling:** Combined score of Shooting attributes for outfield players or Handling for goalkeepers.\n**Passing \/ Kicking:** Combined score of Passing attributes for outfield players or Kicking for goalkeepers.\n**Dribbling \/ Reflexes:** Combined score of Dribbling attributes for outfield players or Reflexes for goalkeepers.\n**Defending \/ Pace:** Combined score of Defending attributes for outfield players or Pace for goalkeepers.\n\nThis dataset offers a rich resource for analyzing player performance, potential, and market value in the world of football\/soccer. It can be utilized for various purposes, including player scouting, performance analysis, and strategic planning for team management. ","412":"All statistics sourced from FBRef and Transfermarkt and scraped using Ruby on Rails. This dataset contains 3 CSVs, \"Teams\" with basic info (just name and FBRef link), \"Players\" with name and summary statistics for the 2023-2024 season as well as their age and value from Transfermarkt, and \"Match Logs\" which has in-depth information for every Premier League game the player's team took part in over the past 2 seasons (or whatever length of time the player was on said team).","413":"**Dataset will be updated during EURO 2024**<br>\n**You may support this project via the upvotes button**\n\n## Description \nThe dataset contains all players & coaches, all matches & results, and main match events in Football\/Soccer UEFA European Championship\/EURO (1960-2024), and Nations League (2019-2023).\n\n* `matches`:\n * `euro`: `1960.csv` ...  `2024.csv` -  all EURO matches.\n * `nations`: `2019.csv`, `2021.csv`, `2023.csv` - all Nations League matches.\n * `friendly_2021-2024.csv` - all friendly matches from 2021 to 2024.\n * `qualifying_1960-2024.csv` - all qualifying matches from 1960 to 2024.\n* `logos` - contains flags of associations\n*  `euro_coaches.csv` - all coaches from 1960 to 2024\n* `euro_lineups.csv` - all players from 1960 to 2024\n* `euro_summary.csv` - table includes basic information about each EURO.\n\n## Related Datasets\n- [Football - FIFA Men's World Cup, 1930 - 2022](https:\/\/www.kaggle.com\/datasets\/piterfm\/fifa-football-world-cup)\n- [Football - FIFA Women's World Cup, 1991 - 2023](https:\/\/www.kaggle.com\/datasets\/piterfm\/football-fifa-womens-world-cup-1991-2023)\n\n## Table columns\n\n## Updates\n2024-06-16 - add lineups & coaches; add associans flags folder.<br>\n2024-06-09 - add friendly, qualifying & Nations League matches.","414":"","415":"","416":"This dataset is collected from a simulated save file of the game Football Manager 23. \n\nNorthampton FC has clawed their way upto the second tier of English football, the Championship in 2030-31 season. After a rough season and nearly escaping relagation, they are focusing heavily on their player recruitment to become an established Championship team. \n\nNorthampton finished 21st in the table, securing only 46 points with a goal difference of -21. Their lack of goalscoring ability upfront and leaky defense at the back has made the club rethink their recruitment strategy.\n\nFrom the list of 422 scouted players, they're actively looking for :\n\n- A clinical striker with a keen eye for goal.\n- A creative midfielder to provide high-quality chances\n- A hard working, physical midfielder to provide cover for the creative midfielder.\n- An aerially dominant Center-back who is also sound in defense. \n- A wingback who covers a lot of ground and dishes out quality crosses inside the final third.\n\nTo accomodate the signings, the Northampton FC board has allowed an extra wage budget of 60k\/week.\n\nYour job as a manager is to find the best recruitments within the wage budget.","417":"2024 NWSL (so far) complete player stats. The dataset will be updated monthly throughout the season as the season progresses. \n\nThe data is mostly scraped from NWSL.com. It has been cleaned, and additional information has been added, such as position.\n\nThe current file is scraped for the current 2024 season through May 25, 2024, and will be updated monthly with updated information as NWSL posts updates.\n\nThere is currently data for 344 players across 40 variables. \n\nHere is the type of information included:\n\n- team\n- player_name\n- position\n- games_played\n- games_started\n- minutes_played\n- goals\n- accurate_pass_percentage\n- assists\n- total_scoring_attempts\n- on_target_scoring_attempts\n- total_attacking_assists\n- tackles\n- fouls_committed\n- fouls_suffered\n- total_offside\n- yellow_cards\n- red_cards\n- accurate_passes\n- total_passes\n- crosses\n- assists_avg_over_90_mins\n- long_balls\n- successful_short_passes\n- turnovers\n- goals_avg_over_90_mins\n- penalty_kick_goals\n- penalty_kick_taken\n- penalty_kick_percentage\n- accurate_shooting_percentage\n- successful_dribble\n- dribble_percentage\n- goals_and_assists\n- tackles\n- tackles_percentage\n- interceptions\n- headed_duel\n- gk_saves\n- gk_long_ball_percentage\n- gk_total_clearance\n\nUse the data to make interesting visualizations, make your own predictions for the season, power your app, do your own analysis, have fun! Just an NWSL superfan here making information available for you. Please message me for any other ideas or questions.\n","418":"For the Pakistan Super League (PSL) Season 9 dataset, you have a couple of good options:\n\n1. Kaggle: You can find a comprehensive dataset for PSL Season 9 on Kaggle. This dataset includes detailed information on matches, players, scores, and other relevant statistics. It's a great resource for analysis and research related to the 2024 season of the PSL. You can access it [here](https:\/\/www.kaggle.com\/datasets\/umerhaddii\/psl-season-9-complete-dataset-2024)\u30105\u2020source\u3011.\n\n2. Open Data Pakistan: Another source for PSL data, including historical data from previous seasons, is available on Open Data Pakistan. This dataset contains extensive records collected from various seasons, including detailed performance metrics. It's available [here](https:\/\/opendata.com.pk\/dataset\/pakistan-super-league-datasets)\u30106\u2020source\u3011\u30107\u2020source\u3011.\n\n","419":"**# Background**\n\n**The Pakistan Super League (PSL)** is a professional Twenty20 cricket league in Pakistan, founded by the Pakistan Cricket Board (PCB) in 2015. The league was conceived to enhance the cricketing talent pool in Pakistan and provide a platform for local players to compete with international stars.\n**Inception and Early Years (2016-2017):**\nThe inaugural season took place in February 2016, with five franchises representing major cities: Islamabad United, Karachi Kings, Lahore Qalandars, Peshawar Zalmi, and Quetta Gladiators. Matches were held in the United Arab Emirates due to security concerns in Pakistan. Islamabad United won the first season, defeating Quetta Gladiators in the final.\n**Expansion and Domestic Growth (2018-2019):**\nThe PSL expanded in 2018 with the addition of a sixth team, Multan Sultans. This period marked a significant effort by the PCB to bring matches back to Pakistan. In 2018, the final was held in Karachi, and by 2019, more matches, including the playoffs and the final, were held in various Pakistani cities, signaling the return of international cricket to the country.\n**Consolidation and Popularity (2020-Present):**\nBy 2020, the entire PSL season was held in Pakistan for the first time. This move bolstered local support and further integrated the league into the national sports culture. The league continued to attract international players, enhancing its competitiveness and entertainment value.\n**Future Expansion (Post PSL-10):**\nThe PSL has continued to thrive, and after the tenth season, the league is set to expand further. Plans are in place to introduce two additional franchises, increasing the number of teams from six to eight. This expansion aims to bring in more regional representation, further develop cricketing talent, and enhance the league's competitiveness.\n**Impact and Future:**\nThe PSL has significantly impacted Pakistan's cricket, offering a lucrative career path for players and improving the country's cricketing infrastructure. It has also played a crucial role in the revival of international cricket in Pakistan. The league's success has paved the way for potential future expansions and innovations.\n\nOverall, the PSL has grown rapidly since its inception, becoming a prominent fixture in the global cricket calendar and contributing to the resurgence of cricket in Pakistan. The planned expansion after PSL-10 underscores the league's commitment to growth and development in the sport.\nOne of the hallmarks of the PSL is its competitive nature, featuring a dynamic mix of local Pakistani players and international cricket stars. This blend of talent not only raises the level of competition but also enhances the global appeal of the league. Over the years, the PSL has grown into a significant fixture on the cricketing calendar, celebrated for its thrilling matches and the high-quality cricket on display.\n\u2003\n## Gratified\nSeason 8 and Season 9 of the Pakistan Super League (PSL) were captivating cricketing spectacles that highlighted the blend of local and international talent. \n**Season 8 of the Pakistan Super League** played from February 13 to March 19, 2023, captivating fans with its intense matches and high-level performances. The tournament was hosted across multiple cities in Pakistan, showcasing the country's passion for cricket. Lahore Qalandars emerged victorious, clinching their second title by defeating Multan Sultans in a thrilling final. The season was marked by remarkable individual performances, with standout players like Fakhar Zaman and Rashid Khan making significant contributions.\n**Season 9 of the Pakistan Super League** followed, held from February 17 to March 18, 2024. This season continued to build on the excitement and competitive spirit of the PSL, with matches hosted in four cities across Pakistan. Islamabad United secured their third title by triumphing over Multan Sultans in a nail-biting final, underscoring their dominance in the league. The tournament featured 34 matches filled with outstanding performances from both local and international stars. Notable players like Babar Azam and Usama Mir grabbed headlines with their exceptional skills and match-winning contributions.\nFor Season 9, a comprehensive dataset is available, containing detailed summaries of each match as well as ball-by-ball information. This data provides a granular view of the performances and dynamics of the season, offering valuable insights into the intricacies of the game.\n\n**Venues: **Karachi, Lahore, Multan and Rawalpindi\n**Time-period:** 9 February to 19 March 2023 (Season 8) and 17 February to 18 March 2024  (Season 9)  \u2003\n\n## T20 Dataset from Cricsheet.org\nThe T20 dataset available on Cricsheet.org is a comprehensive and detailed collection of cricket match data, focusing specifically on the T20 format. This dataset is a valuable resource for cricket analysts, data scientists, and enthusiasts interested in exploring the intricacies of T20 cricket. The data encompasses a wide range of match details, including player performances, match outcomes, and ball-by-ball events, offering a granular view of the dynamics of each game.\nThis dataset opens up numerous possibilities for data-driven insights and advanced cricket analytics, making it an essential asset for anyone looking to explore the fast-paced world of T20 cricket through a data-centric lens.\nThis dataset is also good source for Data Science students to practice.\n\n","420":"## Context##\n\nThis dataset is collected from the simulated season of 2030-31 in Championship in the game Football Manager 23. \n\nWebscrapping is cool and all. But most of the time, you don't get much from the extracted data from sites like FBref, Opta, and so on. \n\nBut if you're a Football Manager player, you can generate your own data by simulating a full season in the game. And that's exactly what i did.\n\n---\n\nP.S Note: This dataset consists of players in the 2030-31 season. So most of the older players have already retired by now and you'll also stumble upon a lot of newly generated young players in the teams.\n\nHere's all the columns:\n\n---\n\nName  --- Player Name\nNat --- Nationality\nClub --- Playing Club\nPosition --- Playing Position\nAge --- Player Age\nHeight --- Player Height(Feet'inches\")\nPreferred Foot --- Strong foot of the player\nApps --- Appearances\/ Matches played\nStarts --- Starting appearance\nMins --- Minuttes\nGls --- Goals\nAst --- Assists\nWage --- Player Wages\nTransfer_Value --- Estimated transfer value of player\nPens --- Penalties Attempted\nPens_S --- Penalties Scored\nPas_A --- Passes Attempted\nPas_% --- Pass completion rate\nPr_Passes --- Progressive Passes\nPres_A --- Press attempted\nPress_C --- Press completed\nBlk --- Block\nShts Blckd --- Shots Blocked\nClear --- Clearance\nHdrs --- Headers\nItc --- Interception\nTck_A --- Tackles attempted\nTck_W --- Tackled Won\nOff --- Offsides\nGl_Mst --- Mistakes leading to Goal\nK_Tck --- Key Tackles\nDistance --- Distance Covered\nDrb --- Dribbles made\nCr_A --- Crosses attempted\nCr_C% --- Cross Completion Ratio\nShots --- Shots taken\nShts_on_target --- Shots on target\nK_Pas --- Key Passes\nYel --- Yellow Cards\nRed --- Red Cards\nxG --- Expected Goals\nxA --- Expected Assists\n\n---\n**Some of you might get the UTF-8 error(UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte) while defining the dataframe. \n\nIn that case, simply add ''encoding =('ISO-8859-1'),low_memory =False'', after calling the location.\n\nExample:\n\ndf = pd.read_csv('\/kaggle\/input\/football-manager-23-championship-3031-dataset\/Championship_2030-31 Simulation data.csv', encoding =('ISO-8859-1'),low_memory =False)\ndf.head()**\n\n---\n\nThe dataset is not cleaned AT ALL. I'll try to clean it up asap. Until then, it's going to be a bit messy to deal with it. Cheers!","421":"2023 complete and comprehensive player stats for 347 NWSL athletes\/players.\nData comes from NWSL.com. Data fields have been cleaned, and additional fields, like player position, have been added for NWSL data science fun.\n\nComplete stats include:\n- Attacking\n- Passing\n- Goals\n- Defending\n- PKs\n- Goalkeeping\n\nand more. 38 variables included. \n\nI have left NaNs in the dataset instead of filling them with 0s because 0 is a meaningful stat in this dataset. NaNs means there was no data available. For example, goalkeeping saves, 0 is a meaningful statistical number. But you can drop and clean NaNs for your own purposes on your own.\n\nPercentages are in whole numbers.\n\n2024 season stats and stadium attendance stats will be added soon. \n\nMessage me for other women's sports ideas and variables to include. ","422":"This dataset contains football match data from the English Premier League for the seasons 2008-2024. Each file includes detailed match statistics, including home and away scores, halftime scores, and various other match details.\n","423":"This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures","424":"These files contain data about football players in Europe's most popular leagues between 2017-2024. There are a wide variety of attributes, the names of teams and players have been properly formatted. If you are unsure what a column is, please visit fbref.com and find the relevant section for a better description of each column. \n\nI used this data to build a Euro 2024 Match predictor (Spoiler: Germany and France Final) using Random Forest Regressor, a machine learning algorithm and I would greatly appreciate any feedback on the project.\nProject link:\nhttps:\/\/github.com\/GurpreetSDeol\/Euro-2024-Match-Predictor-\/tree\/main","425":"","426":"The English Women's Football (EWF) Database is an open database of matches played in the top tiers of women's football in England. It covers all matches played since the 2011 season for the highest division (the Women's Super League) and since the 2014 season for the second-highest division (the Women's Championship).\n\nThe database contains three datasets:\n\n- ewf_matches contains all matches that have been played and has one observation per match per season.\n- ewf_appearances contains all appearances by a team and has one observation per team per match per season.\n- ewf_standings contains all end-of-the-season division tables and has one observation per team per season.\n\nAll three datasets will be updated with the latest information at the end of each season.\n\nSource: The English Women's Football (EWF) Database, May 2024, https:\/\/github.com\/probjects\/ewf-database.\n\nInspiration has been taken from the Fjelstul English Football Database, a similarly structured dataset that covers men's professional football since 1888.\n\n### Data dictionary\n\n#### ewf_matches\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| home_team_id| The unique ID for the home team. Has the format `T-###-T`.|\n| home_team_name| The name of the home team at the match.|\n| away_team_id| The unique ID for the away team. Has the format `T-###-T`.|\n| away_team_name| The name of the away team at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team_score| The score of the home team.|\n| away_team_score| The score of the away team.|\n| home_team_score_margin| The score margin for the home team, equal to home_team_score minus away_team_score.|\n| away_team_score_margin| The score margin for the away team, equal to away_team_score minus home_team_score.|\n| home_team_win| Whether the home team won the match. Possible values are `1` if the home team won the match and `0` otherwise.|\n| away_team_win| Whether the away team won the match. Possible values are `1` if the away team won the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. Possible values are `1` if the match ended in a draw and `0` otherwise.|\n| result| The result of the match. Possible values are `Home team win`, `Away team win`, and `Draw`.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n\n#### ewf_appearances\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team. References `match_id` in the `ewf_matches` dataset.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team at the match.|\n| opponent_id| The unique ID for the team\u2019s opponent. Has the format `T-###-T`.|\n| opponent_name| The name of the team\u2019s opponent at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team| Whether the team was the home team. Possible value are `1` if the team was the home team and `0` otherwise.|\n| away_team| Whether the team was the away team. Possible value are `1` if the team was the away team and `0` otherwise.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| result| The result of the match. Possible values are `Win`, `Loss`, and `Draw`.|\n| win| Whether the team won the match. The possible values are `1` if the team won the match and `0` otherwise.|\n| loss| Whether the team lost the match. The possible values are `1` if the team lost the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. The possible values are `1` if the match ended in a draw and `0` otherwise.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n| points| The number of points the team earned from the match. A team earns `0` points for a loss, `1` point for a draw, or `3` points for a win.|\n\n#### ewf_standings\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| position | The team's final position in the season.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team during the season.|\n| played| The number of matches that the team played.|\n| wins| The number of matches that the team won.|\n| draws| The number of matches that the team drew.|\n| losses| The number of matches that the team lost.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| points| The number of points that the team earned over the whole season (after applying `point_adjustment`).|\n| point_adjustment| The number of points that were deducted by the league due to violations of rules or added by the league due to forfeits.|\n| season_outcome| The outcome for the team following the season. This variable is included to track the movement of teams across seasons more easily. Possible values are `Club folded`, `No change` for when the team remains in their current tier, `Promoted to tier 1` for when the team moves into tier 1 from a lower tier, `Relegated to tier 2` for when the team moves into tier 2 from a higher tier, and `Relegated to tier 3` for when the team moves into tier 3 from a higher tier.|","427":"All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions","428":"**Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n","429":"This dataset contains all the event-by-event actions of World Cup 2022 final match between Argentina vs France. Collected from Statsbomb open data : [https:\/\/github.com\/statsbomb\/open-data](url)\n\nThe dataset contains a total of 4407 recorded events(rows) and 120 columns.\n","430":"The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data.","431":"This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information","432":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","433":"### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv)","434":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","435":"Dataset of Job descriptions and relevant IT skills, soft skills, education and experience required for the job. \nHard skills include IT keywords from job descriptions. It can be used to train LLM models to extract skills from various IT specific job descriptions","436":"Contains a small dataset of labeled IT skills, IT Tools, IT technologies to train named entity recognition (NER) model for extracting skills from job description using spaCy. it will be updated weekly to increase the number of rows.","437":"This Dataset is a collection of popular dark patterns from different E-Commerce and Online store sites. It can be utilized by creating accurate ML Model to detect Dark Patterns in Online Stores and E-Commerces.","438":"## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n```","439":"Did We Solve the Problem?\nThe objective of this analysis was to predict high streaming counts on Spotify and perform a detailed cluster analysis to understand user behavior. Here\u2019s a summary of how we addressed each part of the objective:\n\nPrediction of High Streaming Counts:\n\nImplemented Multiple Models: We utilized several machine learning models including Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN).\nComparison and Evaluation: These models were evaluated based on classification metrics like accuracy, precision, recall, and F1-score. The Gradient Boosting and Random Forest models were found to be the most effective in predicting high streaming counts.\nCluster Analysis:\n\nK-means Clustering: We applied K-means clustering to segment users into three clusters based on their listening behavior.\nDetailed Characterization: Each cluster was analyzed to understand the distinct characteristics, such as average playtime, skip rate, offline usage, and shuffle usage.\nVisualizations: Histograms and scatter plots were used to visualize the distributions and relationships within each cluster.\nResults and Insights\nEffective Models: The Gradient Boosting and Random Forest models provided the highest accuracy and balanced performance for predicting high streaming counts.\nUser Segmentation: The cluster analysis revealed three distinct user segments:\nCluster 1: Users with longer playtimes and lower skip rates.\nCluster 2: Users with moderate playtimes and skip rates.\nCluster 3: Users with shorter playtimes and higher skip rates.\nThese insights can be leveraged for targeted marketing, personalized recommendations, and improving user engagement on Spotify.\n\nConclusion\nYes, we solved the problem. We successfully predicted high streaming counts using effective machine learning models and provided a detailed cluster analysis to understand user behavior. The analysis offers valuable insights for enhancing Spotify\u2019s recommendation system and user experience.","440":"The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data.","441":"This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information","442":"**Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation},","443":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","444":"## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media)","445":"### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv)","446":"","447":"A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user. ","448":"**ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","449":"**ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","450":"**ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","451":"**ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n ","452":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","453":"","454":"Dataset created for training a neural network that can recognize four-player chess pieces. The dataset consists of photos taken based on the expected design, created with a 3D printer, for easy and straightforward recognition.","455":"","456":"The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\""},"keywords":{"0":["movies and tv shows"],"1":["movies and tv shows"],"2":["movies and tv shows"],"3":["movies and tv shows","beginner","intermediate","recommender systems","tabular"],"4":["movies and tv shows","business","computer science","programming"],"5":["popular culture","arts and entertainment","movies and tv shows","image classification"],"6":["movies and tv shows"],"7":["movies and tv shows"],"8":["movies and tv shows"],"9":["arts and entertainment","movies and tv shows"],"10":["anime and manga"],"11":["movies and tv shows"],"12":[],"13":["movies and tv shows"],"14":["movies and tv shows"],"15":["movies and tv shows"],"16":["movies and tv shows"],"17":["arts and entertainment","movies and tv shows","nlp","data analytics","recommender systems"],"18":["movies and tv shows"],"19":["arts and entertainment","movies and tv shows"],"20":[],"21":[],"22":[],"23":["mental health","philosophy","text","english"],"24":["psychology","text"],"25":["psychology","text"],"26":["classification","text","pandas","python","english"],"27":["classification","tabular","news"],"28":["nlp","text mining","binary classification","social networks","text classification"],"29":["mobile and wireless","exploratory data analysis","text mining","multiclass classification","social networks"],"30":["text classification","sentence similarity","english"],"31":["india","politics","nlp","text"],"32":["earth and nature","nlp","social networks"],"33":["mental health","lstm","tabular","multiclass classification","text pre-processing"],"34":["education","computer science"],"35":["intermediate","survey analysis","classification","social networks","sentence similarity"],"36":["arts and entertainment","people and society","education","computer science","nlp","data visualization"],"37":["beginner","nlp","text","text classification"],"38":["text mining","tabular","text","currencies and foreign exchange","text classification"],"39":["internet","email and messaging"],"40":["popular culture","literature","nlp"],"41":["gender","computer science","nlp","text mining","social issues and advocacy"],"42":["politics"],"43":["education"],"44":["computer science"],"45":["text","binary classification","online communities","racial equity","english"],"46":["email and messaging","social networks"],"47":["social science"],"48":["cycling"],"49":[],"50":["intermediate","retail and shopping","pandas","scipy","seaborn"],"51":["earth and nature","business","internet","electronics","beginner","tabular","binary classification"],"52":["law"],"53":["united states","automobiles and vehicles","data visualization"],"54":["brazil","banking","advanced","data analytics","graph"],"55":["united states","categorical","transportation","tabular","urban planning"],"56":["business","intermediate","advanced","data analytics","classification","feature engineering"],"57":["people","computer vision","image","public safety","image classification"],"58":[],"59":["games","cities and urban areas","business","beginner","intermediate","advanced"],"60":["computer vision","deep learning","image","yolo","object detection"],"61":["cycling","exploratory data analysis","data visualization","outlier analysis","regression"],"62":["law"],"63":["beginner","advanced","computer vision","image","object detection"],"64":["automobiles and vehicles","computer vision","deep learning","image","keras"],"65":["law","image","image classification","image augmentation","image generator"],"66":["animals","education","intermediate","tabular"],"67":["animals","deep learning","image"],"68":["earth and nature","animals","classification"],"69":["diseases","public health","data visualization","r"],"70":["research","earth science","animals","science and technology","artificial intelligence","r"],"71":["earth science","animals"],"72":["arts and entertainment","animals","computer vision","image","regression"],"73":["asia","animals","deep learning","image"],"74":["earth and nature"],"75":["fish and aquaria"],"76":["animals","computer vision","image","object detection","yolov8"],"77":[],"78":["public health","data visualization","health conditions","public safety","r","data storytelling"],"79":["earth and nature","animals","computer vision","deep learning","image","multiclass classification","travel"],"80":["global","animals","deep learning","audio","audio classification"],"81":["earth and nature","animals","image"],"82":["alcohol","water bodies","earth and nature","environment","chemistry","energy","public safety"],"83":["europe","animals","beginner","image","automl"],"84":["animals"],"85":["earth and nature","fish and aquaria"],"86":["data visualization","python","image classification","graph","cyber security"],"87":["arts and entertainment"],"88":["health","classification","deep learning","cnn","health conditions","tensorflow"],"89":["computer vision","classification","deep learning","image","binary classification"],"90":["medicine","cancer","image classification"],"91":["medicine","classification","cancer","image classification"],"92":["medicine","cancer","image classification"],"93":["arts and entertainment","medicine","cancer","image classification"],"94":["computer science","computer vision","classification","image","image classification"],"95":["online communities"],"96":["earth and nature"],"97":["agriculture","computer vision","deep learning","image classification"],"98":[],"99":["earth and nature","computer science","computer vision","keras","image classification","image augmentation"],"100":["computer vision","multiclass classification","health conditions","image classification"],"101":["earth science","geology","intermediate","computer vision","image classification"],"102":["research","classification","image"],"103":["health","health conditions"],"104":["artificial intelligence","computer vision","classification","image","online communities"],"105":[],"106":["software","transportation","data cleaning","data visualization","data analytics","data storytelling"],"107":["automobiles and vehicles","beginner","xgboost","decision tree","tabular"],"108":["law"],"109":["earth and nature","automobiles and vehicles"],"110":["categorical","automobiles and vehicles","intermediate","clustering"],"111":["research","categorical","beginner","data analytics","tabular"],"112":["exploratory data analysis","data visualization","feature engineering","deep learning"],"113":["united states","automobiles and vehicles","beginner","classification","data storytelling"],"114":["categorical","law","automobiles and vehicles","intermediate","data analytics","english"],"115":["law","demographics","automobiles and vehicles","exploratory data analysis","public safety"],"116":["cities and urban areas","people and society","transportation","geospatial analysis","public safety"],"117":["cities and urban areas","demographics","transportation"],"118":["beginner","intermediate","exploratory data analysis","data visualization","data analytics"],"119":["europe","law","transportation","travel","urban planning"],"120":["business","law","deep learning","bigquery","audio command detection","audio event classification","data storytelling"],"121":["united states","automobiles and vehicles","data visualization"],"122":[],"123":["automobiles and vehicles"],"124":["automobiles and vehicles","beginner","data visualization","time series analysis","linear regression"],"125":["united states","automobiles and vehicles","beginner","intermediate","english"],"126":["music","exploratory data analysis","data analytics","recommender systems","sql"],"127":["anime and manga"],"128":["games","anime and manga"],"129":["beginner","intermediate","tabular","anime and manga","pandas"],"130":["anime and manga"],"131":["anime and manga"],"132":["anime and manga"],"133":[],"134":["exploratory data analysis","data visualization","feature engineering","recommender systems","anime and manga"],"135":["anime and manga"],"136":["business","banking"],"137":["business","data cleaning","data visualization","time series analysis","tabular"],"138":["lending","banking"],"139":["research","finance","banking","classification","insurance"],"140":["computer science","computer vision","deep learning","neural networks","cnn","image classification"],"141":["arts and entertainment","online communities"],"142":["categorical","image"],"143":["computer science"],"144":[],"145":["computer science","computer vision","feature engineering","image"],"146":["art"],"147":["intermediate","computer vision","classification","cv2","image classification"],"148":["computer vision","lstm","keras","tensorflow","pytorch"],"149":["arts and entertainment","science and technology","computer vision","classification","deep learning","image","cv2"],"150":["languages","artificial intelligence","electronics","intermediate","classification","english"],"151":["languages","earth and nature","classification","image"],"152":["online communities"],"153":["earth and nature","business"],"154":["image","health conditions"],"155":["computer science","computer vision","classification","random forest","numpy","serbian"],"156":["computer vision","image"],"157":[],"158":["literature"],"159":["computer vision","classification","image","yolov5","english"],"160":["ratings and reviews"],"161":["tabular","image"],"162":["intermediate","exploratory data analysis","data cleaning","data visualization","tabular"],"163":["business"],"164":[],"165":["asia","finance","data cleaning","data analytics"],"166":["make-up and cosmetics"],"167":["finance","currencies and foreign exchange"],"168":["exploratory data analysis","data visualization","data storytelling"],"169":[],"170":["business","banking"],"171":["news"],"172":["sports"],"173":["news"],"174":["business","news"],"175":["news"],"176":["news"],"177":["people and society","government","law","news","social networks"],"178":["demographics","international relations","nlp","text","news","middle east"],"179":["news"],"180":["computer vision","multiclass classification","health conditions","image classification"],"181":["advanced","video","python","english"],"182":["advanced","video","python","english"],"183":["news"],"184":["research","classification","image"],"185":["categorical","tabular","text"],"186":[],"187":["news"],"188":[],"189":["news"],"190":[],"191":["nlp","text","news","sinhalese"],"192":[],"193":["music"],"194":["music","artificial intelligence","computer science","data analytics","classification","chinese"],"195":[],"196":["categorical","tabular","news"],"197":[],"198":[],"199":["news"],"200":["nlp","text","news","text classification","english"],"201":["earth and nature","beginner","intermediate","classification","text","news"],"202":["earth and nature"],"203":[],"204":["news"],"205":["news"],"206":["news"],"207":["news"],"208":[],"209":["news"],"210":["news"],"211":[],"212":[],"213":[],"214":[],"215":["business"],"216":["deep learning","image","food","image classification"],"217":["categorical","classification","cnn","image","cooking and recipes","food"],"218":["nutrition","text","cooking and recipes","food"],"219":["deep learning","image","food","image classification"],"220":["cooking and recipes","food"],"221":["earth and nature","business","image","cooking and recipes","food"],"222":["categorical","classification","cnn","image","cooking and recipes","food"],"223":["arts and entertainment","nlp","geospatial analysis","classification","restaurants"],"224":["nutrition"],"225":["alcohol","exploratory data analysis","data analytics","classification","recommender systems"],"226":["languages","literature","computer science","nlp","text mining","deep learning","neural networks"],"227":["education"],"228":["computer science","beginner","intermediate","text","chinese"],"229":["beginner","tabular","question answering","english","mathematics"],"230":["finance","nlp","question answering"],"231":["football","nlp","deep learning","text generation","text conversation"],"232":["data cleaning","text","retrieval question answering","english"],"233":["business","education","science and technology"],"234":[],"235":["internet"],"236":["education"],"237":["computer science","nlp","text-to-text generation","question answering","arabic"],"238":["business","data analytics","tabular","tidyverse","question answering"],"239":["finance","computer science"],"240":["jobs and career"],"241":["artificial intelligence","nlp","text","english"],"242":["board games","image","text","retrieval question answering","retrieval\/ranking"],"243":[],"244":["education","tabular","question answering"],"245":[],"246":["alcohol"],"247":["alcohol"],"248":["alcohol","exploratory data analysis","data analytics","classification","recommender systems"],"249":["healthcare","business","data cleaning","data analytics","health conditions"],"250":["alcohol"],"251":["biology"],"252":["alcohol"],"253":["alcohol"],"254":[],"255":["business"],"256":["business","hotels and accommodations"],"257":["india","housing","intermediate","data visualization","english"],"258":["real estate"],"259":["housing","data visualization","statistical analysis","data analytics","tabular"],"260":["housing","data analytics","tabular","data storytelling"],"261":["housing","exploratory data analysis","data cleaning","data analytics","tabular"],"262":["movies and tv shows","education"],"263":["united states","real estate","neural networks","text","python"],"264":["real estate"],"265":[],"266":["real estate"],"267":["asia","housing","lending","tabular","english"],"268":["literature","exploratory data analysis","data cleaning","data visualization","data analytics","tabular"],"269":["housing","real estate"],"270":[],"271":["india","housing","real estate","regression"],"272":["housing","real estate"],"273":["real estate"],"274":["health","classification","deep learning","cnn","health conditions","tensorflow"],"275":["computer vision","multiclass classification","health conditions","image classification"],"276":["health","health conditions"],"277":["biology","health","medicine","computer science","software"],"278":["health","cnn","covid19","image classification","image augmentation","image generator"],"279":["health","medicine","computer vision","image","yolo","object detection"],"280":[],"281":["deep learning","multiclass classification","keras","tensorflow","image classification"],"282":["health conditions"],"283":["health"],"284":["business","health","medicine","image","canada"],"285":["arts and entertainment"],"286":["healthcare","intermediate","deep learning","image classification","english"],"287":["health"],"288":["arts and entertainment"],"289":["health","deep learning","cnn","image classification"],"290":["education"],"291":["health conditions"],"292":["diseases","health","computer vision","image","image classification"],"293":["diseases","health","medicine","classification","deep learning","image"],"294":["literature"],"295":["languages","literature","computer science","nlp","text mining","deep learning","neural networks"],"296":[],"297":["literature","nlp","pca","nltk","token classification","word2vec skip-gram"],"298":[],"299":["health","health conditions"],"300":["literature","asia","education","beginner","data analytics"],"301":["literature","nlp","deep learning","text","feature extraction","text pre-processing"],"302":["literature","data visualization","deep learning","recommender systems","tabular"],"303":[],"304":[],"305":["geography","business","computer science","exploratory data analysis","data cleaning","data analytics","retail and shopping"],"306":["culture and humanities"],"307":["mental health","healthcare","health","nlp","text generation"],"308":["literature"],"309":[],"310":["earth and nature"],"311":[],"312":[],"313":[],"314":["nlp","text","multitask classification","arabic"],"315":[],"316":[],"317":[],"318":[],"319":[],"320":[],"321":["data visualization","data analytics","deep learning","pandas","urdu"],"322":["africa","social science","artificial intelligence","data analytics","text","arabic"],"323":["intermediate","text","binary classification"],"324":["social networks"],"325":["earth and nature","linguistics","software","nlp","classification","deep learning","news"],"326":["text"],"327":["earth and nature"],"328":[],"329":[],"330":["cancer"],"331":["arts and entertainment","business","eyes and vision"],"332":[],"333":["education"],"334":["artificial intelligence","automobiles and vehicles","intermediate","computer vision","cv2"],"335":["people","business","public safety"],"336":[],"337":["arts and entertainment","earth and nature","people","computer science","neural networks"],"338":["earth and nature"],"339":[],"340":["health","cancer"],"341":["image"],"342":["education"],"343":["public health","data visualization","health conditions","public safety","r","data storytelling"],"344":[],"345":["computer vision","deep learning","eyes and vision","image segmentation"],"346":["biology","cancer"],"347":["healthcare","computer science","programming","health conditions","image segmentation"],"348":["biology","data visualization","data analytics","deep learning","cancer"],"349":["arts and entertainment","people","people and society","image","online communities","image classification","image segmentation"],"350":["healthcare","computer vision","image-to-image","image segmentation","object detection"],"351":["health","computer vision","deep learning","covid19","object detection"],"352":["diseases","exploratory data analysis","data visualization","deep learning","eyes and vision"],"353":[],"354":["people","classification","deep learning","image","online communities","image-to-image"],"355":["health conditions"],"356":["health"],"357":["biology"],"358":["health"],"359":["people","computer vision","image"],"360":["universities and colleges","healthcare","data visualization","deep learning","image","covid19"],"361":["health","artificial intelligence","classification","deep learning","image"],"362":["games","card games","image","yolo","object detection"],"363":["exercise","online communities"],"364":["computer vision","image","multiclass classification","image classification","object detection"],"365":["internet"],"366":["earth and nature"],"367":["earth and nature"],"368":["law","image","yolo","object detection","yolov8"],"369":["programming","beginner","image","object detection"],"370":["earth and nature"],"371":["artificial intelligence","automobiles and vehicles","computer vision","deep learning","object detection"],"372":["law","automobiles and vehicles","computer vision","image","yolo","object detection"],"373":["computer science"],"374":["computer science","computer vision","image","multiclass classification","object detection"],"375":["arts and entertainment","travel"],"376":["artificial intelligence","computer vision","tensorflow","nepali","object detection"],"377":["arts and entertainment","computer science","automobiles and vehicles","computer vision","image","eyes and vision"],"378":["arts and entertainment","artificial intelligence","computer vision","deep learning","image","object detection"],"379":["computer vision","health conditions","yolo","yolov5","object detection","yolov8"],"380":["automobiles and vehicles","deep learning","gpu","object detection","yolov8"],"381":["earth and nature","intermediate","cv2","multilabel classification","image classification","yolov5"],"382":[],"383":["football"],"384":["business","exploratory data analysis","data visualization","retail and shopping","e-commerce services"],"385":["people","computer vision","image","online communities","video classification","cyber security"],"386":["artificial intelligence","image classification"],"387":["finance","advanced","data analytics","classification","automl"],"388":["education","computer science","deep learning","image","adversarial learning","cv2","keras"],"389":["people","computer vision","image","image generator","vietnamese"],"390":["computer science","classification","image"],"391":["people","computer vision","image","image generator"],"392":["india","neural networks","cnn","currencies and foreign exchange","image classification"],"393":["classification","deep learning","multiclass classification","image classification","image feature vector"],"394":["artificial intelligence","computer vision","deep learning","image","image classification"],"395":["arts and entertainment","artificial intelligence","image","image classification","image generator"],"396":["image","video","binary classification","image classification","video classification"],"397":["categorical","advanced","classification","cnn","image"],"398":["computer science","computer vision","deep learning","image","online communities"],"399":["board games","computer vision","classification","image"],"400":["research","people and society","image","social networks","ml ethics"],"401":["politics","middle east"],"402":["football"],"403":["football","intermediate","text","python","other"],"404":["football","beginner","data analytics","text","python"],"405":["football","europe","categorical","middle east"],"406":["football","sports"],"407":["football","sports","data visualization","data analytics","data storytelling"],"408":["football","sports","data analytics","random forest","regression"],"409":["football","statistical analysis"],"410":["football","cricket","india","categorical","text","english"],"411":["football","sports","beginner","data cleaning","data analytics"],"412":["football"],"413":["football","sports","data analytics","tabular"],"414":[],"415":["football"],"416":["games","video games","football","sports","data analytics"],"417":["football"],"418":["football"],"419":["football","cricket","data analytics","tabular"],"420":["video games","football","data cleaning","data analytics","simulations"],"421":["football","sports"],"422":["football"],"423":["football","intermediate","text","python","other"],"424":["football","sports","decision tree"],"425":["football"],"426":["football"],"427":["football","sports","data analytics","random forest","regression"],"428":["football","statistical analysis"],"429":["football"],"430":["psychology","nlp","text","text classification","bert"],"431":["global","categorical","medicine","text"],"432":["health","classification","deep learning","cnn","health conditions","tensorflow"],"433":["classification","binary classification","email and messaging","bert"],"434":["psychology","text"],"435":["computer science","programming","nlp","text","text classification","bert"],"436":["beginner","nlp","classification","text","json"],"437":["earth and nature"],"438":["data visualization","python","image classification","graph","cyber security"],"439":["music"],"440":["psychology","nlp","text","text classification","bert"],"441":["global","categorical","medicine","text"],"442":["arts and entertainment"],"443":["health","classification","deep learning","cnn","health conditions","tensorflow"],"444":["sports"],"445":["classification","binary classification","email and messaging","bert"],"446":[],"447":["computer vision","classification","deep learning","image","binary classification"],"448":["medicine","cancer","image classification"],"449":["medicine","classification","cancer","image classification"],"450":["medicine","cancer","image classification"],"451":["arts and entertainment","medicine","cancer","image classification"],"452":["psychology","text"],"453":[],"454":["computer vision","classification","neural networks","cnn","tensorflow"],"455":["automobiles and vehicles"],"456":["computer science","computer vision","classification","image","image classification"]},"titleNullable":{"0":"IMDB Top 1000 Movies Dataset","1":"IMDB top 1000 dataset","2":"Imdb Movie Dataset from year 1893 to 2020","3":"TMDB TOP 10000 (10K) Movies Dataset","4":"Google Data Analytics Capstone Project: Netflix","5":"Movie Identification Dataset [800 Movies]","6":"IMDb top rated 1000 movies","7":"Top 1000 IMDb movies Dataset ( Web Scraping )","8":"IMDB TOP 1000 MOVIES DATASET","9":"IMDb Top 1000 Movies Dataset","10":"IMDB Anime Data","11":"Top 10000 Voted IMDB Movies Dataset","12":"Dataset","13":"IMDB top 1000 movies","14":"Top 1000 IMDb Movies Dataset","15":"imdb top 1000","16":"IMDB Top 1000 Movies","17":"IMDb Top 10000 Movies Dataset","18":" IMDB Dataset of Top 1000 Movies and TV Shows","19":"IMDB Movies Dataset","20":"clothing_reviews","21":"womens_clothing_reviews","22":"womens_clothing_reviews_new","23":"Reddit Posts on Borderline Personality Disorder","24":"Stress and Anxiety Posts on Reddit","25":"Stress and Anxiety Posts on Reddit","26":"Emotion Classification Dataset","27":"Twitter Financial News Sentiment Dataset","28":"Sentiment Analysis of Tweet Reviews","29":"Twitter Customer Reviews of Popular Smart Phone","30":"Chatgpt DataSet 50000 Tweets from initial month","31":"Indian Political Sentiment on Twitter ","32":"Twitter Emotion Dataset","33":"Emotions Dataset","34":"Clean-NLP with Disaster Tweets Dataset","35":"Tweets Dataset","36":"Emotions","37":"EventClassifier: Twitter Data Set","38":"crypto nft tweets data","39":"Tweets and Engagement Metrics","40":"Short Jokes Dataset","41":"Hate Speech and Offensive Language Detection","42":"indian political tweets","43":"US Airlines Sentiment Analysis in Knime Analytics","44":"Preprocessed Dell Tweets","45":"Web Scrapping Twitter Racism","46":"Twitter Sentiment Analysis - Xitsonga","47":"Urban Mobility Dataset","48":"Yulu Bike Rental Dataset","49":"Natural Language Processing of Chatbot Prompts","50":"Bike rentals","51":"OptiCom Signal Quality Dataset","52":"City Traffic and Vehicle Behavior Dataset","53":"US Automatic Traffic Recorder Stations Data","54":"Economy_Health brazil","55":"US Traffic Congestions (2016-2022)","56":"Traffic Prediction Dataset","57":"Traffic Detection Project","58":"Newcastle's 4 main junctions traffic data","59":"Cities Skyline Springfield","60":"Cars Detection","61":"Bike Sharing Dataset","62":"all traffic signs dataset","63":"Iranian Traffic Sign Detection","64":"Driver Inattention Detection Dataset","65":"Traffic-Signs-Dataset","66":"Healthy and Unhealthy Goat Images","67":"Wildlife","68":"Cat Dataset","69":"FluNet, Global Influenza Programme - WHO","70":"whaleTagData","71":"Local Ocean Conservation Sea Turtle Face Detection","72":"CatFLW","73":"Macaque Monkey Images","74":"Reactive Anomaly Synthetic Data","75":"Sri Lankan Fish Species Dataset","76":"Guinea Pig Detection","77":"sea-animals-image-dataste","78":"COVID-19 & the virus that causes it: SARS-CoV-2.","79":"Hiking Wildlife","80":"Birds, Bats, Grasshoppers Audio","81":"Zebrafish mirror biting","82":"Water Quality -  Every Drop Matters.","83":"IR wildlife","84":"Cat and Dog Detection ","85":"BrackishSGD","86":"Malflow","87":"Different Colors in challenging lightening","88":"Classified NIH Dataset","89":"Cats Dogs Classification dataset","90":"ISIC 2020 JPG 256x256 RESIZED","91":"ISIC 2020 JPG 224x224 RESIZED","92":"ISIC 2019 JPG 256x256 RESIZED","93":"ISIC 2019 JPG 224x224 RESIZED","94":"Dresden Image Database","95":"IMAGE CLASSIFICATION","96":"ImageNet100-INRs-Dataset","97":"Riped and Unriped Tomato Dataset","98":"Song Popularity Classification","99":"Industrial Tools Classification","100":"NIH Chest X-rays Bbox version","101":"Rock Classification Dataset","102":"The Russia-Ukraine War Images on Spanish News","103":"lung x-ray image+clinical text dataset","104":"Signclusive Mediapipe","105":"Iman(357)-Wawan(339)-neww","106":"Road Accident Data Analysis using Microsoft Excel.","107":"KaggleX Skill Assessment Challenge Cohort4 Dataset","108":"Boston - Somerville Traffic Crash Dataset","109":"Vehicle Maintenance Data","110":"Road Accident Data by Vehicle Type","111":"Road Accidents Data","112":"Urban traffic density in cities","113":"Asian-Related Factors in Fatal Crashes In Texas","114":"Road Accidents CSV and EXCEL","115":"Road accidents in the Czech Republic","116":"Road Traffic Collision Data in Northern Ireland","117":"FutureFlow: Navigating Tomorrow's Urban Traffic","118":"Car Accident Dataset","119":"2022 UK Road Safety Data","120":"Road Accident Casualties","121":"US Automatic Traffic Recorder Stations Data","122":"Major Road Accidents","123":"Airbag Recommendation","124":"Hourly Vehicle Crossing data for Time Series","125":"Motor Vehicle Collisions - Crashes-New York ","126":"Spotify dataset ","127":"Anime Dataset 2024","128":"Complete Pokemon Dataset 9th Gen (img + tabular)","129":"Most Popular Anime of all Time","130":"Anime Data from 1970 to 2024","131":"Anime Dataset","132":"Anime Dataset 2024","133":"anime_recommendation_system_weight","134":"Anime Content Based Recommendation System Datasets","135":"MyAnimesList Datasets - 2023","136":"USA Bank Financial Data","137":"Nifty Energy Sector Monthly % Returns Dataset","138":"Leading Indian Bank & CIBIL Real-World Dataset","139":"Loan Approval Dataset","140":"Gesture Recognition System Dataset","141":"Gesture Recognition Data","142":"Hand Gesture Recognition (3 Classes, 64X64)","143":"Enhanced Sign Language MNIST Dataset","144":"Bangla Sign Language Video Dataset","145":"Hand Navigation Landmarks","146":"Hand Gesture Recognition Sample Outputs","147":"Hand Gesture Recognition Dataset for YOLOv8","148":"ISL-CSLTR: Indian Sign Language Dataset","149":"HGR-Dataset","150":"Sensor based (American) Sign Language Recognition","151":"Malaysian Sign Language (MSL) Image Dataset","152":"Masked Gesture Dataset","153":"Myoelectric Control for Hand Motions","154":"ASL Alphabet Wireframes","155":"Serbian Sign Language Dataset","156":"Sign Language Gestures Recognition Data","157":"Sign Language Recognition Dataset","158":"Gesture Classification HaGRID","159":"Hand Gestures Dataset","160":"British Airways Reviews","161":"online review.csv","162":"British Airways Reviews Dataset","163":"Capterra Ticketsystem - Jira, Zendesk - Reviews","164":"Airline Customer Reviews Dataset","165":"commerce Furniture","166":"Online Store Selling","167":"Exchange Rates:USD to PKR,INR, and CNY (2004-2024)","168":"Longest Running Indian Television Series","169":"Top5000 Albums on Spotify Data Analysis","170":"USA Bank Financial Data","171":"fake-news-and-true-news-dataset","172":"US Collegiate Sports Dataset","173":"sentinel","174":"LinkedIn Posts about Layoffs (2023)","175":"Fake News Prediction datasets","176":"celeb-a-2ammu20-a3","177":"Facebook's Supreme Court","178":"israel-hamas-full-articles-nyt","179":"News headlines dataset with company names","180":"NIH Chest X-rays Bbox version","181":" Youtube Channel tmrospacenews","182":" Youtube Channel truecrimenews","183":"celeb-df11parts","184":"The Russia-Ukraine War Images on Spanish News","185":"Unified Dataset for Fake, Spam and Legit data","186":"NewsQA","187":"Classified Arabic News","188":"ag_news_dataset","189":"CNN-Transformer","190":"armenian_news","191":"Derana News","192":"Fake_News_Detection","193":"SocialNet-Weibo-Version-2","194":"SocialNet-Weibo-Version-1","195":"fake_news _Detection","196":"Fake News Detection Data","197":"Fake news detection dataset","198":"Scam Detection | Fake News Labelled  Dataset","199":"fake-news-detection11","200":"fake-and-real-news-dataset","201":"News Detection (Fake or Real) Dataset","202":"Fake News .csv","203":"fake_news_detection_with_nlp_and_lstm","204":"Fake news detection","205":"Fake News Detection","206":"Fake News Detection","207":"Fake News detection","208":"Welfake dataset for fake news","209":"Fake news master","210":"Fake News detection","211":"Retail_Stores_Sales_Dataset","212":"Sales dataset","213":"Dataset Dashboard","214":"Pizza Sales - Excel Mastersheet","215":"Pharma Data Analysis","216":"Indonesian Spices Dataset","217":"Padang Cuisine (Indonesian Food Image Dataset)","218":"Indonesian Food Recipes","219":"Indonesian Spices Dataset","220":"Indian Food Dataset ","221":"Arabic Food 101","222":"Padang Cuisine (Indonesian Food Image Dataset)","223":"Peruvian Food Reviews","224":"Bahama Breeze Menu Nutrition Data","225":"Combined Wine Dataset: Red & White Wines","226":"The LAMBADA Dataset for Word Prediction","227":"SNIPS Natural Language Understanding Benchmark:","228":"college_level_IntroCS_class_dataset|chinese","229":"Sigma Dolphin Filtered and Cleaned","230":"Financial Q&A - 10k","231":"NLP_Neymar_ChatBot_Dataset","232":"CoQA QndA Dataset with ground truth","233":"Astronautics Multiple Choice Questions and Answers","234":"EVJVQA","235":"SE-PQA: Personalized Community Question Answering","236":"GSM8K - Grade School Math 8K dataset for LLM","237":"Arabic Instruct chatbot dataset","238":"A Case Study: Bike Share","239":"A New Benchmark for Financial Question Answering","240":"Visual_Question_Answering-CV-NLP","241":"GenAssocBias","242":"Boardgames_Rulebooks","243":"question-answering-dataset","244":"blooms-taxonomy-dataset","245":"VizWiz_QA","246":"Red Wine Quality","247":"wine quality","248":"Combined Wine Dataset: Red & White Wines","249":"Modeling wine","250":"Red Wine Quality","251":"Red wine quality classifier using a Neural network","252":"Wine Quality Dataset","253":"Red-wine quality","254":"Dataset","255":"Dataset: LQR House Inc. (LQR) Stock Performance","256":"Dataset: Full House Resorts, Inc. (FLL) Stock P...","257":"House Prices India","258":"Bengaluru House Dataset","259":"Transformed Housing Data 2","260":"Transformed Housing Data","261":"Raw Housing Cost Data (imputed)","262":"Dataset: Tree House Education & Accessories Lim...","263":"House_Prices_Dataset","264":"Housing Sales: Factors Influencing Sale Prices","265":"House Price Predication","266":"Housing Price Prediction","267":"Singapore Home Resale Prices (HDB)","268":"Amazon Books Dataset: Genre, Sub-genre, and Books","269":"Portuguese Apartment Listings Dataset","270":"House Prices","271":"Indian Rental House Price","272":"California Real state Dataset","273":"Gurgoan properties (Cleaned) dataset","274":"Classified NIH Dataset","275":"NIH Chest X-rays Bbox version","276":"lung x-ray image+clinical text dataset","277":"Improved_X-Ray_data","278":"Covid-19 X-Ray Classification Dataset","279":"ChestXray 8 Object Detection Yolo and Pascal VOC","280":"VinBigData Chest X-ray - YOLO","281":"Normal Tuberculosis Covid-19 Chest Xrays images","282":"Pulmonary-Chest-X-Ray-Abnormalities","283":"Chest X-Ray Dataset","284":"DS4 Work - Operation Datasets","285":"Chest X-Ray Images","286":"Pneumonia  Tuberculosis  Normal","287":"Chest Radiograph Images (Pneumonia & Normal)","288":"Chest X-ray Images","289":"Covid-19 Chest X-ray","290":"covid-19","291":"chest x-ray images pneumonia dataset","292":"pneumonia CXR dataset","293":"Lung Disease","294":"A Song of Ice and Fire - Dialogues","295":"The LAMBADA Dataset for Word Prediction","296":"BookSales","297":"Percy Jackson Book Series NLP Dataset","298":"Books Dataset","299":"lung x-ray image+clinical text dataset","300":"1000 books data","301":"Book Recommendation System","302":"Open Library Books API - 12k","303":"arabicBooks100","304":"ArabicBooks500","305":"Retail Analysis on Large Dataset","306":"Ancient Inscription Database (BDC_Db)","307":"Ayurveda Texts (English)","308":"Project Geutenberg Books","309":"booksi","310":"Mawqif Dataset","311":"Sarcasm detection","312":"sarcasm-detection-dataset","313":"Sarcasm detection","314":"Arabic YouTube Comments by Khalaya","315":"arabic_Sarcasm_detection","316":"SemEval22 Sarcasm Detection","317":"Sarcasm Detection Publication","318":"Sarcasm Detection","319":"ArSarcasm-V2","320":"Final-Saved-Model","321":"Urdu Sarcastic Tweets Dataset","322":"2.5+ Million Rows Egyptian Datasets Collection","323":"MUStARD (Multimodal Sarcasm Detection Dataset)","324":"Twitter sracastic classification dataset ","325":"Politifact Fact Check Dataset","326":"Bangla SARC","327":"Labeled Dataset for Sarcasm Detection","328":"Sarcasm Detection Data Set","329":"Italian Sarcasm_Detection","330":"ISIC melanoma detection","331":"Multi-Races Human Body Semantic Segmentation Data","332":"Object Detection Using Adaptive Mask RCNN","333":"CVC-ClinicDB Datasets","334":"Parking_Lot_Detection_Counter","335":"Silicone Mask Biometric Attack Dataset","336":"Mask Detection (VOC\/YOLO)","337":"3D Mannequin Dataset for Liveness Detection","338":"Construction Site Safety Image Dataset Roboflow v2","339":"object-detection-case-study-2","340":"Lung nodule infused images","341":"Liveness detection dataset: Cutout 2D attacks","342":"CVC-ClinicDB Datasets","343":"COVID-19 & the virus that causes it: SARS-CoV-2.","344":"Face Mask","345":"Retina Blood Vessel","346":"githut-RSNA-MICCAI-Brain-Tumor-Classification-AI","347":"Processed CT ICH Dataset Images","348":" CBIS-DDSM: Breast Cancer Dataset of JPG Images","349":"Portrait and 26 Photos Re-identification, 19 GB","350":"Surgical Scene Segmentation in Robotic Gastrectomy","351":"MFVT Dataset for Real-time Masked Facial Detection","352":"Intraretinal Cystoid Fluid","353":"face-mask-detection-dataset","354":"Face Mask Detection Dataset - 500 GB of data","355":"Kvasir-Instrument Dataset","356":"Medico automatic polyp segmentation dataset","357":"EndoTect Dataset","358":"Kvasir-SEG Data (Polyp segmentation & detection)","359":"Medical Mask detection","360":"COVID-QU-Ex Dataset","361":"COVID-19 Medical Face Mask Detection Dataset ","362":"playing-cards-dataset | YOLO Object detection","363":"Bangladeshi Native Vehicle Dataset","364":"Car Brand Recognition","365":"IDD Detection Modified","366":"NDDA Dataset","367":"FLIR 2024 Dataset Yolov8 Version","368":"EGYPlate","369":"Roundabout Aerial Images YOLO data","370":"Lab1_Part3_Dataset","371":"Car Detect Dataset","372":"Diverse-LPD - Training Ready","373":"Object Detection using YOLOV3","374":"26 Class Object detection dataset","375":"RSUD20K: Bangladesh Road Scenes Dataset","376":"Nepali Bike and Car images with annotations","377":"Image Obstacle in Public Spaces","378":"Obstacles in Public Spaces for Dist-YOLO","379":"Disabled People Sign Detection","380":"Object Detection Carla self-driving Car","381":"Processed_data_for_yolov5_20ClassObjectDetection","382":"CelebsV2_Faces_224","383":"FF++_Face_224","384":"Contacts, Stores, Products","385":"Asian People - Liveness Detection Video Dataset","386":"Deepfake-dataset (140k + dataset real or fake)","387":"Banknote-authentication-dataset-2024","388":"Cebradata for GAN training","389":"GENERATED Vietnamese Passports Dataset","390":"Fake or Real Competition Dataset","391":"GENERATED USA Passports Augmentation","392":"Indian Coin Images Dataset (incl. fake coins)","393":"Handwritten Signature Identification","394":"CIFAKE: Real and AI-Generated Synthetic Images","395":"AI recognition dataset","396":"Liveness Detection - Zalo AI Challenge 2022","397":"Gemstones","398":"Real vs fake faces","399":"Synthetic Chess Board Images","400":"deepfake and real images","401":"Israeli Parliament (Knesset) Members ","402":"Indian players under 25 in the ISL 23-24 season","403":"Euro 2024 Group Stage FBRef Scrape Data","404":"Teams and players stats from FBRef","405":"Super Lig Player Informations","406":"Fbref Football Leagues Data 2023 2024","407":"UEFA EURO 2024 - Players","408":"2022\/23 Big 5 Football Leagues Player Stats","409":"Turkish Super League","410":"IPL Auction Dataset","411":"Football_players_transfer_values_2024","412":"Premier League Player Statistics 2022-2024","413":"Football - Soccer - UEFA EURO, 1960 - 2024","414":"Football_Player_Dataset_for_YOLOV8_Hosted","415":"Football Player Dataset for YOLOV8","416":"Football Manager England Scout Dataset","417":"2024 NWSL Womens Soccer League Player Stats FULL","418":"PSL Season 9 Dataset","419":"Pakistan Super League","420":"Football Manager-23 Championship 30\/31 dataset","421":"2023 NWSL Womens Soccer League Player Stats FULL","422":"England Premier League (2008-2024)","423":"Euro 2024 Group Stage FBRef Scrape Data","424":"Football Player Performance Data 2017-2024","425":"Football Match Data Date Wise From 1960 to 2024","426":"The English Women's Football Database","427":"2022\/23 Big 5 Football Leagues Player Stats","428":"Turkish Super League","429":"FIFA Worldcup 2022 Final Dataset: ARG vs FRA","430":"People Slurs Dataset","431":"Medicine Dataset","432":"Classified NIH Dataset","433":"phishing-email-classifier-bert","434":"Stress and Anxiety Posts on Reddit","435":"IT Skills from Jobs","436":"Labeled IT Job skills","437":"Dark Pattern Dataset","438":"Malflow","439":"Million Song Data Analysis 2","440":"People Slurs Dataset","441":"Medicine Dataset","442":"Different Colors in challenging lightening","443":"Classified NIH Dataset","444":"US Collegiate Sports Dataset","445":"phishing-email-classifier-bert","446":"Classification","447":"Cats Dogs Classification dataset","448":"ISIC 2020 JPG 256x256 RESIZED","449":"ISIC 2020 JPG 224x224 RESIZED","450":"ISIC 2019 JPG 256x256 RESIZED","451":"ISIC 2019 JPG 224x224 RESIZED","452":"Stress and Anxiety Posts on Reddit","453":"Skin Disease Classification","454":"4 Players Chess Pieces","455":"Car Classification","456":"Dresden Image Database"},"subtitleNullable":{"0":"\"Release Year, Duration, Ratings, Metascores, Vote Counts, and Plot Summaries\"","1":"","2":"","3":"This dataset contains the TOP 10K movies fetched from TMDB API as of March 2024.","4":"Insights from historical data","5":"Frames from 800 top-rated movies for building movie identification models","6":"","7":"","8":"","9":"Web Scraped data from IMDb Website","10":"Most Popular Animes of all Time on IMDB","11":"","12":"","13":"This dataset contains all the IMDb top 1000 movies ","14":"Discover the Greatest Movies of All Time - IMDb's Top 1000 Movie Rankings","15":"Dataset consist of top 1000 imdb movies","16":"Exploring what makes movies stand out from the rest","17":"Feature Film, Rating Count at least 10,000 (Sorted by IMDb Rating Descending)","18":"","19":"Top 1000 Movies by IMDB rating","20":"","21":"","22":"","23":"CLEANED almost 6000 posts about borderline personality","24":"4,000 Reddit posts from people experiencing stress\/anxiety.","25":"4,000 Reddit posts from people experiencing stress\/anxiety.","26":"Emotion-labeled Dataset: Unveiling Sentiments in Textual Content","27":"A Twitter Financial Dataset for Clarification ","28":"Building a predictive model based on positive and negative tweets of twitter","29":"Building a predictive sentiment analysis model using machine learning","30":"Obtained by Twitter API v2, Best for Topic Modelling and Sentiment Analysis","31":"Unveiling Public Sentiment on Politics: A Comprehensive Analysis of India","32":"Unveiling the Emotional Tapestry of Social Media","33":"Emotions are conscious mental reactions","34":"NLP with Disaster Tweets: Data Preprocessing","35":"Sentiment analysis using 1M+ tweets dataset","36":"Where Words Paint the Colors of Feelings","37":"Analyzing Public Opinions: Twitter Data with Labels for Different Categories","38":"crypto and nft related tweets and sentiments for NLP, market trends, etc","39":"Twitter Data containing temporal, geographical, and general tweet metrics.","40":"Humorous Short Jokes","41":"Hate Speech and Offensive Language Detection on Twitter","42":"","43":"Made by a group of students from FORE School of Management","44":"","45":"Dataset generated and used in the final project of UAA-ICI-S7-M1","46":" Xitsonga Twitter Sentiment Analysis Dataset","47":"Synthetic data on urban mobility for enhancing urban planning","48":"Rental Trends and User Behavior","49":"","50":"This is Yulu dataset. Yulu is India\u2019s leading micro-mobility service provider.","51":"OptiCom Sig Quality: Signals' perf metrics for optical comms.","52":"\"Urban Mobility Insights: Exploring City Traffic and Vehicle Behavior\"","53":"Vehicle Traffic Counts and Locations at US ATR Stations","54":"Contains data from the World Bank's data portal. There is also a consolidated.","55":"Comprehensive Dataset of 33 Million U.S. Traffic Congestion Events","56":"Real Traffic Prediction Dataset ","57":"This dataset contains various images of traffic. Images mostly taken from Turkey","58":"","59":"","60":"Cars Detection Dataset","61":"Regression and anamoly detection using Bike Sharing Dataset","62":"","63":"Annotated Dashcam images to detect traffic signs ","64":"","65":"Unlock Road Safety: Explore 52 Types of Traffic Signs in High-Resolution Imagery","66":"For Educational, Research or Commercial Purpose","67":"","68":"A dataset of 29843 cat pictures (64x64), compiled together for training models.","69":"The Importance of Monitoring the Global Uptrend of Influenza.","70":"Automated behavioral classification using multisensor data collected from whales","71":"Create a model that detects the bounding box around a sea turtle\u2019s face","72":"Cat Facial Landmarks in the Wild","73":"Macaque Monkey detection (Object Detection)","74":"Reactive Synthetic Data Augmentation of the UCSD Anomaly Detection Dataset","75":"Top 16 fish species consumed in Sri Lanka","76":"Hand-labeled images of guinea pigs for object detection","77":"","78":"Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?","79":"Dataset containing Images of Wildlife Animals","80":"\"Avian & Insect Harmonies: Recordings from Xeno-Canto\"","81":"Zebrafish aggressive behaviour in front of mirror","82":"Water water everywhere, but not a drop to drink!","83":"Infrared Small Object Detection for Wildlife Conservation","84":"","85":"Expansion of the brackish dataset with synthetically generated data","86":"Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg","87":"Different color classification in different lightening Environment","88":"NIH x-ray dataset in classified format.","89":"contains total of 400 images of cats and dogs for making a classification model.","90":"","91":"","92":"","93":"ISIC 2019 resized dataset","94":"","95":"","96":"Implicit neural representation of ImageNet100 as part of work Implicit Zoo","97":"Annotated Images for Tomato Ripeness Classification","98":"ML Classification  Models","99":"See It, Sort It: An Image Dataset for Industrial Tool Quality Control ","100":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","101":"Classify Different Types of Rocks (Stones)","102":"Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War","103":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","104":"Blacked Out Background Using Mediapipe Dataset","105":"","106":"Analyzing Road Safety: An Interactive Excel Dashboard for Accident Data Insights","107":"Used Car Price Prediction Dataset","108":"Exploring Traffic Incidents, Causes, and Trends in Somerville","109":"","110":"Road Accident Data by Vehicle Type and Provices in Sri Lanka - 2012","111":"\"Roadway Woes: A Comprehensive Dataset on Road Accidents\"","112":"contains traffic densities of cities","113":"Unveiling Asian Influence: Understanding Contributing Factors in Fatal Crashes","114":"CSV NUMERICAL DATA SET ","115":"Detailed dataset of road accidents in the Czech Republic (2016-2022)","116":"Statistics on injury road traffic collisions (RTCs) from 2017 to 2022","117":"Decoding Urban Motion: Analyzing Traffic Density in the Cities of Tomorrow","118":"Road Accident Records in Kensington and Chelsea (January 2021)","119":"Casualty, Collision and Vehicle information for vehicular accidents in the UK","120":"Understanding Accident Severity for Effective Road Management","121":"Vehicle Traffic Counts and Locations at US ATR Stations","122":"","123":"Recommend Airbags for vehicles","124":"Hourly Vehicle data on an Highway, good dataset for beginers","125":"Dataset of Motor Collisions or Crashes in New York","126":"A Comprehensive Collection of Spotify Tracks Across Various Genres","127":"Latest dataset for Animes","128":"Pokemon image dataset + Base stats will 9th Generation","129":"IMDb's Most Popular Anime of All Time","130":"Explore Anime Evolution: Comprehensive Dataset and Scraping Script (1970-2024)","131":"\"Anime Series Statistics and Metadata\"","132":"Latest dataset for Animes","133":"","134":"Dive into Dynamic Datasets for Tailored Recommendations!","135":"Listed Animes, Users and Ratings on MyAnimeList (MAL)","136":"Data set for Tableau practice","137":"Monthly Percentage Returns of Nifty Energy Index Components, Ready for Analysis","138":"Banking and CIBIL Data for Predictive Credit Risk Analysis","139":"Exploring Loan Applicant Characteristics and Risk Assessment","140":"Computer Vision Task","141":"","142":"","143":"Enhanced Sign Language MNIST Dataset for Hand Gesture Recognition","144":"","145":"Deciphering Hand Gestures: A Comprehensive Image Dataset for Gesture Recognition","146":"","147":"Diverse Hand Gestures: A collection of 20,000 images featuring a variety of hand","148":"Dataset for Continuous Sign Language Translation and Recognition","149":"","150":"","151":"","152":"A dataset comprising 14,000 images of hand gestures.","153":"Myoelectric Control for Hand Motions with Myo Armband","154":"A dataset of wireframe images pulled from ASL alphabet images. ","155":"From Hands to Words: A Dataset for Sign Language Translation and Recognition","156":"","157":"","158":"Gesture images including thumbs_up(like) and thumbs_down(dislike).","159":"Empower Your ML Models with Hand Gesture Dataset !!","160":"","161":"Online shopping sentiment analysis flikcart ","162":"Exploring British Airways Customer Reviews","163":"Cappterra Ticketsystem Reviews - Jira, Zoho, ServiceNow, Zendesk - Text, Ratings","164":"Customers Reviews in Airline Industry","165":"This dataset comprises 2,000 entries scraped from AliExpress.","166":"","167":"","168":"A Journey Through Timeless Classics","169":"Data Analysis of Spotify's top 5000 albums of all time.","170":"Data set for Tableau practice","171":"","172":"US Collegiate Sports Dataset from 2015 to 2019 ","173":"","174":"Huge corpus of textual data about LinkedIn posts related to layoffs ","175":"","176":"","177":"Meta's oversight-board & outcomes impacting decisions around content-moderation","178":"","179":"News headlines, company names, top headlines date wise ","180":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","181":" Youtube Channel tmrospacenews","182":" Youtube Channel truecrimenews","183":"","184":"Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War","185":"A Diverse Text Dataset for Classifying Fake, Spam, and Legit Information","186":"","187":"","188":"","189":"","190":"","191":"Sinhala Ada Derana News Articles Dataset (2008-2024) over 200,000 articles","192":"","193":"A publicly available dataset based on the microblogging (Weibo)  social platform","194":"A public dataset of fake news identification for the microblogging social platfo","195":"","196":"Tabular summary about each news article","197":"","198":"","199":"","200":"nlp fake news detection dataset","201":"The Battle Against Misinformation: A Text Classification Dataset","202":"fake news as tweets in tweeter","203":"","204":"","205":"The Battle Against Misinformation: A Text Classification Dataset","206":"","207":"","208":"","209":"","210":"","211":"","212":"","213":"","214":"","215":"","216":"A dataset consisting of classifications of Indonesian spice images","217":"Masakan Padang (Dataset Gambar Makanan Indonesia)","218":"14000 recipes of chicken, lamb, beef, egg, tofu, tempe, and fish","219":"A dataset consisting of classifications of Indonesian spice images","220":"Spice Tales: Exploring the Flavors of Indian Cuisine - An Indian Food Dataset","221":"Data about 16 traditional and famous dishes in Jordan","222":"Masakan Padang (Dataset Gambar Makanan Indonesia)","223":"Analyze the sentiment of one of the best cuisine","224":"Bahama Breeze Restaurant Menu Nutritional Dataset","225":"Comprehensive data on the chemical and quality aspects of wines","226":"Evaluating text understanding through word prediction","227":"","228":"Json format with (instruction input output)","229":"Cleaned and Filtered Version Of Sigma Dolphin","230":"A Comprehensive Financial Question-Answer Dataset from Company Filings","231":"","232":"Processed version of https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversation","233":"AstroMCQA is designed for comparative assessment of LLMs in Astronautics domain","234":"","235":"","236":"GSM8K use for test and evaluate LLM  Math solve performance","237":"Fine tuning LLMs on Arabic language for better performance","238":"Divvy tripdata on bike 2023 ","239":"Financial Question Answering","240":"","241":"Bias Detection Dataset","242":"Pdfs for different rulebooks of boardgames","243":"","244":"","245":"VizWiz Question Answering dataset which answer type is \"other\".","246":"","247":"","248":"Comprehensive data on the chemical and quality aspects of wines","249":"We propose a data mining approach to predict human wine.","250":"Evaluating factors influencing the quality and characteristics of red wine","251":"","252":"This dataset is public available for research.","253":"","254":"","255":"Stock Performance Dataset","256":"Stock Performance Dataset","257":"Detailed Analysis of House Prices in Urban India: Trends and Insights.","258":"Bengaluru House Prediction","259":"Data regarding Sale Prices of several houses and their respective features","260":"Data regarding Sale Prices of several houses and their respective features","261":"Data regarding Sale Prices of several houses and their respective features","262":"Dr. Jagadish Tawade & Nitiraj Kulkarni","263":"House Prices Dataset","264":"Exploring Property Characteristics, Location, and Amenities in Real Estate Marke","265":"Predicting the house price","266":"","267":"Transacted resale flat prices w\/ distance to MRT & CBD. Official SG Gov data.","268":"Exploring Amazon's Book Diversity","269":"Comprehensive Dataset of Apartment Listings in Portugal from Imovirtual","270":"","271":"Regression analysis, mutiple regression,linear regression, prediction","272":"","273":"","274":"NIH x-ray dataset in classified format.","275":"880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset","276":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","277":"NIH Chest X-rays validated images","278":"Covid-19 Patient Chest X-Ray Dataset (Positive and Negative)","279":"YOLO and Pascal VOC Scheme Dataset for Chest X-ray 8 Object Detection","280":"","281":"","282":"","283":"","284":"Data Science for Business - Operation Department","285":"","286":"X Ray Images Dataset -- 3 Classes -- ","287":"11,236 Images, 2 Categories","288":"","289":"","290":"a chest X-ray images of person diagnosed negative and positive with COVID-19","291":"","292":"6,126 images and corresponding lung masks, 4 categories","293":"Lung Disease classification","294":"Dialogue dataset from George R. R. Martin's \"A Song of Ice and Fire\" books","295":"Evaluating text understanding through word prediction","296":"","297":"A Comprehensive Text Corpus of the First Five Books for Natural Language Process","298":"","299":"Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co","300":"This dataset contains prices along with star rantings of one thousand books.","301":"Dataset of different books","302":"Open Library Books API with more than 12k book registers!","303":"","304":"","305":"In this dataset i founded so many insights also in last i developed Recom.. Sys.","306":"Ancient Inscription Image Database from Maharashtra","307":"Texts on Ayurveda in English containing 20+ Books and 2000+ Articles","308":"An arbitrary collection of public domain works from Project geutenberg.","309":"","310":"Stance Detection in Arabic Language Shared Task","311":"","312":"","313":"","314":"Arabic YouTube Comments: A Multi-tagged Dataset by Khalaya","315":"","316":"","317":"","318":"","319":"","320":"","321":"Towards Computational Sarcastic Tweets Identification : An Open-Source Dataset ","322":"","323":"Sarcasm Detection Dataset","324":"Tweets data for sarcastic detection & categorical classificaiton of sarcasm","325":"High-quality dataset with 21k fact check statements between 2008 to 2022","326":"BanglaSarc: A Dataset for Sarcasm Detection ","327":"","328":"","329":"","330":"","331":"","332":"","333":"Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/","334":"Computer Vision Project. ","335":"Anti spoofing dataset with Silicone 3D mask attacks (7000 videos)","336":"","337":"Explore 3D mannequins for anti-spoofing models (1000+ images)","338":"Modification on the original dataset, where some classes were removed\/renamed.","339":"","340":"Infusing Tumor Masks for Enhanced Classification","341":"Face Spoofing dataset Cutout Mask: 1,5K+ participants ","342":"Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/","343":"Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ?","344":"","345":"Retina Blood Vessel for segmentation","346":"Competition RSNA-MICCAI Brain Tumor Radiogenomic Classification","347":"The final images and masks obtained after processing Physionet Dataset","348":"Curated Breast Imaging Subset of DDSM dataset with JPG images","349":"27 photos of the same person - face recognition dataset","350":"Surgical scene segmentation in robotic gastrectomy with real and synthetic data","351":"A new masked facial dataset with more accurate mask-wearing conventions","352":"The diversified ocular disorder","353":"","354":"301 132 images, 4 types of masks worn, 75 283 unique faces, face dataset ","355":"Endoscopy instrument dataset (https:\/\/datasets.simula.no\/kvasir-instrument\/)","356":"polyp seg. data(https:\/\/multimediaeval.github.io\/editions\/2020\/tasks\/medico\/)","357":" polyp segmentation  and GI classification dataset (https:\/\/endotect.com\/)","358":"Colorectal polyp data(1000 im, GT & BB) (https:\/\/datasets.simula.no\/kvasir-seg\/)","359":"6000 images of people wearing masks, scarves, face shields and other accessories","360":"","361":"COVID-19 CoronaVirus Face Mask Collection ","362":"Comprehensive Playing Cards Dataset generated for YOLO Object Detection","363":" Bangladeshi Native Vehicle Dataset: Cataloging Traditional Transportation","364":"image datasets training images: 11060 and  test images: 2274 classifies 27 cars","365":"A modified version of IDD-Detection dataset for YOLO object detection pipelines.","366":"[CVPR 2024] Intriguing Properties of Diffusion Models","367":"","368":"EGYPlate: Egyptian Car License Plate Detection Dataset for Object Detection","369":"","370":"","371":"Cars Object Detection Dataset","372":"License Plate Detection - Ready to Train (YOLO)","373":"","374":"Comprehensive 26-Class Object Detection Dataset for Urban Scenes","375":"RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving","376":"","377":"Annotated Images of Obstacles in Public Spaces","378":"Annotated Images of Obstacles for Object Detection in Public Spaces","379":"","380":"","381":"VOC_2012 Ready to use Dataset for yolo 20 class object detection","382":"DeepFake dataset celebs v2 images","383":"DeepFake dataset FF++","384":"Comprehensive Simulated Business Environment for CRM and Retail Analysis","385":"Face anti spoofing with photos and videos of asian people","386":"","387":"Build a Model that detect fake Currency Notes","388":"This datasets(205k images) will be used for GAN models training","389":"Images of Vietnamese Passports. ALL DATA IS GENERATED","390":"2023 fake or real image discrimination competition dataset","391":"Dataset of USA Passports with Augmentation. ALL DATA IS GENERATED","392":"Contains images of Indian coins (real and fake) of denominations Rs 1, 2 and 5.","393":"Simplified version of handwritten signatures dataset in only 1MB by FCIS-ASU","394":"Can Computer Vision detect when images have been generated by AI?","395":"a dataset of human generated art  and AI generated art.","396":"Replay-attack Video Anti-Spoofing Dataset","397":"","398":"Discriminate Real and Fake Face Images","399":"Photorealistic synthetic images representing pictures of chess game states","400":"Detect if any images is real image of deepfake image","401":"All members of the Israeli Parliament with affiliation","402":"Find players that can play for India to qualify for the world cup","403":"In-Depth Data from Euro 2024 Group Stage on FBRef","404":"Data from FBRef about top leagues and relative teams and players","405":"Turkiye Super Lig 2023-2024 Player Informations (Number, Team, Skills, Value...)","406":"","407":"Basic info of players from UEFA EURO 2024, collected on Transfermarkt","408":"Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/","409":"Turkish Super League 2023\/2024 Mid-Season Player Statistics","410":"Data about Indian Premier League Auction of all teams","411":"Football players transfer value based on their ratings, skills and attributes","412":"Game by game statistics of all current PL players for the past 2 seasons","413":"Football matches, players & events for EURO 1960-2024 & Nations League 2019-2023","414":"","415":"","416":"A scout database for Northampton FC in Football Manager 23(Season 2030-31)","417":"2024 NWSL Season (so far) player stats for 343 players. Updated monthly.","418":"","419":"Season 8 & 9 ball-by-ball","420":"A dataset of the players in a simulated Championship season(2030-31)","421":"Comprehensive player positions and stats for NWSL athletes - 2023","422":"This dataset contains football match data from the English Premier League","423":"In-Depth Data from Euro 2024 Group Stage on FBRef","424":"Football Player Performance Data for Europe's top leagues ","425":"","426":"Open database of matches played in the top tiers of women's football in England.","427":"Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/","428":"Turkish Super League 2023\/2024 Mid-Season Player Statistics","429":"FIFA Worldcup 2022 FInal Dataset: ARG vs FRA: Event to Event data","430":"The dataset appears to be a collection of text","431":"Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries","432":"NIH x-ray dataset in classified format.","433":"BERT Fine-Tuned For Classification on Phishing Email Dataset","434":"4,000 Reddit posts from people experiencing stress\/anxiety.","435":"Extract IT skills from Job Descriptions","436":"labeled IT skills in job descriptions.","437":"Custom Dark Pattern Dataset - IIT(BHU)","438":"Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg","439":"Data Analysis Designed to Predict High Stream Count on Spotify","440":"The dataset appears to be a collection of text","441":"Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries","442":"Different color classification in different lightening Environment","443":"NIH x-ray dataset in classified format.","444":"US Collegiate Sports Dataset from 2015 to 2019 ","445":"BERT Fine-Tuned For Classification on Phishing Email Dataset","446":"","447":"contains total of 400 images of cats and dogs for making a classification model.","448":"","449":"","450":"","451":"ISIC 2019 resized dataset","452":"4,000 Reddit posts from people experiencing stress\/anxiety.","453":"","454":"Pieces Dataset for training","455":"","456":""},"descriptionNullable":{"0":"This dataset provides comprehensive information about top-rated movies from IMDB. It includes the title of each movie, the year it was released, and its duration in hours and minutes. The dataset also provides the IMDB rating and the number of votes the movie has received on the platform. Additionally, it features the Metascore from Metacritic, indicating the critical reception of each movie. The content rating, which indicates the appropriate audience for the movie, is also included. Finally, a brief plot summary is provided for each movie, offering an overview of the storyline. This dataset is ideal for movie enthusiasts, researchers, and data analysts interested in exploring and analyzing the characteristics of top-rated movies.\n\n\n\n\n\n\n","1":"","2":"The IMDb movie dataset and IMDb rating dataset provide comprehensive information about movies listed in the IMDb database along with their ratings. Here's a brief description of each:\n\n1. IMDb Movie Dataset:\n   - This dataset contains detailed information about movies, including their titles, original titles, release years, publication dates, genres, durations, countries of production, languages, directors, writers, production companies, actors, descriptions, average votes, total votes, budgets, gross incomes (both in the USA and worldwide), metascores, and reviews from users and critics.\n   - Each row represents a unique movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset offers insights into various aspects of movies, such as their production details, cast and crew, plot summaries, and reception through ratings and reviews.\n\n2. IMDb Rating Dataset:\n   - This dataset provides ratings and voting statistics for movies listed in the IMDb database.\n   - It includes information such as IMDb title IDs, weighted average votes, total votes, mean and median votes, distribution of votes across different rating levels (from 1 to 10), and average votes and votes counts segmented by gender and age groups.\n   - Additionally, it offers ratings from specific voter groups, such as top 1000 voters, US voters, and non-US voters.\n   - Each row corresponds to a movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset enables analyses of movie ratings across different demographics and provides insights into the audience's perception and reception of movies listed on IMDb.\n\nIMDB Movie Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. title: Title of the movie.\n3. original_title: Original title of the movie (may differ from the localized title).\n4. year: Year of release of the movie.\n5. date_published: Date when the movie was published or released.\n6. genre: Genre(s) to which the movie belongs.\n7. duration: Duration of the movie in minutes.\n8. country: Country or countries where the movie was produced or filmed.\n9. language: Language(s) spoken in the movie.\n10. director: Director(s) of the movie.\n11. writer: Writer(s) of the screenplay or story for the movie.\n12. production_company: Production company or companies involved in producing the movie.\n13. actors: Main actors or cast members of the movie.\n14. description: Brief description or summary of the movie's plot or storyline.\n15. avg_vote: Average rating or vote score given to the movie by IMDb users.\n16. votes: Total number of votes received by the movie on IMDb.\n17. budget: Budget allocated for producing the movie.\n18. usa_gross_income: Gross income or revenue generated from the movie's release in the United States.\n19. worlwide_gross_income: Gross income or revenue generated from the movie's release worldwide.\n20. metascore: Metascore rating assigned to the movie by critics (if available).\n21. reviews_from_users: Number of user reviews or ratings submitted for the movie.\n22. reviews_from_critics: Number of reviews or ratings given by critics for the movie.\n\nIMDB Rating Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. weighted_average_vote: Weighted average rating or vote score given to the movie.\n3. total_votes: Total number of votes received by the movie.\n4. mean_vote: Mean or average vote score given to the movie.\n5. median_vote: Median vote score given to the movie.\n6. votes_10: Number of votes rating the movie as 10.\n7. votes_9: Number of votes rating the movie as 9.\n8. votes_8: Number of votes rating the movie as 8.\n9. votes_7: Number of votes rating the movie as 7.\n10. votes_6: Number of votes rating the movie as 6.\n11. votes_5: Number of votes rating the movie as 5.\n12. votes_4: Number of votes rating the movie as 4.\n13. votes_3: Number of votes rating the movie as 3.\n14. votes_2: Number of votes rating the movie as 2.\n15. votes_1: Number of votes rating the movie as 1.\n16. allgenders_0age_avg_vote: Average vote score given by viewers of all genders in the 0-18 age group.\n17. allgenders_0age_votes: Number of votes from viewers of all genders in the 0-18 age group.\n18. allgenders_18age_avg_vote: Average vote score given by viewers of all genders in the 18-30 age group.\n19. allgenders_18age_votes: Number of votes from viewers of all genders in the 18-30 age group.\n20. allgenders_30age_avg_vote: Average vote score given by viewers of all genders in the 30-45 age group.\n21. allgenders_30age_votes: Number of votes from viewers of all genders in the 30-45 age group.\n22. allgenders_45age_avg_vote: Average vote score given by viewers of all genders in the 45+ age group.\n23. allgenders_45age_votes: Number of votes from viewers of all genders in the 45+ age group.\n24. males_allages_avg_vote: Average vote score given by male viewers of all ages.\n25. males_allages_votes: Number of votes from male viewers of all ages.\n26. females_allages_avg_vote: Average vote score given by female viewers of all ages.\n27. females_allages_votes: Number of votes from female viewers of all ages.\n28. top1000_voters_rating: Average rating given by the top 1000 voters.\n29. top1000_voters_votes: Number of votes from the top 1000 voters.\n30. us_voters_rating: Average rating given by voters from the United States.\n31. us_voters_votes: Number of votes from voters from the United States.\n32. non_us_voters_rating: Average rating given by voters from outside the United States.\n33. non_us_voters_votes: Number of votes from voters from outside the United States.","3":"","4":"Introduction:\n\nIn this case study the skills that I acquired from Google Data Analytics Professional Certificate Course is demonstrated. These skills will be used to complete the imagined task which was given by Netflix. The analysis process of this task will be consisted of following steps. Ask, Prepare, Process, Analyze, Share and Act.\n\n\nScenario:\n\nThe Netflix Chief Content Officer, Bela Bajaria, believes that companies success depends on to provide the customers what they want. Bajaria stated that the goal of this task is to find most wanted contents of the movies which will be added to the portfolio. Most of the movie contracts are signed before they come to the theaters, and it is hard to know if the customers really want to watch that movie and if the movie will be successful. There for my team wants to understand what type of content a movies success depends on. From these insights my team will design an investment strategy to choose the most popular movies that are expected to be in theaters in the near future. But first, Netflix executives must approve our recommendations. To be able to do that we must provide satisfying data insights along with professional data visualizations. \n\n\nAbout the Company:\n\n\nAt Netflix, we want to entertain the world. Whatever your taste, and no matter where you live, we give you access to best-in-class TV series, documentaries, feature films and games. Our members control what they want to watch, when they want it, in one simple subscription. We\u2019re streaming in more than 30 languages and 190 countries, because great stories can come from anywhere and be loved everywhere. We are the world\u2019s biggest fans of entertainment, and we\u2019re always looking to help you find your next favorite story.\n\nAs a company Netflix knows that it is important to acquire or produce movies that people want to watch. \n\nThere for Bajaria has set a clear goal: Define an investment strategy that will allow Netflix to provide customers the movies what they want to watch which will maximize the Sales.\n\n\n\nAsk:\n\nBusiness Task: \nTo find out what kind of movie customers wants to watch and if the content type really has a correlation with the movie success.\nStakeholders: \n\nBela Bajaria: She joined Netflix in 2016 to oversee unscripted and scripted series. Bajaria also responsible from the content selection and strategy for different regions. \n\nNetflix content analytics team: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Netflix content strategy.\n\nNetflix executive team: The notoriously detail-oriented executive team will decide whether to approve the recommended content program.\n\n\nPrepare:\n\nI start my preparation procedure by downloading every piece of data I'll need for the study. Top 1000 Highest-Grossing Movies of All Time.csv will be used. Additionally, 15 Lowest-Grossing Movies of All Time.csv was found during the data research and this dataset will be analyst as well. The data has been made available by IMDB and shared this two following URL addresses: https:\/\/www.imdb.com\/list\/ls098063263\/ and https:\/\/www.imdb.com\/list\/ls069238222\/ .\n\n\nProcess:\n\nData Cleaning: \n\nSQL: To begin the data cleaning process, I opened both csv file in SQL and conducted following operations: \n\n\u2022\tChecked for and removed any duplicates. \n\u2022\tChecked if there any null values. \n\u2022\tRemoved the columns that are not necessary.\n\u2022\tTrim the Description column to have only gross profit in it. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\n\u2022\tRenamed the Description column as Gross_Profit. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\nFollwing SQL codes were used during the data cleaning:\n#SQL CODE used for Highest Grossing Movies DATASET\n\nSELECT \nPosition,\nSUBSTR(Description,34,12) as Gross_Profit,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\n FROM `even-electron-400301.Highest_Gross_Movies.1` \n\n#SQL CODE used for Lowest Grossing Movies DATASET\n\nSELECT \nPosition,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\nFROM `even-electron-400301.Lowest_Grossing_Movies.2` \nOrder By Position\n\n\n\n\nAnalyze:\n\nAs a starter, I want to reemphasize the business task once again. Is content has a big impact on a movie\u2019s success?\n\nTo answer this question, there were a few information that I projected that I could pull of and use it during my analysis. \n\n\u2022\tAverage gross profit \n\u2022\tNumber of Genres\n\u2022\tTotal Gross Profit of the most popular genres\n\u2022\tThe distribution of the Gross income on Genres\n\n\nI used Microsoft Excel for the bullet points above. The operations to achieve the values above are as follows:\n \n\u2022\tAverage function for Average Gross profit in 1000 Highest-Grossing Movies of All Time.\n\u2022\tCreated a pivot table to work on Genres and Gross_Profit\n\u2022\tAnalyzed all genre which consist different categories (exp: Action, Adventure, Horrror ...) in it by using filtering for specific categories like, action, adventure etc. \n\u2022\tThe content analysis mentioned in the last bullet point implemented also for 15 Lowest-Grossing Movies of All Time dataset. \n\nShare:\nThe dashboard I created for the project is also posteed on tableau, you can find the dashboard can be found here\n\nFor the visualizations, it was aimed to emphasize the only the data which demonstrates the key differences between Genre and Gross Profit. \nFollowing visualizations were included into the dashboard;\n\n\u2022\tGross income distributions on movie genres\n\u2022\tGenre\u2019s category distribution for highest grossing movies\n\u2022\tGenre\u2019s category distribution for lowest grossing movies\n\n\n\nHere is a summary of the most meaningful insights for content and movie success:\n\nHighest Grossing Movies: \n\n\u2022\tAmong the 1000 most profited movies the most common genre is \u201cAction, Adventure, Sci-Fi\u201d, 64 movies have this genre type in the list.\n\u2022\t The following genre type is \u201cAction, Adventure, Thriller\u201d with 33 movies. \n\u2022\tEach Genre has several categories in it. During analysis it was observed that some categories has occurred more than other categories. \n\u2022\tAmong the highest grossing movies there were clear difference between different categories that is used in genres. According to my analysis among the 1000 movies 520 movies possessed adventure category and 484 movies had action category in it. \n\u2022\tMovies with horror genre is one of the least sucessfull ones in the 1000 highest grossing movies list in terms of gross profit.\n\nLowest Grossing Movies: \n\u2022\tThere is no movie in the lowest gross income movies with action or adventure genres.\n\u2022\t30% of the 15 least grossing movies has horror genre which is the most common genre in this list.\n\nAbout the analysis: \n\nThe data collected from secondary resources was limited to come up with a detailed analysis. Since the analysis has conducted with the limited data, it might affect the insights and might not demonstrate us the real-life distribution which will lead to wrong strategies especially when it comes to different regions. \n\nAct: \n\nBased on my analysis, following actions are recommended which I believe that it will help content department to create an effective strategy for content creation and selection. \n\n\n\u2022\tAction\/Adventure base strategy: When Netflix signing with a new movie or creating a new one, we can add more movies with action and adventure category to our portfolio. \n\u2022\tHorror Movies: Netflix should give less priority to horror movies. Statistically, horror movies are the least successful movies among all. And to increase the number of movies which will be invested it will increase the probability to fail.\n\u2022\tCombination of the forces: Netflix can try to combine the most successful categories in terms of gross profit. For instance, comedy with action or animation and action to increase the attraction. \n\n\nBy implementing these strategies, Netflix can make more meaningful decision on content creation and sign more precise movie agreements which are demanded by the audience. \n","5":"Google's GEMINI can identify a movie if you show a few frames of it. Cool stuff! Can you build a model that can do the same thing?\n\nThe dataset contains frames from approximately 800 top-rated movies. Each movie has around 1000 frames, totaling up to 800,000 frames in the dataset. \n\n&gt; If you find this dataset valuable for building movie identification models, don't forget to hit the upvote button! \ud83d\ude0a\ud83d\udc9d \n\nTop rated movies were identified from my [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) dataset\n\n### Checkout my top datasets\n- [Top Spotify Songs in 73 Countries](https:\/\/www.kaggle.com\/datasets\/asaniczka\/top-spotify-songs-in-73-countries-daily-updated)\n- [Wages by Education in the USA](https:\/\/www.kaggle.com\/datasets\/asaniczka\/wages-by-education-in-the-usa-1973-2022)\n- [Amazon Products Dataset (1.4M Products)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-products-dataset-2023-1-4m-products)\n- [TMDB 950K Movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies)\n- [Amazon Kindle Books (130K Books)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-kindle-books-dataset-2023-130k-books)\n\n\n## Interesting Task Ideas:\n\n1. Create a movie identification system that can predict the title of a movie based on extracted frames.\n2. Train an AI model to identify similar scenes or genres using movie frames.\n3. Develop a content-based recommendation system that suggests movies based on visual similarity.\n4. Explore the correlation between movie ratings and the visual elements captured in the frames. Use [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) for ratings\n5. Train deep learning models like VIT or ResNet using the dataset to develop powerful movie identification systems.\n\n## Specs:\n\n- Each frame has a height of 256px.\n- Width will vary since different movies have different aspect ratios.\n- Roughly 1000 frames were extracted from each movie.\n- All frames for a movie will exist under the folder with the movie's name.\n- Movie names are in the format of `Movie name (Year of release)`.\n- To get more details on movies, use my TMDB movies dataset. Movie names are not a 100% match. Filter by release year and try semantic similarity or vector similarity.\n\n---\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@jakobowens1?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Jakob Owens<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/clap-board-roadside-jakob-and-ryan-CiUR8zISX60?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n  ","6":"The dataset have seven features : \n1) Movie Name\n2) Category\n3) Release year\n4) Duration\n5) Rating\n6) Meta score\n7) Votes .","7":"","8":"","9":"This dataset contains information about top 1000 IMDB movies, including their titles, certificates, durations, genres, IMDb ratings, Metascores, directors, cast members, the number of votes they received, grossed earnings, and plot summaries. The data is a curated list of highly acclaimed and popular movies.\n\nColumns\/Variables:\n\nMovie Name: The title of the movie.\nCertificate: The certificate or rating assigned to the movie.\nDuration: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nIMDb Rating: The IMDb rating of the movie.\nMetascore: The Metascore rating of the movie.\nDirector: The director of the movie.\nStars: The main cast members of the movie.\nVotes: The number of user votes\/ratings the movie has received.\nGrossed in $: The gross earnings in dollars (if available).\nPlot: A brief summary or plot description of the movie.\nSize: The dataset contains 1000 rows and 11 columns.\n\n\nData Quality: The dataset appears to be well-structured and complete. There are no missing values, and it seems to be ready for analysis.\n\nUse Cases: This dataset can be used for various analyses, such as exploring the relationship between IMDb ratings and Metascores, identifying top-rated directors, or understanding the distribution of movie ratings across genres.","10":"This Dataset displays only the top 1000 Anime of all time on IMDB. It is in the same order as displayed on the IMDB website which is filtered by - \"Most Popular Movies and TV Shows tagged with keyword \"anime\". [**Source**](https:\/\/www.imdb.com\/search\/keyword\/?keywords=anime&mode=detail&page=1&ref_=kw_nxt&sort=moviemeter,asc)\n\n\n**DATA DICTIONARY:**<br>\n`Title`: The name of the Anime.\n`Release Year`: The year the anime was released and till which year it is Ended, The OA in the values tells that the anime is still airing.\n`Genre`: Categories where the anime belongs.\n`Duration`: Anime running time in minutes.\n`Rating`: Ratings given by IMDb registered users (on a scale of 1 to 10)\n`Description`: Summary of the Anime which gives a brief description what it is about.\n`No. of Votes`: Number of votes cast by IMDb registered users.\n\n\n*Note: There are NA values present in most of the columns, because the value is not available on the source*","11":"","12":"","13":"The IMDb Top 1000 Movies dataset is a treasure trove of information about the most esteemed films in cinematic history. It includes a comprehensive collection of attributes that can fuel numerous insightful analyses and visualizations.\n\nTitle: The title of the movie.\nYear: The year the movie was released.\nRating: The IMDb rating of the movie, reflecting its overall popularity and reception.\nDuration: The runtime of the movie in minutes.\nGenre: The genre(s) to which the movie belongs (e.g., Drama, Action, Adventure).\nIMDb Rank: The rank of the movie within the IMDb Top 1000 list.\nMetascore: The Metascore assigned to the movie, offering a critical review of its quality.\nDirector: The director(s) responsible for bringing the movie to life.\nStars: The lead actors and actresses who play pivotal roles in the movie.\nVotes: The number of user votes the movie has received on IMDb, reflecting its popularity.\nGross: The gross earnings of the movie at the box office or through various distribution channels.\nDescription: A brief synopsis or overview of the movie's plot, theme, or significance.","14":"Welcome to the \"Top 1000 IMDb Movies Dataset\" - a treasure trove of cinematic excellence! This comprehensive collection presents the most celebrated and beloved movies, as rated and ranked by IMDb users. Whether you're a movie enthusiast, a data scientist, or a filmmaker seeking inspiration, this dataset promises a thrilling journey through the world of cinema's finest creations.\n\n**Content**:\n\nThis dataset boasts a rich array of movie attributes to help you delve into the realm of top-rated films:\n\nMovie Name: The title of each movie, representing iconic masterpieces that have left an indelible mark on the film industry and audiences worldwide.\n\nYear of Release: The year when each movie was released, providing valuable context for historical and chronological analyses.\n\nWatch Time: The duration of each movie, allowing you to identify captivating films for various viewing preferences.\n\nMovie Rating: IMDb's user ratings, serving as a benchmark for gauging audience reception and appreciation.\n\nMetascore of Movie: Metascores from renowned critics, offering insights into the films' critical acclaim and recognition.\n\nGross Earnings: The worldwide box office earnings, reflecting the commercial success and popularity of each movie.\n\nVotes: The number of votes cast by IMDb users, indicating the films' popularity and reach.\n\nDescription: Brief summaries that provide a glimpse into the captivating plots and themes of these cinematic marvels.\n\n**Usage**:\n\nThe \"Top 1000 IMDb Movies Dataset\" opens up a world of possibilities for movie aficionados and data enthusiasts alike:\n\nFilm Analysis and Insights: Dive into data-driven analyses to uncover intriguing patterns, trends, and correlations among highly-rated movies.\n\nRating Predictions: Develop predictive models to estimate movie ratings based on other attributes and gain a deeper understanding of factors influencing user perceptions.\n\nBox Office Performance: Examine the relationship between critical acclaim, ratings, and box office success, providing valuable insights for filmmakers and producers.\n\nGenre Exploration: Investigate the distribution of top-rated movies across different genres to identify trends in audience preferences.\n\nTemporal Trends: Analyze changes in movie ratings, box office performance, and viewer preferences over time to reveal shifts in cinematic taste and trends.\n\n**Caution**:\n\nWhile this dataset is a treasure trove for movie analysis, it is essential to respect copyright and intellectual property rights. Users are encouraged to use the data responsibly and comply with IMDb's terms of use and any applicable legal restrictions.\n\n**Conclusion**:\n\nThe \"Top 1000 IMDb Movies Dataset\" offers an immersive and data-driven exploration of cinema's greatest works, igniting creativity, fostering discussions, and enriching our appreciation for the art of storytelling. Let the magic of these cinematic gems captivate you as you embark on an exciting journey into the world of acclaimed movies!","15":"","16":"Data of the top 1000 movies scraped from the IMDB website. Dataset also duplicates a movie in another row if the movie consists of more than one genre. Missing values present in 'gross(M)' and 'metascore' column represented as 0.00 and 0 respectively.\n\nEdit: Version 3. I have included the raw data from scraping the website for anyone interested in cleaning the data and transforming data types.\nI have also updated the csv file to include the director of the movie\n\nDISCLAIMER:  The grossing values of movies from IMDb only include numbers from US and Canada and are thus not representative of the values on a global scale.","17":"This dataset consists of a meticulously collected collection of 10,000 feature films from IMDb, one of the most popular and authoritative sources for movie information. The movies included in this dataset are sorted based on their IMDb ratings in descending order. The dataset covers a wide range of genres, directors, and stars, providing a comprehensive overview of highly regarded films across various categories.The scraping process was performed on June 17, 2023.\n\nDataset Columns:\n\nID: Unique identifier for each movie in the dataset.\nMovie Name: The title of the movie.\nRating: The IMDb rating for the movie.\nRuntime: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nMetascore: The Metascore rating for the movie (if available).\nPlot: A brief summary or description of the movie's plot.\nDirectors: The director(s) of the movie.\nStars: The main cast or actors featured in the movie.\nVotes: The number of votes\/ratings received by the movie.\nGross: The gross revenue generated by the movie (if available).\nLink: The IMDb link to access the full details and additional information about the movie.","18":"","19":"","20":"","21":"","22":"","23":"This dataset contains 5,879 Reddit posts discussing Borderline Personality Disorder (BPD) , posts are about people telling own experiencies with this disorder either they suffer from or have met people or raised by bpd parents \n\nThis dataset provides a valuable resource for analyzing the discourse around BPD on social media, studying patterns in user interactions, and understanding the experiences and challenges faced by individuals with BPD.\n\n**Columns:**\n**Title**: The title of the Reddit post.\n**Score**: The score of the post, reflecting its popularity and engagement.\n**ID**: The unique identifier of the post.\n**Comments**: The number of comments the post received.\n**Creation Time**: The timestamp when the post was created\n**Content**: The main text of the post, detailing the user's thoughts, experiences, and discussions about BPD\n\nThis dataset can be used for various purposes, including sentiment analysis, natural language processing, mental health research","24":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","25":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","26":"This dataset presents a comprehensive collection of textual content annotated with corresponding sentiments: happy, sad, and angry. Each entry in the dataset contains a piece of text along with its associated sentiment label, providing a rich resource for sentiment analysis and emotion detection tasks.","27":"# Introduction \nThe Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment.\n\nThe dataset holds 11,932 documents annotated with 3 labels:\n```python\nsentiments = {\n    \"LABEL_0\": \"Bearish\", \n    \"LABEL_1\": \"Bullish\", \n    \"LABEL_2\": \"Neutral\"\n}  \n```\n\nThe data was collected using the Twitter API. The current dataset supports the multi-class classification task.\n\n# Data Splits\nThere are 2 splits: train and validation. Below are the statistics:\n\nDataset Split\tNumber of Instances in Split: \n\nTrain -----------9,938\n\nValidation-------2,486\n\n# Licensing Information\nThe Twitter Financial Dataset (sentiment) is released under the MIT License.\n\n# Source \nhttps:\/\/huggingface.co\/datasets\/zeroshot\/twitter-financial-news-sentiment","28":"**Context**\nThe objective of this task is to build a model based on pre-processed tweets. For the sake of simplicity, we say a tweet contains a negative review if it has a racist or hate sentiment associated with it. So, the task is to predict the labels on the test dataset after building a model.\n\n**Content**\nIn the dataset, a labelled train data is given where label '0' denotes the tweet is positive \ud83d\ude0a and label '1' denotes the tweet is negative \u2639\ufe0f\n\n**Acknowledgements**\nDataset is provided by [Analytics Vidhya](http:\/\/https\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/)\n","29":"**Context**\n\nThis dataset is a part of our research work titled \"Opinion Mining of Customer Reviews Using Supervised Learning Algorithms\". If you use this dataset then please cite our work.\nYou can find the article in https:\/\/ieeexplore.ieee.org\/document\/9733435\n\n**Content**\n\nNowadays, a lot of people express their opinions on various topics using social networking sites. Twitter has become a famous social networking site where people can express their opinions to the point and so it has become a great source for opinion mining. In this research, the goal was to train and build a model that can automatically and accurately categorize the opinion of customer tweet reviews about popular cell phone brands. We have used python TextBlob library for getting the polarity values of all the tweet reviews of the dataset. We have also used Support Vector Machine (SVM), Na\u00efve Bayes, Logistic Regression, Decision Tree and Random Forest algorithms along with Bag of Words and TF-IDF vectorizers separately to train and build the model. We have investigated the opinions using five classes which are Strongly Positive, Positive, Neutral, Negative and Strongly Negative.\n\n\n**When referencing this dataset please cite the below paper**\n\n**Bibtex**\n@inproceedings{arif2021opinion,\ntitle={Opinion Mining of Customer Reviews Using Supervised Learning Algorithms},\nauthor={Arif, Shibbir Ahmed and Hossain, Taslima Binte},\nbooktitle={2021 5th International Conference on Electrical Information and Communication Technology (EICT)},\npages={1--6},\nyear={2021},\norganization={IEEE}\n}","30":"","31":"This dataset provides a comprehensive collection of public sentiment and discourse related to Indian politics. The entries cover a wide range of opinions, news, social media posts, and other forms of public communication. \nEach entry is meticulously labeled with a sentiment score, capturing the polarity of the opinion from strongly negative to strongly positive. \n\nThis dataset is structured to facilitate detailed sentiment analysis and examination of political sentiments in India.\n\nUse Cases\n\nThis dataset is ideal for:\n\nSentiment Analysis: Researchers can use this dataset to train and evaluate sentiment analysis models specifically tailored to the political context in India.\nTrend Analysis: Analysts can track the evolution of public sentiment over time, identifying key events that influenced public opinion.\nPolitical Studies: Scholars can investigate the relationship between public sentiment and political events, figures, and policies in India.\nNatural Language Processing (NLP): NLP practitioners can leverage this dataset for various tasks such as text classification, opinion mining, and more.\n","32":"Title: Twitter Emotion Dataset: Unveiling the Emotional Tapestry of Social Media\n\nDescription:\n\nDive into the intricate world of human emotions expressed through Twitter messages with our meticulously curated dataset. Each entry in this comprehensive collection features a text segment extracted from Twitter, accompanied by a corresponding label denoting the predominant emotion conveyed by the message. The emotions are thoughtfully categorized into six distinct classes: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\nThis dataset is tailor-made for researchers, data scientists, and enthusiasts keen on exploring the dynamic emotional landscape within the realm of social media. Whether you're delving into sentiment analysis, emotion classification, or text mining, our Twitter Emotion Dataset provides a rich foundation for unraveling the subtle nuances that define human emotions in the digital age.\n\n**Dataset Highlights:**\n- Over 416809 Twitter messages covering a diverse range of topics and user demographics.\n- Manually annotated labels ensure high-quality emotional classification.\n- Tweets sourced from a variety of geographies and languages, capturing a global perspective on emotions in social media.\n- A balanced distribution across the six emotion categories for robust model training.\n\n**Potential Applications:**\n- Sentiment Analysis: Explore sentiment trends and patterns to understand how users express emotions over time.\n- Emotion Classification: Develop models to accurately predict and categorize emotional states within tweets.\n- Text Mining: Extract valuable insights from the vast pool of emotional data to inform decision-making and marketing strategies.\n\n**Why Use Our Dataset?**\n- Enrich your research with a diverse and well-labeled dataset, ensuring the reliability and accuracy of your findings.\n- Leverage the global scope of the dataset for cross-cultural and multilingual analyses of emotional expression.\n- Contribute to advancements in emotional intelligence research and its applications in social media analytics.\n\n**Dataset Conclusions:**\nUncover the hidden gems within our Twitter Emotion Dataset and draw meaningful conclusions about the evolving emotional landscape of social media. Whether you're identifying sentiment shifts during significant events or tracking the ebb and flow of joy, anger, and surprise, this dataset provides the tools to delve into the intricacies of human emotion in the digital realm. As you analyze and model, contribute to the collective understanding of emotional intelligence on social platforms and pave the way for innovative applications in communication, mental health, and beyond. Join us in decoding the emotional tapestry of Twitter and shaping the future of emotion-aware technologies.","33":"&gt;# Content:\nThis Emotion Classification dataset is designed to facilitate research and experimentation in the field of natural language processing and emotion analysis. It contains a diverse collection of text samples, each labelled with the corresponding emotion it conveys. Emotions can range from happiness and excitement to anger, sadness, and more.\n&gt;# About emotions.csv file\nEach entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into **six categories**: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within social media.\n- **text**: Description of context\n- **label**: The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\n&gt;# Usecase:\n- **Sentiment Analysis**: Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification**: Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis**: Explore linguistic patterns and expressions associated with different emotional states.","34":" Introduction\n\n Raw tweet data is often messy and requires cleaning and normalization before building an effective NLP model.  Here's a breakdown of common preprocessing steps and their purpose:\n\n 1. Data Gathering\n\nObtaining a suitable dataset of labeled tweets (e.g., from Twitter, existing NLP datasets, or Kaggle competitions).\n 2. Removing HTML Tags\n\nHTML tags can clutter the text and don't contribute to understanding a tweet's content or sentiment.\n 3. Removing URLs\n\nURLs often don't add significant meaning for disaster classification and may introduce unnecessary variability.\n 4. Converting to Lowercase\n\nMakes the text case-insensitive, ensuring that \"Disaster\" and \"disaster\" are treated as the same word, improving word frequency analysis.\n 5. Removing Emojis\n\nWhile emojis can carry sentiment, they require sophisticated techniques to interpret consistently. Basic preprocessing often removes them, although more advanced models might incorporate emoji analysis.\n 6. Removing Punctuation\n\nPunctuation marks rarely contribute to core meaning in disaster classification and can introduce noise.\n 7. Removing Stop Words\n\nRemoving common words like \"the,\" \"and,\" etc., that have little semantic value. This reduces computational load and lets the model focus on more informative words.\n 8. Handling Abbreviations\/Slang\n\nExpanding abbreviations and slang terms (e.g., \"lol\" -&gt; \"laughing out loud\") aids in understanding the full meaning of the text and makes the vocabulary more standardized.\n 9. Stemming\n\nReduces different forms of words to their root (e.g., \"flooding,\" \"flooded\" -&gt; \"flood\"), potentially helping the model generalize better.\n 10. Spelling Correction\n\nFixing typos ensures words are correctly interpreted and can make word frequencies more accurate.\n 11. Tokenization\n\nSplits the text into individual words or meaningful units (e.g., \"New York\" is often better treated as a single token) to prepare the data for further analysis and model input.","35":"&gt;## Context\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = negative, 4 = positive) and can be used to detect sentiment.\n## Content\nIt contains the following 6 fields:\n- target: the polarity of the tweet (0 = negative and 4 = positive)\n- ids: The id of the tweet ( 2087)\n- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n- user: the user that tweeted.\n- text: the text of the tweet.","36":"# Introduction:\n\n&gt;Welcome to the \"Emotions\" dataset \u2013 a collection of English Twitter messages meticulously annotated with six fundamental emotions: anger, fear, joy, love, sadness, and surprise. This dataset serves as a valuable resource for understanding and analyzing the diverse spectrum of emotions expressed in short-form text on social media.\n\n[![DOI](https:\/\/zenodo.org\/badge\/DOI\/10.34740\/KAGGLE\/DSV\/7563141.svg)](https:\/\/www.kaggle.com\/nelgiriyewithana\/datasets)\n\n\n# About the Dataset:\n\n&gt;Each entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within the realm of social media.\n\n# Key Features:\n\n&gt;- **text**: A string feature representing the content of the Twitter message.\n- **label**: A classification label indicating the primary emotion, with values ranging from 0 to 5.\n\n# Potential Use Cases:\n\n&gt;- **Sentiment Analysis:** Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification:** Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis:** Explore linguistic patterns and expressions associated with different emotional states.\n\n# Sample Data:\n\nHere's a glimpse of the dataset with a few examples:\n\n| text                                                  | label |\n|-------------------------------------------------------|-------|\n| that was what i felt when i was finally accept...      | 1     |\n| i take every day as it comes i'm just focussin...      | 4     |\n| i give you plenty of attention even when i fee...      | 0     |\n\n<br>\nIf you find this dataset useful consider giving it a vote! \ud83d\ude0a\u2764\ufe0f \n","37":"Explore the diverse landscape of social media with this annotated Twitter dataset. Categorized into 'Political,' 'Positive,' 'Protest,' 'Riot,' 'Terror,' 'Disaster,' and 'Other,' these tweets provide valuable insights into public sentiments and trending topics. Whether you're interested in understanding political discourse, positive expressions, social movements, or crisis events, this dataset offers a comprehensive look into various aspects of online conversations. Use it for sentiment analysis, trend detection, or studying public reactions across a spectrum of categories.","38":"NO MORE UPDATES\n[nitter.net](https:\/\/nitter.net) the twitter mirror we were pulling from has shutdown indefinitely.\n\n* this is a twitter tweet dataset on crypto, nfts, exchanges, and popular forums like wallstreetbets  \n* pulling tweets live from twitter ~3-4 days worth of tweets from the marked date of the files  \n\n\n## search queries in `_q.csv`\nthis file contains all the queries for different cryptocurrencies and exchanges \/ forums that were searching on twitter to gather the data.\n\n## tweets `yymmdd.*` files\n- these are the tweets data files\n- note they are marked sol and binance but they contain results from all queries\n\n## columns\n```\n|-------------------------|-------------------------------------------------------|\n| field                   | explanation                                           |\n|-------------------------|-------------------------------------------------------|\n| username                | who posted the tweet                                  |\n|-------------------------|-------------------------------------------------------|\n| tweet_raw               | unfiltered tweets *have links whitespace special chs. |\n|-------------------------|-------------------------------------------------------|\n| tweet_text              | cleaned tweet text                                    |\n|-------------------------|-------------------------------------------------------|\n| date                    | date tweet was posted                                 |\n|-------------------------|-------------------------------------------------------|\n| happy\/sad\/etc           | these are emotion \/ sentiment scoring                 |\n|-------------------------|-------------------------------------------------------|\n| afinn\/bing\/sid\/bertweet | more sentiment scoring                                |\n|-------------------------|-------------------------------------------------------|\n```\n\n\n","39":"### This is the data wrangled and cleaned from an existing dataset, and it is the output of my notebook: [https:\/\/www.kaggle.com\/code\/paakhim10\/analyzing-and-classifying-twitter-sentiments](url)\n\n####You can use this dataset for:\n\n&gt;\u2022 Machine Learning\n1. Sentiment analysis\n2. Spam detection\n3. Text classification\n4. Trend identification (temporal and geographical)\n5. Building recommendation systems\n\n&gt;\u2022 Exploratory Data Analysis\n1. Geospatial or temporal mapping\n2. Hashtags Trend Detection\n3. User Engagement Metrics\n\n##### Credits:\nIf you use this dataset in your research, please credit the original authors.\nIf you use this dataset in your research, please credit [https:\/\/data.world\/kjensen18](url)","40":"_____\n# Short Jokes Dataset\n### Humorous Short Jokes\nBy Fraser Greenlee (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n_____\n\n### About this dataset\n> \n> This dataset offers a valuable resource for various applications such as natural language processing, sentiment analysis, joke generation algorithms, or simply for entertainment purposes. Whether you're a data scientist looking to analyze humor patterns or an individual seeking some quick comedic relief, this dataset has got you covered.\n> \n> By utilizing this dataset, researchers can explore different aspects of humor and study the linguistic features that make these short jokes amusing. Moreover, it provides an opportunity for developing computer models capable of generating similar humorous content based on learned patterns.\n> \n\n### How to use the dataset\n> \n> \n> - **Understanding the Columns:**\n>    - `text`: This column contains the text of the short joke.\n>    - `**text`: No information is provided about this column.\n> \n> - **Exploring the Jokes:**\n>    - Start by exploring the `text` column, which contains the actual jokes. You can read through them and have a good laugh!\n>   \n> - **Analyzing the Jokes:**\n>    - To gain insights from this dataset, you can perform various analyses:\n>      - Sentiment Analysis: Use Natural Language Processing techniques to analyze the sentiment of each joke.\n>      - Categorization: Group jokes based on common themes or subjects, such as animals, professions, etc.\n>      - Length Distribution: Analyze and visualize the distribution of joke lengths.\n>    \n> - **Creating New Content or Applications:**\n>   Since this dataset provides a large collection of short jokes, you can utilize it creatively:\n>     - Generating Random Jokes: Develop an algorithm that generates new jokes based on patterns found in this dataset.\n>     - Humor Classification: Build a model that predicts if a given piece of text is funny or not using machine learning techniques.\n> \n> - **Sharing Your Findings:** \n>   If you make interesting discoveries or create unique applications using this dataset, consider sharing them with others in Kaggle community.\n> \n> Please note that no information regarding dates is available in train.csv; therefore, any temporal analysis or date-based insights won't be feasible with this specific file.\n> \n\n### Research Ideas\n> - Analyzing humor patterns: This dataset can be used to analyze different types of humor and identify patterns or common elements in jokes that make them funny. Researchers and linguists can use this dataset to gain insights into the structure, wordplay, or comedic techniques used in short jokes.\n> - Natural language processing: With the text data available in this dataset, it can be used for training models in natural language processing (NLP) tasks such as sentiment analysis, joke generation, or understanding humor from written text. NLP researchers and developers can utilize this dataset to build and improve algorithms for detecting or generating funny content.\n> - Social media analysis: Short jokes are popular on social media platforms like Twitter or Reddit where users frequently share humorous content. This dataset can be valuable for analyzing the reception and impact of these jokes on social media platforms. By examining trends, engagement metrics, or user reactions to specific jokes from the dataset, marketers or social media analysts can gain insights into what type of humor resonates with different online communities.\n> Overall, this dataset provides a rich resource for exploring various aspects related to humor analysis and NLP tasks while offering opportunities for sociocultural studies related to online comedy culture\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name   | Description                                   |\n|:--------------|:----------------------------------------------|\n| **text**      | The actual content of the short jokes. (Text) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Fraser Greenlee (From Huggingface)](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes).\n\n","41":"_____\n# Hate Speech and Offensive Language Detection\n### Hate Speech and Offensive Language Detection on Twitter\nBy hate_speech_offensive (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n_____\n\n### About this dataset\n> This dataset, named hate_speech_offensive, is a meticulously curated collection of annotated tweets with the specific purpose of detecting hate speech and offensive language. The dataset primarily consists of English tweets and is designed to train machine learning models or algorithms in the task of hate speech detection. It should be noted that the dataset has not been divided into multiple subsets, and only the train split is currently available for use.\n> \n> The dataset includes several columns that provide valuable information for understanding each tweet's classification. The column count represents the total number of annotations provided for each tweet, whereas hate_speech_count signifies how many annotations classified a particular tweet as hate speech. On the other hand, offensive_language_count indicates the number of annotations categorizing a tweet as containing offensive language. Additionally, neither_count denotes how many annotations identified a tweet as neither hate speech nor offensive language.\n> \n> For researchers and developers aiming to create effective models or algorithms capable of detecting hate speech and offensive language on Twitter, this comprehensive dataset offers a rich resource for training and evaluation purposes\n\n### How to use the dataset\n> \n> - Introduction:\n> \n> - Dataset Overview:\n>    - The dataset is presented in a CSV file format named 'train.csv'.\n>    - It consists of annotated tweets with information about their classification as hate speech, offensive language, or neither.\n>    - Each row represents a tweet along with the corresponding annotations provided by multiple annotators.\n>    - The main columns that will be essential for your analysis are: count (total number of annotations), hate_speech_count (number of annotations classifying a tweet as hate speech), offensive_language_count (number of annotations classifying a tweet as offensive language), neither_count (number of annotations classifying a tweet as neither hate speech nor offensive language).\n> \n> - Data Collection Methodology:\n>     The data collection methodology used to create this dataset involved obtaining tweets from Twitter's public API using specific search terms related to hate speech and offensive language. These tweets were then manually labeled by multiple annotators who reviewed them for classification purposes.\n> \n> - Data Quality:\n>     Although efforts have been made to ensure the accuracy of the data, it is important to acknowledge that annotations are subjective opinions provided by individual annotators. As such, there may be variations in classifications between annotators.\n> \n> - Preprocessing Techniques:\n>     Prior to training machine learning models or algorithms on this dataset, it is recommended to apply standard preprocessing techniques such as removing URLs, usernames\/handles, special characters\/punctuation marks, stop words removal, tokenization, stemming\/lemmatization etc., depending on your analysis requirements.\n> \n> - Exploratory Data Analysis (EDA):\n>     Conducting EDA on the dataset will help you gain insights and understand the underlying patterns in hate speech and offensive language. Some potential analysis ideas include:\n>     - Distribution of tweet counts per classification category (hate speech, offensive language, neither).\n>     - Most common words\/phrases associated with each class.\n>     - Co-occurrence analysis to identify correlations between hate speech and offensive language.\n> \n> - Building Machine Learning Models:\n>     To train models for automatic detection of hate speech and offensive language, you can follow these steps:\n>    a) Split the dataset into training and testing sets for model evaluation purposes.\n>    b) Choose appropriate features\/\n\n### Research Ideas\n> - Sentiment Analysis: This dataset can be used to train models for sentiment analysis on Twitter data. By classifying tweets as hate speech, offensive language, or neither, the dataset can help in understanding the sentiment behind different tweets and identifying patterns of negative or offensive language.\n> - Hate Speech Detection: The dataset can be used to develop models that automatically detect hate speech on Twitter. By training machine learning algorithms on this annotated dataset, it becomes possible to create systems that can identify and flag hate speech in real-time, making social media platforms safer and more inclusive.\n> - Content Moderation: Social media platforms can use this dataset to improve their content moderation systems. By using machine learning algorithms trained on this data, it becomes easier to automatically detect and remove offensive or hateful content from the platform, reducing the burden on human moderators and improving user experience by keeping online spaces free from toxic behavior\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name                  | Description                                                                                            |\n|:-----------------------------|:-------------------------------------------------------------------------------------------------------|\n| **count**                    | The total number of annotations for each tweet. (Integer)                                              |\n| **hate_speech_count**        | The number of annotations classifying a tweet as hate speech. (Integer)                                |\n| **offensive_language_count** | The number of annotations classifying a tweet as offensive language. (Integer)                         |\n| **neither_count**            | The number of annotations classifying a tweet as neither hate speech nor offensive language. (Integer) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [hate_speech_offensive (From Huggingface)](https:\/\/huggingface.co\/datasets\/hate_speech_offensive).\n\n","42":"The \"Indian Political Tweets\" dataset is a collection of tweets related to the Indian political party BJP during election campaigns. The tweets were collected using Twitter API and Github sources. The tweets were preprocessed using various techniques to clean the data. This included removing duplicate tweets, removing retweets, removing mentions and URLs, and correcting spelling and grammar errors. The remaining tweets were then tokenized, lemmatized, and stop words were removed to prepare the data for sentiment analysis.\nThe dataset contains two labels - positive and negative - which indicate the sentiment of the tweet towards the political party being mentioned (BJP ). The sentiment analysis was performed using the Vader sentiment analysis tool, which assigned a score to each tweet based on the presence of positive and negative words in the text. Tweets with a positive score were labeled as positive, while tweets with a negative score were labeled as negative.\nThe dataset includes a total of 10210 number of tweets, with an approximately equal number of positive and negative tweets for both BJP . The dataset is suitable for data analysis and machine learning tasks such as text classification, sentiment analysis, and topic modeling.","43":"Project: **Predictive Model for Twitter US Airline Comments Classification**\nThe project was developed by a group of students from FORE School of Management, It was aimed to develop a predictive model for classifying passengers' tweets about different US airlines into three categories: positive, negative, and neutral. The primary objective was to predict a tweet's classification based on its content.\n****The workflow and all the steps in the workflow have been explained in the attached project report with the dataset***","44":"A collection of tweets about the international computer technology business Dell can be found in the \"Preprocessed Dell Tweets\" dataset. To make sentiment analysis and natural language processing jobs easier, these tweets have undergone meticulous preprocessing. The dataset was first made up of tweets that were scraped from Twitter; preprocessing was done to clean and organize the data.\n\n**Important characteristics:**\n\n **Text:** The preprocessed text of the tweets is contained in this column, making it appropriate for natural language processing and text analysis.\n\n **Sentiment:** To facilitate machine learning activities, the sentiment column has been converted into numeric values. The following is how sentiment labels have been encoded:\n0: Neutral\n1: Positive\n2: Negative\n\n**Possible Applications:**\nThe dataset is perfect for machine learning, sentiment analysis, and sentiment classification tasks. This dataset can be used by researchers and data scientists to test and refine sentiment analysis methods. Efficient sentiment models for prediction can be developed thanks to the numerical sentiment labels.\n\n**Data Preprocessing:**\nTo prepare the dataset for analysis, the following preprocessing steps were applied:\n\n- Punctuation and special characters were removed from the text.\n- URLs and hyperlinks were stripped from the text.\n- Text was converted to lowercase for uniformity.\n- Stopwords (common words with limited analytical value) were removed.\n- Tokenization, stemming, and lemmatization were performed to normalize the text data.\n\n\n","45":"As part of my studies at the *Aguascalientes Autonomous University* in the *Intelligent Computing Engineering* program, during our seventh semester, we took a course called \"Metaheuristics 1.\" Taught by Professor **Francisco Javier Luna Rosas**, we were going to undertake a quite ambitious project planned for the entire semester.\n\nThe project consisted of several phases where we would perform web scraping of a social network on a specific sensitive topic on a massive scale, aiming to simulate a Big Data operation for sentiment analysis of this data and classify it as positive or negative. Additionally, we were required to propose a load balancer that would handle the distributed processing, providing us with speed in the project's execution time.\n\nThe teacher said that, since this project was very complex, it needed to be accomplished in teams. So with that, the team I collaborated with was formed. Teaming up with [Andrea Melissa Almeida Ortega](https:\/\/github.com\/Melissa-AO), [\u00d3scar Alonso, Flores Fern\u00e1ndez](https:\/\/github.com\/Dem0n2000), [Dariana G\u00f3mez Garza](https:\/\/github.com\/DariGmz), Fernando Francisco Gonz\u00e1lez Arenas, and [Hiram Efra\u00edn Orocio Garc\u00eda](https:\/\/github.com\/hiram57ef) we began sentiment analysis focusing on the topic of racism using the social network Twitter and conducted web scraping with a developer account using the Python library Tweepy. The procedure took place throughout the second semester of 2021, and at that time, the Twitter API download limit was approximately 900 requests every 15 minutes. Therefore, we developed a program to continuously make these requests, aiming to gather the maximum number of tweets possible and effectively simulate Big Data for the project. The number of tweets obtained after intensive web scraping reached a total of **6,942,021 tweets**, resulting in a **1.14 GB** file. Here it's uploaded separating the total scrapping each of the team members could do.\n\nA simple genetic algorithm procedure was applied, serving as a dynamic load balancer among the six collaborating project computers to perform parallel processing as quickly as possible. The NLTK library in Python was used for lemmatization procedures on the tweets. The Random Forest model was chosen as the classifier for this sentiment analysis, implemented using the sci-kit learn library in Python.\n\nThe classifier achieved an accuracy of **0.9999157** as evaluated through a confusion matrix. The project can be reviewed at this link [here](https:\/\/github.com\/Joul24py\/UAA-ICI\/tree\/main\/23-S7-M1-ClassExercises\/04-FinalProject).","46":"he Xitsonga Twitter Sentiment Analysis Dataset is a carefully curated collection of tweets written in the Xitsonga language, with the primary objective of discerning the emotional tones expressed in these tweets. This dataset is specifically designed to classify tweets into one of three distinct emotional categories: \"Positive,\" \"Neutral,\" or \"Negative.\" or \"irrelevant\"\n\nNotable Features:\nIn this dataset, you will find several key characteristics:\n\nLanguage Focus: All the tweets included in this dataset are composed in the Xitsonga language, ensuring that the sentiment analysis is tailored to the linguistic nuances of this specific language.\n\nEmotion Classification: Each tweet within the dataset has been manually categorized into one of three emotional labels: \"Positive\" for tweets conveying positive sentiments, \"Neutral\" for tweets that exhibit no significant emotional tone, and \"Negative\" for those expressing negative emotions or viewpoints.\n\nComment-Centric: The primary emphasis of this dataset is on the analysis of user comments and conversations on the Twitter platform, making it particularly useful for understanding the sentiments and reactions of Xitsonga-speaking Twitter users.\n\nDiverse Content: The dataset encompasses tweets from a broad spectrum of topics and subject matters, offering a comprehensive representation of discussions and discourse on Twitter in the Xitsonga language.\n\nSubstantial Data Size: With a substantial number of tweets, this dataset provides ample data for the development, training, and evaluation of sentiment analysis models.","47":"This dataset contains synthetic data generated to model urban mobility patterns. It includes detailed information on public transportation usage, traffic flow, bike-sharing programs, and pedestrian movement, enriched with additional contextual factors like weather conditions, holidays, and events. The dataset is designed to support urban planners and transportation authorities in making data-driven decisions to improve urban mobility and reduce traffic congestion.\n\nColumns:\n1. timestamp: Date and time of the record.\n2. public_transport_usage: Number of public transport users per hour.\n3. traffic_flow: Number of vehicles passing a specific point per hour.\n4. bike_sharing_usage: Number of users utilizing bike-sharing services per hour.\n5. pedestrian_count: Number of pedestrians recorded per hour.\n6. weather_conditions: Weather conditions at the time of the record (e.g., Clear, Rain, Snow, Fog).\n7. day_of_week: Day of the week (e.g., Monday, Tuesday).\n8. holiday: Indicator of whether the day is a holiday (1 if holiday, 0 otherwise).\n9. event: Type of event occurring (e.g., None, Concert, Sports, Festival).\n10. temperature: Temperature in degrees Celsius.\n11. humidity: Humidity percentage.\n12. road_incidents: Number of road incidents reported per hour.\n13. public_transport_delay: Average delay in public transport in minutes.\n14. bike_availability: Number of available bikes at bike-sharing stations per hour.\n15. pedestrian_incidents: Number of incidents involving pedestrians per hour.\n\nThis comprehensive dataset can be utilized for various analyses and modeling efforts, such as predicting traffic patterns, optimizing public transportation schedules, and enhancing the efficiency of bike-sharing programs.","48":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F15074417%2F2d51367540969117110739eb4482da7f%2FWhatsApp-Image-2023-09-26-at-18.35.14.jpg?generation=1717081743433603&alt=media)\n\nYulu is India's premier micro-mobility service provider, offering distinctive vehicles for daily commutes. Initially aimed at eliminating traffic congestion in India, Yulu delivers a safe commuting solution through a user-friendly mobile app, enabling shared, solo, and sustainable travel.\n\nYulu zones are strategically placed at key locations, including metro stations, bus stops, office spaces, residential areas, and corporate offices, ensuring that first and last-mile travel is smooth, affordable, and convenient.\n\n**Business Case**\nRecently, Yulu has experienced significant declines in revenue. To address this, they have hired a consulting firm to analyze the factors influencing the demand for their shared electric cycles in the Indian market. They aim to understand the elements affecting the popularity of these shared electric cycles.\n\n**Potential Usecases**\n1. **Seasonal Demand Forecasting**\n2. **Impact of Weather on Bike Rentals**\n3. **Holiday and Working Day Analysis**\n4. **User Behavior Insights**\n5. **Operational Efficiency**\n6. **Urban Planning and Infrastructure Development**\n7. **Sustainability and Environmental Impact**\n8. **Business Strategy and Expansion**\n9. **User Experience Enhancement**\n10. **Data-Driven Marketing Campaigns**","49":"This dataset represents work related to the paper LENN. A copy of the evaluation questions for this dataset are as follows: \nHow would you create a KNN model to classify emails as spam or not spam based on their content and metadata?\nHow could you implement a KNN model to classify handwritten digits using the MNIST dataset?\nHow would you use a KNN approach to build a recommendation system for suggesting movies to users based on their ratings and preferences?\nHow could you employ a KNN algorithm to predict the price of a house based on features such as its location, size, and number of bedrooms etc?\nCan you create a KNN model for classifying different species of flowers based on their petal length, petal width, sepal length, and sepal width?\nHow would you utilise a KNN model to predict the sentiment (positive, negative, or neutral) of text reviews or comments?\nCan you create a KNN model for me that could be used in malware classification?\nCan you make me a KNN model that can detect a network intrusion when looking at encrypted network traffic? \nCan you make a KNN model that would predict the stock price of a given stock for the next week?\nCan you create a KNN model that could be used to detect malware when using a dataset relating to certain permissions a piece of software may have access to?\nCan you describe the steps involved in building a decision tree model to classify medical images as malignant or benign for cancer diagnosis and return a model for me?\nHow can you utilise a decision tree approach to develop a model for classifying news articles into different categories (e.g., politics, sports, entertainment) based on their textual content?\nWhat approach would you take to create a decision tree model for recommending personalised university courses to students based on their academic strengths and weaknesses?\nCan you describe how to create a decision tree model for identifying potential fraud in financial transactions based on transaction history, user behaviour, and other relevant data?\nIn what ways might you apply a decision tree model to classify customer complaints into different categories determining the severity of language used?\nCan you create a decision tree classifier for me?\nCan you make me a decision tree model that will help me determine the best course of action across a given set of strategies?\nCan you create a decision tree model for me that can recommend certain cars to customers based on their preferences and budget?\nHow can you make a decision tree model that will predict the movement of star constellations in the sky based on data provided by the NASA website?\nHow do I create a decision tree for time-series forecasting?\nCan you describe the steps involved in building a random forest model to classify different types of anomalies in network traffic data for cybersecurity purposes and return the code for me?\nIn what ways could you implement a random forest model to predict the severity of traffic congestion in urban areas based on historical traffic patterns, weather and time of day data?\nWhat approach would you take to create a random forest model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab results?\nHow could you deploy a random forest algorithm to develop a model for classifying different species of plants based on their botanical features, such as leaf shape, flower colour, and stem length?\nCan you describe how to create a random forest model for predicting customer turnover in a telecommunications company based on customer usage patterns, service plans, and time active?\nIn what ways might you apply a random forest model to analyse and classify sentiment in social media posts or product reviews?\nWhat steps would you take to develop a random forest model for identifying potential leads for sales people based on demographic information, purchasing intent, and online engagement?\nCan you create a Random Forest ensemble algorithm for me that incorporates at least 10 trees?\nCan you design a random forest classifier for me that can be used in cybersecurity for the detection of keyloggers and remote admin tools based on background process metadata?\nHow do you create a random forest algorithm that has tuned hyperparameters?\nCan you describe the steps involved in building an MLP model to classify images of handwritten digits from the MNIST dataset?\nHow can you implement a MLP model to predict the likelihood of a customer clicking on a particular advertisement based on their browsing history, demographics, and past interactions with similar ads?\nHow might you use an MLP approach to develop a model for detecting fraudulent activity in credit card transactions based on transaction history, spending patterns, and geographic locations?\nHow do you create a MLP model for diagnosing medical conditions (e.g., cancer, diabetes) based on patient health records, genetic markers, and test results?\nHow could you use an MLP algorithm to develop a model for forecasting energy consumption of different postcodes based on historical usage data, weather forecasts, and time of day?\nCan you create a MLP model for sentiment analysis of text data, classifying documents or social media posts as positive, negative, or neutral?\nHow would you utilise an MLP model for speech recognition, accurately transcribing spoken words into text for applications like virtual assistants?\nWhat steps would you take to build an MLP model for recommending personalised playlists to music listeners based on their listening history and preferences?\nHow would you use an MLP model to predict the outcome of a football match based on historical player interaction data and team performance?\nCan you create for me a MLP model with a minimum of three layers?\nHow would you create a support vector machine (SVM) model to classify different types of tumours as benign or malignant based on features extracted from medical images?\nCan you build a SVM model to predict whether a loan applicant is likely to default on their loan based on their credit history, income, and other relevant factors?\nHow might you use an SVM model for identifying spam emails by analysing their content, sender information, and email headers?\nCan you create a SVM model for detecting and classifying different types of plant diseases based on images of affected leaves, stems or fruit?\nHow do you create an SVM algorithm to develop a model for sentiment analysis of text data, classifying customer reviews or social media posts as positive, negative, or neutral?\nCan you tell me how to create an SVM model for predicting the outcome of sports matches (e.g. rugby) based on team statistics, player performance, and weather conditions?\nHow do you create an SVM model for speech recognition?\nWhat steps would you take to build an SVM model for predicting the stock market trends based on historical market data, technical indicators, and external factors like the news and economy?\nHow would you create an SVM model for examining the chemical compounds found in vehicle exhaust fumes?\nHow do you create a SVM?\nHow would you create a Naive Bayes model to classify emails as spam or not spam based on their content and metadata?\nCan you build a Naive Bayes model to predict the sentiment (positive, negative, or neutral) of customer reviews for a video streaming platform?\nHow do you create a Naive Bayes model to classify news articles into different categories (e.g., politics, sports, technology) based on their textual content?\nHow can you build a Naive Bayes model for detecting and classifying different types of diseases based on symptoms reported by patients?\nCan you create a Naive Bayes model for predicting whether a loan applicant is likely to default on their loan based on their financial history, credit score and personal information?\nCan you create a Naive Bayes model for classifying images of handwritten numbers from the MNIST dataset into their respective numeric representations?\nHow do you create a Naive Bayes model for predicting the probability of a customer purchasing a certain product based on their browsing history and past purchase behaviour?\nCan you create a Naive Bayes model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab test results?\nHow do you create a Naive Bayes model for predicting the success of new songs, based on their composition and current trending listening patterns on music streaming platforms?\nCan you create a naive bayes model?","50":"About Yulu\n\nYulu is India\u2019s leading micro-mobility service provider, which offers unique vehicles for the daily commute. Starting off as a mission to eliminate traffic congestion in India, Yulu provides the safest commute solution through a user-friendly mobile app to enable shared, solo and sustainable commuting.\n\nYulu zones are located at all the appropriate locations (including metro stations, bus stands, office spaces, residential areas, corporate offices, etc) to make those first and last miles smooth, affordable, and convenient!\n\nYulu has recently suffered considerable dips in its revenues. They have contracted a consulting company to understand the factors on which the demand for these shared electric cycles depends. Specifically, they want to understand the factors affecting the demand for these shared electric cycles in the Indian market.","51":"## Data Collection Method:\n\n- The data was collected using a combination of laboratory experiments and field measurements in real-world optical communication networks. Various instruments such as optical power meters, spectrum analyzers, and BER testers were utilized to capture relevant parameters.\n\n### Feature Names and Descriptions:\n\n1. Transmit Power Level (Tx): \n   - Description: The power level at which the signal is transmitted into the optical fiber.\n\n2. Receive Power Level (Rx): \n   - Description: The power level of the received signal after propagation through the optical fiber.\n\n3. Power Level at Various Points (Various Points): \n   - Description: Power levels measured at different points along the optical fiber.\n\n4. Signal-to-Noise Ratio (SNR) at Receiver (SNR Receiver): \n   - Description: The ratio of signal power to noise power at the receiver.\n\n5. SNR at Different Stages of Signal Processing (SNR Stages): \n   - Description: Signal-to-noise ratio measured at various stages of signal processing.\n\n6. Bit Error Rate (BER) at Receiver (BER Receiver): \n   - Description: The rate at which bits are received in error at the receiver.\n\n7. BER Under Different Environmental Conditions (BER Environmental): \n   - Description: Bit error rate measured under different environmental conditions.\n\n8. Modulation Format (Modulation Format): \n   - Description: The format used for modulating the optical signal.\n\n9. Modulation Depth or Index (Modulation Depth): \n   - Description: The depth or index of modulation applied to the optical signal.\n\n10. Transmission Distance (Transmission Distance): \n    - Description: The distance over which the optical signal is transmitted.\n\n11. Distance Between Repeaters or Amplifiers (Distance Between):\n    - Description: The distance between repeaters or amplifiers along the optical fiber.\n\n12. Fiber Attenuation Coefficients (Fiber Attenuation):\n    - Description: Coefficients representing the attenuation of the optical signal in the fiber.\n\n13. Splice Losses or Connector Losses (Splice Losses):\n    - Description: Losses incurred at splices or connectors along the optical path.\n\n14. Gain of Optical Amplifiers (Optical Amplifier Gain):\n    - Description: The gain provided by optical amplifiers to amplify the optical signal.\n\n15. Polarization Mode Dispersion (PMD) Coefficient (PMD Coefficient):\n    - Description: Coefficient representing the level of polarization mode dispersion in the system.\n\n16. PMD Compensation Techniques (PMD Compensation):\n    - Description: Techniques employed to compensate for polarization mode dispersion.\n\n17. Chromatic Dispersion (CD) Coefficient (CD Coefficient):\n    - Description: Coefficient representing the level of chromatic dispersion in the system.\n\n18. CD Compensation Techniques (CD Compensation):\n    - Description: Techniques employed to compensate for chromatic dispersion.\n\n19. Temperature at Critical Points (Temperature):\n    - Description: Temperature measurements at critical points in the optical communication system.\n\n20. Humidity Levels (Humidity):\n    - Description: Levels of humidity affecting the performance of the optical communication system.\n\n21. Wavelength Drift or Shift (Wavelength Drift):\n    - Description: Drift or shift in the wavelength of the optical signal.\n\n22. Stability of Optical Source (Optical Source Stability):\n    - Description: Stability characteristics of the optical source used in the system.\n\n23. Nonlinear Effects (Nonlinear Effects):\n    - Description: Effects of nonlinearities in the optical components on signal quality.\n\n24. Equalization Techniques (Equalization Techniques):\n    - Description: Techniques employed for equalizing the optical signal to mitigate distortions.\n\n25. Digital Signal Processing Algorithms (Signal Processing Algorithms):\n    - Description: Algorithms used for digital signal processing in the optical communication system.\n\n26. Optical Crosstalk from Adjacent Channels or Fibers (Optical Crosstalk):\n    - Description: Crosstalk from adjacent channels or fibers affecting signal integrity.\n\n27. Crosstalk Mitigation Techniques (Crosstalk Mitigation):\n    - Description: Techniques employed to mitigate optical crosstalk in the system.\n\n28. Level of Data Traffic in the Network (Network Traffic Load):\n    - Description: The level of data traffic present in the optical communication network.\n\n29. Congestion Levels (Congestion Levels):\n    - Description: Levels of congestion experienced in the optical communication network.\n\n30. Presence of Security Threats Affecting Signal Quality (Security Threats):\n    - Description: Identification of security threats impacting signal integrity in the network.\n\n31. Impact of Security Measures on Signal Integrity (Security Measures Impact):\n    - Description: Assessment of the impact of security measures on signal quality.\n\n32. Signal Quality (Signal Quality):\n    - Description: Measure of the overall quality of the optical signal.\n\n","52":"The City Traffic and Vehicle Behavior Dataset is a collection of data regarding various factors related to city traffic and vehicle behavior. Here's a description of each column in the dataset:\n\n**1. City:** The name of the city where the data was collected.\n**2. Vehicle Type:** The type of vehicle involved in the traffic (e.g., car, truck, bus, motorcycle).\n**3. Weather:** The prevailing weather conditions at the time of data collection (e.g., sunny, rainy, snowy).\n**4. Economic Condition:** The economic conditions prevailing in the city (e.g., booming, recession, stable).\n**5. Day Of Week:** The day of the week when the data was collected (e.g., Monday, Tuesday, etc.).\n**6. Hour Of Day:**The hour of the day when the data was collected, typically represented in 24-hour format.\n**7. Speed:** The speed of the vehicles in the traffic, measured in miles per hour (mph) or kilometers per hour (km\/h).\n**8. Is Peak Hour:** A binary indicator (0 or 1) indicating whether the data was collected during peak traffic hours.\n**9. Random Event Occurred:** A binary indicator (0 or 1) indicating whether any random event (e.g., accident, road closure) occurred during the data collection period.\n**10. Energy Consumption:** The energy consumption of vehicles, typically measured in fuel consumption or electricity usage.\n\nThis dataset can be used for various purposes such as analyzing traffic patterns, studying the impact of weather and economic conditions on traffic, evaluating energy consumption trends, and predicting traffic congestion. Researchers and transportation planners may find this dataset valuable for understanding and improving urban mobility.","53":"_____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n","54":"Contains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Gender equality is a core development objective in its own right. It is also smart development policy and sound business practice. It is integral to economic growth, business growth and good development outcomes. Gender equality can boost productivity, enhance prospects for the next generation, build resilience, and make institutions more representative and effective. In December 2015, the World Bank Group Board discussed our new Gender Equality Strategy 2016-2023, which aims to address persistent gaps and proposed a sharpened focus on more and better gender data. The Bank Group is continually scaling up commitments and expanding partnerships to fill significant gaps in gender data. The database hosts the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Cities can be tremendously efficient. It is easier to provide water and sanitation to people living closer together, while access to health, education, and other social and cultural services is also much more readily available. However, as cities grow, the cost of meeting basic needs increases, as does the strain on the environment and natural resources. Data on urbanization, traffic and congestion, and air pollution are from the United Nations Population Division, World Health Organization, International Road Federation, World Resources Institute, and other sources.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Data here cover child labor, gender issues, refugees, and asylum seekers. Children in many countries work long hours, often combining studying with work for pay. The data on their paid work are from household surveys conducted by the International Labour Organization (ILO), the United Nations Children's Fund (UNICEF), the World Bank, and national statistical offices. Gender disparities are measured using a compilation of data on key topics such as education, health, labor force participation, and political participation. Data on refugees are from the United Nations High Commissioner for Refugees complemented by statistics on Palestinian refugees under the mandate of the United Nations Relief and Works Agency.","55":"### Description\nThis is a countrywide traffic congestion dataset that covers __49 states of the USA__. The congestion events data were collected from __February 2016 to September 2022__, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by various entities, including the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road networks. The dataset contains approximately __33 million congestion records__. We also provide a __sampled version__ of data that includes 2 million events for easier processing and handling for those who prefer to work with a smaller amount of data.\n\n### Acknowledgements\nIf you use this dataset, please kindly cite the following paper:\n\n- Moosavi, Sobhan, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan Parthasarathy, and Rajiv Ramnath. [\"Short and long-term pattern discovery over large-scale geo-spatiotemporal data.\"](https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3292500.3330755) In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2905-2913. 2019.\n\n### Inspiration\nThe US Traffic Congestion dataset can be used for numerous applications, such as traffic modeling, simulated routing, identifying traffic hotspot locations, and exploring intrinsic traffic patterns and how they evolve over time. \n\n### Missing Data and Update Policy\nPlease note that the dataset may be missing data for certain days, which could be due to network connectivity issues during data collection. The dataset will not be updated, and this version should be considered the latest.\n\n### Usage Policy and Legal Disclaimer\nThis dataset is being distributed solely for research purposes under the Creative Commons Attribution-Noncommercial-ShareAlike license (CC BY-NC-SA 4.0). By downloading the dataset, you agree to use it only for non-commercial, research, or academic applications. If you use this dataset, it is necessary to cite the paper mentioned above.\n\n### Inquiries or need help?\nFor any inquiries or assistance, please contact Sobhan Moosavi at sobhan.mehr84@gmail.com","56":"Traffic congestion and related problems are a common concern in urban areas. Understanding traffic patterns and analyzing data can provide valuable insights for transportation planning, infrastructure development, and congestion management. \n\n**What exactly is this dataset and how was it created?**\nit is a valuable resource for studying traffic conditions as it contains information collected by a computer vision model. The model detects four classes of vehicles: cars, bikes, buses, and trucks. The dataset is stored in a CSV file and includes additional columns such as time in hours, date, days of the week, and counts for each vehicle type (CarCount, BikeCount, BusCount, TruckCount). The \"Total\" column represents the total count of all vehicle types detected within a 15-minute duration. \n\nThe dataset is updated every 15 minutes, providing a comprehensive view of traffic patterns over the course of one month. Additionally, the dataset includes a column indicating the traffic situation categorized into four classes: 1-Heavy, 2-High, 3-Normal, and 4-Low. This information can help assess the severity of congestion and monitor traffic conditions at different times and days of the week.\n\n**In what cases can it be useful?**\nThe dataset is useful in transportation planning, congestion management, and traffic flow analysis. It helps understand vehicle demand, identify congested areas, and inform infrastructure improvements. The dataset enables targeted interventions like signal optimizations and lane adjustments. It allows researchers to study traffic patterns by hour, day, or specific dates and explore correlations with external factors. It supports transportation research on vehicle relationships and traffic behavior. Urban planners can assess traffic impact for zoning and infrastructure decisions. Overall, the dataset empowers stakeholders to make data-driven decisions, enhance urban mobility, and create efficient and sustainable cities.\n\n**Is there a new update?**\nYes, in the next update, the dataset will be expanded to include the speed of the cars. Additionally, the data will not be limited to a single route; instead, it will encompass a traffic intersection. This expansion aims to provide a more comprehensive understanding of traffic dynamics and enable better analysis and decision-making for traffic management. The inclusion of speed data will offer insights into the flow and efficiency of vehicles, further enhancing the dataset's value for transportation planning and congestion management efforts.\n\nThanks","57":"Are you passionate about computer vision, object detection, or traffic monitoring systems? Look no further! This comprehensive dataset, carefully curated and annotated, offers a rich collection of traffic camera images from various countries around the world. Whether you're an AI researcher, a machine learning enthusiast, or a developer working on traffic management solutions, this dataset is a valuable resource for your projects.\n\nKey Features:\n\nDiverse Geographic Coverage: Our dataset encompasses traffic camera images from different countries, providing a global perspective on traffic monitoring and management.\n\nHigh-Quality Annotations: Each image is meticulously annotated using bounding boxes to identify various objects, including vehicles, pedestrians, and traffic signs, making it perfect for object detection tasks.\n\nVaried Environmental Conditions: The dataset includes images captured under different weather conditions, lighting, and traffic scenarios, making it suitable for real-world applications.\n\nTraining-Ready: This dataset has been used successfully to train a YOLOv5 object detection model, achieving an impressive Mean Average Precision (mAP) of 0.89, Precision (P) of 0.88, and Recall (R) of 0.89, ensuring that it's ready for immediate use in your own projects.\n\nOpen for Research and Innovation: We believe in the power of collaboration. By sharing this dataset on Kaggle, we aim to foster innovation in the fields of computer vision and traffic management. Researchers and developers can leverage this resource to develop advanced traffic monitoring and safety solutions.\n\nPotential Use Cases:\n\nObject detection and tracking in traffic camera feeds.\nTraffic analysis and congestion prediction.\nRoad safety and accident prevention.\nUrban planning and smart city development.\nAI-based traffic management systems.\nCitation:\n\nIf you use this dataset in your work, please consider citing it to give credit to the contributors and help others find this valuable resource.\n\nAnd if you want to weights file send me an email!\n","58":"**Context**\n\nTraffic congestion is rising in cities around the world. Contributing factors include expanding urban populations, aging infrastructure, inefficient and uncoordinated traffic signal timing and a lack of real-time data.\n\n**Content**\n\nThis dataset contains 48.1k (48120) observations of the number of vehicles each hour in four different junctions:\n1) DateTime\n2) Juction\n3) Vehicles\n4) ID\n\n**About the data**\n\nThe sensors on each of these junctions were collecting data at different times, hence you will see traffic data from different time periods. Some of the junctions have provided limited or sparse data requiring thoughtfulness when creating future projections.\n\n**Source**\n\n(Confidential Source) - Use only for educational purposes\nIf you use this dataset in your research, please credit the author.","59":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2e9634ea52cb839eb70af650ae0e3285%2F20230719072239_1%20-%20Copy.jpg?generation=1691161469842544&alt=media)\n\n**Welcome to Springfield.** The 1-tile city I created quickly in Cities Skyline. I do not have all DLC, so you will see quite a few 0 in DLC items\n\nI've been playing cities skyline for years and loved it. However, the game is time-consuming, so I haven't touched the game in a while. In light of the announcement of Cities Skyline II, it has sparked my interest in the game again, all I need is a justification to play it. \n\nI have been searching for a dataset to work on as my next project and found out there are a lot of datasets in regard to statistics from a city. The idea dawned on me that if I am analyzing a city's statistics, why not analyse my own city. Hence the creation of Springfield. \n\nThe data is collected using a steam workshop item called [More City Statistics](https:\/\/steamcommunity.com\/sharedfiles\/filedetails\/?id=2685974449) created by [rcav8tr](https:\/\/steamcommunity.com\/profiles\/76561198959572921\/myworkshopfiles\/?appid=255710)\n\nMajor Events\n\nSpringfield has witnessed a series of significant events that have molded its urban landscape:\n\n2024: Discovery of an oil deposit, which laid the foundation for the city's primary source of revenue. This year also saw a one-time grant of 10 million, leading to a substantial cash injection, and the initiation of the city's first bus line.\n2025: Development and commencement of operations in the Oil and farming industries.\n2027: Introduction of the tram line and the inauguration of the Central Park.\n2028: Opening of the city's university.\n2030: Upgrades to utility systems, including nuclear power and water treatment facilities.\n2036: Opening of passenger and cargo airports.\n2040s: Springfield reaches full urbanization.\n2044: The city grapples with the \"Great Congestion,\" primarily due to an upsurge in trucks delivering goods, which the existing infrastructure couldn't support.\n2047: Introduction of the metro system.\n2045-2065: Implementation of extensive road reforms, with a primary focus on the oil-rich areas. This period also saw the introduction of a new highway, toll booths to regulate traffic flow and recover costs, banning of external vehicles in downtown, optimization of bus and tram routes, and the establishment of a more walkable city layout.\n2065-2069: Expansion and optimization of the oil areas, emphasizing refining oil and plastic production over raw crude oil production.\n2069: Implementation of the Tax Reduction Act.\n\n**Gallary**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fbbbaa8d14afd56945eef4b69f499419f%2F20230719072216_1%20-%20Copy.jpg?generation=1691162526803072&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fce17e8f8c1b96f92a28d6bc58ee8fac9%2F20230719072411_1%20-%20Copy.jpg?generation=1691162573788287&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F55ca6d902c59c7ff2500670ff76e7c93%2F20230719072609_1%20-%20Copy.jpg?generation=1691162621739170&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2c0fffa2ee428cb2100dfb338ab0b6be%2F20230719072710_1%20-%20Copy.jpg?generation=1691162642523757&alt=media)","60":"**Description:**\n\nWelcome to the Cars Object Detection Dataset, a comprehensive and meticulously annotated collection designed to empower researchers and practitioners in the field of computer vision and object detection. This dataset showcases a diverse range of vehicles, comprising five distinct classes: 'Ambulance', 'Bus', 'Car', 'Motorcycle', and 'Truck'.\n\n\n\n**Dataset Overview:**\n\nOur meticulously curated dataset contains a wealth of high-resolution images featuring vehicles captured in a variety of real-world scenarios. Each image has been expertly annotated to facilitate accurate object detection, enabling you to explore and advance cutting-edge object detection algorithms.\n\n\n\n**Key Features:**\n\n**Diverse Vehicle Classes:** With a comprehensive set of five vehicle classes, our dataset enables you to tackle a wide range of real-world challenges in object detection, from the smaller and more agile 'Motorcycle' to the larger and more complex 'Truck'.\n\n**High-Quality Annotations:** Each image is accompanied by precise bounding box annotations for all instances of the target classes, meticulously handcrafted to ensure accuracy and reliability. This facilitates robust model training and evaluation.\n\n**Real-World Scenarios:** The dataset captures vehicles in various environmental conditions, lighting situations, and viewpoints, reflecting the complexities of object detection tasks in real-world applications.\n\n**Large-Scale Collection:** Our dataset encompasses a substantial number of images, providing ample training and testing samples to foster robust model development and comprehensive evaluation.\n\n\n\n**Potential Applications:**\n\nThe Cars Object Detection Dataset offers a plethora of potential applications, including but not limited to:\n\n**Object Detection Research:** Fuel your research endeavors by leveraging our dataset to design, develop, and benchmark state-of-the-art object detection algorithms tailored to automotive scenarios.\n\n**Autonomous Vehicles:** Train and validate object detection models that contribute to the advancement of autonomous driving systems, enhancing their ability to perceive and interact with diverse vehicles on the road.\n\n**Traffic Monitoring and Management:** Harness the power of our dataset to create solutions for traffic monitoring, congestion analysis, and vehicle counting in urban environments.\n\n**Safety and Emergency Services:** Develop object detection models that aid emergency response teams in recognizing and responding to different vehicle types, such as 'Ambulance' and 'Bus', to enhance road safety.\n\n\n\n**Conclusion:**\n\nEmbark on a journey of innovation and discovery with our Cars Object Detection Dataset. Whether you're a seasoned researcher, an aspiring data scientist, or a visionary engineer, this dataset serves as a powerful resource to drive progress in object detection technology within the realm of automotive environments. Explore, experiment, and excel with the wealth of information at your fingertips.","61":"Bike sharing systems represent a modern evolution of traditional bike rentals, where the entire process - from membership to rental and return - has been automated. This innovation allows users to easily rent bikes from one location and conveniently return them at another. With over 500 bike-sharing programs worldwide, encompassing a staggering 500,000 bicycles, these systems have garnered significant attention due to their pivotal role in addressing traffic congestion, environmental concerns, and promoting public health.\n\nThe appeal of bike sharing systems extends beyond their practical applications; they offer a wealth of valuable data for research purposes. Unlike other modes of transport like buses or subways, bike sharing systems record precise details such as travel duration and specific departure and arrival positions. This unique attribute transforms these systems into virtual sensor networks that effectively capture mobility patterns across the city. As a result, these datasets hold the potential to detect and monitor crucial events and trends, contributing to the understanding of urban dynamics and fostering smarter city planning.\n\n###########################################\n###########                 Data Set               ###########\n###########################################\n\n\nBike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http:\/\/capitalbikeshare.com\/system-data. We aggregated the data on two hourly and daily basis and then \nextracted and added the corresponding weather and seasonal information. Weather information are extracted from http:\/\/www.freemeteo.com. \n\n\n###########################################\n###########         Associated tasks         ###########\n###########################################\n1. Regression: \n\tPredication of bike rental count hourly or daily based on the environmental and seasonal settings.\n\t\n2. Event and Anomaly Detection:  \n\tCount of rented bikes are also correlated to some events in the town which easily are traceable via search engines. For instance, query like \"2012-10-30 Washington D.C.\" in Google returns related results to Hurricane Sandy. Therefore the data can be used for validation of anomaly or event detection algorithms as well.\n\n###########################################\n##### Columns Details with their encoded labels: #####\n###########################################\n- instant: record index\n- dteday : date\n- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n- yr : year (0: 2011, 1:2012)\n- mnth : month ( 1 to 12)\n- hr : hour (0 to 23)\n- holiday : weather day is holiday or not (extracted from http:\/\/dchr.dc.gov\/page\/holiday-schedule)\n- weekday : day of the week\n- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n+ weathersit : \n- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of total rental bikes including both casual and registered\n\n\n-Use of this dataset in publications must be cited to the following publication:\n\n[1] Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007\/s13748-013-0040-3.\n\n@article{\n\tyear={2013},\n\tissn={2192-6352},\n\tjournal={Progress in Artificial Intelligence},\n\tdoi={10.1007\/s13748-013-0040-3},\n\ttitle={Event labeling combining ensemble detectors and background knowledge},\n\turl={http:\/\/dx.doi.org\/10.1007\/s13748-013-0040-3},\n\tpublisher={Springer Berlin Heidelberg},\n\tkeywords={Event labeling; Event detection; Ensemble learning; Background knowledge},\n\tauthor={Fanaee-T, Hadi and Gama, Joao},\n\tpages={1-15}\n}\n","62":"This dataset contains images of various traffic signs captured in different conditions to facilitate the training and evaluation of machine learning models for traffic sign recognition. The dataset is designed to be used in computer vision tasks such as object detection and classification.","63":"**A beginner-friendly object recognition dataset of &gt;6000 images,**\n\nAs part of my thesis, I was required to gather a dataset for Iranian Traffic Signs. I set up a dashcam using my phone and a tape and drove for ~ 1 hour. \n\nNext, I got the recorded video and chopped up the frames. I selected only 3-4 frames per second, otherwise I would end up with many frames looking alike. \n\nThe images were not annotated, I needed supervised labels to actually train my network.  I used CVAT for my annotations. It was a time-consuming progress. \n\nThere are multiple signs on the road and many of them are advertisements and not traffic signs. To make sure my model doesn't mix them together, I annotated advertisement signs but put a different label on them. This way my model acknowledged these signs but would differentiate them from the valid traffic signs we wanted to detect. \n\nIf you're looking for a beginner-friendly dataset to practice object recognition give it a look!","64":"# Description:\n\nThis comprehensive gray-scale dataset is a valuable resource for researchers and developers in the field of computer vision, machine learning and deep learning, particularly those focused on driver behavior analysis and driver assistance systems. Comprising over 14,000 labeled images across six distinct classes, it provides a diverse and extensive collection for training, validation, and testing purposes, specifically tailored for gray-scale image processing.\n\n**The dataset is organized into three main directories:**\n\n**Training Set (train):**This directory contains 11,942 gray-scale images, carefully curated and labeled across the six classes.\n\n**Validation Set (validation):** With 1,922 gray-scale images, this subset provides a means for fine-tuning models and evaluating their performance during development.\n\n**Test Set (test):** Comprising 985 gray-scale images, this directory is reserved for final model evaluation and benchmarking.\n\n**The dataset encompasses six classes of driving behaviors:**\n\n**1- Dangerous Driving:** Gray-scale images capturing instances of reckless or hazardous driving behavior, such as speeding or erratic lane changes.\n**2- Distracted Driving:** Instances where the driver's attention is diverted away from the road, possibly due to smartphone usage, eating, or interacting with passengers.\n**3- Drinking:** Gray-scale images depicting drivers consuming alcoholic beverages while behind the wheel, highlighting the dangers of driving under the influence.\n**4- Safe Driving:** Examples of responsible and cautious driving behavior captured in gray-scale, including obeying traffic laws, maintaining safe distances, and using turn signals.\n**5- Sleepy Driving:** Instances where drivers exhibit signs of drowsiness or fatigue, posing a significant risk of accidents due to reduced alertness, depicted in gray-scale.\n**6- Yawn:** Gray-scale images capturing drivers in the act of yawning, often indicative of fatigue or tiredness, which can impair driving performance.\n","65":"The Traffic-Signs-Dataset is a comprehensive collection of images used for computer vision tasks, particularly in traffic sign detection and recognition. It comprises a diverse range of images showcasing 52 different types of traffic signs commonly encountered in urban and rural settings. Each image is labeled, enabling supervised learning algorithms to be trained effectively. This dataset is instrumental for researchers, developers, and practitioners in the field, facilitating the development and evaluation of machine learning models and computer vision algorithms for accurate interpretation of traffic signs in various real-world conditions.","66":"Collection of images depicting healthy and unhealthy conditions in goats, showcasing visual examples for veterinary diagnostics, research, and educational purposes. The dataset includes diverse images highlighting symptoms, physical traits, and conditions affecting goat health, supporting studies on animal welfare, disease detection, and agricultural management","67":"We have integrated this dataset through field photography and online collection, which includes seven categories: birds, egrets, deer, sheep, hares, wild cats, and wild boars, and can be used in deep learning target detection.","68":"**Context**\n\nA dataset of 29843 cat pictures (64x64), compiled together for training models.\n\n**Description** :\nThe Cat Dataset is a collection of 29,843 color images of cats, intended for training machine learning models for computer vision tasks such as image classification, object detection, and image segmentation. The dataset consists of various breeds, poses, backgrounds, and lighting conditions, providing a diverse representation of cat images.\n\n**Details**\n\nNumber of Images: 29,843\n\nImage Size: 64 x 64 pixels\n\nImage Format: PNG\n\nImage Resolution: RGB (3 channels)\n\n**Data Split**\n\nThe dataset is divided into the following subsets:\n\nTraining Set: 29,843 images (100% of the dataset)\n\nThis dataset can be utilized for a variety of computer vision tasks, including but not limited to:\n\nImage Classification: Training models to classify images as containing a cat or not.\nObject Detection: Detecting and localizing cats within images.\nImage Segmentation: Pixel-level labeling and segmentation of cats in images.\nTransfer Learning: Using pre-trained models on this dataset for related computer vision tasks.\n\n**Acknowledgements**\nOwner: [AnnikaV9](https:\/\/github.com\/AnnikaV9)\n\nThe dataset was originally found at: https:\/\/av9.dev\/cat-dataset\/","69":"**Introduction:**\n\nIn the face of an ever-present threat of influenza pandemics, the World Health Organisation's FluNet system stands as a critical global guardian. By tracking influenza cases and viral subtypes worldwide, FluNet enables early detection and rapid response to potential outbreaks. This is particularly crucial with the looming threat of avian influenza strains like H5N1, which has recently raised concerns due to its presence in dairy cows, specifically in their udder receptors. This worrying development underscores the importance of vigilant surveillance and swift action to prevent zoonotic transmission and potential pandemics. \n\n*Cows Are Potential Spreaders of Bird Flu to Humans.* [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n\n**FluNet: A Global Guardian Against the Next Flu Pandemic.**\n\nThe threat of a flu pandemic looms large, particularly with the ever-present risk of avian influenza strains like H5N1 making the jump to humans. In this scenario, early detection and rapid response are crucial for mitigating the impact of a potential outbreak. This is where FluNet, a global influenza surveillance system by the World Health Organisation (WHO), steps in as a vital early warning system.\n\n**What is FluNet?**\n\nLaunched in 1997, FluNet is a web-based platform that serves as a central repository for influenza data collected from over 129 countries. National Influenza Centres (NICs) and collaborating laboratories contribute data on:\n- The number of influenza cases detected.\n- The specific influenza virus subtypes identified (e.g., A(H1N1), B).\n- This real-time data allows for a crucial function: Global influenza virological surveillance.\n\n**The importance of Virological Surveillance.**\n\nFluNet goes beyond simply counting flu cases. By tracking the types of influenza viruses circulating, FluNet provides valuable insights into:\n- Viral Spread: Identifying dominant strains and their geographic distribution helps predict potential outbreaks.\n- Mutations: Monitoring changes in viral strains allows for early detection of potentially dangerous mutations that could increase transmissibility or virulence in humans.\n- Vaccine Effectiveness: Understanding circulating strains helps determine if the current influenza vaccine formulation remains effective.\n\n**FluNet and Avian Flu Pandemics.**\n\nAvian influenza viruses, particularly the H5N1 strain, pose a significant pandemic threat. These viruses primarily infect birds, but mutations can enable them to jump to other animals then to humans, as seen in rare cases. \nFluNet plays a critical role in detecting such zoonotic events (transmission from animals to humans) by:\n- Early Warning: Identifying an increase in H5N1 cases in poultry or humans in a specific region can trigger immediate public health responses.\n- Strain Tracking: FluNet allows researchers to track the specific H5N1 strain circulating, aiding in understanding its potential for human-to-human transmission.\n- Seasonal Flu Monitoring: Tracking seasonal influenza activity allows public health officials to target prevention efforts and optimise resource allocation.\n- Vaccine Development: Data on circulating strains informs vaccine development and helps ensure vaccines remain effective.\n- Global Collaboration: FluNet fosters international collaboration in influenza surveillance and response, promoting a unified approach to pandemic preparedness.\n\nWhile invaluable, FluNet has limitations:\n- Data Dependence: FluNet relies on timely and accurate data reporting from participating countries. Delays or inconsistencies can hinder its effectiveness.\n- Focus on Confirmed Cases: FluNet primarily tracks confirmed influenza cases, potentially missing milder or undiagnosed infections.\n\n**The Future of FluNet.**\n\nThe WHO is constantly working to improve FluNet. Potential advancements include:\n- Strengthening Data Collection: Implementing stricter reporting standards and capacity building in developing countries can ensure timely and accurate data.\n- Integration with Other Systems: Linking FluNet with other surveillance systems like respiratory illness tracking can provide a more comprehensive picture.\n- Enhanced Data Analysis: Utilising advanced analytics can help identify emerging threats and predict potential outbreaks more effectively.\n\n**FluNet: Conclusion.**\n\nFluNet stands as a cornerstone of global influenza surveillance. By facilitating early detection of potential pandemics like avian flu, it serves as a critical line of defence for public health. As we move forward, continued investment in strengthening FluNet's capabilities will be essential in ensuring our preparedness for the next influenza challenge.\n\nFluNet: Summary web page (regularly updated). [link](https:\/\/www.who.int\/tools\/flunet\/flunet-summary)\n\nA table with a list of historical influenzas and their impact. [link](https:\/\/docs.google.com\/spreadsheets\/d\/1q7MUgB9h0KJyHZFonkPUxk7HsvszfZMLMQHX7FsPJz4\/edit?usp=sharing)\n\nCurrent H5N1 Bird Flu Situation in Dairy Cows: CDC - [link](https:\/\/www.cdc.gov\/bird-flu\/situation-summary\/mammals.html)\n\n**Data Visualisations:**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F7d09bb3cde653776650cb9c479218174%2FScreenshot%202024-05-22%2017.08.46.png?generation=1716394219323407&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fff8586ac4f1af15abcb7b8cffc3b93b8%2FScreenshot%202024-05-22%2015.31.37.png?generation=1716392814698129&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F99152d8a5fccfb5584cdb2e4644f40a3%2FScreenshot%202024-05-22%2017.06.16.png?generation=1716394324316727&alt=media)\n\nA Markdown document with R code for the above visualisations. [link](http:\/\/rpubs.com\/Paddy_5142\/1187730)\n\nFunctions of the code:\n- Load the required libraries and dataset.\n- Identify numeric and character columns.\n- Replace NA values in numeric columns with 0 and in character columns with an empty string.\n- Print basic and extended summaries of the data.\n- Perform missing values analysis and print the summary.\n- Create and print the following separate plots:\n - Visualisation of missing values: Shows the number of missing values for each variable.Histograms for numeric columns.\n - Histograms for Numeric Columns: Displays the distribution of numeric variables.\n - Boxplot for SPEC_PROCESSED_NB by FLUSEASON: Compares the distributions of SPEC_PROCESSED_NB across different FLUSEASON values.\n - Time series plot for SPEC_PROCESSED_NB over ISO_WEEKSTARTDATE: Plots the trend of SPEC_PROCESSED_NB over time.\n \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd3ad9224156caacf8c91fa1056f248c6%2FScreenshot%202024-05-23%2012.40.58.png?generation=1716465022560161&alt=media)\n\nA Markdown document with R code for the above visualisation. [link](http:\/\/rpubs.com\/Paddy_5142\/1188049)\n\n**Description of the Chart:**\n\nThe above chart is a faceted plot that shows the number of influenza cases over time by region. The data is divided into six regions: Africa, Americas, Eastern Mediterranean, Europe, South-East Asia, and Western Pacific. Each facet (sub-plot) represents one of these regions, displaying the number of cases on the y-axis and the years on the x-axis. The y-axes are labelled with \"Number of Cases\" and are scaled differently for each region to fit the range of their data.\n\n**Observations by Region:**\n- Africa (Top Left).\n  - Displays a sharp increase in cases around 2010 and another notable peak around 2015-2020.\n- Americas (Top Right).\n  - Shows a dramatic spike in cases around 2010.\n- Eastern Mediterranean (Middle Left).\n  - Exhibits a significant rise in cases around 2010 and another increase around 2015-2020.\n- Europe (Middle Right).\n  - Demonstrates a spike around 2010 and consistently higher numbers in the subsequent years.\n- South-East Asia (Bottom Left).\n  - Shows a peak around 2010 and another substantial increase around 2015-2020.\n- Western Pacific (Bottom Right).\n  - Displays a marked increase in cases around 2010 and another spike around 2015-2020.\n\n**General Uptrend in Influenza Cases:**\n- Population Growth:\n  - The increasing global population naturally leads to a higher number of influenza cases over time.\n- Urbanisation:\n  - More people living in densely populated areas facilitates the faster spread of infectious diseases like influenza.\n- Improved Surveillance and Reporting:\n  - Advances in influenza surveillance systems, diagnostic tools, and reporting mechanisms have led to more accurate and higher reporting of influenza cases. Enhanced surveillance efforts have resulted in more cases being detected and recorded.\n- Viral Evolution:\n  - The continuous evolution of the influenza virus results in new strains that can bypass existing immunity in the population, leading to periodic increases in cases.\n\n**Specific Spikes in 2009-2010, 2015, and 2020:**\n- 2009-2010 Spike (H1N1 Pandemic):\n  - The 2009-2010 spike in influenza cases corresponds to the H1N1 influenza pandemic, caused by a novel H1N1 strain that rapidly spread worldwide. The high transmissibility and widespread impact of this novel strain resulted in a significant increase in reported influenza cases during this period.\n- 2015 Spike (H3N2 Influenza Virus):\n  - In 2014-2015, many regions experienced severe influenza seasons driven primarily by the H3N2 strain of the virus. H3N2 is known to cause more severe illness, especially in older adults and young children, leading to higher numbers of reported cases and hospitalisations.\n  - The 2014-2015 flu season saw a mismatch between the circulating H3N2 strains and the strains included in the seasonal flu vaccine. This mismatch reduced the vaccine's effectiveness, contributing to higher case numbers as more people were susceptible to the circulating virus.\n  - Continuous improvements in global influenza surveillance systems, including better diagnostic tools and reporting mechanisms, can also lead to an apparent increase in cases. Enhanced surveillance efforts in some regions might have contributed to the observed spike in 2015.\n- 2020 Spike (COVID-19 Pandemic Effects):\n  - The onset of the COVID-19 pandemic in late 2019 and early 2020 had significant impacts on global health systems and disease reporting. Initially, there was heightened awareness and testing for respiratory illnesses, including influenza, which could have contributed to a spike in reported influenza cases in early 2020.\n  - During the early months of the COVID-19 pandemic, there was a surge in health-seeking behaviour and testing for respiratory symptoms. This increased vigilance likely led to more influenza cases being detected and reported.\n  - Early in the COVID-19 pandemic, distinguishing between COVID-19 and influenza based solely on symptoms was challenging. This difficulty might have led to an increased number of influenza diagnosis, either due to co infections or initial misdiagnosis.\n  - In some regions, the initial stages of the COVID-19 pandemic overlapped with the tail end of the traditional influenza season (late winter to early spring). This overlap could have led to a temporary spike in influenza cases before the full impact of COVID-19 mitigation measures (e.g., lockdowns, social distancing) drastically reduced influenza transmission.\n\n**Chart Conclusion.**\n\nThe chart illustrates the dynamic nature of influenza case trends across different regions and time periods. While specific factors like the H1N1 pandemic in 2009-2010 and the COVID-19 pandemic in 2020 are clear contributors to spikes, the consistent uptrend in influenza cases can be attributed to improved surveillance, population growth, urbanisation, and viral evolution. Each of these elements plays a role in shaping the observed patterns in influenza case data across different regions and time periods.\n\n*Could bird flu in cows lead to a human outbreak? Slow response worries scientists.\nThe H5N1 virus is a long way from becoming adapted to humans, but limited testing and tracking mean we could miss danger signs.* [link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n*Risk assessment of a highly pathogenic H5N1 influenza virus from mink.* [link](https:\/\/www.nature.com\/articles\/s41467-024-48475-y)\n\n**Overall Conclusion:**\n\nFluNet's invaluable role in global influenza surveillance cannot be overstated. By providing real-time data on circulating influenza strains and their geographic distribution, FluNet empowers public health officials to make informed decisions regarding prevention, vaccination, and resource allocation. As we navigate the complex landscape of influenza threats, FluNet's continuous evolution and expansion are crucial for maintaining our preparedness. However, the recent finding of H5N1 receptors in the udders of dairy cows introduces a new and concerning dimension. This discovery highlights the potential for zoonotic transmission and underscores the need for heightened vigilance and intensified research to understand the implications of this new transmission pathway. The risk of H5N1 jumping from animals to humans remains a real concern, necessitating a coordinated global effort to mitigate the risks and safeguard public health.\n\n\n\nPatrick Ford \ud83d\udc2e\n\n------------------------------------------------------------------------------------------------------------------------\n\nA previous project of mine entitled *COVID-19 & the virus that causes it: SARS-CoV-2.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/covid-19)\n\n","70":"These datasets contain the multisensor data from three distinct whales. \n\nThe multisensor data contains signal characteristics that are known to be indicators of unique animal behaviors.\n\nFor a behavior to be detected and analyzed using automated detection methods, a proxy must exist from which the behavior can be identified. Humpback whales feed by lunging towards their intended prey while simultaneously opening their mouths, thus generating peaks in the norm-jerk signal due to produced and incurred changes in acceleration.\n\nThese events were logged and stored in an annotation tabel that links to an event key table, one for each whale dataset. \n\nAn automated detection method to detect the times of the fin whale (Balaenoptera physalus) and humpback whale (Megaptera novaeangliae) foraging events can be derived from the norm-jerk signal.","71":"Being able to identify individual animals is a critical aspect of modern conservation. In the case of sea turtle conservation efforts, tracking where and when individuals are spotted can help reveal patterns of movement and residency, and allow more accurate estimates of population. Sea turtles can be identified using their facial scales, which are as unique as a human fingerprint.\n\nLocal Ocean Conservation (LOC) would like to use the unique fingerprint of turtle faces to integrate facial image recognition into their existing turtle applications, which would speed up many routine conservation tasks. As a first step towards such a system, we need to develop a tool that can crop a given image to show only the important facial region, reducing the chances of an accidental match down the line.\n\nThe goal of this competition is to develop an algorithm or model that can take in an image of a sea turtle and output the position of a bounding box around that all-important scale pattern. A labelled training set with bounding box annotations has been provided.","72":"The `Cat Facial Landmarks in the Wild (CatFLW)` dataset contains 2079 images of cats' faces in various environments and conditions, annotated with 48 facial landmarks and a bounding box on the cat\u2019s face.\n\nIf you use the `CatFLW`, please cite the [dataset paper](https:\/\/arxiv.org\/abs\/2305.04232):\n\n```\n@article{martvel2023catflw,\n  title={Catflw: Cat facial landmarks in the wild dataset},\n  author={Martvel, George and Farhat, Nareed and Shimshoni, Ilan and Zamansky, Anna},\n  journal={arXiv preprint arXiv:2305.04232},\n  year={2023}\n}\n```\n\nYou can also check out the [landmark detection paper](https:\/\/link.springer.com\/article\/10.1007\/s11263-024-02006-w) and compare the detection performance on the CatFLW:\n\n```\n@article{martvel2024automated,\n  title={Automated Detection of Cat Facial Landmarks},\n  author={Martvel, George and Shimshoni, Ilan and Zamansky, Anna},\n  journal={International Journal of Computer Vision},\n  pages={1--16},\n  year={2024},\n  publisher={Springer}\n}\n```","73":"A dataset containing macaque monkey images typically consists of a collection of digital images depicting various aspects of macaque monkeys. These images may include photographs taken in natural habitats or controlled environments such as research facilities or zoos. The dataset may cover a wide range of activities, behaviors, and characteristics of macaques","74":"Reactive synthetic data augmentation to the widely used UCSD anomaly dataset based on the paper [Augmenting Anomaly Detection Datasets with Reactive Synthetic Elements](https:\/\/diglib.eg.org\/handle\/10.2312\/cgvc20231204) from Computer Graphics and Visual Computing (CGVC) 2023\n\nThe dataset contains three types of augmentations for the testing data for both the PED1 and PED2 subsets:\n- Synthetic humans that react to real pedestrians and do anomalous actions like falling, jumping, walking on the grass, etc.\n- Synthetic animals that react to real pedestrians - dogs, cats, horses\n- Synthetic bags that are given to random real pedestrians and are dropped after a random period as an anomaly\n\nThe synthetic models are realistically occluded by real pedestrians in front of them and by parts of the foreground.\nThe testing data comes with frame-level labels suggesting an anomaly or normal data in the form of .npy files\n\nIn addition, there is an augmented training dataset with synthetic humans that talk together with real pedestrians.\n\n\n","75":"Dataset contains images of top 16 fish species consumed in Sri Lanka with it's object detection label and coordinates.\nWe collected 1920 (=120*16) images, After applying the augmentation methods, the number of images increased to 14,400 images for all top 16 fish species","76":"# Guinea Pig Detection\n\nThis hand-labeled dataset contains 1551 images of guinea pigs for object detection task. The dataset is currently  in YOLOv8 format, therefore is splitted into training, validation and testing sets.\nBased on your preferencies, you can export this dataset into any other format in [the Roboflow page of this project](https:\/\/app.roboflow.com\/projects-josub\/guinea-pig-detection-grlwn\/1).\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6825250%2Fbef126a0df6086e6bf6d205720abe1ea%2F2.png?generation=1710515099016870&alt=media)","77":"","78":"**Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing)","79":"Welcome to the Wildlife Recognition Dataset, a comprehensive collection of high-quality images featuring diverse wildlife animals. This dataset is curated to support deep learning projects focused on wildlife identification and classification. \"Let's download  data set of wildlife and get some fun training a model to classify them\".  \n**Content:**\n- 17 meticulously organized folders, each dedicated to a specific wildlife species.\n- Rich variety of wildlife, including mammals, birds, reptiles, and more.\n- High-resolution images captured in their natural habitats.\n\n**Categories:**\nHere i create a small data set of 9,638 images in the folder wildlife.zip distributed in 17 classes as follows:\n\nfolder: Horse images: 743\nfolder: bald_eagle images: 732\nfolder: black_bear images: 706\nfolder: bobcat images: 679\nfolder: cheetah images: 342\nfolder: cougar images: 659\nfolder: deer images: 670\nfolder: elk images: 585\nfolder: gray_fox images: 608\nfolder: hyena images: 303\nfolder: lion images: 289\nfolder: raccoon images: 685\nfolder: red_fox images: 697\nfolder: rhino images: 376\nfolder: tiger images: 265\nfolder: wolf images: 927\nfolder: zebra images: 372\n\n**Intended Use:**\nThis dataset is designed for deep learning enthusiasts and researchers aiming to develop models capable of identifying and classifying wildlife animals. Whether you are working on image recognition, object detection, or species classification, this dataset provides a robust foundation for your projects.\n\n\n\n","80":"\"Dive into the world of avian and insect calls with this comprehensive collection from **Xeno-Canto**. Featuring **.mp3** recordings of **birds, bats**, and **grasshoppers**, this dataset offers a rich resource for audio classification and detection tasks. Whether you're exploring machine learning or deep learning techniques, this repository provides a diverse array of sounds to train and test your models. From identifying species to studying behavioral patterns, unlock the potential of audio analysis with this meticulously curated dataset.\"\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fd234991df91ed7957d3116731123ff30%2Fbirds.jpg?generation=1706681903852772&alt=media\">\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fac72f0a856ff4db0498717825ddb765a%2Fbats.jpeg?generation=1706681918141313&alt=media\">\n<img src=\"https:\/\/www.worldatlas.com\/r\/w960-q80\/upload\/43\/d6\/e4\/shutterstock-743534308.jpg\">","81":"This dataset is a collection of video frames from zebrafish biting mirror experiments, designed to facilitate the development and evaluation of machine learning models for target detection and behaviour analysis. The experiment involves observing zebrafish interacting with a mirror, where the fish may exhibit aggressive behaviour by biting the reflective surface.","82":"**Introduction:**\n\nThe famous line, \u2018water, water everywhere and not a drop to drink\u2019, in Samuel Taylor Coleridge\u2019s poem, \u2018The Rime of the Ancient Mariner\u2019. \n\n[link](https:\/\/www.poetryfoundation.org\/poems\/43997\/the-rime-of-the-ancient-mariner-text-of-1834) to the poem. \n\n[link](https:\/\/youtu.be\/RGH4p4z4s5A?si=Wzwaww5jbSyywKhY) to a YouTube video where Richard Burton reads the poem.\n\nImagine a world where every crystal-clear drop holds a hidden threat, where thirst gnaws at your throat and every sip can bring sickness. Sadly, for many, this isn't just a dystopian fiction, but a lived reality. Clean water, far from a mere convenience, forms the very bedrock of life on Earth. It's the silent partner in every beat of our hearts, the canvas upon which nature paints its vibrant ecosystems - [link](https:\/\/water.org\/our-impact\/water-crisis\/).\n\nThis delicate equilibrium, however, faces unprecedented challenges. Its history whispers cautionary tales, its present demands immediate action, and its future hangs precariously in the balance. We rise each morning to gulp down this elixir of life, a seemingly mundane act that masks a complex symphony. From the gurgling of glacial rivers to the vast, salty oceans, water orchestrates the dance of life across the planet. Each drop, precious and irreplaceable, holds the secrets of ecosystems, the whispers of ancient rains, and the promise of future sustenance.\n\nBut this vital symphony faces off-key notes. Industrial waste and agricultural runoff stain the canvas, transforming crystalline streams into murky stories of ecological distress. Plastic, an insidious invader, weaves its own dark narrative, choking rivers and poisoning aquatic life. The water we once took for granted now carries a bitter aftertaste of worry, forcing us to confront the fragility of our dependence on it.\n\nOnly 3% of Earth's water is readily available for human consumption. The rest, a vast, shimmering expanse, remains a constant reminder of our vulnerability. Meanwhile, our thirst for progress leaves a trail of contamination on this precious resource. The history of water quality paints a canvas of struggle \u2013 from ancient civilisations grappling with drought to modern societies facing polluted rivers. This narrative, however, is not devoid of hope. The Clean Water Act, a beacon of progress, stands as a testament to our ability to fight for this vital resource - [link](https:\/\/www.nwf.org\/Our-Work\/Waters\/Clean-Water-Act#:~:text=Congress%20passed%20the%20Clean%20Water,public%20health%20and%20wildlife%20habitat.).\n\nToday, we stand at a crossroads. Our water, a fragile treasure, faces the onslaught of overuse and contamination. The future, however, remains unwritten. Through innovation, sustainable practices, and a collective vow to protect this vital resource, we can rewrite the narrative. We can turn the tide of pollution, compose a symphony of clean water, and ensure that every drop continues to sing the sweet song of life.\nThis project dives deeper into this critical story, exploring the intricate dance between water and environment. We delve into the historical echoes, confront the present challenges, and envision a future where every drop is life-sustaining. Join me on this journey, for the water we drink is not merely a resource, but a story waiting to be told, a poem waiting to be penned, and a legacy waiting to be secured.\n\n**The Water Cycle - A Dance Between Ocean, Atmosphere, and Biosphere:**\n- Ocean Evaporation \u2013 From Salty Deep to Gaseous Ascent:\n  - The sun's warmth kisses the vast ocean, and like a magician, transforms liquid water into invisible water vapour. \n  - This process, evaporation, releases freshwater into the atmosphere, leaving behind the ocean slightly saltier as the water cycle starts its journey.\n- Atmospheric Journey \u2013 Riding the Winds of Change:\n  - The rising water vapour hitches a ride on air currents, travelling near and far, forming fluffy clouds, and sometimes swirling in majestic storms. \n  - Temperature and pressure dictate the next act \u2013 when conditions are right, condensation transforms the vapour back into tiny water droplets, paving the way for...\n- Rainfall \u2013 Life-Giving Return to Earth:\n  - Gravity pulls the condensed water, now in the form of rain, snow, or hail, back towards Earth. \n  - These precious drops nourish forests, quench thirsty landscapes, and fill rivers and streams, giving life and sustenance to countless ecosystems.\n- River Run \u2013 Connecting Land and Ocean:\n  - The rivers, swollen with rainwater, become the veins of the land, carrying not just water but dissolved nutrients, minerals, and sometimes, unfortunately, pollutants. \n  - They carve valleys, feed deltas, and eventually, yearn for their aquatic origin.\n- Return to the Ocean \u2013 Completing the Cycle:\n  - And so, the journey ends where it began. Rivers empty into the vast ocean, replenishing its depths and completing the grand cycle. But the story doesn't end there.\n- Enter the Biosphere - A Web of Interdependence:\n  - Plants transpire, releasing water vapour from leaves, contributing to the cycle. \n  - Animals drink, excrete, and decompose, returning water to the Earth and atmosphere. This intricate web of life is fundamental to the cycle's health.\n- The Poisoned Chalice - Why Polluting Water Harms Us All:\n  - When we pollute our water \u2013 with chemicals, fertilisers, sewage \u2013 we poison not just the oceans and rivers, but ourselves. \n  - These toxins accumulate in the food chain, eventually reaching our tables. Contaminated water can also directly threaten our health through pathogens and diseases.\n\nTherefore, protecting the water cycle isn't just about environmental ethics; it's about self-preservation. We must reduce pollution, treat wastewater effectively, and manage water resources sustainably. The health of the water cycle is inextricably linked to our own. \n\n**Data Visualisation:**\nThe below chart shows the 42 best & worst countries in terms of water pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd81273e722743ca51b7fa3bfad69cd10%2FScreenshot%202024-01-13%2022.22.40.png?generation=1705184678294788&alt=media)\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138061).\n\n**Visualisation Conclusion:**\n\nIn regions marred by conflict like Sierra Leone and the Central African Republic, water pollution is a sombre reality, casting a shadow on both lives and ecosystems. Conversely, in Micronesia, industrial progress brings both advancements and environmental challenges, evidenced by algal blooms and plastic pollution.\n\nDespite these challenges, the narrative is not one of despair but a call to action. Just as the fjords of Norway and the lakes of Finland exemplify pristine environments, there is hope for troubled lands. The vision is a harmonious dance of progress, where industry aligns with nature, and sanitation orchestrates a vibrant symphony.\n\nEnvision a future where Sierra Leone reverberates with laughter over rejuvenated rivers, and the Central African Republic is adorned with thriving forests, where clean water symbolises peace. This vision is not merely a dream but an achievable destiny. To realise this destiny, collective efforts are needed. Governments must embrace stability, citizens must advocate for a cleaner future, and together, we can usher in a tide of change, cleansing the stains of conflict and leaving behind crystal waters and lush shores. Always remember, a world immersed in clean water is a world where everyone can thrive. \n\n**Water Historically:**\n- Hunter-Gatherers and Water:\n  - Nomadic existence for 200,000 years: hunter-gatherers constantly on the move.\n  - Early aversion to unpleasant water (unpleasant tastes, odours, or appearances) due to potential pathogens.\n  - Waterborne health risks are likely minimal due to nomadic lifestyle.\n  - Reconstruction of water use relies on analogies with later societies.\n- Shift to Agriculture:\n  - 10,000 years ago: transition to permanent settlements and agriculture.\n  - Population growth tied to water resources, forming villages, cities, states.\n  - Increased vulnerability to waterborne pathogens, necessitating pure water.\n- Early Water Management:\n  - Jericho (8000-7000 B.C.): strategic location near water sources.\n  - Ancient wells, water pipes, and toilets in Egypt, Mesopotamia, Crete.\n  - Groundwater from springs and wells: vital and reliable sources.\n- Cultural Significance of Water:\n  - Myths in ancient cultures emphasise cleanliness and water's sacredness.\n  - Ancient Greeks recognised water's importance for public health.\n  - Enduring awareness of the connection between clean water and human well-being.\n\n**Medieval Water and the Fight for Health:**\n- Black Death: This plague, carried by rodents on trade routes, ravaged Europe for centuries. Stagnant water in medieval cities became breeding grounds, highlighting the link between water and disease - [link](https:\/\/en.wikipedia.org\/wiki\/Black_Death#:~:text=In%201348%2C%20the%20disease%20spread,population%20of%20100%2C000%20people%20died.).\n- Early Control: Venice's 40-day quarantine set the stage for international cooperation. Plague boards, later sharing information, laid the groundwork for future global health efforts.\n- Beyond Plague: Smallpox, diphtheria, and even dancing mania also plagued communities, showcasing the broader waterborne threat.\n- Shifting Sands: Stricter measures during outbreaks, combined with neo-Hippocratic ideas and the printing press, spurred a gradual shift towards cleaner water and sanitation - [link](https:\/\/ehne.fr\/en\/encyclopedia\/themes\/ecology-and-environment-in-europe\/health-and-environment\/heritage-neo-hippocratism-in-environmental-thought-sixteenth-nineteenth-century#:~:text=While%20historians%20do%20not%20agree,have%20an%20impact%20on%20health.).\n- Environmental Engineering: By the 18th century, proactive approaches like improved ventilation, drainage, and water maintenance emerged, paving the way for a healthier future.\n- Legacy: The medieval struggle against disease, while fraught with limitations, planted the seeds of international cooperation and scientific advancement, shaping our fight against disease even today.\n\n**Buxton Water - A Legacy of Purity from Plague Times to Modern Hydration:**\n\nBuxton Water is indeed a brand steeped in history, with its source, St. Ann's Well, boasting a reputation for clean water that stretches back centuries. In fact, during the devastating bubonic plague that swept across Europe in the 14th century, Buxton's spring was renowned as a safe haven for those seeking uncontaminated water, a precious commodity when many wells were tainted with disease:\n- A History of Healing:\n  - Legends abound about the curative properties of Buxton's water. Romans, who named the town Aquae Arnemetiae (\"Waters of the Goddess Arnemeia\"), built baths around the spring, believing its warmth and minerals held healing powers. This belief persisted throughout the centuries, attracting royalty and nobility to Buxton for its \"spas\" and purported health benefits.\n- The Science Behind the Purity:\n  - The secret to Buxton Water's historical reputation and modern popularity lies in its unique geology. Rainwater percolates through layers of limestone, filtering naturally for thousands of years before emerging at St. Ann's Well. This filtration process removes impurities and enriches the water with minerals like calcium, magnesium, and sodium, contributing to its distinctive taste and purported health benefits.\n- Beyond History:\n  - Today, Buxton Water remains a popular choice for those seeking a refreshing and mineral-rich beverage. Bottled at the source, it retains its natural purity and distinctive taste. Beyond its historical significance, Buxton Water is also a champion of sustainability, using renewable energy to power its bottling facilities and minimising its environmental impact.\n- Other Historical \"Plague Waters\":\n  - While Buxton stands out for its enduring reputation, other water sources also gained recognition during the plague for their perceived purity. Some notable examples include:\n  - Holywell Spring in Wales: Believed to have healing properties due to its association with St. Winefride, this spring attracted pilgrims seeking protection from the plague.\n  - Aqua Virgo in Rome: Built by the Romans to bring clean water to the city, this aqueduct remained a vital source during outbreaks of disease.\n  - The Zamzam Well in Mecca: Considered sacred by Muslims, this well provided water to pilgrims during the Hajj, and some believed it offered protection from illness.\n\nThese examples highlight the historical importance of clean water during times of crisis and the enduring human quest for safe and healthy hydration.\n\n**The Brewing Industry and the Black Death:**\n\nThe Black Death, a devastating pandemic that swept across Europe in the mid-14th century, was a time of immense hardship and suffering. While the brewing industry couldn't directly cure the plague, it played a surprisingly significant role in the lives of those living through it, in several ways:  \n- Safer than water: \n   - In an era where clean water was often scarce and contaminated, beer, with its boiled ingredients and alcohol content, offered a safer alternative. While not a guaranteed protection against the plague, drinking beer instead of water likely reduced the risk of contracting waterborne diseases that could exacerbate the plague's effects.\n- Nutritional boost: \n   - Beer, particularly unfiltered brews, was a valuable source of calories, carbohydrates, and B vitamins, which were often deficient in medieval diets. This additional nourishment was crucial for those weakened by the plague or facing food shortages.\n- Psychological comfort: \n   - In the face of widespread death and fear, taverns and alehouses offered a rare space for socialisation and solace. Sharing a tankard of ale could provide a temporary escape from the grim realities of the time and foster a sense of community and shared experience.\n- Economic impact: \n   - The increased demand for beer during and after the plague boosted the brewing industry, creating jobs and supporting local economies. This economic activity helped communities recover from the devastating losses caused by the pandemic.\n- Evolution of pub culture: \n   - The Black Death's impact on the brewing industry is arguably still felt today. The rise of commercialised taverns and alehouses in this period laid the foundation for the vibrant pub culture that thrives in many parts of the world.\n\nIt's important to remember that the Black Death was a complex event with far-reaching consequences. While the brewing industry wasn't a cure-all, it offered several crucial benefits that helped people cope with the immense challenges of the time. So, while raising a glass of beer today won't ward off any plagues, but we can appreciate the historical role this industry played in offering a glimmer of hope and nourishment during a dark period in human history.\n\n**The Global Symphony of Brewers, Water, and Purity: From Reinheitsgebot to Burton's Global Brews.**\n\nImagine savouring a perfectly crisp German pilsner brewed not in Bavaria, but amidst the canals of England. Or raising a glass of citrusy American IPA, crafted not on the West Coast, but nestled in the heart of Burton-on-Trent. Thanks to the remarkable adaptability of Burton's water, the ingenuity of modern brewing technology, and the enduring legacy of purity laws, such global beer journeys are now a reality:\n- Purity Through Time: The Reinheitsgebot's Lasting Influence.\n  - While not solely focused on water purity, the German Reinheitsgebot of 1516, declaring beer ingredients could only be barley, hops, water, and yeast, indirectly emphasised the crucial role of this \"liquid canvas.\" German brewers traditionally favour soft water, low in minerals, allowing the nuances of malt and hops to shine through. This focus on purity and precision in water selection laid the foundation for the meticulous control brewing water receives today.\n- Burton's Mineral Masterclass: A Canvas for Global Brews.\n  - But not all beer thrives on soft water. The English town of Burton-on-Trent boasts hard water, rich in calcium sulphate, lending distinct maltiness and subtle bitterness to its iconic pale ales. This unique profile, however, presents an opportunity \u2013 a chance to replicate the brewing conditions of other countries, even without crossing borders.\n- Modern Alchemy: From Burton to Bavaria and Beyond.\n  - Enter the reverse osmosis plant, a water sculptor of the modern age. This technology meticulously removes unwanted elements, creating a blank canvas for brewers to paint with specific mineral additions. For a German pilsner, softening Burton's water takes centre stage. Reverse osmosis paves the way for precise additions of calcium carbonate and magnesium sulphate, mimicking the soft water profile of traditional German pilsner regions. The result? A faithful Burton-brewed pilsner, where malt sweetness and hop bitterness dance in perfect harmony, just like their Bavarian counterparts.\nBut the symphony of water extends beyond Europe. Crafting an American IPA in Burton requires a different melody. Here, brewers retain the hardness of Burton's water while adjusting the mineral composition. Gypsum enhances malt sweetness, while carefully controlled hop additions bring out the signature citrusy aromas and sharp bitterness of American IPAs. Burton's water, once again, transforms, becoming a stage for American brewing tradition to flourish.\n- A Toast to Water: The Unsung Hero of Global Brewing.\n  - From the Reinheitsgebot's emphasis on quality to the art of Burtonisation and the precision of reverse osmosis, water emerges as the unsung hero of global brewing. Its purity, mineral symphony, and adaptability allow brewers to craft beers that transcend borders, carrying the soul of one country within the heart of another. So, raise a glass to water, the foundation upon which every great beer, whether born in Burton or beyond, is built.\n\n**War and Water:**\n- Water as a Weapon:\n - Deprivation: Cutting off the besieged city's water supply was a common siege tactic. Armies would divert rivers, contaminate wells, or build dams, forcing surrender through dehydration and disease.\n  - Flooding: Flooding enemy fortifications could weaken walls, create temporary bridges, or flush out defenders.\n  - Contamination: Poisoning water sources with animal carcasses, diseased bodies, or even industrial waste was a cruel but effective way to spread illness and demoralise the enemy.\n- Protecting Water Sources:\n  - Fortification: Cities often prioritised building strong defences around wells, springs, and aqueducts, knowing their critical importance.\n  - Hidden Reserves: Secret cisterns and wells within city walls allowed defenders to access water even if external sources were compromised.\n  - Purification Tactics: Boiling, filtering through sand or charcoal, and exposing water to sunlight were early methods used to combat contaminated water.\n- Historical Examples:\n  - The Siege of Masada (73 AD): Roman forces cut off Jewish rebels' access to water, contributing to their ultimate defeat.\n  - The Siege of Constantinople (1453): Ottomans diverted the city's main water supply, forcing Byzantines to rely on less secure sources and weakening their defences.\n  - The Dutch Revolt (1568-1648): Spanish forces flooded Dutch lowlands in an attempt to drive out rebels, leading to strategic shifts and ecological changes.\n- Modern Concerns:\n  - Weaponization of water: Concerns persist about the potential for deliberate contamination of water sources during modern conflicts.\n  - Climate change and water security: Increased droughts and flooding threaten water access in conflict zones, potentially exacerbating tensions and humanitarian crises.\n  - Protecting water infrastructure: The vulnerability of critical water systems to cyber attacks and sabotage poses new challenges for military and civilian authorities.\n\n**The Two Sides of the Dam - Benefits and Damage of Large Dams:**\n\nLarge dams, like the Hoover Dam, are complex structures with a mixed bag of consequences. They offer undeniable benefits, while also raising concerns about environmental and social impacts. Let's dive into both sides of the coin:\n- Benefits:\n  - Hydro-power: Dams harness the power of flowing water to generate electricity, often considered a clean and renewable energy source. For example, the Hoover Dam supplies power to millions in the American Southwest.\n  - Flood Control: By regulating river flows, dams can mitigate flooding, protecting lives and infrastructure. The Hoover Dam helps prevent flooding in the Grand Canyon and downstream communities.\n  - Irrigation: Stored water from dams can be used for agricultural irrigation, improving crop yields and food security in arid regions. The Hoover Dam provides crucial water for farms across California, Arizona, and Nevada.\n  - Navigation: Controlled water levels can improve river navigation, facilitating transportation and trade. The Mississippi River system heavily relies on dams for this purpose.\n- Damage:\n  - Ecosystem Disruption: Dams disrupt the natural flow of rivers, impacting downstream ecosystems. They can block fish migration, reduce nutrient-rich sediment flow, and harm riverine habitats. The Hoover Dam has significantly reduced the Colorado River's flow into Mexico, impacting its ecology and agriculture. [link](https:\/\/www.theguardian.com\/environment\/2019\/oct\/21\/the-lost-river-mexicans-fight-for-mighty-waterway-taken-by-the-us)\n  - Displacement and Resettlement: Dam construction often displaces communities, particularly indigenous populations, from their ancestral lands. The Three Gorges Dam in China displaced at least 1.3 million people. [link](https:\/\/www.britannica.com\/topic\/Three-Gorges-Dam)\n  - Loss of Cultural Heritage: Archaeological sites and cultural artefacts can be submerged or destroyed during dam construction.\n  - Siltation and Sedimentation: Dams trap sediment, reducing its availability downstream and leading to erosion and loss of fertile land. This is a significant concern for the Nile Delta.\n  - Safety Concerns: Dam failures can have catastrophic consequences, causing flooding and loss of life. The Brumadinho dam disaster in Brazil in 2019 is a tragic example. [link](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0303243420300192#:~:text=On%2025th%20January%202019%2C%20the,missing%20as%20of%20January%202020.)\n- The Hoover Dam Case:\n  - The Hoover Dam exemplifies both the benefits and challenges of large dams. While it provides vital water and electricity to millions, its impact on the Colorado River's flow has caused ecological and social problems in Mexico. Balancing these issues requires careful planning, environmental mitigation measures, and international cooperation.\n- Moving Forward:\n  - Large dams should be considered with caution, only after exhaustive cost-benefit analyses and comprehensive environmental impact assessments. Sustainable alternatives like solar and wind power should be explored whenever feasible. Moreover, transparency and community engagement are crucial to ensure dams do not exacerbate social injustice or environmental degradation.\nUltimately, the decision to build a large dam should be made with a nuanced understanding of its potential benefits and damage, weighing them carefully against alternative solutions and prioritising the long-term well-being of both humans and the environment.\nRemember, the water cycle and the ecosystems it supports are delicate. We must tread carefully and make informed decisions about how we interact with them, ensuring their health and our own for generations to come.\n\n**Modern Water Contamination:**\n- **Antibiotic Resistance:**\n  - Sources: Wastewater from hospitals, farms using antibiotics in animal feed, and improper disposal of unused medications contribute to the presence of antibiotics in water.\n  - Problem: Bacteria can develop resistance to antibiotics, making it harder to treat infections and potentially leading to super-bugs, becoming untreatable by existing antibiotics. This can increase the risk of serious illnesses, longer hospital stays, and even death.\n  - Impact: The World Health Organisation has called antibiotic resistance one of the \"ten greatest threats to global health in today's world.\"\n  - Antibiotic stewardship: Promoting responsible use of antibiotics in healthcare and agriculture can help reduce their presence in the environment.\n- **Increased Oestrogen:**\n  - Sources: Birth control pills, hormone replacement therapy medications, and agricultural runoff containing livestock hormones can contribute to elevated oestrogen levels in water.\n  - Problem: Oestrogen in water can disrupt the endocrine system of aquatic life, leading to feminization in male fish, reduced fertility, and population decline. In humans, exposure to oestrogen in water through drinking or swimming may be linked to certain cancers and reproductive problems.\n  - Impact: The widespread presence of oestrogen in water poses a threat to both ecological and human health, requiring improved wastewater treatment and responsible use of hormones.\n  - Improved wastewater treatment: Upgrading wastewater treatment plants to remove antibiotics, hormones, and other contaminants is crucial.\n  - Public awareness: Raising public awareness about these issues and encouraging responsible disposal of medications and personal care products can contribute to reducing pollution.\n  - Microplastics: These tiny plastic fragments can adsorb antibiotics and other contaminants, potentially increasing their bio-availability and posing additional risks to aquatic life and human health. \n- **Glyphosate:** The most widely used herbicide globally, primarily found in Roundup weed killer. While its effects on land are well-studied, its presence in water raises growing concerns.\n  - Minimal direct use: Glyphosate isn't applied directly to water bodies for weed control. However, it can reach water indirectly.\n  - Runoff from agricultural fields, lawns, and gardens. Rainfall and irrigation can carry glyphosate residues into rivers, lakes, and groundwater.\n  - Glyphosate is routinely used pre-harvest to help ripen or dry out crops. [link](https:\/\/ahdb.org.uk\/pre-harvest-glyphosate#:~:text=Pre%2Dharvest%20glyphosate%3A%20best%20practice,particularly%20valuable%20in%20wet%20seasons.)\n  - Industrial discharge: Factories producing or using glyphosate may release wastewater containing the herbicide.\n  - Atmospheric deposition: Glyphosate can travel through the air and eventually settle on water surfaces.\n- Dangers and Concerns:\n  - Impact on aquatic life: While glyphosate itself is relatively non-toxic to fish and invertebrates at current levels, concerns exist about potential indirect effects:\n  - Disruption of aquatic ecosystems: Glyphosate can harm algae, the base of the food chain, impacting entire ecosystems.\n  - Increased susceptibility to disease: Glyphosate may weaken aquatic organisms, making them more vulnerable to infections and environmental stressors.\n- **Human health concerns:** Though the EPA considers glyphosate safe at current levels in drinking water, some studies suggest potential risks.\n  - Cancer: The International Agency for Research on Cancer classified glyphosate as \"probably carcinogenic to humans,\" though evidence remains inconclusive.\n  - Developmental effects: Some studies suggest potential harm to fetal development at high exposure levels, requiring further research.\n  - Contamination of human breast milk: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9322831\/)  \n  - Glyphosate use, toxicity and occurrence in food: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8622992\/#:~:text=Glyphosate%20is%20applied%20intensively%20in,in%20human%20urine%20%5B6%5D.)\n  - Antibiotic resistance: Some concerns exist about potential links between glyphosate use and the development of antibiotic resistance in bacteria.\n- Regulations: Different countries have varying regulations regarding glyphosate levels in drinking water. \n  - Detection and treatment: Glyphosate can be detected in water through sophisticated testing methods. However, removing it from drinking water remains a challenge, requiring advanced treatment technologies.\n  - Ongoing Research: More research is needed to fully understand the long-term impacts of glyphosate in water, especially on human health and aquatic ecosystems.\n  - Raising awareness: Public education is crucial to understand the potential risks of glyphosate in water and promote responsible herbicide use.\n  - Supporting research: Continued research on glyphosate's environmental and health impacts is essential for informing regulatory decisions and protecting public health.\n  - Exploring alternatives: Investigating and promoting safer alternatives to glyphosate-based herbicides can help minimize potential risks to water quality and ecosystems.\n- Emerging contaminants: New and poorly understood chemicals, such as PFAS (per- and polyfluoroalkyl substances), are increasingly being detected in water, raising concerns about their potential health effects.\n  - A document entitled - PFAS: The 'Forever Chemicals' With a Toxic Legacy: [link](https:\/\/docs.google.com\/document\/d\/1dKyWDo2BGMwqpOnWbRqxBbZ5_jdccbKlAZKENm3Hn4c\/edit?usp=sharing)\n\n**Data Visualisation:** The below chart shows the 42 best & worst countries in terms of fine particulate matter, from the data (WHO_PM.csv) for the years 2014 - 2019. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F365fd85dc0eccd344966dc3a7fe48a02%2FScreenshot%202024-01-13%2023.08.00.png?generation=1705187441671576&alt=media)\nA Markdown document with the R code for the above plot. The code analyses air pollution and air quality data and calculates average values for each location - [link](http:\/\/rpubs.com\/Paddy_5142\/1138053).\n\nParticle pollution can transport contaminants, such as toxic heavy metals and organic compounds, which can accumulate in fish tissues and be ingested by humans. These pollutants can harm aquatic life through various processes, such as physical damage, ingestion, bio-accumulation, light attenuation, and toxicity. [link](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S1532045623001011#:~:text=Particle%20pollution%20can%20transport%20contaminants,%2C%20light%20attenuation%2C%20and%20toxicity.)\n\n**See current wind, weather, ocean, and pollution conditions, as forecast by supercomputers, on an interactive global animated map:**\n- Earth Nullschool.\n- Updated every three hours.  \n- The following link will take you to: **Wind & Particulate Matter &lt; 2.5 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm2.5\/orthographic)\n- The following link will take you to: **Wind & Particulate Matter &lt; 10 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm10\/orthographic=-354.03,-1.14,182)\n- Click anywhere on the global map to see particulate matter in \u00b5g\/m3 (concentration of an air pollutant, given in micrograms (one-millionth of a gram) per cubic meter of air).\n\n**Data Visualisation:** The below chart shows the 25 best & worst countries in terms of water and air pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fbe8a7a353496b2c9503e53ea615f0ad8%2FScreenshot%202024-01-14%2014.25.31.png?generation=1705243641434813&alt=media)\n\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138105).\n\nFrom the above visualisation: \n- Palau is the cleanest country for both air and water quality. [link](https:\/\/pristineparadisepalau.com\/)\n- Palau, the Federated States of Micronesia, and Eritrea are tied in first place for air quality.\n- Palau and Liechtenstein are tied in first place for water quality.\n- Contrary to its water pollution, the Federated States of Micronesia is in second place for air quality.\n\n*A project of mine entitled 'What about the Wind?' which explores its origins, its role in a changing climate, the ways we harness its strength, and its impact on human lives:* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind)\n\n**Plastic in Water: A Ubiquitous Threat from Land to Sky.**\n\nPlastic, once a revolutionary material, has become a pervasive pollutant, infiltrating every corner of our environment, including our water sources. From the vast oceans to the raindrops falling from the sky, plastic has become a ubiquitous threat.\n\n- Plastic in Rainwater:\n  - Microplastics: Tiny plastic fragments, less than 5 millimetres in size, are now routinely found in rainwater around the world, even in remote locations like the Himalayas and the Pyrenees. These microplastics can originate from various sources, including car tires, synthetic clothing, and the breakdown of larger plastic debris.\n  - Microplastics in rain: Studies have shown that microplastics can be incorporated into raindrops as they form in clouds. This means that even pristine rainwater can now be contaminated with plastic particles.\n  - Potential health risks: The long-term health effects of ingesting microplastics are still unknown, but concerns exist about their potential to harm human health. Microplastics can act as sponges for pollutants and may release harmful chemicals into the body.\n- Atomised Plastic from the Ocean:\n  - Ocean plastic breakdown: The vast gyres of plastic waste in our oceans are constantly breaking down into smaller and smaller pieces under the influence of sunlight, waves, and chemical reactions.\n  - Sea spray and atomisation: Wind and waves create sea spray, which can send tiny plastic particles, known as atomised plastic, into the atmosphere. These particles are even smaller than microplastics, ranging from 1 to 10 micrometers in size.\n  - Plastic rain: Atomised plastic can be carried by air currents for long distances and eventually fall back to Earth as rain, contaminating land and water resources.\n- The Impact of Plastic in Water:\n  - Harm to aquatic life: Plastic pollution in water can entangle and suffocate marine animals, disrupt their feeding habits, and introduce harmful chemicals into the food chain.\n  - Threats to human health: Microplastics and atomised plastic can be ingested by humans through drinking water and eating seafood, potentially posing health risks.\n  - Environmental damage: Plastic pollution can disrupt ecosystems, reduce biodiversity, and impact water quality.\n- What We Can Do:\n  - Reduce plastic use: Making conscious choices to avoid single-use plastics and opting for reusable alternatives can help stem the tide of plastic pollution.\n  - Proper waste management: Ensuring proper disposal and recycling of plastic waste can prevent it from ending up in our environment.\n  - Support research: Investing in research to understand the full impact of plastic pollution and develop solutions for its mitigation is crucial.\n  - Policy changes: Advocating for stricter regulations on plastic production and consumption can help address this global challenge.\n\n**Rain and Contaminants:**\n\nWhile some contaminants can reach the atmosphere and potentially return to Earth through rain or other forms of precipitation, several factors prevent all contaminants from following this cycle: \n- Chemical properties: Not all contaminants are easily evaporated or transported through the air. Heavy metals, for example, tend to remain dissolved in water and are unlikely to become part of raindrops.\n- Biological degradation: Some contaminants can be broken down by microorganisms in the ocean before reaching the atmosphere. This breakdown process prevents them from joining the water cycle.\n- Deep-sea sequestration: Certain pollutants, like heavy metals or persistent organic pollutants (POPs), can sink down to the ocean floor and become trapped in sediments, effectively removed from the cycle.\n- Human intervention: We can mitigate the presence of some contaminants through wastewater treatment and pollution control measures, preventing them from reaching the ocean in the first place.\n\nTherefore, while rain can be a pathway for transporting certain contaminants, it's not a guaranteed endpoint for all pollutants present in seawater. Understanding the specific properties and fate of different contaminants is crucial for assessing their potential environmental impact and developing effective mitigation strategies.\n\n**Modern Water Treatment: Bristol, UK.** [link](https:\/\/www.bristolwater.co.uk\/)\n\nThe water treatment process in Bristol by Bristol Water is a fascinating journey, transforming raw river water into the fresh, clean tap water we all rely on. Here's a detailed breakdown:\n- Sources:\n  - Surface Water: Primarily sourced from reservoirs in the Mendip Hills, fed by rivers like the Chew and the Yeo.\n  - Groundwater: Used in drier periods, sourced from boreholes in limestone aquifers.\n- Treatment stages:\n  - Screening: Removing large debris like leaves and branches.\n  -  Pre-treatment: Adding a coagulant (aluminum sulfate) to bind small particles together.\n  - Sedimentation: Allowing the large clumps to settle out in tanks.\n  - Filtration: Passing the water through sand filters to remove remaining particles.\n  - Activated Carbon Filtration: Removing dissolved contaminants like pesticides and organic matter using highly absorbent activated carbon.\n  - Disinfection: Adding chlorine to kill bacteria and viruses.\n  - pH Adjustment: Balancing the water's acidity for corrosion control and improved taste.\n  - Quality Control: Continuous monitoring and testing at various stages to ensure safe, high-quality water.\n- Bristol Water's \"Wicked Water Treatment\" Initiative: [link](https:\/\/www.bristolwater.co.uk\/wicked-water-treatment#:~:text=We%20use%20wicked%20water%20treatment,can%20leak%20into%20our%20rivers.)\n  - A public education program explains the treatment process in a fun and engaging way, particularly for children.\n  - Emphasises sustainable practices like energy efficiency and waste reduction.\n  - Offers tours of treatment works for a firsthand look at the process.\n- Additional notes:\n  - Bristol Water uses a multi-barrier approach, meaning each stage contributes to overall water quality and safety.\n  - Treatments may vary depending on the specific source and water quality.\n  - The company invests heavily in research and development to improve efficiency and adapt to new challenges.\n\nWastewater in Bristol, and indeed most places, typically undergoes one treatment cycle before being discharged back into the environment, not being recycled in the traditional sense. However, the water used in the treatment process itself is often recycled!\n\n- Here's a breakdown of the water usage in Bristol's wastewater treatment:\n  - Raw water intake: Around 180 million litres of water are drawn from reservoirs and aquifers daily for both drinking water and wastewater treatment.\n  - Treatment process: Only a small portion, around 1%, of this water is actually used in the various treatment stages like screening, sedimentation, and filtration. The majority of the water simply carries the wastewater through the system.\n  - Treated water discharge: The treated wastewater, now significantly cleaner, is released back into rivers or the Severn Estuary.\n  - Treatment plant water recycling: The water used within the treatment plant for tasks like back-washing filters and machinery is often treated and recycled internally. This reduces the overall freshwater intake needed for the process.\n\nSo, while the wastewater itself isn't directly recycled for drinking or other purposes, there are efforts to minimise freshwater use within the treatment plants themselves. Additionally, research into wastewater reuse for non-potable applications like irrigation or industrial processes is ongoing, and Bristol Water is exploring these possibilities as well. [link](https:\/\/corporate.wessexwater.co.uk\/our-purpose\/investment-schemes\/bristol-water-recycling-centre)\n\n**The Rise of Ocean Radioactivity: A Shadow of Fukushima.**\n\nThe vast expanse of our oceans, once considered pristine and boundless, now faces a growing threat: increased radioactivity. This rise is largely attributed to the 2011 Fukushima Daiichi nuclear power plant disaster in Japan, where meltdowns and explosions released significant amounts of radioactive material into the Pacific Ocean.\n\n- Fukushima's Radioactive Footprint:\n  - Cesium-137: The primary radioactive isotope released from Fukushima, cesium-137, has a half-life of 30 years, meaning it will persist in the environment for decades to come.\n  - Ocean Contamination: Cesium-137 has been detected in Pacific Ocean waters thousands of miles from the accident site, contaminating marine life and raising concerns about long-term impacts on the food chain.\n  - Pacific Currents: Ocean currents play a crucial role in dispersing radioactive material, carrying it across vast distances and potentially exposing marine ecosystems far beyond the immediate vicinity of the accident.\n- Impacts on Marine Life:\n  - Bio-accumulation: Radioactive isotopes can accumulate in the tissues of marine organisms, increasing their radiation exposure and potentially posing risks to predators higher up the food chain, including fish consumed by humans.\n  - Disruption of Ecosystems: Radiation can harm the reproduction and development of marine life, potentially disrupting the delicate balance of ocean ecosystems.\n  - Long-Term Uncertainty: The full extent of the long-term consequences of Fukushima's radioactive releases on ocean health and human health through seafood consumption is still being studied and debated. - [link](https:\/\/www.globalresearch.ca\/28-signs-that-the-west-coast-is-being-absolutely-fried-with-nuclear-radiation-from-fukushima\/5355280)\n  - Possible Immune System Compromise: Cesium-137. - [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Beyond Fukushima:\n  - Other Contributors: While Fukushima is a major source of ocean radioactivity, it's not the only one. Nuclear testing, accidents at other nuclear power plants, and even naturally occurring radioactive elements contribute to the overall burden of radioactivity in the oceans.\n- Global Concern: The increasing presence of radioactive materials in our oceans raises concerns about the long-term health of marine ecosystems and the potential risks to human health through seafood consumption. This issue demands international cooperation and continued research to mitigate the risks and protect our oceans.\n- What We Can Do:\n  - Support Research: Continued research on the impacts of ocean radioactivity and the development of monitoring and mitigation strategies is crucial.\n  - Sustainable Seafood Practices: Choosing sustainably sourced seafood and supporting responsible fishing practices can help minimise exposure to radioactive contaminants.\n  - Advocate for Change: Raising awareness about ocean radioactivity and its potential consequences can encourage policymakers to implement stricter regulations on nuclear activities and invest in cleaner energy sources.\n\n**A Future of Abundant and Healthy Oceans: A Collective Responsibility.**\n\nOur journey through the history of water revealed its profound connection to human health and the fragility of its purity. From the challenges of hunter-gatherers to the complexities of modern water management, one truth remains constant: clean water is the lifeblood of our planet and all its inhabitants.\n\nHowever, our oceans, once vast and seemingly limitless, now face a multitude of threats. Increased radioactivity from nuclear accidents like Fukushima casts a long shadow, while the invisible menace of antibiotic resistance, oestrogen contamination, glyphosate residues, and the ubiquitous presence of plastic paint a worrying picture of a polluted sea.\n\nYet, amidst these challenges lies a powerful opportunity. By acknowledging the gravity of these threats and taking collective action, we can forge a future where our oceans are brimming with vibrant life, free from the burden of modern contaminants. This necessitates a multifaceted approach:\n- Research and monitoring: Understanding the extent and long-term impacts of these threats through robust research and continuous monitoring is crucial to informing effective response strategies.\n- Policy and regulation: Implementing stricter regulations on pollution sources, such as nuclear activities, agricultural practices, and plastic production, is essential to curbing the tide of contamination.\n- Technological innovation: Developing innovative solutions like advanced water treatment technologies and cleaner energy sources can offer sustainable pathways towards ocean health.\n- Individual responsibility: Making conscious choices in our daily lives, from reducing plastic use to supporting responsible seafood practices, can collectively make a significant difference.\n\nThe health of our oceans is not just an environmental concern; it is a matter of human health and the very viability of life on Earth. By standing together in this endeavour, we can ensure that future generations inherit a legacy of abundant and healthy oceans, forever teeming with life, forever a source of wonder and sustenance.\nRemember, clean water is not a luxury, it is a birthright. Let us protect it, cherish it, and ensure its abundance for all.\n\n**Cultivating Without Soil: Hydroponics, Aeroponics, and the Future of Farming.**\n\nTraditional soil-based agriculture has served humanity for millennia, but as we face challenges like limited land, water scarcity, and climate change, innovative methods like hydroponics and aeroponics are emerging as sustainable alternatives. Let's delve into these fascinating techniques and explore their potential for the future of farming, even in the vast expanse of space!\n\nHydroponics: Growing in a Watery Embrace.\n\n- Imagine lush vegetables thriving not in soil, but in a nutrient-rich water solution. That's the magic of hydroponics! Plants' roots are suspended in a variety of mediums like rockwool, perlite, or even just water, while receiving a precisely balanced blend of nutrients and oxygen through the solution. This method offers several advantages:\n  - Water efficiency: Hydroponics uses up to 90% less water than traditional agriculture, making it ideal for arid regions or periods of drought.\n  - Increased yields: Precise control over nutrients and environment leads to faster growth and higher yields compared to soil-based methods.\n  - Reduced pest and disease problems: Soil-borne diseases are less of a concern with hydroponics, leading to fewer pesticides and healthier crops.\n  - However, hydroponics also requires significant upfront investment in equipment and infrastructure. Maintaining the correct pH and nutrient balance in the water solution is crucial, and any technical glitches can have a rapid impact on the plants.\n\nAeroponics: Dancing on Air.\n\nTake hydroponics up a notch, and you have aeroponics! In this method, plant roots are suspended in midair and misted with a nutrient-rich solution at regular intervals. This constant exposure to oxygen fosters even faster growth and eliminates the need for a growing medium altogether.\n\n- Aeroponics boasts all the benefits of hydroponics, with some additional advantages:\nEven better root aeration: Direct exposure to air maximises oxygen uptake, leading to even more vigorous plant growth.\n  - Reduced risk of root disease: Without any growing medium, bacteria and fungal diseases have less chance of taking hold.\n  - However, aeroponics demands even stricter control over the nutrient solution and misting frequency. Any interruption in the misting system can quickly stress or even kill the plants.\n\nSoil vs. Soil-less: Weighing the Options.\n- While hydroponics and aeroponics offer numerous benefits, traditional soil-based agriculture remains dominant for several reasons:\n  - Lower cost: Setting up and maintaining soil-less systems requires significant investment, making soil-based farming more accessible, especially for small-scale operations.\n  - Wider range of crops: Not all plants thrive in soil-less systems, while soil can accommodate a diverse range of crops.\n  - Resilience: Soil-based systems offer inherent buffering against fluctuations in temperature, pH, and nutrient levels, making them more forgiving of minor errors.\n\nUltimately, the choice between soil and soil-less methods depends on factors like the type of crop, climate, resources available, and desired level of control.\n\nPure Water: The Lifeblood of Soil-less Systems.\n\nThe importance of clean water is paramount in hydroponics and aeroponics. Impurities in the water can clog systems, disrupt nutrient balance, and harm the plants. Therefore, using filtered or even reverse osmosis-treated water is crucial for optimal results.\n\nSpace faring Sprouts: Farming Beyond Earth.\n\nThe potential of soil-less agriculture extends beyond Earth's soil. In the context of space exploration and potential long-duration missions, hydroponics and aeroponics offer solutions for growing fresh food in the limited space and harsh conditions of spacecraft and lunar or Martian outposts.\nImagine astronauts enjoying salads and herbs grown right on board, reducing reliance on pre-packaged food and providing psychological benefits of cultivating their own sustenance. Research is already underway to develop closed-loop systems that recycle water and nutrients, minimising waste and maximising efficiency.\n\nThe Future of Farming: A Tapestry of Techniques.\n\nAs the world's population grows and resources become scarcer, innovative agricultural methods like hydroponics and aeroponics will undoubtedly play a crucial role in feeding the future. While traditional soil-based agriculture will remain relevant, these soil-less techniques offer promising solutions for sustainable and efficient food production, both on Earth and potentially beyond. So, the next time you bite into a crisp lettuce or juicy tomato, remember that the future of farming might not involve soil at all, but a carefully orchestrated dance of water, nutrients, and air.\n\n**Quenching the Thirst: Unveiling the Secrets of Desalination and Distilled Water.**\n\nWater, the elixir of life, is a precious resource, and with growing populations and climate change, its scarcity is becoming a pressing concern. Enter desalination, the process of removing salt and other minerals from seawater, making it fit for human consumption and other uses. Let's dive into the fascinating world of desalination and explore the journey towards the purest water, distilled water, through ancient and modern methods.\n\nFrom Ancient Ingenuity to Modern Marvels: Desalination's Journey.\n\nThe quest for fresh water from saline sources is as old as civilisation itself. Ancient Greeks and Romans used simple methods like sun evaporation and filtration through clay pots to desalinate seawater. Fast forward to the 19th century, steam-powered distillation became the dominant method, with the first large-scale desalination plant built in 1872 in Santiago, Chile.\n\nToday, desalination has evolved into a sophisticated field, employing a variety of technologies.\n\n- Thermal Desalination: This method uses heat to evaporate seawater, leaving behind salt and other minerals. The most common thermal desalination technologies are:\n  - Multi-stage flash distillation (MSF): Seawater is heated in a series of chambers, and the vapour condenses on cool surfaces to produce fresh water.\n  - Multi-effect distillation (MED): Similar to MSF, but uses waste heat from one stage to heat the next, making it more energy-efficient.\n  - Membrane Desalination: This method uses semi-permeable membranes that allow water molecules to pass through while rejecting salt and other impurities. The most common membrane desalination technology is:\n  - Reverse osmosis (RO): High pressure forces seawater through a membrane, leaving behind concentrated brine and producing fresh water.\n\nDistilled Water: Purity Reborn.\n\nDistillation, the oldest and arguably the purest method of water purification, involves boiling water and collecting the condensed vapour. This process removes virtually all impurities, including minerals, salts, and even some bacteria and viruses. Distilled water is often used in laboratories, medical applications, and certain industrial processes where ultra-pure water is required:\n- Ancient Echoes in Modern Purity:\n  - The principles of distillation are not lost on modern technology. Vacuum distillation, a more advanced method, operates at lower temperatures and pressures, further reducing the energy consumption and preserving the delicate properties of the water. This makes it a preferred choice for producing high-purity water for sensitive applications.\n- The Future of Desalination and Distilled Water:\n  - As the need for clean water grows, so does the importance of desalination and distilled water production. Advancements in technology are making these processes more efficient and affordable, opening up possibilities for wider adoption. Research is also ongoing in developing sustainable energy sources for desalination, such as solar and wind power, to reduce its environmental footprint.\n- The Choice Between Purity and Practicality:\n  - While distilled water offers unparalleled purity, it also lacks essential minerals that are beneficial for human health. \n   - Additionally, the energy consumption of some desalination methods raises concerns about sustainability. Therefore, the choice between desalinated, distilled, or other types of purified water depends on the specific needs and context.\n\nIn conclusion, desalination and distilled water production are vital tools in our quest for clean water. Understanding their history, methods, and limitations helps us make informed decisions about utilising these technologies to ensure a sustainable future with enough water for all.\n\n**Water, water everywhere and not a drop to drink!**\n\nThe tale of the Robertson family's 38-day ordeal adrift in the Pacific Ocean following their yacht's sinking in 1972 is indeed an incredible testament to human resilience and ingenuity. Their story, echoing Samuel Taylor Coleridge's \"Rime of the Ancient Mariner,\" is one of facing the very element that sustains life while desperately battling for it:\n- The Ordeal Begins:\n  - The Robertsons, a British family of six - Dougal and Lyn, along with their children Douglas, Anne, and twins Neil and Sandy - set sail from Falmouth, England, in 1971 on a voyage around the world aboard their 43-foot wooden schooner, Lucette. \n  - Sadly, their dream adventure took a horrific turn when, 200 miles off the coast of the Galapagos Islands, a pod of orcas attacked their vessel, sinking it within minutes.\n- Adrift in a Tiny Dinghy:\n  - The family grabbed a few items and abandoned ship and managed to board their inflatable rubber life raft and nine-foot fiberglass dinghy Ednamair. \n  - They only had enough water for ten days and emergency rations for three days on the raft.\n  - Lyn had taken their papers, the logbook, and a bag of onions, and they had a kitchen knife, a tin of biscuits, ten oranges, six lemons, half a pound of glucose sweets and flares.   \n  - Faced with the agonising reality of being surrounded by water yet desperately parched, they knew they had to think outside the box to survive.\n- Desperate Measures for Survival:\n  - In their fight for survival, the Robertsons resorted to several unconventional techniques, some bordering on the macabre. \n  - They rationed their food meticulously, collecting rainwater in any way they could, even squeezing moisture from their clothes. \n  - To combat dehydration, they employed saltwater enemas, a risky procedure that, while unpleasant, helped to retain some fluids.\n- A Gruesome Necessity:\n  - Perhaps the most shocking aspect of their survival story is their reliance on turtle blood. Driven by sheer desperation, they captured and killed sea turtles, consuming their flesh and drinking their blood to quench their thirst and stave off starvation. This act, while understandably controversial, was a life-or-death decision made in the face of unimaginable circumstances.\n- A Beacon of Hope:\n  - After 38 gruelling days adrift, their ordeal finally came to an end when they were spotted and rescued by a Japanese fishing vessel. \n  - Emaciated and sunburnt but alive, the Robertsons had cheated death against all odds. \n  - Their story became a global sensation, a testament to the indomitable human spirit and the lengths we will go to for survival.\n- Lessons Learned:\n  - The Robertson family's experience left an indelible mark on them and the world. It serves as a stark reminder of the power and unpredictability of nature, the importance of preparation and resourcefulness in the face of adversity, and the depths of human strength and resilience. It is a story that continues to inspire and captivate, a testament to the will to live even in the face of the most desperate odds.\n  - While their methods may raise eyebrows, their tale underscores the desperate lengths humans will go to for survival. It's a story that serves as a powerful reminder of the human spirit's enduring strength and the importance of hope in the face of seemingly insurmountable challenges. [link](https:\/\/nmmc.co.uk\/2022\/05\/the-50th-anniversary-of-the-robertson-family-rescue\/)\n\n**Navigating the Hydration Highway: Bottled Water, Tap Water, Filters, and Distillers.**\n\nHydration \u2013 the essential fuel for our daily adventures. But when it comes to choosing our drink, we're often greeted by a crossroads: bottled water, shimmering in its plastic shell, or tap water, the familiar flow from our taps\/faucets. Let's explore the advantages and downsides of each, along with their trusty sidekicks \u2013 water filters and home distillers \u2013 to find the perfect lane on the hydration highway.\n\nBottled Water: Convenience with Caveats.\n\n- Advantages:\n  - Convenience: Portable and readily available, whether scaling mountains or stuck in traffic, bottled water keeps us quenched.\n  - Taste: Some prefer the perceived \"cleaner\" taste, especially in areas with chlorine-tinged tap water.\n  - Perception of Purity: Often marketed as purer than tap water, though bottled water quality can vary greatly.\n- Disadvantages:\n  - Cost: Significantly more expensive than tap water, making it an unsustainable daily option.\n  - Environmental Impact: Mountains of plastic bottles generate enormous waste, posing a major environmental threat.\n  - Limited Mineral Content: While often perceived as purer, bottled water may lack essential minerals naturally found in tap water.\n\nTap Water: The Everyday Essential.\n- Advantages:\n  - Cost-Effective: The most affordable hydration option, readily available at the turn of a tap.\n  - Mineral Content: Naturally rich in essential minerals important for health.\n  - Lower Environmental Impact: No plastic waste generated, significantly reducing environmental footprint.\n- Disadvantages:\n  - Taste: Can have a chlorinated taste or be affected by local water quality issues.\n  - Purity Concerns: Depending on location, tap water may contain contaminants or require additional filtration.\n  - Accessibility: Not always readily available in all locations or situations.\n\nWater Filters: On-Demand Purification.\n\nEnter the water filter, the trusty sidekick for tap water warriors. Filter jugs, tap\/faucet attachments, and reverse osmosis systems offer various levels of filtration, removing contaminants like chlorine, sediment, and even some heavy metals.\n\n- Advantages:\n  - Improved Taste: Removes unpleasant odours and tastes, enhancing the palatability of tap water.\n  - Reduces Contaminants: Provides an extra layer of protection against potential contaminants in tap water.\n  - Cost-Effective: A more sustainable and affordable option compared to bottled water.\n- Disadvantages:\n  - Maintenance: Filters require regular cleaning and replacement, adding to the cost and effort.\n  - Limited Removal: May not remove all contaminants, depending on the type of filter and local water quality.\n\nHome Distillers: Crafting Purest H2O.\n\nFor those seeking the ultimate water purity, home distillers offer a fascinating option. By boiling and condensing water, these devices remove virtually all impurities, including minerals, salts, and even some bacteria.\n\n- Advantages:\n  - Purest Water: Produces the purest drinking water available, ideal for sensitive applications.\n  - Removes Contaminants: Eliminates virtually all impurities, including those not addressed by filters.\n  - Flexibility: Allows control over the mineral content of the produced water.\n- Disadvantages:\n  - Cost: Home distillers can be expensive and require energy to operate.\n  - Slow Production: Water production is slower compared to other options.\n  - Mineral Removal: Removes essential minerals naturally found in water, requiring supplementation.\n\nThe Verdict: Hydrate Wisely.\n\nUltimately, the perfect hydration choice depends on individual needs, priorities, and access to clean tap water:\n- Remember:\n  - Choose wisely: Consider convenience, cost, environmental impact, and desired water purity.\n  - Embrace filters: Tap water with a filter can be a cost-effective and sustainable option. [link](https:\/\/www.who.int\/publications\/i\/item\/924156251X)\n  - Distill with caution: While pure, distilled water may require mineral supplementation.\n  - Reduce plastic: Opt for reusable bottles and recycle diligently. [link](https:\/\/bottledwater.org\/packaging\/#:~:text=Plastic%20bottled%20water%20containers%20are,and%20HDPE%20for%209.2%20percent.)\n\n**Conclusion: A Ripple of Responsibility in the Aquatic Symphony.**\n\nWater, the essence of life, stands at a pivotal juncture. While its life-sustaining properties nourish us, it confronts an array of threats. From the subtle encroachment of chemical pollutants to the relentless grasp of microplastics, the aquatic landscape endures a silent siege. Heavy metals clandestinely inhabit its depths, unsettling the fragile equilibrium of ecosystems, while the looming shadows of industrial activities stretch across its pristine purity. The very wellspring of life\u2014its groundwater\u2014stands contaminated, posing risks to health and well-being.\n\nIn this challenging panorama, glimmers of hope persist. Acknowledging the intricate interconnectedness of life, we must craft a nuanced and collective response. Clearer and more stringent regulations should resound like a clarion call, reverberating across landscapes burdened by unsustainable practices. Sustainable agriculture, efficient waste management, and international cooperation must waltz in unison, choreographed by a commitment to shared responsibility.\n\nHowever, our focus cannot be exclusively global; it must also turn inward. Every choice we make, from the type of water we consume to the source we rely on, sends ripples outward, influencing the intricate web of life. Within every decision for a reusable bottle and each conserved drop lies the potential for transformative impact.\n\nNavigating the Aquatic Symphony demands not only personal initiative but also an unwavering awareness of the global tableau we collectively paint. Let us, therefore, stride ahead with a revitalised commitment\u2014toward sustainability, equity, and a future where each sip of water heralds a shared resource accessible to all. Ultimately, the power to generate a wave of change resides within our individual choices, capable of cleansing the stains of pollution and revitalising the lifeblood of our planet.\n\nEmbarking on this journey, guided by a shared aspiration, envisions a world where every human revels in the brilliance of clean, safe water. May each droplet murmur a testament to our collective responsibility\u2014a ripple of hope resonating across the expansive canvas of the Aquatic Symphony.\n\n\nPatrick Ford \ud83c\udf0a\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) to my project: *We did not weave the web of life !* Where I concentrate on some important factors that will affect humanity's potential to survive on planet Earth.\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind) to my project: *What about the Wind?*\nFrom a gentle rustle of leaves to the howl of a hurricane, wind is an unseen force that shapes our world. Revered and feared throughout history, wind has inspired myths, driven ships, and fuelled revolutions in energy. We'll explore its origins, its role in a changing climate, the ways we harness its strength, its impact on human lives and the effect on water quality. Our journey will blend science, history, and technology, and even take us beyond Earth to examine the wild winds of other planets.\n\n-----------------------------------------------------------------------------------------------------------------\n\nA short poem about rivers - [link](https:\/\/docs.google.com\/document\/d\/1LVyiP43r8A357T6gyisozJ0Faeis_Yx3QlxcKgIeM1g\/edit?usp=sharing).\n\n","83":"Monitoring of protected areas to curb illegal activities like poaching is a monumental task. Real-time data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which allow surveillance at night when poaching typically occurs. However, it is still a challenge to accurately and quickly process large amounts of the resulting TIR data. The Benchmarking IR Dataset for Surveillance with Aerial Intelligence (BIRDSAI, pronounced \u201cbird\u2019s-eye\u201d) is a long-wave thermal infrared (TIR) dataset containing nighttime images of animals and humans in Southern Africa. The dataset allows for testing of automatic detection and tracking of humans and animals with both real and synthetic videos, in order to protect animals in the real world.","84":"**Description**\n\nThis dataset comprises a collection of images that are categorized into two main classes: cats and dogs. Each class further contains subcategories based on color, namely white and black.\n\nThe dataset includes the following structure:\n\n    Cats\n        White: Images of cats with white fur.\n        Black: Images of cats with black fur.\n    Dogs\n        White: Images of dogs with white fur.\n        Black: Images of dogs with black fur.\n\nThe dataset is well-structured and labeled, making it suitable for tasks like image classification. It is particularly useful for training and evaluating models designed to classify animal types (cats or dogs) and distinguish their colors (white or black).\nContents\n\n    Cats (White): [Number of images: 400]\n    Cats (Black): [Number of images: 400]\n    Dogs (White): [Number of images: 400]\n    Dogs (Black): [Number of images: 400]\n\nThese images are sourced from diverse collections, ensuring a wide variety of cat and dog breeds across various backgrounds and environments. The dataset's labeling and diversity provide a robust foundation for developing and testing image classification models.\n**Potential Use Cases**\n\n    Image classification: Developing models capable of accurately distinguishing between cats and dogs based on their colors.\n    Color-based analysis: Identifying the prevalence or distinguishing features of different fur colors in cats and dogs.\n\nThe dataset's diversity, labeling, and balanced distribution among classes make it a valuable resource for both training and evaluating machine learning models, especially those focused on image classification tasks related to pets' color and type.","85":"This dataset was created for a 7.th semester project which uses and expands upon \"The Brackish Dataset\" https:\/\/www.kaggle.com\/datasets\/aalborguniversity\/brackish-dataset?rvi=1 by adding synthetically generated data. Github for instructions and code https:\/\/github.com\/Sebastian-Whitehead\/MED7-TheFishening\n\nSynthetic data generation in the context of data scarcity and diversity could help bolster the effectiveness of object detection. This paper proposes a synthetic framework for generating realistic underwater fish data, using BOIDs and high-definition rendering. The synthetic data is used to train and test an EfficientDet model, a state-of-the-art object detection algorithm, on the Brackish data set, an underwater fish detection data set captured in the turbid waters of the Limfjord, Denmark. The results demonstrate that there is no significant disparity observed when synthetic data is incorporated with the Brackish method, assumed to be attributed to the limited number of epochs trained. Even so, the inclusion of synthetic data exhibits a promising potential for enhancing the model. Within this paper, the challenges and limitations of such an approach are explored, allowing for potential insights in promising fields for future work.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F44fb71a061901d29601aeb9b96cc4a54%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0256_prediction1.jpg?generation=1702645091439627&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F3b2cfa75d850ae55ad8dae170a134e2d%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0225_prediction2.jpg?generation=1702645118933655&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F35e5beeb9125c1fcc08a312763187a18%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0156_prediction3.jpg?generation=1702645127887965&alt=media)","86":"## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n```","87":"**Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation},","88":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","89":"A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user. ","90":"**ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","91":"**ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","92":"**ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","93":"**ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n ","94":"The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\"","95":"","96":"This data is release as part of work of ***[Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes.](https:\/\/github.com\/qimaqi\/Implicit-Zoo)***\nFollowing the standard procedure for ImageNet-1K classification, we resize images to 256x256 and then center crop them to 224x224 for further processing. To better fit the high-res images, we employ a 4-layer 256 width SIREN. The normalization parameters are consistent with those used for CIFAR-10. We increase the training iterations to 3000, each taking 50.24 secs for each image.\n","97":"This dataset contains annotated images of tomatoes at various stages of ripeness. It is designed to support research and development in agricultural automation, specifically for training machine learning models to distinguish between ripe and unripe tomatoes. The dataset includes annotated images created using an [annotation lab](https:\/\/github.com\/sumn2u\/annotate-lab), ensuring precise and accurate labeling of ripeness status.\n\n","98":"Classification:\n\u25aa Split your dataset into 80% training and 20% testing. \n\u25aa Train at least 3 different models to classify each sample into distinct \nclasses.\n\u25aa Choose at least two hyperparameters to vary. Study at least three \ndifferent choices for each hyperparameter. When varying one \nhyperparameter, all the other hyperparameters should be fixed\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6407243%2F0e9835ec9319dc098fafd88676ca7ae3%2FScreenshot%20(25).png?generation=1719419036630363&alt=media)","99":"This dataset contains images of industrial tools, specifically focusing on screws and ball screws, categorized as either defective or non-defective. The dataset is intended for use in training and evaluating machine learning models for automated defect detection in industrial settings.\n\nThis dataset is primarily designed for educational and research purposes, specifically to aid in the development and evaluation of machine learning models. The core objective is to provide a collection of images featuring industrial tools, like screws and ball screws, categorized as either defective or non-defective. This classification task allows researchers and students to experiment with various machine learning algorithms, aiming to train models that can accurately distinguish between faulty and functional tools based solely on image data.\n\nThe dataset is comprised of two main files: \"train\" and \"test\". Both files contain images of industrial tools, specifically focusing on screws and valves. Each image is categorized into one of two classes: \"defective\" or \"non-defective\". This classification is crucial for training machine learning models that can automatically distinguish faulty tools from functional ones based solely on image data. The \"train\" file serves as the foundation for the model's learning process, while the \"test\" file is used to evaluate the model's performance on unseen data. By analyzing the accuracy on the test data, we can gauge the model's effectiveness in real-world scenarios of identifying defects in industrial tools.","100":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","101":"The dataset includes 4212 images.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 640x640 (Stretch)\n\nThe following augmentation was applied to create 3 versions of each source image:\n* 50% probability of horizontal flip\n* Randomly crop between 0 and 20 percent of the image\n* Random shear of between -10\u00b0 to +10\u00b0 horizontally and -10\u00b0 to +10\u00b0 vertically\n* Random brigthness adjustment of between -15 and +15 percent\n* Random exposure adjustment of between -10 and +10 percent\n* Salt and pepper noise was applied to 0.1 percent of pixels\n\n**Types OF Rocks**\n\n* Basalt\n* chert\n* Clay\n* Conglomerate\n* Diatomite\n* gypsum\n* olivine-basalt\n* Shale-(Mudstone)\n* Siliceous-sinter","102":"This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n","103":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","104":"This dataset includes images of all 26 letters of the alphabet and the \"space\" sign, totaling 27 classes. The dataset includes images from five different signers, with each signer providing 100 samples for each letter. This effort resulted in a total of 500 samples per letter, amounting to 13,500 images.\n\n**Dataset Collection Process:**\n\u2022\tSigners: 5 different signers\n\u2022\tSamples per Letter: 500\n\u2022\tTotal Images: 13,500\n\u2022\tImage Size: 640x480 pixels (resized to 224x224 pixels for training)\n\n**Data Preprocessing**\nAfter collecting the images, we resized them to 224x224 pixels to standardize the input size for the model. This resizing ensures that all images are compatible with the input requirements of the neural network. We then drew the hand landmarks on all the images and blacked out the background in order for the model to focus only on the landmarks when it came to training a CNN model using images.","105":"Flower Samples Dataset\n\nThis dataset contains images of two different types of flowers: roses and orchid. Each image is labeled with its corresponding flower type. The dataset is suitable for tasks such as image classification or object recognition. The images are captured from various angles and under different lighting conditions to provide a diverse set of samples for training and evaluation purposes. Researchers and developers can use this dataset to train machine learning models to classify images of roses and orchid accurately.\n\nAdenium obesum (Desert Rose)\n\nAdenium obesum, commonly known as the Desert Rose, is a flowering plant native to the Sahel regions of Africa and the Arabian Peninsula. This plant is admired for its striking appearance, featuring a swollen stem base (caudex) that stores water, making it well-suited to arid environments. The Desert Rose produces vibrant, trumpet-shaped flowers in shades of pink, red, and white. Its glossy, dark green leaves contrast beautifully with its flowers, adding to its ornamental appeal. Due to its ability to thrive in dry conditions, Adenium obesum is popular among bonsai enthusiasts and gardeners who favor drought-tolerant plants.\n\nCatharanthus roseus (Madagascar Periwinkle)\n\nCatharanthus roseus, also known as Madagascar Periwinkle or Vinca, is a perennial plant native to Madagascar. This hardy and versatile plant is celebrated for its continuous blooming habit, producing abundant pink, white, or red flowers throughout the year. It is often used in gardens and landscaping due to its ability to thrive in a variety of conditions, from full sun to partial shade. Beyond its ornamental value, Catharanthus roseus is renowned in the pharmaceutical industry for its alkaloid compounds, which have been used in the development of important cancer treatment drugs, such as vincristine and vinblastine.","106":"This project entails the development of an advanced data analysis dashboard using Microsoft Excel to evaluate and visualize road accident and casualty statistics for the years 2021 and 2022. The primary objective is to provide a comprehensive, user-friendly interface that offers insightful analysis into the patterns, trends, and factors contributing to road accidents and their resultant casualties.\n\n** Column Descriptions:\n\n1. **Accident_Index**: A unique identifier for each accident recorded in the dataset. This ensures that each accident can be referenced individually.\n\n2. **Accident Date**: The date on which the accident occurred. This includes day, month, and year.\n\n3. **Month**: The month in which the accident occurred, extracted from the accident date. This helps in analyzing monthly trends.\n\n4. **Year**: The year in which the accident occurred, extracted from the accident date. Useful for annual trend analysis.\n\n5. **Day_of_Week**: The day of the week on which the accident occurred. This can help in understanding if there are more accidents on specific days.\n\n6. **Junction_Control**: Indicates the type of control present at the junction where the accident occurred (e.g., traffic lights, stop sign).\n\n7. **Junction_Detail**: Provides more detailed information about the junction's layout (e.g., T-junction, roundabout).\n\n8. **Accident_Severity**: The severity of the accident, which can range from slight, serious, to fatal.\n\n9. **Latitude**: The geographical latitude where the accident occurred. This helps in mapping the location of accidents.\n\n10. **Light_Conditions**: Describes the lighting conditions at the time of the accident (e.g., daylight, darkness with street lighting).\n\n11. **Local_Authority_(District)**: The local authority district where the accident took place. This is useful for regional analysis.\n\n12. **Carriageway_Hazards**: Any hazards present on the carriageway that might have contributed to the accident (e.g., roadworks, oil spill).\n\n13. **Longitude**: The geographical longitude where the accident occurred. Used in conjunction with latitude for mapping.\n\n14. **Number_of_Casualties**: The total number of casualties resulting from the accident. This includes all levels of injury severity.\n\n15. **Number_of_Vehicles**: The number of vehicles involved in the accident. This helps in understanding the scale of the accident.\n\n16. **Police_Force**: The police force that responded to and recorded the accident. This can indicate jurisdiction and reporting standards.\n\n17. **Road_Surface_Conditions**: Describes the condition of the road surface at the time of the accident (e.g., dry, wet).\n\n18. **Road_Type**: The type of road where the accident occurred (e.g., single carriageway, dual carriageway).\n\n19. **Speed_limit**: The speed limit in effect on the road where the accident occurred. This can be a factor in accident severity.\n\n20. **Time**: The exact time of day when the accident occurred. This can help in analyzing patterns related to time.\n\n21. **Urban_or_Rural_Area**: Indicates whether the accident occurred in an urban or rural area. This can influence accident characteristics.\n\n22. **Weather_Conditions**: The weather conditions at the time of the accident (e.g., fine, raining). This can impact driving conditions.\n\n23. **Vehicle_Type**: The type of vehicle(s) involved in the accident. This can help in understanding which types of vehicles are most frequently involved in accidents.**","107":"This Dataset is **forked from the Kaggle Cohort-4 Skill Assessment Challenge.** \nThe dataset for this competition (both train and test) was generated from a deep learning model fine-tuned on the [Used Car Price Prediction Dataset](https:\/\/www.kaggle.com\/datasets\/taeefnajib\/used-car-price-prediction-dataset) dataset. Feature distributions are close to, but not exactly the same, as the original. **Feel free to use the original dataset** as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance, but that is not required.\n\n**train.csv ** - the training dataset; refer to the original dataset link above for column descriptions\n\n**test.csv **- the test dataset; your objective is to predict the value of the target Price\n\n\n**Brand & Model:** Identify the brand or company name along with the specific model of each vehicle.\n\n**Model Year:** Discover the manufacturing year of the vehicles, crucial for assessing depreciation and technology advancements.\n\n**Mileage:** Obtain the mileage of each vehicle, a key indicator of wear and tear and potential maintenance requirements.\n\n**Fuel Type:** Learn about the type of fuel the vehicles run on, whether it's gasoline, diesel, electric, or hybrid.\n\n**Engine Type:** Understand the engine specifications, shedding light on performance and efficiency.\n\n**Transmission:** Determine the transmission type, whether automatic, manual, or another variant.\n\n**Exterior & Interior Colors:** Explore the aesthetic aspects of the vehicles, including exterior and interior color options.\n\n**Accident History:** Discover whether a vehicle has a prior history of accidents or damage, crucial for informed decision-making.\n\n**Clean Title:** Evaluate the availability of a clean title, which can impact the vehicle's resale value and legal status.\n\n***Price:*** Access the listed prices for each vehicle, aiding in price comparison and budgeting.\n","108":"\n# Crash Data in Somerville, USA\n\nThis dataset provides detailed information about traffic crashes in Somerville, USA, including various factors and circumstances related to each incident.\n\n## Columns Description\n\n- **Crash Number**: Unique identifier for each crash event.\n- **Date and Time of Crash**: Date and time when the crash occurred.\n- **Police Shift**: Shift of the police responding to the crash.\n- **Crash Location**: Location where the crash occurred.\n- **Light Conditions**: Conditions of light at the time of the crash (e.g., day, night).\n- **Weather Conditions**: Weather conditions during the crash.\n- **Road Surface**: Type of road surface at the crash location.\n- **Road Contributing Circumstances**: Contributing factors or circumstances related to the road condition.\n- **Traffic Control Device Type**: Type of traffic control device present at the crash location.\n- **Roadway Intersection Type**: Type of intersection where the crash occurred.\n- **Trafficway Description**: Description of the trafficway involved in the crash.\n- **Manner of Collision**: Manner in which the collision occurred (e.g., rear-end, head-on).\n- **First Harmful Event**: Initial event that caused harm in the crash sequence.\n- **First Harmful Event Location**: Location where the first harmful event occurred.\n- **Speed Limit**: Speed limit at the crash location.\n- **Work Zone**: Indicates if the crash occurred within a work zone.\n- **Count Fatal Injury**: Number of fatalities resulting from the crash.\n- **Count Suspected Serious Injury**: Number of suspected serious injuries.\n- **Count Suspected Minor Injury**: Number of suspected minor injuries.\n- **Count Possible Injury**: Number of possible injuries.\n- **Count No Apparent Injury**: Number of individuals with no apparent injuries.\n- **Count Unknown Injury**: Number of injuries with unknown severity.\n- **Count Not Reported Injury**: Number of injuries not reported.\n- **Total Non-Motorists**: Total number of non-motorists involved in the crash.\n- **Pedestrian Involvement (Non-Motorist)**: Indicates pedestrian involvement in non-motorist incidents.\n- **Cyclist Involvement (Non-Motorist)**: Indicates cyclist involvement in non-motorist incidents.\n- **Other Non-Motorist Involvement**: Involvement of other non-motorists in the crash.\n- **Hit and Run Flag**: Indicates if the crash was a hit-and-run incident.\n- **Latitude**: Geographic latitude coordinate of the crash location.\n- **Longitude**: Geographic longitude coordinate of the crash location.\n- **Ward**: Ward where the crash occurred.\n- **Block Code**: Code representing the specific block of the crash location.\n\nThis dataset is valuable for analyzing traffic safety, identifying trends, and understanding factors contributing to crashes in Somerville. It can be used for research, policy-making, and implementing interventions to improve road safety and reduce accidents.\n\n\n\n\nThis data set contains Somerville crashes that occurred from May 2018 to present. Crash reports are completed when a motor vehicle crash occurs on a public way and involves at least one of the following: Any person is killed, any person is injured,  or damage is in excess of $1,000 to any one vehicle or other property. Data does not include crashes that are under active investigation, nor those that occur on state roads, which are under the jurisdiction of the Massachusetts State Police.   State crash data may be accessed on the Massachusetts Department of Transportation.\n\n\nData source : https:\/\/catalog.data.gov\/dataset\/police-data-crashes","109":"### Vehicle Maintenance Dataset\n\n#### Overview\nThis dataset provides synthetic data related to vehicle maintenance to help predict whether a vehicle requires maintenance or not based on various features.\n\n#### Features\n\n1. **Vehicle_Model**: Type of the vehicle (Car, SUV, Van, Truck, Bus, Motorcycle)\n2. **Mileage**: Total mileage of the vehicle\n3. **Maintenance_History**: Maintenance history of the vehicle (Good, Average, Poor)\n4. **Reported_Issues**: Number of reported issues\n5. **Vehicle_Age**: Age of the vehicle in years\n6. **Fuel_Type**: Type of fuel used (Diesel, Petrol, Electric)\n7. **Transmission_Type**: Transmission type (Automatic, Manual)\n8. **Engine_Size**: Size of the engine in cc (Cubic Centimeters)\n9. **Odometer_Reading**: Current odometer reading of the vehicle\n10. **Last_Service_Date**: Date of the last service\n11. **Warranty_Expiry_Date**: Date when the warranty expires\n12. **Owner_Type**: Type of vehicle owner (First, Second, Third)\n13. **Insurance_Premium**: Insurance premium amount\n14. **Service_History**: Number of services done\n15. **Accident_History**: Number of accidents the vehicle has been involved in\n16. **Fuel_Efficiency**: Fuel efficiency of the vehicle in km\/l (Kilometers per liter)\n17. **Tire_Condition**: Condition of the tires (New, Good, Worn Out)\n18. **Brake_Condition**: Condition of the brakes (New, Good, Worn Out)\n19. **Battery_Status**: Status of the battery (New, Good, Weak)\n20. **Need_Maintenance**: Target variable indicating whether the vehicle needs maintenance (1 = Yes, 0 = No)\n\n#### Target Variable\n- **Need_Maintenance**: Indicates whether the vehicle requires maintenance or not based on specified conditions.\n\n#### Data Range\n- Total number of records: 50,000\n\n#### Source\nThis dataset is synthetic and was generated using Python. It is intended for educational and research purposes.\n\n#### Acknowledgements\n- The dataset was generated using Python and the data is synthetic.\n\n\n","110":"The inspiration behind creating this dataset stems from the need to analyze and understand road safety trends, identify risk factors, and develop targeted interventions to reduce road accidents. By categorizing accidents based on vehicle types, researchers, policymakers, and transportation authorities can gain insights into the unique challenges and characteristics associated with different types of vehicles on the road. This dataset aims to serve as a valuable resource for research, analysis, and policymaking efforts aimed at improving road safety and reducing the impact of accidents on society.\n\n","111":"\"Road Accidents Dataset\":\n\nDescription: This comprehensive dataset provides detailed information on road accidents reported over multiple years. The dataset encompasses various attributes related to accident status, vehicle and casualty references, demographics, and severity of casualties. It includes essential factors such as pedestrian details, casualty types, road maintenance worker involvement, and the Index of Multiple Deprivation (IMD) decile for casualties' home areas.\n\nColumns:\n\n1.Status: The status of the accident (e.g., reported, under investigation).\n\n2.Accident_Index: A unique identifier for each reported accident.\n\n3.Accident_Year: The year in which the accident occurred.\n\n4.Accident_Reference: A reference number associated with the accident.\n\n5.Vehicle_Reference: A reference number for the involved vehicle in the accident.\n\n6.Casualty_Reference: A reference number for the casualty involved in the accident.\n\n7.Casualty_Class: Indicates the class of the casualty (e.g., driver, passenger, pedestrian).\n\n8.Sex_of_Casualty: The gender of the casualty (male or female).\n\n9.Age_of_Casualty: The age of the casualty.\n\n10.Age_Band_of_Casualty: Age group to which the casualty belongs (e.g., 0-5, 6-10, 11-15).\n\n11.Casualty_Severity: The severity of the casualty's injuries (e.g., fatal, serious, slight).\n\n12.Pedestrian_Location: The location of the pedestrian at the time of the accident.\n\n13.Pedestrian_Movement: The movement of the pedestrian during the accident.\n\n14.Car_Passenger: Indicates whether the casualty was a car passenger at the time of the accident (yes or no).\n\n15.Bus_or_Coach_Passenger: Indicates whether the casualty was a bus or coach passenger (yes or no).\n\n16.Pedestrian_Road_Maintenance_Worker: Indicates whether the casualty was a road maintenance worker (yes or no).\n\n17.Casualty_Type: The type of casualty (e.g., driver\/rider, passenger, pedestrian).\n\n18.Casualty_Home_Area_Type: The type of area in which the casualty resides (e.g., urban, rural).\n\n19.Casualty_IMD_Decile: The IMD decile of the area where the casualty resides (a measure of deprivation).\n\n20.LSOA_of_Casualty: The Lower Layer Super Output Area (LSOA) associated with the casualty's location.\n\nThis dataset provides valuable insights for analyzing road accidents, identifying trends, and implementing safety measures to reduce casualties and enhance road safety. Researchers, policymakers, and analysts can leverage this dataset for evidence-based decision-making and improving overall road transportation systems.","112":"Dataset Overview\n\nThis dataset provides a detailed view of traffic data in a futuristic urban environment, containing over 1.2 million records. Each record represents a unique snapshot of various factors affecting traffic conditions in six fictional cities.\n\nFeatures\n\nCity: Name of the city (e.g., MetropolisX, SolarisVille).\nVehicle Type: Type of vehicle (e.g., Car, Flying Car).\nWeather Conditions: Current weather (e.g., Clear, Rainy).\nEconomic Conditions: Economic state of the city (e.g., Booming, Recession).\nDay of Week: Day of the week.\nHour of Day: Hour of the day when the data was recorded.\nSpeed: Recorded vehicle speed.\nEnergy Consumption: Estimated energy consumption based on vehicle type and speed.\nIs Peak Hour: Indicator if the record was during peak traffic hours.\nRandom Event Occurred: Indicator if a random event (e.g., accidents, road closures) occurred.\nTraffic Density: Density of traffic at the time of recording.\nFile Format\n\nThe dataset is provided in CSV format, suitable for analysis in various data processing tools and programming languages.\n\nPotential Uses\n\nThis dataset can be utilized for a variety of studies and analyses, including:\n\nUnderstanding traffic patterns in futuristic urban environments.\nAnalyzing the impact of factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\nDeveloping and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n","113":"This dataset provides insights into fatal car crashes with a focus on Asian-related factors, including vehicle make and model. It includes information on lighting conditions, weather, state, city, and driver behavior. Explore the correlation between these factors and fatal accidents involving Asian car brands.","114":"\n****Dataset Description: Road Accident Records****\n\nThis dataset contains detailed records of road accidents occurring within a specific geographic region over a defined period. The data encompasses various attributes related to each accident, providing valuable insights into factors contributing to road safety issues.\n\n**Attributes Included:**\n\n1. **Accident ID:** A unique identifier assigned to each accident for reference and tracking purposes.\n2. **Date and Time:** The date and time when the accident occurred, facilitating temporal analysis and trend identification.\n3. **Location:** The precise geographical coordinates or address where the accident took place, enabling spatial analysis and mapping.\n4. **Severity:** The severity level of the accident, categorized based on the extent of injuries, property damage, or fatalities.\n5. **Weather Conditions:** Information about weather conditions prevailing at the time of the accident, such as clear, rainy, foggy, or snowy weather.\n6. **Road Conditions:** Description of road conditions, including dry, wet, icy, or slippery surfaces.\n7. **Vehicle Involved:** Details about vehicles involved in the accident, including types, models, and counts.\n8. **Contributing Factors:** Factors contributing to the accident, such as speeding, distracted driving, drunk driving, road defects, or mechanical failures.\n9. **Injuries and Fatalities:** Number of individuals injured or killed as a result of the accident.\n10. **Vehicle Maneuvers:** Description of maneuvers or actions taken by vehicles involved, such as turning, braking, or overtaking.\n\n**Potential Uses:**\n\n1. **Safety Analysis:** Identifying high-risk locations, times, and conditions to develop targeted safety measures and interventions.\n2. **Predictive Modeling:** Building predictive models to anticipate and prevent accidents based on historical patterns and contributing factors.\n3. **Policy Formulation:** Informing policymaking and infrastructure planning to improve road safety and reduce accident rates.\n4. **Public Awareness Campaigns:** Designing targeted awareness campaigns to educate drivers about common risk factors and safe driving practices.\n5. **Law Enforcement:** Supporting law enforcement efforts by identifying areas with high accident rates and enforcing traffic regulations accordingly.\n\n**Data Source:**\n\nThe dataset is sourced from official accident reports, police records, or other reliable sources authorized to collect and maintain such data. Care has been taken to ensure accuracy and completeness, although some discrepancies may exist due to reporting errors or data collection limitations.\n\n**Note:** Use of this dataset for research, analysis, or other purposes should adhere to relevant data privacy and ethical guidelines, ensuring responsible use and respect for individual privacy rights.","115":"The police of Czech Republic regularly gathers and releases detailed data on traffic incidents throughout the nation, typically on an monthly basis. This dataset covers various aspects such as geographic locations, weather conditions, vehicle types, casualty counts, and vehicle maneuvers. The wealth of information makes it a compelling and extensive dataset for analysis and research purposes.\n\n- Most coded variables in the data have been translated into text strings using lookup tables, making the analysis more efficient and user-friendly.\n- After analyzing the dataset, I've filled in the majority of NaN values.\n\nContent\nThe data come from the [police of Czech Republic](https:\/\/www.policie.cz\/clanek\/statistika-nehodovosti-900835.aspx) \n\nThe dataset comprises of two csv files:\n\n- road_accidents_czechia_2016_2022.csv: every line in the file represents a unique traffic accident, featuring various properties related to the accident as columns\n\n- pedestrian.csv: Each line in the file corresponds to a distinct pedestrian-involved traffic incident, with various incident-related attributes captured as columns.\n","116":"## Content  \nThe [**Police Service of Northern Ireland (PSNI)**](https:\/\/www.psni.police.uk\/) compiles data on **road traffic collisions (RTCs)** that result in injuries. This data is used to monitor and identify trends in the number of individuals killed or injured (either seriously or slightly) due to RTCs on Northern Ireland\u2019s roads.\n\nThe PSNI\u2019s injury collision data for Northern Ireland, combined with those for England, Scotland and Wales, provide a comprehensive overview of all such collisions across the UK.\n\nThis dataset is retrieved from data.gov.uk, a website that is built and maintained by the Government Digital Service. It contains four csv files:\n1. **casualty2017-2022.csv**: The casualties involved in collisions, including casualty information (whether the casualty was a driver, passenger, pedestrian or cyclist) and vehicle involved.\n2. **collision2017-2022.csv**: The conditions under which the collisions occurred, including collision severity, number of vehicles and casualties involved, time, location, weather, road conditions and carriageway hazards.\n3. **vehicle2017-2022.csv**: The vehicles involved in each collision, including type of vehicle, operation at the time of the collision, object involved and driver information.\n4. **vehicle-index.csv**: A list of variables and values for vehicle2017-2022.csv.\n  \n## Possible Explorations\n- Assess how environmental conditions affect accident likelihood and severity.\n- Examine the influence of time (day and hour) on accident occurrence and severity.\n- Show differences in casualty demographics.\n- Identify regional variations in road traffic collisions.\n- Analyze trends in traffic accidents and fatality rates.\n\n## Important Note\nThe following variables are documented for collisions resulting in fatal and serious injuries only:  \n&gt;**Collision Records**  \n1. a_jdet - Junction Detail\n2. a_jcont - Junction Control\n3. a_pedphys - Pedestrian Crossing Facilities - Physical\n4. a_pedhum - Pedestrian Crossing Facilities - Human\n5. a_light - Light Conditions\n6. a_weat - Weather Conditions\n7. a_roadsc - Road Surface Conditions\n8. a_speccs - Special Conditions at Site\n9. a_chaz - Carriageway Hazard\n10. a_scene - Did a Police Officer Attend the Scene of the Collision  \n  \n&gt;**Casualty Records**  \n1. c_loc - Pedestrian Location\n2. c_move - Pedestrian Movement\n3. c_pcv - Bus or Coach Passenger\n4. c_pedinj - Pedestrian casualty injured in the course of on the road work  \n  \n&gt;**Vehicle Records**  \n1. v_junc - Junction Location of Vehicle at Time of Impact\n2. v_skid - Skidding \/ Overturning\n3. v_hit - First Object Hit in Carriageway\n4. v_leave - Vehicle Leaving Carriageway\n5. v_hitoff - First Object Hit off Carriageway\n6. v_forreg - Foreign Registered Vehicle\n\n## Licence\nUK Open Government Licence (OGL)\n\n## Acknowledgements  \nThe data were publicly visible and published by the Police Service of Northern Ireland.","117":"## Overview\n\nThis dataset provides a comprehensive look at traffic data in a futuristic urban setting. It includes over 1.2 million records, each representing a unique snapshot of various factors influencing traffic conditions in six fictional cities.\n\n## Features\n\n- **City**: The name of the city (e.g., MetropolisX, SolarisVille).\n- **Vehicle Type**: Type of vehicle in use (e.g., Car, Flying Car).\n- **Weather Conditions**: Current weather conditions at the time of data capture (e.g., Clear, Rainy).\n- **Economic Conditions**: Economic state of the city at the time of the record (e.g., Booming, Recession).\n- **Day of Week**: The day of the week.\n- **Hour of Day**: The hour of the day when the data was recorded.\n- **Speed**: Recorded speed of the vehicle.\n- **Energy Consumption**: An estimate of energy consumption based on vehicle type and speed.\n- **Is Peak Hour**: Indicator of whether the record was during peak traffic hours.\n- **Random Event Occurred**: Indicator of whether a random event (like accidents or road closures) occurred.\n- **Traffic Density**: The density of traffic at the time of recording.\n\n## File Format\n\nThe dataset is provided in a CSV format, suitable for analysis in various data processing tools and programming languages.\n\n## Potential Uses\n\nThis dataset can be used for a range of studies and analyses, including but not limited to:\n\n- Understanding traffic patterns in futuristic urban environments.\n- Analyzing the impact of various factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\n- Developing and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n\n---\n\n*Note: This is a simulated dataset created for analytical and educational purposes.*","118":"This dataset provides detailed records of road accidents that occurred during *January 2021*. It includes information such as the accident date, day of the week, junction control, accident severity, geographical coordinates, lighting and weather conditions, vehicle details, and more. The data is valuable for analyzing and understanding the factors contributing to road accidents in this urban area, aiding in the development of strategies for improved road safety.\n\n\n**Accident_Index:** A unique identifier for each accident record.\n\n**Accident Date:** The date on which the accident occurred (format: DD\/MM\/YYYY).\n\n**Day_of_Week:** The day of the week when the accident took place.\n\n**Junction_Control** : Describes the type of junction control at the accident location (e.g., \"Give way or uncontrolled\").\n\n**Junction_Detail:** Provides additional details about the junction where the accident occurred (e.g., \"T or staggered junction\").\n\n**Accident_Severity:** Indicates the severity of the accident (e.g., \"Serious\").\n\n**Latitude:** The geographic latitude of the accident location.\n\n**Light_Conditions:** Describes the lighting conditions at the time of the accident (e.g., \"Daylight\").\n\n**Local_Authority_(District)**: The local authority district where the accident occurred.\n\n**Carriageway_Hazards:** Describes any hazards present on the carriageway at the time of the accident (e.g., \"None\").\n\n**Longitude:** The geographic longitude of the accident location.\n\n**Number_of_Casualties:** The total number of casualties involved in the accident.\n\n**Number_of_Vehicles:** The total number of vehicles involved in the accident.\n\n**Police_Force:** The police force that handled the accident.\n\n**Road_Surface_Conditions:** Describes the surface conditions of the road at the time of the accident (e.g., \"Dry\").\n\n**Road_Type:** Specifies the type of road where the accident occurred (e.g., \"One way street\").\n\n**Speed_limit:** The speed limit applicable to the road where the accident occurred.\n\n**Time:** The time of day when the accident happened (format: HH:MM).\n\n**Urban_or_Rural_Area:** Indicates whether the accident occurred in an urban or rural area.\n\n**Weather_Conditions:** Describes the weather conditions at the time of the accident (e.g., \"Fine no high winds\").\n\n**Vehicle_Type:** Specifies the type of vehicle involved in the accident (e.g., \"Car,\" \"Taxi\/Private hire car\").","119":"This dataset is sourced from the United Kingdom's Open Data website (https:\/\/www.data.gov.uk\/).\n\nIn this dataset, you will find extensive information on the characteristics of individual vehicular accidents. The data is split into three main datasets and one data guide which can be used to reference dummy variables.\n\nThe three main data sets are:\n\n- Casualty\n- Collision\n- Vehicle\n\nThe data sets can be combined using the 'accident_index' key across the data sets.","120":"## **Introduction:**\n\nRoad accidents pose significant threats to public safety and necessitate a comprehensive understanding of various factors influencing their occurrence. This article explores key aspects related to accident severity and emphasizes the importance of effective road management strategies.\n\n##**Exploring Geographic and Temporal Aspects of Road Incidents:**\n\nTo enhance road safety, it is crucial to delve into the geographic and temporal dimensions of road incidents. Analyzing the locations and times at which accidents frequently occur enables authorities to implement targeted interventions. This section discusses the significance of spatial and temporal analysis in devising proactive safety measures.\n\n##**A Comprehensive Dataset for Traffic Incident Research:**\n\nA robust dataset forms the foundation for meaningful research in traffic incident analysis. This segment highlights the need for comprehensive data collection, emphasizing variables such as road infrastructure, vehicle types, and driver demographics. The article emphasizes the importance of open and accessible datasets to facilitate research and policy development.\n\n##**Impacts of Weather and Road Conditions on Accident Rates:**\n\nWeather and road conditions play a pivotal role in determining accident rates. This section explores the correlations between adverse weather, poor road conditions, and increased accident severity. Understanding these relationships can aid in developing strategies to mitigate risks during inclement weather.\n\n##**Identifying Hotspots and Risk Factors in Road Safety:**\n\nEffective road management involves identifying accident hotspots and understanding the underlying risk factors. By employing data-driven analysis techniques, authorities can pinpoint areas with high accident rates and implement targeted interventions. This portion of the article discusses methodologies for hotspot identification and risk factor analysis.\n\n##**Data-driven Approaches to Reduce Road Accidents:**\n\nHarnessing the power of data is essential for developing proactive strategies to reduce road accidents. This section focuses on data-driven approaches, including predictive modeling and machine learning, to identify potential accident scenarios and implement preventive measures. The integration of technology and analytics is crucial for achieving substantial improvements in road safety.\n\n##**Traffic Collision Analysis for Urban Planning Strategies:**\n\nUrban planning plays a crucial role in shaping road safety outcomes. This part of the article explores how traffic collision analysis can inform urban planning strategies. By incorporating safety considerations into urban design, cities can create environments that minimize the risk of accidents and enhance overall road safety.\n\n##**Patterns of Driver Behavior and Their Influence on Accidents:**\n\nUnderstanding patterns of driver behavior is paramount for effective road management. This section examines the impact of driver behavior on accident rates and discusses how insights into these patterns can inform targeted educational campaigns and enforcement strategies.\n\n##**Conclusion:**\n\nIn conclusion, this article emphasizes the multifaceted nature of road safety and the importance of a holistic approach to accident prevention. By considering factors such as geographic and temporal aspects, comprehensive datasets, weather and road conditions, hotspots and risk factors, data-driven approaches, urban planning, and driver behavior, authorities can formulate effective road management strategies to enhance public safety.","121":"_____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n","122":" Road Accident Details:\nThis dataset comprises 5000 entries detailing road accidents. Each entry contains 15 columns covering crucial accident-related information. It includes unique accident IDs, accident locations, dates, severity levels, weather conditions during the accidents, types of vehicles involved, injury counts, fatalities, causes (such as human error or weather conditions), road types (like highways or city streets), vehicle speeds, alcohol involvement, road conditions, and time of the accidents. The dataset is diverse, offering insights into the circumstances, factors, and outcomes of these accidents, crucial for analysis and pattern recognition in road safety.\n\nAccident Investigation and Reporting:\nComplementing the first dataset, this collection of 5000 entries provides detailed information regarding accident investigations and subsequent reporting. It contains 15 columns, including accident IDs linking it to the first dataset, accident descriptions, police station details involved in the investigation, investigation statuses (such as pending or completed), reporters (like witnesses or involved parties), insurance claim information, fault attributions, injured persons' names and contact information, details about vehicle damages, repair costs, and legal proceedings. This dataset offers a comprehensive view of the post-accident processes, legal aspects, and financial implications, aiding in understanding the aftermath and resolution of road accidents.","123":"The Airbag Recommendation Dataset: An Analysis and Insights\n\nIn recent decades, the automotive industry has witnessed substantial advancements in safety measures, notably the integration of airbag systems to reduce the impact of collisions and safeguard occupants. The efficacy of airbags largely depends on various factors such as their placement, design, and the dynamics of the crash scenario. To better understand and improve airbag systems, researchers and engineers heavily rely on datasets that capture real-world collision data, aiding in the refinement and enhancement of these safety mechanisms.\n\nThe Airbag Recommendation Dataset serves as a pivotal resource within this domain. This dataset aggregates information from a multitude of sources, encompassing diverse vehicular collisions, encompassing crash-test data, real-world accidents, and simulations. It consolidates information on the performance of different airbag systems across various makes and models of vehicles, shedding light on their effectiveness under different crash conditions.\n\nThis dataset encompasses a wide array of variables and features, offering a comprehensive view of airbag deployment scenarios. Some of the crucial data points included in this dataset are:\n\n1. **Vehicle Information:** Details about the vehicle involved in the crash, including make, model, year, and specifications relevant to airbag deployment.\n  \n2. **Crash Parameters:** Information about the crash, including impact speed, collision angle, and severity.\n\n3. **Occupant Details:** Data pertaining to the occupants involved, such as their seating position, age, and whether they were wearing seat belts at the time of the crash.\n\n4. **Airbag Deployment:** Details about the airbag system, including the type of airbags deployed (frontal, side, curtain), their deployment timing, and effectiveness in mitigating injury.\n\n5. **Injury Assessment:** Information about injuries sustained by the occupants, ranging from minor to severe, correlated with the airbag deployment scenarios.\n\nAnalyzing this dataset provides valuable insights into various aspects of airbag deployment:\n\n1. **Effectiveness of Different Airbag Systems:** By comparing the performance of diverse airbag systems across different vehicles and collision scenarios, researchers can ascertain which designs are more effective in specific crash situations. For instance, frontal airbags might excel in head-on collisions, while side airbags might prove more beneficial in T-bone accidents.\n\n2. **Optimal Deployment Strategies:** Understanding the correlation between deployment timing and injury mitigation is critical. This dataset aids in evaluating the timing and sequence of airbag deployment to maximize protection while minimizing potential harm caused by airbag impact.\n\n3. **Occupant Positioning and Protection:** The dataset helps in determining the impact of passenger positioning within the vehicle concerning airbag effectiveness. This insight is pivotal in recommending safe seating positions for occupants of different ages and sizes.\n\n4. **Improving Design and Engineering:** Engineers and vehicle manufacturers can use this dataset to refine airbag designs and systems, optimizing their performance in real-world crash scenarios.\n\n5. **Policy and Regulation Implications:** Insights from this dataset can influence safety standards, regulations, and policies related to airbag systems, ensuring higher safety standards in the automotive industry.\n\nHowever, while the Airbag Recommendation Dataset holds immense potential, it also comes with challenges and limitations. Data quality, consistency across different sources, and the representation of various vehicle types and crash scenarios are some of the challenges that researchers need to address when utilizing this dataset.\n\nIn conclusion, the Airbag Recommendation Dataset stands as a valuable asset for researchers, engineers, policymakers, and automotive manufacturers alike. Its insights play a crucial role in advancing the safety standards of airbag systems, contributing significantly to the ongoing efforts to enhance vehicle safety and reduce the severity of injuries sustained in automotive accidents. Continued research and analysis of this dataset are pivotal in driving innovation and improving safety measures within the automotive industry.","124":"The data contains hourly vehicle counts of particular highway in the year of 1995 beginning from the month of August. The data is of around 70 days span.\n\nThe dataset is good for beginners to practice on Time series","125":"The Motor Vehicle Collisions crash table contains details on the crash event. Each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC. The police report (MV104-AN) is required to be filled out for collisions where someone is injured or killed, or where there is at least $1000 worth of damage (https:\/\/www.nhtsa.gov\/sites\/nhtsa.dot.gov\/files\/documents\/ny_overlay_mv-104an_rev05_2004.pdf). \n\nIt should be noted that the data is preliminary and subject to change when the MV-104AN forms are amended based on revised crash details. For the most accurate, up to date statistics on traffic fatalities, please refer to the NYPD Motor Vehicle Collisions page (updated weekly) or Vision Zero View (updated monthly).\n\nDue to success of the CompStat program, NYPD began to ask how to apply the CompStat principles to other problems. Other than homicides, the fatal incidents with which police have the most contact with the public are fatal traffic collisions. Therefore in April 1998, the Department implemented TrafficStat, which uses the CompStat model to work towards improving traffic safety. Police officers complete form MV-104AN for all vehicle collisions. The MV-104AN is a New York State form that has all of the details of a traffic collision. Before implementing Trafficstat, there was no uniform traffic safety data collection procedure for all of the NYPD precincts. Therefore, the Police Department implemented the Traffic Accident Management System (TAMS) in July 1999 in order to collect traffic data in a uniform method across the City. TAMS required the precincts manually enter a few selected MV-104AN fields to collect very basic intersection traffic crash statistics which included the number of accidents, injuries and fatalities. As the years progressed, there grew a need for additional traffic data so that more detailed analyses could be conducted. The Citywide traffic safety initiative, Vision Zero started in the year 2014. Vision Zero further emphasized the need for the collection of more traffic data in order to work towards the Vision Zero goal, which is to eliminate traffic fatalities. Therefore, the Department in March 2016 replaced the TAMS with the new Finest Online Records Management System (FORMS). FORMS enables the police officers to electronically, using a Department cellphone or computer, enter all of the MV-104AN data fields and stores all of the MV-104AN data fields in the Department\u2019s crime data warehouse. Since all of the MV-104AN data fields are now stored for each traffic collision, detailed traffic safety analyses can be conducted as applicable.\n\n","126":"# Description for Spotify Songs Dataset on Kaggle\n__________________________________________________________________________________________________________________\n##Dataset Title: Spotify Songs Dataset\n**Description**:\nThis dataset contains a collection of songs fetched from the Spotify API, covering various genres including \"acoustic\", \"afrobeat\", \"alt-rock\", \"alternative\", \"ambient\", \"anime\", \"black-metal\", \"bluegrass\", \"blues\", \"bossanova\", \"brazil\", \"breakbeat\", \"british\", \"cantopop\", \"chicago-house\", \"children\", \"chill\", \"classical\", \"club\", \"comedy\", \"country\", \"dance\", \"dancehall\", \"death-metal\", \"deep-house\", \"detroit-techno\", \"disco\", \"disney\", \"drum-and-bass\", \"dub\", \"dubstep\", \"edm\", \"electro\", \"electronic\", \"emo\", \"folk\", \"forro\", \"french\", \"funk\", \"garage\", \"german\", \"gospel\", \"goth\", \"grindcore\", \"groove\", \"grunge\", \"guitar\", \"happy\", \"hard-rock\", \"hardcore\", \"hardstyle\", \"heavy-metal\", \"hip-hop\", \"holidays\", \"honky-tonk\", \"house\", \"idm\", \"indian\", \"indie\", \"indie-pop\", \"industrial\", \"iranian\", \"j-dance\", \"j-idol\", \"j-pop\", \"j-rock\", \"jazz\", \"k-pop\", \"kids\", \"latin\", \"latino\", \"malay\", \"mandopop\", \"metal\", \"metal-misc\", \"metalcore\", \"minimal-techno\", \"movies\", \"mpb\", \"new-age\", \"new-release\", \"opera\", \"pagode\", \"party\", \"philippines-opm\", \"piano\", \"pop\", \"pop-film\", \"post-dubstep\", \"power-pop\", \"progressive-house\", \"psych-rock\", \"punk\", \"punk-rock\", \"r-n-b\", \"rainy-day\", \"reggae\", \"reggaeton\", \"road-trip\", \"rock\", \"rock-n-roll\", \"rockabilly\", \"romance\", \"sad\", \"salsa\", \"samba\", \"sertanejo\", \"show-tunes\", \"singer-songwriter\", \"ska\", \"sleep\", \"songwriter\", \"soul\", \"soundtracks\", \"spanish\", \"study\", \"summer\", \"swedish\", \"synth-pop\", \"tango\", \"techno\", \"trance\", \"trip-hop\", \"turkish\", \"work-out\", \"world-music\". Each entry in the dataset provides detailed information about a song, including its name, artists, album, popularity, duration, and whether it is explicit.\n__________________________________________________________________________________________________________________\n**Data Collection Method**:\nThe data was collected using the Spotify Web API through a Python script. The script performed searches for different genres and retrieved the top tracks for each genre. The fetched data was then compiled and saved into a CSV file.\n__________________________________________________________________________________________________________________\n**Columns Description**:\nid: Unique identifier for the track on Spotify.\nname: Name of the track.\ngenre: genre of the song.\nartists: Names of the artists who performed the track, separated by commas if there are multiple artists.\nalbum: Name of the album the track belongs to.\npopularity: Popularity score of the track (0-100, where higher is more popular).\nduration_ms: Duration of the track in milliseconds.\nexplicit: Boolean indicating whether the track contains explicit content.\n__________________________________________________________________________________________________________________\n**Potential Uses**:\nThis dataset can be used for a variety of purposes, including but not limited to:\n\n- Music Analysis: Analyze the popularity and characteristics of songs across different genres.\n- Recommendation Systems: Develop and test music recommendation algorithms.\n- Trend Analysis: Study trends in music preferences and popularity over time.\n- Machine Learning: Train machine learning models for tasks like genre classification or popularity prediction.\n__________________________________________________________________________________________________________________\n**Acknowledgements**:\nThis dataset was created using the Spotify Web API. Special thanks to Spotify for providing access to their extensive music library through their API.\n__________________________________________________________________________________________________________________\n**License**:\nThis dataset is made available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You are free to use, modify, and distribute this dataset, provided you give appropriate credit to the original creator.\n__________________________________________________________________________________________________________________","127":"This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.","128":"## Complete Pok\u00e9mon Dataset 9th Gen (img + tabular)\nThe Complete Pok\u00e9mon Dataset 9th Gen (img + tabular) is a comprehensive collection of information covering the entirety of the 9th generation Pok\u00e9mon. It combines both image data of Pok\u00e9mon sprites and tabular data containing their base statistics and index ranks. This dataset serves as a valuable resource for researchers, developers, and enthusiasts interested in exploring and analyzing the characteristics of Pok\u00e9mon in the latest generation.\n\n## **Dataset Overview**:\n### Image Data:\n\n**Pok\u00e9mon Sprites**: High-quality images representing the visual appearance of each Pok\u00e9mon in the 9th generation.\nFormat: Images are provided in a standardized format for easy integration into various applications and analysis tools.\nIndexing: Each image is indexed to correspond with the respective entry in the tabular data for seamless cross-referencing.\n\n### Tabular Data:\n\n**Base Stats**: Comprehensive information on the base statistics of each Pok\u00e9mon, including HP, Attack, Defense, Special Attack, Special Defense, and Speed.\n**Index Rank**: An index rank assigned to each Pok\u00e9mon based on various attributes, such as overall power, rarity, or popularity.\n**Additional Attributes**: Any additional relevant attributes, such as Pok\u00e9mon type, abilities, evolutionary line, and regional variants.","129":"**Dataset Description: Most Popular Anime of All Time**\n\nThis dataset compiles information on the most popular anime titles of all time, drawing its data from IMDb.com. IMDb, known for its extensive database of films and television series, serves as a reputable source for aggregating ratings, reviews, and other relevant details.\n\n****Key Features:**\n\n**Name:** The name of the anime series, representing the title under consideration.\n\n**Type: **Categorization of the anime, distinguishing between series, movies, or other formats.\n\n**Aired: **The release year or range of years during which the anime was originally aired.\n\n**Rating:** IMDb ratings, reflecting the overall reception and viewer satisfaction with each anime.\n\n**Votes:** The number of votes received by each anime on IMDb, providing insights into its popularity and audience engagement.\n\n**Description:** A brief summary or description of the anime's storyline, offering a glimpse into its thematic elements.\n\n**Studio:** Information about the production studio responsible for creating the anime, shedding light on the creative forces behind its development.\n\n**Data Collection Methodology:**\n\nThe dataset is populated through web scraping IMDb.com, utilizing Python libraries such as BeautifulSoup for parsing HTML and extracting relevant information. IMDb's comprehensive database ensures the inclusion of a diverse array of anime titles, from classic series to contemporary releases.\n\n**Potential Applications:**\n\nThis dataset can be a valuable resource for anime enthusiasts, researchers, and data analysts interested in exploring trends, patterns, and the factors contributing to the popularity of anime titles. The information contained herein facilitates comparative analyses of different anime series, aiding in the identification of trends in viewer preferences over time.\n\n**Usage Considerations:**\n\nUsers are encouraged to exercise caution and verify the dataset periodically, as IMDb ratings and details may change with time. Additionally, proper attribution to IMDb as the source is recommended when utilizing the dataset for research or any other purpose.","130":"This dataset contains a comprehensive collection of anime titles spanning the years 1970 to 2024. The data was collected from MyAnimeList using web scraping techniques. It includes essential information about each anime, such as its unique ID, title, genre, description, studio, release year, and user ratings. The dataset offers a valuable resource for exploring the evolution of anime over the decades and understanding trends in the industry. Researchers, anime enthusiasts, and data analysts can use this dataset to analyze various aspects of anime production and consumption, including popular genres, top-rated studios, and changes in audience preferences over time. The dataset is presented in a CSV format and is suitable for a wide range of data analysis and machine learning applications.\n\n## Columns in the data.json :\nmal_id: Unique identifier for the anime entry.\ntitles: List of titles associated with the anime. In this case, \"Attack No.1\".\ntype: Type of the anime, e.g., TV series.\nsource: Source material of the anime, here it's based on a manga.\nepisodes: Number of episodes in the anime (104 in this case).\nrating: Audience rating category, PG-13 in this example.\nscore: Average score given to the anime by users.\nscored_by: Number of users who have scored the anime.\nrank: Ranking of the anime based on score or popularity.\npopularity: Popularity ranking of the anime.\nmembers: Number of members who have added this anime to their list.\nfavorites: Number of users who have favorited this anime.\nsynopsis: Plot summary or synopsis of the anime.\nstudios: Production studio responsible for creating the anime.\ngenres: List of genres the anime belongs to (e.g., Drama, Sports).\nthemes: List of themes present in the anime (e.g., Team Sports).\n\n## Columns in the  user_recommendation.csv :\nmal_id: This column represents the ID of an anime that users have watched or interacted with.\nmal_id_recomm: This column lists the IDs of anime recommended by users for a specific mal_id.\nvotes: The votes column indicates the number of votes or recommendations given by users for the recommendation of mal_id_recomm for mal_id.\n\n\nThe dataset is ready for exploration, analysis, and visualization to uncover insights into the world of anime and its dynamic landscape.","131":"The \"Anime TV Shows Database\" is a comprehensive collection of data related to various anime TV shows. This dataset provides a wealth of information about anime series, including details about their production, ratings, related shows, genres, and more.\n\n\nKey Features and Attributes:\n\nBasic Information: Includes the title of the show, its type (e.g., TV, OVA, Movie), the year of release, and the season it premiered.\n\nProduction and Studios: Contains information about the studios responsible for producing the anime series, along with studio locations.\n\nRanking and Ratings: Provides details on the ranking of each show, user ratings, and any available reviews.\n\nRelated Shows and Franchises: Lists related shows and provides information about any franchises to which the show belongs.","132":"This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry.","133":"","134":"# Description\n\nOur dataset comprises comprehensive user preference data gathered from 73,516 avid anime enthusiasts, spanning across 12,294 diverse anime titles. Each individual user has the autonomy to curate their own completed anime list, supplemented with personal ratings reflecting their viewing experience. This rich compilation of user-generated ratings forms the backbone of our dataset, offering invaluable insights into the nuanced preferences and tastes of anime enthusiasts worldwide.\n\n# Key Features\n\n##### User Profiles:-\nExplore the preferences and behaviors of over 73,000 users, each with their unique anime consumption habits and rating patterns.\n\n##### Anime Titles:-\nDive into a vast collection of 12,000+ anime titles, ranging from timeless classics to contemporary releases across various genres and themes.\n\n##### Completed Lists:-\nGain access to users' completed anime lists, providing a glimpse into the breadth and depth of their viewing history.\n\n##### Ratings:-\nUncover users' subjective evaluations of anime titles, quantified through personalized ratings, offering a granular understanding of viewer satisfaction and engagement.\n\n# Content\n\n##### Anime.csv\n\nanime_id - unique id identifying an anime.\nname - full name of anime.\ngenre - comma separated list of genres for this anime.\ntype - movie, TV, OVA, etc.\nepisodes - how many episodes in this show. (1 if movie).\nrating - average rating out of 10 for this anime.\nmembers - number of community members that are in this anime's\n\"group\".\n\n##### Rating.csv\n\nuser_id - non identifiable randomly generated user id.\nanime_id - the anime that this user has rated.\nrating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).\n","135":"<h1>\ud83d\udda5\ufe0f My Animes List Datasets - 2023 \ud83d\udda5\ufe0f<\/h1>\n\n    Listed Animes, Users and Ratings on MyAnimeList (MAL)\n\n----\n\n<h2>\ud83d\udcc5 Extraction Period \ud83d\udcc5<\/h2>\n\nThe data was extracted between August 1th in 2023 and October 6th in 2023 via Python Programming Language and using [anime-data-scrapper](https:\/\/github.com\/CSFelix\/anime-data-scrapper) available as a Repository on GitHub.\n\n---\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    anime-dataset-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all listed Animes in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Anime ID** - anime id on MyAnimeList;\n- **Name** - anime original name;\n- **English Name** - English version name;\n- **Other Name** - Japanese version name;\n- **Score** - weighted\/Bayesian average score. You can check out for more details at [\"How are MyAnimeList scores calculated?\"](https:\/\/myanimelist.net\/info.php?go=topanime);\n- **Genres** - related genres;\n- **Synopsis** - briefly description;\n- **Type** - type of animation (movie, anime, OVA...);\n- **Episodes** - number of episodes. Movies are considered having 1 episode;\n- **Aired** - period when anime was aired;\n- **Premiered** - season when the anime was released;\n- **Status** - current status (airing, hiatus, finished...);\n- **Producers** - related production companies;\n- **Licensors** - related streaming platforms and licensors;\n- **Studios** - related animation studios;\n- **Source** - source material of the anime (originated from manga, light novel, movie or tv);\n- **Duration** - duration of the movie or each episode;\n- **Rating** - age restriction;\n- **Rank** - rank position on MyAnimeList website (based on Score criteria);\n- **Popularity** - popularity position on MyAnimeList;\n- **Favorites** - number of users that marked the anime as favorite;\n- **Scored By** - number of users that rated the anime;\n- **Members** - number of users that added the anime to the watch list;\n- **Image Url** - banner url.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-details-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all users in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Mal ID** - user id;\n- **Username** - nickname;\n- **Gender** - user gender;\n- **Birthday** - birthday;\n- **Location** - user's location or country;\n- **Joined** - the joined date on MyAnimeList Platform (ISO format);\n- **Days Watched** - total number of days the user spent on MyAnimeList;\n- **Mean Score** - the average score the user gives to the watched animes;\n- **Watching** - number of animes currently being watched by the user;\n- **Completed** - number of animes finished by the user;\n- **On Hold** - number of animes that the user stopped watching but kept it into its list;\n- **Dropped** - number of animes that the user stopped watching and removed from its list;\n- **Plan to Watch** - number of animes that the user has added into the list but did not started watching;\n- **Total Entries** - total number of animes into the user's list;\n- **Rewatched** - number of animes rewatched;\n- **Episodes Watched** - number of episodes watched from all animes.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-score-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all ratings from all users on MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **User ID** - user id on MyAnimeList Platform;\n- **Username** - nickname;\n- **Anime ID** - anime id on MyAnimeList Platform;\n- **Anime Title** - anime original name;\n- **Rating** - score that the user rated the anime.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    .\/benchmark\/full-benchmark.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nData about all Recommendation System Algorithms' performances.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Iteration** - iteration running number;\n- **Algorithm** - filtering recommendation and data basis approach;\n- **Execution Time** - execution time in seconds;\n- **Average CPU Usage** - average CPU usage in percent over the iteration;\n- **Minimum CPU Usage** - minimum CPU usage in percent over the iteration;\n- **Maximum CPU Usage** - maximum CPU usage in percent over the iteration;\n- **Average RAM Usage** - average RAM usage in percent over the iteration;\n- **Minimum RAM Usage** - minimum RAM usage in percent over the iteration;\n- **Maximum RAM Usage** - maximum RAM usage in percent over the iteration.\n\n----\n\n<h2>\ud83d\udd10 Others Datasets \ud83d\udd10<\/h2>\n\nAll others datasets has been created from one of the three previous ones.\n\n----\n\n<h2>\ud83c\udf89 Acknowledgements \ud83c\udf89<\/h2>\n\nThanks to:\n\n1. [Sajid](https:\/\/www.kaggle.com\/dbdmobile) for providing the [Anime Dataset 2023](https:\/\/www.kaggle.com\/datasets\/dbdmobile\/myanimelist-dataset) that inspired this whole dataset.\n\n----\n\n<h2>\u2696\ufe0f Dataset License \u2696\ufe0f<\/h2>\n\n    MIT License\n\nMore details are available on the [Open Source Initiative - The MIT License](https:\/\/opensource.org\/license\/mit\/).","136":"**Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis.","137":"- **\"This dataset contains monthly percentage returns of constituent stocks comprising the Nifty Energy Index.\"\n\n\"Each observation includes the percentage change in stock prices for leading energy companies listed on the National Stock Exchange of India (NSE).\"\n\n\"The dataset spans multiple years, offering insights into the volatility and performance trends within the energy sector.\"\n\n\"Researchers and analysts can utilize this dataset to analyze market dynamics, assess risk, and develop predictive models for energy stock movements.\"\n\n\"Ideal for studying correlations between individual stock performances and broader market trends affecting the energy sector.\"**","138":"#### **Description:**\n\nExplore a comprehensive dataset combining internal banking data and CIBIL credit information from a leading Indian bank. This dataset is ideal for developing predictive credit risk models and gaining valuable financial insights. It includes detailed information on customer transactions, credit scores, and more, providing a robust foundation for advanced analytics and risk assessment.\n\n#### Features for Internal Banking Data \n\n| Variable Name               |                         Description                                   |\n|---------------------------|-----------------------------------------------|\n| Total_TL                  | Total trade lines\/accounts in Bureau          |\n| Tot_Closed_TL             | Total closed trade lines\/accounts             |\n| Tot_Active_TL             | Total active accounts                         |\n| Total_TL_opened_L6M       | Total accounts opened in last 6 Months        |\n| Tot_TL_closed_L6M         | Total accounts closed in last 6 months        |\n| pct_tl_open_L6M           | Percent accounts opened in last 6 months      |\n| pct_tl_closed_L6M         | Percent accounts closed in last 6 months      |\n| pct_active_tl             | Percent active accounts                       |\n| pct_closed_tl             | Percent closed accounts                       |\n| Total_TL_opened_L12M      | Total accounts opened in last 12 Months       |\n| Tot_TL_closed_L12M        | Total accounts closed in last 12 months       |\n| pct_tl_open_L12M          | Percent accounts opened in last 12 months     |\n| pct_tl_closed_L12M        | Percent accounts closed in last 12 months     |\n| Tot_Missed_Pmnt           | Total missed Payments                         |\n| Auto_TL                   | Count of Automobile accounts                  |\n| CC_TL                     | Count of Credit card accounts                 |\n| Consumer_TL               | Count of Consumer goods accounts              |\n| Gold_TL                   | Count of Gold loan accounts                   |\n| Home_TL                   | Count of Housing loan accounts                |\n| PL_TL                     | Count of Personal loan accounts               |\n| Secured_TL                | Count of secured accounts                     |\n| Unsecured_TL              | Count of unsecured accounts                   |\n| Other_TL                  | Count of other accounts                       |\n| Age_Oldest_TL             | Age of oldest opened account                  |\n| Age_Newest_TL             | Age of newest opened account                  |\n\n\n\n#### Features for External Cibil Data \n\n**Variable Description**\n\n| Variable                         | Description                                      |\n|----------------------------------|--------------------------------------------------|\n| time_since_recent_payment        | Time Since recent Payment made                   |\n| time_since_first_deliquency      | Time since first Delinquency (missed payment)    |\n| time_since_recent_deliquency     | Time Since recent Delinquency                    |\n| num_times_delinquent             | Number of times delinquent                       |\n| max_delinquency_level            | Maximum delinquency level                        |\n| max_recent_level_of_deliq        | Maximum recent level of delinquency              |\n| num_deliq_6mts                   | Number of times delinquent in last 6 months      |\n| num_deliq_12mts                  | Number of times delinquent in last 12 months     |\n| num_deliq_6_12mts                | Number of times delinquent between last 6 months and last 12 months |\n| max_deliq_6mts                   | Maximum delinquency level in last 6 months       |\n| max_deliq_12mts                  | Maximum delinquency level in last 12 months      |\n| num_times_30p_dpd                | Number of times 30+ dpd                          |\n| num_times_60p_dpd                | Number of times 60+ dpd                          |\n| num_std                          | Number of standard Payments                      |\n| num_std_6mts                     | Number of standard Payments in last 6 months     |\n| num_std_12mts                    | Number of standard Payments in last 12 months    |\n| num_sub                          | Number of substandard payments - not making full payments |\n| num_sub_6mts                     | Number of substandard payments in last 6 months  |\n| num_sub_12mts                    | Number of substandard payments in last 12 months |\n| num_dbt                          | Number of doubtful payments                      |\n| num_dbt_6mts                     | Number of doubtful payments in last 6 months     |\n| num_dbt_12mts                    | Number of doubtful payments in last 12 months    |\n| num_lss                          | Number of loss accounts                          |\n| num_lss_6mts                     | Number of loss accounts in last 6 months         |\n| num_lss_12mts                    | Number of loss accounts in last 12 months        |\n| recent_level_of_deliq            | Recent level of delinquency                      |\n| tot_enq                          | Total enquiries                                  |\n| CC_enq                           | Credit card enquiries                            |\n| CC_enq_L6m                       | Credit card enquiries in last 6 months           |\n| CC_enq_L12m                      | Credit card enquiries in last 12 months          |\n| PL_enq                           | Personal Loan enquiries                          |\n| PL_enq_L6m                       | Personal Loan enquiries in last 6 months         |\n| PL_enq_L12m                      | Personal Loan enquiries in last 12 months        |\n| time_since_recent_enq            | Time since recent enquiry                        |\n| enq_L12m                         | Enquiries in last 12 months                      |\n| enq_L6m                          | Enquiries in last 6 months                       |\n| enq_L3m                          | Enquiries in last 3 months                       |\n| MARITALSTATUS                    | Marital Status                                   |\n| EDUCATION                        | Education level                                  |\n| AGE                              | Age                                              |\n| GENDER                           | Gender                                           |\n| NETMONTHLYINCOME                 | Net Monthly Income                               |\n| Time_With_Curr_Empr              | Time with current Employer                       |\n| pct_of_active_TLs_ever           | Percent active accounts ever                     |\n| pct_opened_TLs_L6m_of_L12m       | Percent accounts opened in last 6 months to last 12 months |\n| pct_currentBal_all_TL            | Percent current balance of all accounts          |\n| CC_utilization                   | Credit card utilization                          |\n| CC_Flag                          | Credit card Flag                                 |\n| PL_utilization                   | Personal Loan utilization                        |\n| PL_Flag                          | Personal Loan Flag                               |\n| pct_PL_enq_L6m_of_L12m           | Percent enquiries PL in last 6 months to last 12 months |\n| pct_CC_enq_L6m_of_L12m           | Percent enquiries CC in last 6 months to last 12 months |\n| pct_PL_enq_L6m_of_ever           | Percent enquiries PL in last 6 months to last 6 months |\n| pct_CC_enq_L6m_of_ever           | Percent enquiries CC in last 6 months to last 6 months |\n| max_unsec_exposure_inPct         | Maximum unsecured exposure in percent            |\n| HL_Flag                          | Housing Loan Flag                                |\n| GL_Flag                          | Gold Loan Flag                                   |\n| last_prod_enq2                   | Latest product enquired for                      |\n| first_prod_enq2                  | First product enquired for                       |\n| Credit_Score                     | Applicant's credit score                         |\n| Approved_Flag                    | Priority levels                                  |\n\n","139":"This dataset provides insights into loan applicants' characteristics and their risk assessment. It comprises information on various attributes of loan applicants, including demographic details, financial status, employment history, and ownership status. The dataset includes both numerical and categorical features, making it suitable for diverse analytical approaches.\n\nKey Features:\n\n1. Id: Unique identifier for each loan applicant.\n2. Income: The income level of the applicant.\n3. Age: Age of the applicant.\n4. Experience: Years of professional experience.\n5. Married\/Single: Marital status of the applicant.\n6. House_Ownership: Indicates whether the applicant owns or rents a house.\n7. Car_Ownership: Indicates whether the applicant owns a car.\n8. Profession: Occupation or profession of the applicant.\n9. CITY: City of residence of the applicant.\n10. STATE: State of residence of the applicant.\n11. CURRENT_JOB_YRS: Duration of employment in the current job.\n12. CURRENT_HOUSE_YRS: Duration of residence in the current house.\n13. Risk_Flag: Binary indicator of loan risk, where 1 represents a flagged risky applicant and 0 represents a non-risky applicant.\n\nThis dataset contains 252,000 entries and provides a comprehensive overview of loan applicants' profiles, enabling analysis and modeling for risk assessment and decision-making in lending processes.","140":"The following steps were taken to prepare the dataset:\n\n1.1. Video Recording: Videos of the gestures (thumbs up, fist, open palm) were recorded for both the left and right hands.\n1.2. Frame Extraction: Frames were extracted from the videos, resulting in a total of 250 images for each gesture from both the left and right hands. Each image is of dimension: 64X64\n\nThere are 3 gestures : fist, open_palm and thumbs_up\n\nEach image\/frame has a name starting with \u2018l\u2019 or \u2018r\u2019 where \u2018l\u2019 refers to left and \u2018r\u2019 refers to right. \nAnd second character as \u2018f\u2019 ,\u2018t\u2019 , or \u2018p\u2019 which stands for fist, thumbs up and open palm, respectively.\nFor example, image named as lf_0011 means it is an image of left fist taken from video\u2019s 11th frame. \nIt was carried out with the help of OpenCV (cv2) and os libraries as shown in code in figure 1 and the frames were downloaded as a zip file with the help of code in figure 2.\n\n1.3. Dataset Splitting: The total dataset consisted of 1500 images (250 images per gesture per hand).\nThe dataset was split into training and testing sets:\nTraining Set: 300 images (150 images for left and right hands combined) for each gesture.\nTesting Set: 200 images (100 images for left and right hands combined) for each gesture.\n","141":"# Gesture Recognition Data in Online Conference Scenes\n\n\n## Description\n2,341 People Gesture Recognition Data in Meeting Scenes includes Asians, Caucasians, blacks, and browns, and the age is mainly young and middle-aged. It collects a variety of indoor office scenes, covering meeting rooms, coffee shops, libraries, bedrooms, etc. Each person collected 18 pictures and 2 videos. The pictures included 18 gestures such as clenching a fist with one hand and heart-to-heart with one hand, and the video included gestures such as clapping.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1292?source=Kaggle\n\n## Data size\n2,341 people, each person collects 2 videos and 18 images\n## Race distribution\n786 Asians, 1,002 Caucasians, 401 black, 152 brown people\n## Gender distribution\n1,209 males, 1,132 females\n## Age distribution\nfrom teenagers to the elderly, mainly young and middle-aged\n## Collection environment\nindoor office scenes, such as meeting rooms, coffee shops, libraries, bedrooms, etc.\n## Collection diversity\ndifferent gestures data, different races, different age groups, different meeting scenes\n## Collection equipment\ncellphone, using the cellphone to simulate the perspective of laptop camera in online conference scenes\n## Collection content\ncollecting the  gestures data in online conference scenes\n## Data format\n.mp4, .mov, .jpg\n## Accuracy rate\nthe accuracy exceeds 97% based on the accuracy of the actions; the accuracy of action naming is more than 97%\n# Licensing Information\nCommercial License\n","142":"There are 300 images in train folder and 200 images in test folder. Dimension of each image is 64px X 64px. Each image contains one of the following three hand gestures :\n- Open Palm\n- Thumbs Up\n- Fist","143":"The Enhanced Sign Language MNIST dataset is a comprehensive collection of grayscale images representing American Sign Language (ASL) gestures. This dataset serves as an enhancement to the original Sign Language MNIST dataset, providing a more diverse and extensive set of hand gesture samples for machine learning tasks.\n\nInspired by the need for more challenging benchmarks in image-based machine learning, this dataset is consistent with the original [Sign Language MNIST dataset](https:\/\/www.kaggle.com\/datasets\/datamunge\/sign-language-mnist\/data) to acquire a self-generated dataset, resulting in a more robust and varied collection of hand gesture images. The original Sign Language MNIST dataset, available on Kaggle, provided a solid foundation with 27,455 training cases and 7,172 test cases, each representing a label (0-25) mapped to an alphabetic letter A-Z (excluding J and Z). \n\nThe Enhanced Sign Language MNIST dataset builds upon this foundation by incorporating additional images generated through a process involving various image manipulation techniques. These techniques include hand tracking using MediaPipe, cropping, grayscale conversion, and resizing, to create approximately 1400 samples of each alphabetic letter. The enhanced dataset contains 69,252 samples in total, with 55,402 samples for training and validation, and 13,850 samples for testing.\n\nThis dataset is invaluable for researchers and developers working on sign language recognition, hand gesture detection, and related computer vision tasks. It offers a challenging benchmark for evaluating the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in recognizing ASL gestures.\n\nThe dataset is divided into training and testing sets following the methodology outlined in [Oladayo's research (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2), ensuring the consistency and reproducibility of experimental setups. The experimentation framework incorporated four distinct Convolutional Neural Network (CNN) models: CNN1, CNN2, CNN3, and CNN4. Additionally, four diverse data augmentation techniques were employed, denoted as DAM1, DAM2, DAM3, and DAM4. Notably, DAM1 represents the scenario where no data augmentation is applied.\n\n\nCNN2 achieved a remarkable 99.89% validation accuracy on the enhanced test samples and 99.78% on the generated test samples. Training the model on a GPU\/TPU took approximately 209 seconds (3.5 minutes), which is close to the results reported in the research report. This success underscores the effectiveness of sample generation in enhancing the model's performance, showcasing its superiority over traditional data augmentation methods.\n\nWith the Enhanced Sign Language MNIST dataset, researchers can explore new approaches to sign language recognition, develop more robust machine learning models, and ultimately contribute to the advancement of assistive technologies for the deaf and hard-of-hearing community.\n\nIf you use this code or the datasets in your research, please cite the following dissertation:\n[Oladayo Luke. (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2). Enhancing Sign Language Recognition and Hand Gesture Detection using Convolutional Neural Networks and Data Augmentation Techniques. (Doctoral dissertation, Nova Southeastern University).","144":"The Bangla Sign Language Video Dataset is a comprehensive collection of videos captured from the Badhir School in Dhaka, Bangladesh. This dataset is meticulously curated, containing over 8000 videos showcasing sign language gestures corresponding to 40 common Bangla words. Developed with the aim of facilitating research and development in the field of sign language recognition and interpretation, this dataset serves as a valuable resource for both academia and industry professionals.\n\nAs part of the preprocessing pipeline for the Bangla Sign Language Video Dataset, advanced techniques were employed to detect the hand region and remove the background from the captured videos. This crucial preprocessing step enhances the quality and usability of the dataset for subsequent analysis and research endeavors in sign language recognition and interpretation.\n\nHand Region Detection:\nUtilizing state-of-the-art computer vision algorithms, the hand region in each video frame was accurately identified. MediaPipe, a robust framework for building multimodal perceptual pipelines, was employed for its efficiency and reliability in hand detection tasks. By isolating the hand region, the dataset ensures a focused and standardized input for subsequent processing stages, laying the foundation for precise gesture analysis.\n\nBackground Removal:\nFollowing hand region detection, sophisticated background removal techniques were applied to eliminate extraneous visual elements from the video frames. This process enhances the clarity and saliency of the hand gestures, minimizing distractions and facilitating more accurate feature extraction during subsequent analysis stages. Through meticulous background removal, the dataset offers researchers and developers clean and unambiguous data for training and evaluation purposes.","145":"\nThe \"Hand Gesture Landmark\" dataset is an image dataset designed for hand gesture recognition tasks. It encompasses a variety of hand navigation movements, each categorized into one of nine distinct classes: \n\n anticlockwise,\n clockwise,\n up,\n down,\n left,\n right,\n forward,\n backward. \n\nThe dataset is generated utilizing the capabilities of both Mediapipe and OpenCV technologies.\n\nStructured into separate directories for training, evaluation, and testing, the dataset ensures effective model training and evaluation processes. Within the training directory, there are 2000 images for each class, providing ample data for robust model learning. The test directory contains 300 images for each class, enabling comprehensive model testing across various scenarios. Lastly, the evaluation directory includes 900 images, likely intended for further validation or fine-tuning of trained models. With its comprehensive coverage of hand gestures and sufficient data distribution, the \"Hand Gesture Landmark\" dataset serves as a valuable resource for researchers and developers working on gesture recognition applications.","146":"","147":"This dataset comprises 20,000 images of hand gestures, divided into 10 distinct classes. Each class represents a specific hand gesture commonly used in gesture recognition tasks. The dataset is organized into \"train\" and \"val\" folders, adhering to the structure recommended for YOLOv8 data preparation.\n\nThe hand gestures included in the dataset are meticulously labeled to facilitate training and evaluation of YOLOv8-based models for hand gesture recognition applications. Researchers, developers, and enthusiasts interested in exploring deep learning-based approaches for gesture recognition tasks can utilize this dataset to train and validate their models effectively.\n\nPlease note that the dataset is provided solely for research and educational purposes. Any commercial use or redistribution of this dataset should be done with appropriate permissions and acknowledgments. We hope this dataset contributes to advancements in gesture recognition technology and encourages further exploration in this field.","148":"Sign language is a cardinal element for communication between deaf and dumb community. Sign language has its own grammatical structure and gesticulation nature. Research on SLRT focuses a lot of attention in gesture identification. Sign language comprises of manual gestures performed by hand poses and non-manual features expressed through eye, mouth and gaze movements. \n\nThe sentence-level completely labelled Indian Sign Language dataset for Sign Language Translation and Recognition (SLTR) research is developed. The ISL-CSLTR dataset assists the research community to explore intuitive insights and to build the SLTR framework for establishing communication with the deaf and dumb community using advanced deep learning and computer vision methods for SLTR purposes. This ISL-CSLTR dataset aims in contributing to the sentence level dataset created with two native signers from Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India and four student volunteers from SASTRA Deemed University, Thanjavur, Tamilnadu. The ISL-CSLTR corpus consists of a large vocabulary of 700 fully annotated videos, 18863 Sentence level frames, and 1036 word level images for 100 Spoken language Sentences performed by 7 different Signers. This corpus is arranged based on signer variants and time boundaries with fully annotated details and it is made available publicly.\n\nThe main objective of creating this sentence level ISL-CSLRT corpus is to explore more research outcomes in the area of SLTR. This completely labelled video corpus assists the researchers to build framework for converting spoken language sentences into sign language and vice versa. This corpus has been created to address the various challenges faced by the researchers in SLRT and significantly improves translation and recognition performance. The videos are annotated with relevant spoken language sentences provide clear and easy understanding of the corpus data.\n\nAcknowledgements:\nThe research was funded by the Science and Engineering Research Board (SERB), India under Start-up Research Grant (SRG)\/2019\u20132021 (Grant no. SRG\/2019\/001338). And also, we thank all the signers for their contribution in collecting the sign videos and the successful completion of the ISL-CSLTR corpus. We would like to thank Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India for their support and contribution. ","149":"# Hand Gesture Recognition Dataset\n\n## Overview\n\nThis dataset contains images for hand gesture recognition, supporting research and development in the field of human-computer interaction, particularly in augmented reality (AR) and virtual reality (VR) applications.\n\n## Dataset Description\n\nThe dataset consists of hand images captured using a standard camera, with corresponding labels for hand gestures. The text input method is based on hand-gesture recognition using a trained neural network. The process involves hand segmentation, gesture recognition, and hand movement tracking using a convex hull algorithm.\n\n## License\n\nThis dataset is provided under the license of Nizamuddin and Nooruddin. Please refer to the LICENSE.md file for detailed licensing information.\n\n## Usage\n\nResearchers and developers are encouraged to use this dataset for various applications, including but not limited to:\n\n- Human-Computer Interaction\n- Augmented Reality (AR) Systems\n- Virtual Reality (VR) Systems\n\n## Dataset Structure\n\nThe dataset is organized into folders containing hand images and corresponding labels. The file structure is as follows:\n\n- `images\/`: Directory containing hand images.\n- `labels.csv`: CSV file containing labels for each hand image.\n\n## Citation\n\nIf you use this dataset in your work, please cite the following paper:\n\nN. Nooruddin, R. Dembani and N. Maitlo, \"HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices,\" 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Toronto, ON, Canada, 2020, pp. 744-751, doi: 10.1109\/SMC42975.2020.9283348.\n\n**Keywords:** Human computer interaction; Solid modeling; Image segmentation; Computational modeling; Neural networks; Gesture recognition; Computer architecture; Augmented Reality; Virtual Reality; Hand Segmentation; Hand Gesture Recognition; Text-Based Input; CNN.\n\n@InProceedings{Nooruddin_2020_SMC,\n    author    = {Nooruddin, N. and Dembani, R. and Maitlo, N.},\n    title     = {HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices},\n    booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},\n    month     = {October},\n    year      = {2020},\n    pages  &nbsp;&nbsp;&nbsp;=&nbsp;{744-751}\n}\n\nPlease visit our linkedin for further details:\n- [Mr. Nizamuddin Maitlo](https:\/\/www.linkedin.com\/in\/nizamuddin-maitlo-b27417a0\/)\n- [Dr. Nooruddin Noonari](https:\/\/www.linkedin.com\/in\/noonari\/)\n","150":"This dataset features detailed sensor data from smart gloves designed to translate sign language into spoken language. The gloves were equipped with **5 flex sensors** and a \"**[Grove - 6 - Axis Accelerometer & Gyroscope](https:\/\/wiki.seeedstudio.com\/Grove-6-Axis_AccelerometerAndGyroscope\/)**\" for each hand. The dataset consists of different files, each recorded by a different individual, showcasing a variety of hand movements and signs. Key features include:\n**- `Flex-Left\/Right-p-Frame-n` :** Measures the degree of flexion for each finger over 20 frames, with 'n' indicating the frame and 'p' the finger number.\n**- `Acceleration-X\/Y\/Z-Left\/Right-Frame-n` :** Captures the hand's acceleration in 3D space across 20 frames.\n**- `Orientation-X\/Y\/Z-Left\/Right-Frame-n` :** Details the hand's orientation in 3D space for each frame.\n So for each hand, there are 3 types of measurements (Flex, Acceleration, Orientation), each with 3 axes (X, Y, Z) for Acceleration and Orientation, and 5 measurements for Flex (one per finger), resulting in 11 measurements per hand per frame. Multiplied by 20 frames, and then doubled for both hands, this amounts to **440 columns**. Additionally, there's a **\"SIGN\" column**, which is the target for each data entry, identifying the specific sign language gesture.\n\nEach recording in the dataset represents a sequence of 20 frames, capturing detailed motion for both left and right hands.\n\n**This dataset is not yet complete and is still being collected.**","151":"This dataset is a collection of images designed for training machine learning models in the recognition and classification of Malaysian Sign Language (MSL) signs. It encompasses three distinct categories, each critical for a comprehensive understanding of MSL:\n\nAlphabets (Alphabet_MSL): This segment consists of images depicting individual letters of the MSL alphabet. Each image is a clear, focused representation of a hand gesture corresponding to an MSL letter, formatted in .jpg for optimal compatibility with machine learning algorithms.\n\nSingle Words (SingleWord_MSL): Here, key basic words used in MSL communication are visually represented. This category is vital for models intended to recognize common vocabulary in sign language.\n\nNumbers (Number_MSL): This part contains images illustrating numeric signs in MSL, covering a range from simple digits.","152":"A dataset comprising 14,000 images of hand gestures has been created. These images were manually captured. The dataset, as yet unnamed, offers a substantial resource for various applications in computer vision and machine learning, particularly in the realm of gesture recognition and sign language interpretation. The dataset's size and diversity make it a valuable asset for researchers and developers.\n\nPotential uses for this dataset include training and evaluating machine learning models for gesture recognition, enabling research in computer vision, supporting accessibility and assistive technology development, and contributing to educational and research initiatives. Depending on the dataset's licensing and labeling, it can be made publicly available for broader use within the research community, fostering advancements in the field.\n\nDocumentation, labeling, and clear licensing terms are essential considerations for sharing the dataset with the wider community.\n\n\n\n\n","153":"This dataset comprises surface electromyography (sEMG) recordings acquired for myoelectric control applications. Collected using the Myo Armband device, the dataset encompasses ten distinct hand motion classes. These recordings serve as a valuable resource for researchers and practitioners in the fields of biomedical signal processing, machine learning, and human-computer interaction. The dataset's accessibility and diversity make it suitable for training, testing, and optimizing sEMG-based hand gesture recognition algorithms. By sharing this dataset, we aim to foster advancements in myoelectric control technologies and contribute to the development of more intuitive and efficient human-machine interfaces.","154":"The Hand Gesture Recognition Dataset is a comprehensive collection designed for training and evaluating machine learning models for hand gesture recognition tasks. This dataset comprises 24 distinct classes, representing each letter of the alphabet excluding 'J' and 'Z'. Each gesture is depicted against a consistent black background, enhancing the clarity of the hand signs. The visual representation of the gestures is presented as wireframes, providing a clear outline of the hand's position.\n\n\nKey Features:\n\n24 Distinct Classes: The dataset includes signs corresponding to all letters of the alphabet, except 'J' and 'Z'. These two classes require video input so they were removed from the still image dataset. \n\nConsistent Background: All gestures are set against a black background, ensuring uniformity across the dataset. This consistent background simplifies preprocessing tasks and ensures that models are trained on signs alone, without any background distractions.\n\nPrecise Hand Landmarks: The dataset's accuracy is enhanced by utilizing Google MediaPipe to extract detailed hand landmarks. These landmarks are essential for understanding the intricate movements of fingers, enabling the development of highly accurate gesture recognition models.\n","155":"The dataset is a collection of hand gesture and expression videos captured using a webcam and processed using the OpenCV and Mediapipe libraries. It consists of 33 classes, each representing a different hand gesture or expression. For each class, there are 30 videos, and each video contains 75 frames. The dataset is organized into directories, with each class having its own directory. Within each class directory, there are 30 subdirectories numbered from 0 to 29, each containing 75 frames as numpy arrays.\n\nThe keypoints of the hand in each frame were detected using the Mediapipe library, resulting in numpy arrays that represent the coordinates of these keypoints. These keypoints are crucial for understanding the hand's pose and movement in each frame. The dataset is intended to support various applications, such as gesture recognition and machine learning model development, offering a substantial resource for research and development in these domains.","156":"**Description**\nThe data diversity includes multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, and multiple light conditions. In terms of data annotation, 21 landmarks, gesture types, and gesture attributes were annotated. This dataset can be used for tasks such as gesture recognition and sign language translation.\nFor more details, please visit: https:\/\/www.nexdata.ai\/datasets\/980?source=Kaggle\n\n**Specifications**\nData size\n180,717 images, including 83,012 images of static gestures, 97,705 images of dynamic gestures\nPopulation distribution\nthe race distribution is Asian, the gender distribution is male and female, the age distribution is mainly young people and middle-aged people\nCollection environment\nincluding indoor scenes and outdoor scenes\nCollection diversity\nincluding multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, multiple light conditions\nDevice\ncellphone\nData forma\nthe image data format is .jpg, the annotation file format is .json\nCollecting content\nsign language gestures were collected in different scenes\nAnnotation content\n21 landmarks annotation (each landmark includes the attribute of visible or invisible), gesture type annotation, gesture attributes annotation (left hand or right hand)\nAccuracy\naccuracy requirement: the point location errors in x and y directions are less than 3 pixels, which is considered as a qualified annotation; accuracy of landmark annotation: the annotation part (each landmark) is regarded as the unit, the accuracy rate shall be more than 95%\n\n**Get the Dataset**\nThis is just an example of the data. To access more sample data or request the price, contact us  at info@nexdata.ai","157":"The Hand Sign Recognition Image Dataset is a rich collection of high-quality images featuring diverse **single-hand signs**. With comprehensive coverage, annotations, and multi-modal data, it's an ideal resource for developing sign language recognition models. Whether for sign language translation or gesture-based interaction, this dataset supports a range of applications and is ready for machine learning integration.","158":"## Data Sources:\nThis Dataset is collected from HAnd Gesture Recognition Image Dataset(HaGRID) and its 512p version created by [Innominate817](https:\/\/www.kaggle.com\/innominate817). I modify it  for gesture classification.\nHaGRID: https:\/\/github.com\/hukenovs\/hagrid\nHaGRID-512p: https:\/\/www.kaggle.com\/datasets\/innominate817\/hagrid-classification-512p\n## Data Analysis:\nit includes 2 gestures: thumbs up (like) and thumbs down(dislike).\nThe statistics for each sample are as follows:\n| shape | train | valid | test | total|\n| --- | --- | --- | --- | --- |\n| like | 16554 | 5518 | 5519 | 27591|\n| dislike | 17007 | 5669 | 5669 | 28345|\n\n## Usage:\nThis dataset can be used in gesture classification. My demo can be seen on GitHub: https:\/\/github.com\/RuiNov1st\/InternWork\/tree\/main\/L6_GestureClassification. I use pretrained ResNet18 as base model and obtain its capability to recognize 'thumb up' and 'thumb down' gestures through retraining. The accuracy in the test set can reach 99%. I also take some my own gestures photos for testing, but now the performance is not good.\nAny suggestions will be greatly appreciated! \n","159":"### TRAIN \/ TEST SPLIT:\n\n**Training Set:** 70% of the dataset with 642 images\n**Validation Set:** 19% of the dataset with 178 images\n**Testing Set:** 10% of the dataset with 94 images\n### PREPROCESSING:\n\n**Auto-Orient:** Applied to ensure consistent orientation of images\n**Resize:** All images have been stretched to a standard size of 640x640 for uniformity\n\n### AUGMENTATIONS:\n\nNo augmentations were applied to the dataset, preserving the original image integrity.\n\n**_annotations.csv contains following columns:**\n\n**filename:** The name of the image file associated with the annotation.\n**width:** The width of the image in pixels.\n**height:** The height of the image in pixels.\n**class:** The class label of the hand gesture present in the image.\n**xmin:** The x-coordinate of the top-left corner of the bounding box around the hand gesture.\n**ymin:** The y-coordinate of the top-left corner of the bounding box around the hand gesture.\n**xmax:** The x-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n**ymax:** The y-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n\n### Here are some potential use cases for this dataset:\n\n**Gesture-Controlled User Interfaces:** The dataset can be used to build gesture-controlled user interfaces for various devices and applications, such as smartphones, tablets, computers, and IoT devices.\n**Sign Language Translation:** Hand gestures are an essential part of sign language used by the hearing-impaired community. This dataset can be used to develop sign language recognition systems that can translate sign language into text or speech, enabling better communication with non-sign language users.\n\n","160":"The dataset containing reviews of British Airways has been meticulously gathered from the Skytrax website. This dataset is intended for comprehensive data analysis and visualization. By analyzing this data, we aim to uncover insights and trends regarding customer experiences and satisfaction levels with British Airways. The visualization of this data will further enhance the understanding by presenting the findings in a clear and engaging manner, making it easier to identify patterns and areas for improvement.","161":"The [\/kaggle\/input\/online-review-csv\/online_review.csv ](url)file contains customer reviews from Flipkart. It includes the following columns:\n\nreview_id: Unique identifier for each review.\nproduct_id: Unique identifier for each product.\nuser_id: Unique identifier for each user.\nrating: Star rating (1 to 5) given by the user.\ntitle: Summary of the review.\nreview_text: Detailed feedback from the user.\nreview_date: Date the review was submitted.\nverified_purchase: Indicates if the purchase was verified (true\/false).\nhelpful_votes: Number of users who found the review helpful.\nreviewer_name: Name or alias of the reviewer.\nUses\nSentiment Analysis: Understand customer sentiments.\nProduct Improvement: Identify areas for product enhancement.\nMarket Research: Analyze customer preferences.\nRecommendation Systems: Improve recommendation algorithms.\nThis dataset is ideal for practicing data analysis and machine learning techniques.","162":"This dataset captures customer reviews of **British Airways** obtained from the website[ https:\/\/www.airlinequality.com\/airline-reviews\/scoot](url). It consists of raw, uncleaned data directly scraped from the source, providing a rich resource for honing skills in **data cleaning, data visualization, analysis, and potentially predicting customer sentiment**.\n\nThe dataset includes various attributes such as review text, ratings, date of travel, and possibly more contextual information relevant to customer experiences with British Airways. Given its raw state, the dataset presents an opportunity to practice preprocessing techniques to ensure data quality and prepare it for deeper analytical insights.","163":"## Capterra Reviews for Ticket Systems\n\n### Dataset Description\n\nThis dataset contains reviews of various ticket systems from Capterra, providing insights into the features, pros, and cons as experienced by users. The dataset is designed to facilitate analysis and comparison of different ticket systems based on user feedback. Each review includes ratings for various aspects such as ease of use, customer service, features, and overall value for money, as well as the likelihood to recommend the system.\n\n### Content\n\nThe dataset consists of the following columns:\n\n- **ticket_system**: The name of the ticket system being reviewed.\n- **title**: The title of the review.\n- **overall_text**: The overall text of the review.\n- **pros_text**: The positive aspects of the ticket system as mentioned by the reviewer.\n- **cons_text**: The negative aspects of the ticket system as mentioned by the reviewer.\n- **overall_rating**: The overall rating given by the reviewer (out of 5).\n- **ease_of_use**: The rating for ease of use (out of 5).\n- **customer_service**: The rating for customer service (out of 5).\n- **features**: The rating for features (out of 5).\n- **value_for_money**: The rating for value for money (out of 5).\n- **likelihood_to_recommend**: The likelihood to recommend the system (out of 10).\n\nIn addition, the dataset includes columns for ten key features, indicating whether each feature was mentioned positively, negatively, or not at all in the review. The feature columns are:\n\n- **Ticket Creation and Assignment**\n- **Status Tracking and Updates**\n- **Priority and SLA Management**\n- **Reporting and Analytics**\n- **Automated Ticket Routing**\n- **Knowledge Base Integration**\n- **Customer and Agent Portals**\n- **Multi-Channel Support (Email, Chat, Phone)**\n- **Email Notifications and Alerts**\n- **Customizable Workflows**\n\nEach feature column contains:\n- `1` if the feature was mentioned positively,\n- `-1` if the feature was mentioned negatively,\n- `0` if the feature was not mentioned.\n\n### Usage\n\nThis dataset can be used for:\n- **Sentiment Analysis**: Analyzing user sentiments towards different features of ticket systems.\n- **Feature Importance**: Identifying which features are most positively or negatively received.\n- **Comparative Analysis**: Comparing different ticket systems based on user reviews and ratings.\n- **Market Research**: Understanding customer needs and preferences in the ticketing system market.\n\n### Acknowledgements\n\nThe reviews in this dataset are sourced from Capterra. The dataset is intended for educational and research purposes.\n\n### Keywords\n\nTicket systems, Capterra reviews, customer service, sentiment analysis, feature analysis, user feedback, software reviews, helpdesk solutions, comparative analysis, market research\n\n### License\n\nThe dataset is available for public use. If you use this dataset, please credit the original source and this dataset on Kaggle.\n\n### Contact\n\nFor any questions or contributions regarding this dataset, please reach out via Kaggle or through the provided contact information in the dataset repository.\ndata@softoft.de","164":"","165":"this graph was created in PowerBi,Loocker studio and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fea63e695d326e3f57b044725376024c2%2Fgraph1.jpg?generation=1718574239511915&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fc285144b368e9b25b2d576d5bf6d4b81%2Fgraph2.png?generation=1718574245118394&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F80adbea61286526a5910b803a8d4822e%2Fgraph3.png?generation=1718574250267201&alt=media)\n\n\nAliExpress Data is vast collection of information and statistics gathered from the AliExpress online marketplace. It includes details about products, sellers, prices, customer reviews, ratings, and other relevant data points. This data can be analyzed to gain insights into market trends, consumer behavior, and business performance, helping businesses make informed decisions and optimize their operations on the platform.\nExamples of AliExpress data include product listings, customer reviews, seller ratings, and transaction details. AliExpress data is used for various purposes such as market research, competitor analysis, trend identification, and customer behavior analysis. In this page, you\u2019ll find the best data sources for AliExpress data, AliExpress dataset, AliExpress data analysis.","166":"This is a synthetic dataset of an online store selling natural cosmetics. It is closely approximated to real data, with logic maintained in the number of sales during certain periods, the influence of user ratings on products, and even the seasonality of some product sales.","167":"## Introduction:\n\nThis dataset contains historical daily exchange rates for the US Dollar (USD) against three major currencies: Pakistani Rupee (PKR), Indian Rupee (INR), and Chinese Yuan (CNY). The data covers the period from January 2004 to June 2024, providing a comprehensive view of currency fluctuations over time.\n\n## About the Dataset\n\nThis dataset contain daily exchange rate from (2004 - 2024) of Dollar to PKR, Dollar to INR, Dollar to Chinese Yuan and contains 9 columns.\n\n## Columns:\n\n- `Date`: The date of the exchange rate.\n- `Open_pkr, High_pkr, Low_pkr, Close_pkr, Adj Close_pkr`: Exchange rate details from USD to PKR.\n- `Open_inr, High_inr, Low_inr, Close_inr, Adj Close_inr`: Exchange rate details from USD to INR.\n- `Open, High, Low, Close, Adj Close`: Exchange rate details from USD to CNY.\n- `PKR_pct_change`: Daily percentage change in the exchange rate from USD to PKR.\n- `INR_pct_change`: Daily percentage change in the exchange rate from USD to INR.\n- `CNY_pct_change`: Daily percentage change in the exchange rate from USD to CNY.\n\n## Source:\nThe data has been compiled from various reliable financial sources to ensure accuracy and consistency.\n\n## Usage:\nThis dataset is valuable for financial analysts, economists, and researchers interested in studying currency trends, economic events, and their impacts on exchange rates. It can also be used for machine learning projects involving time series analysis, forecasting, and financial modeling.\n\n## Acknowledgements:\nWe acknowledge the data sources for providing the necessary exchange rate information.","168":"This dataset provides detailed information on the longest-running Indian television series in the Hindi language, specifically focusing on non-fiction genres. The data encompasses various attributes, including the length of time each show has been on air, the title, the network it was broadcasted on, the genre, start and end dates, the number of episodes, and additional notes. Below is a summary of the dataset structure:\n\n## Dataset Columns:\n- **Length (years)**: The duration for which the show has been running, expressed in years.\n- **Title**: The name of the television series.\n- **Network**: The TV network(s) that aired the show.\n- **Genre**: The category or type of the show (e.g., Agriculture, Music, Reality, Game show, Talk show, Cooking show, Stunt based Reality show, Cultural).\n- **Start date**: The date when the show first premiered.\n- **End date**: The date when the show ended, if applicable.\n- **No. of episodes**: The total number of episodes aired.\n- **Notes**: Additional information or significant achievements related to the show.","169":"This script, a potent analytical tool, is a potential game-changer for music industry stakeholders such as data analysts, marketing strategists, and business decision-makers. It enables users to predict music release dates and trends with precision, leveraging historical data. The script's modularity ensures it can seamlessly adapt to new datasets and analytical methods. In real-life scenarios, the script's placeholder variables and paths can be effortlessly replaced with data fields and file locations, enhancing its practical value. The script's visualisations, predictive modelling, and time series analysis are pivotal for music streaming data-driven decision-making. ","170":"**Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis.","171":"","172":"## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media)","173":"","174":"This dataset contains around 136k posts scrapped form linkedIn from Dec 2022 to May 2023, specifically about the mass layoffs which have occurred during that time.\n\nThe data was scraped on a regular basis through the time period and many of the entries were deduplicated for better usability. ","175":"","176":"","177":"In 2020, Meta (then Facebook) created a quasi-independent, 20 member oversight board with the power to override the company's decisions around content-moderation. These are their major cases and outcomes. ","178":"This dataset comprises articles from the New York Times, covering the israel-hamas war. It is ideal for Natural Language Processing (NLP) tasks such as text classification, sentiment analysis, and named entity recognition (NER). The dataset includes the following features:\n\ncomprises all articles related to israel hamas war published by the nytimes during the year 2024 until 26\/june.\n\nheadline: The headline of the article.\ntext: The full text of the article.","179":"Dataset Overview\nThis dataset comprises a collection of 324 news headlines from various leading companies, structured over several months in 2024. Each entry in the dataset is associated with a specific company and date, along with the primary news headline of that day and the headline from the previous day.\n\nContent\nCompany Name: Indicates the company to which the news is related.\nDate: The publication date of the news.\nNews Headlines: The headline of the news corresponding to the company and date.\nPrevious Day Headlines: News headline from the day before, for the same company.\nPotential Uses\nThis dataset is ideal for temporal text analysis, news trend monitoring, and sentiment analysis. Researchers and data scientists can explore changes in news coverage over time, analyze the impact of news on stock prices, or use the dataset for training machine learning models for text classification or summarization.\n\nAcknowledgements\nThis data was compiled and curated with the intent to provide a rich source for news headline analysis related to major companies. It is presented here for educational and informational purposes.\n\nInspiration\nSome questions and tasks this dataset could inspire include:\n\nHow do headlines about a company change over time?\nCan the impact of news on a company's public perception be quantified?\nDeveloping models that predict the sentiment conveyed in the headlines.\nComparing news coverage between companies within the same time frame.","180":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","181":"Description for the tmrospacenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024.","182":"Description for the truecrimenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024.","183":"","184":"This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n","185":"# Information Classification Dataset\n\n## Description\n\nThis dataset is designed for classifying information into three categories: fake, spam, and legit. It combines data from multiple sources to provide a diverse and comprehensive collection.\n\n## Source Attribution\n\n1. **YouTube Spam and Ham Data**:\n   - Source: [YouTube Spam Collection](https:\/\/doi.org\/10.24432\/C58885)\n   - Description: Contains spam and legitimate comments from YouTube videos.\n\n2. **Email Spam Dataset**:\n   - Source: [Email Spam Collection](https:\/\/www.kaggle.com\/datasets\/venky73\/spam-mails-dataset)\n   - Description: Collection of spam and legitimate emails.\n\n3. **SMS Spam Dataset**:\n   - Source: [SMS Spam Collection](https:\/\/www.kaggle.com\/datasets\/uciml\/sms-spam-collection-dataset)\n   - Description: Dataset of spam and legitimate SMS messages.\n\n4. **WELFake Dataset**:\n   - Source: [WELFake Fake News Dataset](https:\/\/www.kaggle.com\/datasets\/vcclab\/welfake-dataset)\n   - Description: Contains fake and legitimate news articles.\n\n5. **GossipCop Dataset**:\n   - Source: [GossipCop Fake News Dataset](Retrieved from https:\/\/github.com\/KaiDMML\/FakeNewsNet )\n   - Description: Dataset of fake and legitimate news articles from GossipCop.\n\nThe dataset was carefully cleaned and processed to ensure quality and consistency. Duplicate entries were removed.\n\n## Files\n\n- `data.csv`: The main dataset file.\n- `README.md`: Detailed description of the dataset creation process and source attribution.\n","186":"","187":"","188":"","189":"","190":"","191":"# Sinhala Ada Derana News Articles Dataset (2008-2024)\n\nThis dataset is a comprehensive collection of news articles from [Sinhala Ada Derana](https:\/\/sinhala.adaderana.lk\/), spanning from 2008 to 2024. It includes **over 200,000** articles, offering a rich resource for a variety of applications in both machine learning and non-machine learning domains.\n\n## Dataset Features:\n- **ID:** Unique identifier for each news article from the website.\n- **Title:** The title of the news article.\n- **Description:** The full text of the news article.\n- **Date:** The publication date of the news article.\n- **URL:** The URL of the image associated with the news article.\n\n## Potential Uses:\n1. **Natural Language Processing (NLP):**\n   - Sentiment analysis\n   - Topic modeling\n   - Named entity recognition\n   - Language translation\n   - Text summarization\n\n2. **Machine Learning:**\n   - Embedding generation\n   - News classification\n   - Fake news detection\n\n3. **Non-Machine Learning Applications:**\n   - Historical analysis\n   - Media studies\n   - Sociopolitical research\n\n## Additional Information:\n- **Historical Context:** This dataset provides a detailed view of the historical news landscape in Sri Lanka, capturing the socio-political and cultural narratives over a significant period.\n- **Data Format:** The dataset is structured in a user-friendly format, making it easy to parse and analyze.\n\nThis dataset serves as a valuable resource for researchers, data scientists, and anyone interested in the evolution of news media in Sri Lanka. It provides insights into the past events, public sentiment, and media trends in the country, offering a broad range of possibilities for analysis and application.\n\nFeel free to explore, analyze, and build upon this dataset to uncover interesting patterns and insights.\n","192":"","193":"# Description\n\n**Weibo V2**: Includes **11,329** number of news from the Chinese microblogging social media platform. There are **5,661 fake news** items and **5,668 real news** items. Comparable to version 1 (V1), version 2 (V2) expands the data magnitude on the basis of V1. Meanwhile, V2 provides news **multi-modal data**, including **news posts**, **comment collections**, **images**, **videos** and **voice information**. As a result, V2 provides a better simulation of the real environment of social networks, thus supporting downstream tasks. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n```","194":"# Description\n\n**Weibo V1**: Includes data on 2,067 news items from the microblogging platform in the half of 2023. There are **1,000 fake news** and **1,067 real news**. The dataset consists of comment data on news spreads and contains **user** and **comment information**. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n```","195":"","196":"The internet and social media have led to a major problem\u2014fake news. Fake news is false information presented as real news, often with the goal of tricking or influencing people. It's difficult to identify fake news because it can look very similar to real news.\nThe Fake News detection dataset deals with the problem indirectly by using tabular summary statistics about each news article to attempt to predict whether the article is real or fake. This dataset is in a tabular format and contains features such as word count, sentence length, unique words, average word length, and a label indicating whether the article is fake or real.","197":"","198":"","199":"","200":"# **Fake News detection dataset**\n## \nDataset separated in two files:\n1. Fake.csv (23502 fake news article)\n2. True.csv (21417 true news article)\n\nDataset columns:\n1. Title: title of news article\n2. Text: body text of news article\n3. Subject: subject of news article\n4. Date: publish date of news article","201":"The Fake News Detection Dataset is created to assist researchers, data scientists, and machine learning enthusiasts in tackling the challenge of distinguishing between genuine and false information in today's digital landscape inundated with social media and online channels. With thousands of news items labeled as either \"Fake\" or \"Real,\" this dataset provides a robust foundation for training and testing machine learning models aimed at automatically detecting deceptive content.\n\nEach entry in the dataset contains the full text of a news article alongside its corresponding label, facilitating the development of supervised learning projects. The inclusion of various types of content within the news articles, ranging from factual reporting to potentially misleading information or falsehoods, offers a comprehensive resource for algorithmic training.\n\nThe dataset's structure, with a clear binary classification of news articles as either \"Fake\" or \"Real,\" enables the exploration of diverse machine learning approaches, from traditional methods to cutting-edge deep learning techniques.\n\nBy offering an accessible and practical dataset, the Fake News Detection Dataset aims to stimulate innovation in the ongoing battle against online misinformation. It serves as a catalyst for research and development within the realms of text analysis, natural language processing, and machine learning communities. Whether it's refining feature engineering, experimenting with state-of-the-art transformer models, or creating educational tools to enhance understanding of fake news, this dataset serves as an invaluable starting point for a wide range of impactful projects.","202":"The research on fake news and machine learning focuses on using advanced techniques to detect and classify fake news from real news. Various studies have shown the effectiveness of machine learning models, such as Naive Bayes Classifier, in distinguishing between fake and true news based on language patterns and content analysis\n\nEfforts have been made to automate the detection of fake news due to the limitations of human ability in consistently identifying fake news. Machine learning and natural language processing techniques have been employed to create tools that can analyze language patterns and classify news articles as real or fake, demonstrating the potential of machine learning in this task\n1\n5\n. While some deep learning models have been explored, they may require more data to predict accurately for fake news detection","203":"","204":"","205":"Overview\nIn an era where information spreads rapidly through social media and other digital channels, distinguishing between real and fake news has become increasingly challenging. The Fake News Detection Dataset aims to provide researchers, data scientists, and machine learning enthusiasts with a robust dataset for training models to automatically detect fake news with high accuracy.\n\nDataset Description\nThis dataset comprises thousands of news items labeled as \"Fake\" or \"Real\", providing a rich foundation for developing and testing machine learning models capable of identifying deceptive information. Each entry in the dataset consists of a news text and a corresponding label, offering a straightforward yet powerful resource for supervised learning projects.\n\nData Fields\nText: The full text of the news article. This field includes the body of the article, representing a mix of factual reporting, opinions, and potentially misleading information or falsehoods.\nLabel: A categorical label indicating whether the news article is \"Fake\" or \"Real\". This binary classification makes it suitable for a wide range of machine learning approaches, from traditional models to advanced deep learning techniques.\n\nInspiration\nThe Fake News Detection Dataset is designed to inspire the development of innovative algorithms that can contribute to the fight against misinformation online. By providing a readily accessible and useable dataset, we aim to foster research and development in text analysis, natural language processing, and machine learning communities. Whether you're exploring feature engineering, experimenting with the latest transformer models, or developing educational tools to help understand fake news, this dataset offers a starting point for a myriad of impactful projects.","206":"","207":"","208":"A larger and more generic Word Embedding over Linguistic Features for Fake News Detection (WELFake) dataset of 72,134 news articles with 35,028 real and 37,106 fake news. Merged four popular news datasets (i.e. Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent over-fitting of classifiers and to provide more text data for better ML training.\n\nThe dataset contains four columns: Serial number (starting from 0); Title (about the text news heading); Text (about the news content); and Label (0 = fake and 1 = real).\n\nThere are 78098 data entries in csv file out of which only 72134 entries are accessed as per the data frame. \n\n[Here is the demo project on this dataset!] (https:\/\/youtu.be\/IGOCtD4HFFQ)","209":"","210":"","211":"","212":"","213":"","214":"","215":"The dataset is a comprehensive sales record from Gottlieb-Cruickshank, detailing various transactions that took place in Poland in January 2018. The data includes information on customers, products, and sales teams, with a focus on the pharmaceutical industry. Below is a detailed description of the dataset:\n\n### Dataset Description\n\n**Columns:**\n\n1. **Distributor:** The name of the distributing company, which is consistent across all records as \"Gottlieb-Cruickshank.\"\n2. **Customer Name:** The name of the customer or the purchasing entity.\n3. **City:** The city in Poland where the customer is located.\n4. **Country:** The country of the transaction, consistently listed as \"Poland.\"\n5. **Latitude:** The latitude coordinate of the customer's city.\n6. **Longitude:** The longitude coordinate of the customer's city.\n7. **Channel:** The distribution channel, either \"Hospital\" or \"Pharmacy.\"\n8. **Sub-channel:** Specifies whether the sub-channel is \"Private,\" \"Retail,\" or \"Institution.\"\n9. **Product Name:** The name of the pharmaceutical product sold.\n10. **Product Class:** The classification of the product, such as \"Mood Stabilizers,\" \"Antibiotics,\" or \"Analgesics.\"\n11. **Quantity:** The number of units sold.\n12. **Price:** The price per unit of the product.\n13. **Sales:** The total sales amount, calculated as Quantity * Price.\n14. **Month:** The month of the transaction, which is \"January\" for all records.\n15. **Year:** The year of the transaction, which is \"2018\" for all records.\n16. **Name of Sales Rep:** The name of the sales representative handling the transaction.\n17. **Manager:** The manager overseeing the sales representative.\n18. **Sales Team:** The sales team to which the sales representative belongs, such as \"Delta,\" \"Bravo,\" or \"Alfa.\"\n\n### Example Rows:\n\n1. **Row 1:**\n   - **Customer Name:** Zieme, Doyle and Kunze\n   - **City:** Lublin\n   - **Product Name:** Topipizole\n   - **Product Class:** Mood Stabilizers\n   - **Quantity:** 4\n   - **Price:** 368\n   - **Sales:** 1472\n   - **Sales Rep:** Mary Gerrard\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\n2. **Row 2:**\n   - **Customer Name:** Feest PLC\n   - **City:** \u015awiecie\n   - **Product Name:** Choriotrisin\n   - **Product Class:** Antibiotics\n   - **Quantity:** 7\n   - **Price:** 591\n   - **Sales:** 4137\n   - **Sales Rep:** Jessica Smith\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\nThis dataset can be utilized for various analyses, including sales performance by city, product, and sales teams, as well as geographical distribution of sales within Poland. It provides valuable insights into the pharmaceutical sales strategies and their execution within a specific time frame.","216":"This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes.","217":"# Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613","218":"### Context  \n\nIndonesian foods are well-known for their rich taste. There are many spices used even for daily foods. This dataset may give insight on how to prepare Indonesian food, in many ways. \n\n### Content  \n\nThis dataset contains 14000 recipes divided in 7 categories:  \n- `dataset-ayam.csv` (chicken recipes)  \n- `dataset-kambing.csv` (lamb recipes)   \n- `dataset-sapi.csv` (beef recipes)   \n- `dataset-telur.csv` (egg recipes)  \n- `dataset-tahu.csv` (tofu recipes)  \n- `dataset-ikan.csv` (fish recipes)  \n- `dataset-tempe.csv` (tempe recipes)  \n\nFor each category, there are 5 columns:  \n- Title  \n- Ingredients  \n- Steps  \n- Love  \n- URL\n\n### Acknowledgements\n\nAll the data were taken from Cookpad on 23 Feb 2018.\n\n\n### Inspiration\n\nDare to find out what is the unique recipe? The most strange? Or the common way to cook particular ingredients.   \nCan you create your own recipe based on this dataset?","219":"This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes.","220":"Known for its aromatic spices, bold flavors, and regional specialties, Indian food is a delightful journey for the taste buds. From the tandoori delights of the North to the coconut-infused dishes of the South, each region offers a unique culinary experience. Indian cuisine not only caters to various palates but also holds a significant place in Indian traditions and celebrations.\nThis Dataset is inspired by the Dataset \"Indian Food 101\" By NEHA PRABHAVALKAR.\nIn addition to that, it also contains the image URLs for all the dishes.","221":"# **Context**\nThere's a story behind every dataset and here's your opportunity to share yours.\n\n# **Content**\nJordan cuisine consists of a variety of regional and traditional to the Arabic subcontinent. Given the diversity in soil, climate, culture, ethnic groups, and occupations, these cuisines vary substantially and use locally available spices, herbs, vegetables, and fruits.\n\nThis dataset consists of information about various Jordan dishes, their ingredients, their place of origin, etc.\n\n# **Challenge**\nThe first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions\/image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.\n\n#**About**\nFor more information: http:\/\/arartawil.com\/\n","222":"# Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613","223":"Pd: The banner image was obtained [here](https:\/\/radioambulante.org). Credits to them.\n\n### Context \nPeru is one of best culinary destination in the world. This country has diverse climates and ecological floors, where various crops have been developed. In this way, it has a lot of natural and unique inputs. So, peruvian food is a cuisine of opposites: hot and cold on the same plate. Acidic tastes melding with the starchy. Robust and delicate at the same time. This balance occurs because traditional Peruvian food relies on spices and bold flavors, ranging from the crisp and clean to the heavy and deep. Each flavor counters or tames the other. While many people see Peru as a land of cloud-topped mountains and ruins of ancient civilizations, Peru\u2019s true treasure is its rich culinary heritage. Ingredients and cooking techniques from Africa, Europe, and East Asia come together in a delightful melange that is utterly unique the world over. But what kind of food do Peruvians eat? And, **what restaurants should you visit?** \ud83d\ude0a \n\n![gg](https:\/\/th.bing.com\/th\/id\/R.2ba072e671a2fdb017da8c135fc2bc08?rik=g8HU8568d8RHog&riu=http%3a%2f%2f2.bp.blogspot.com%2f-KhLPAIsagTQ%2fVie_1VlvTUI%2fAAAAAAAAACc%2fzBjaekoIkUM%2fs1600-r%2f2.jpg&ehk=stHuhD5xQj5P5tx3MNqlLctSB72pLLTMm2A7Wi%2b6Msk%3d&risl=&pid=ImgRaw)\n\n### Content\nThis kaggle dataset contains information scraped from GooglePlaces and Tripadvisor using Selenium, Requests, BeautifulSoup and Rvest. More info about the used web-scraping in this [github repository](https:\/\/github.com\/Lazaro-97\/satisfaction-in-restaurants)\nThe content here have a lot (at the moment not all) of restaurants reviews in Lima, Peru between 2010 and 2021. In total exist more of 8791 restaurants and more of 1160666 reviews. With a total of 20 features with a high diversity: geospatial, text, date ,categoric and numeric feafures!\n\nThis has two general sections. The first is the *Restaurants*. This contains general and geospatial information.  The second is the *Reviews*. This contains the interaction between user and restaurant, with this way is possible to see the satisfaction of the client with a service.\nExist a possible third section: the *Users*. This information maybe will be added in two months.\n\nAbout the collection methodology, this is explained below:\n\n-**The sample:** The scraped reviews are the most recent reviews in all possible restaurants in the province of Lima. \n\n-**Set of items:** In one way, the users. In other way: the restaurants.\n\n-**Set of variables:** Exist two general tables. See the information below\n\nThe following diagram and table summarise all.\n\n**Table 1:** Restaurants\n|Variable|Description|\n| --- | ---|\n| Id | Id of the restaurant |\n|Name| Name of the restaurant |\n|Tag| The category of the restaurant |\n|x, y| Geospatial information and exact location of restaurant |\n|District|District where the restaurant is located |\n|Direction|District where the restaurant is located|\n|Stars|Mean Stars of restaurant in all time|\n|N_reviews|Number of reviews of restaurant in all time |\n|Min_Price| Minimum price in the menu of restaurant |\n|Max_Price| Maximum price in the menu of restaurant |\n|Platform| Platform where the information was downloaded |\n\n**Table 2:** Reviews\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review |\n|Id_nick| Id of the user. With this is possible to get the profile link |\n|Date| Date when the review was written |\n|Service| Id of the restaurant. Conection with Table 1 |\n|Review|Content of the review. This describe the satisfaction of the user |\n|Title|Title of the review. Only available in Tripadvisor|\n|Score|Punctuation in the review|\n|Likes|Number of votes in the publication|\n|Platform|Platform where the information was downloaded |\n\nAlso, exist auxiliar information related with the sentiment and emotion. This probabilities was obtained with a Spanish NrcLexicon, however, that results is not ok. Anyway, that is a reference and you can propose a fine tuning here. In adittion, also exist the probability to get a specific star, however, this was obtained with a simple logistic regression. Also i showed the information about Spanish NrcLexicon and Geospatial Borders. The author ands more information you can find [there](https:\/\/github.com\/jboscomendoza\/lexicos-nrc-afinn) and [there](https:\/\/www.geogpsperu.com\/2018\/02\/limite-distrital-politico-shapefile-ign.html).\n\n**Table 3:** Models\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review. Conection with Table 1 |\n|Positive| Probability of review that it will  show positive sentiment|\n|Negative| Probability of review that it will show negative sentiment|\n|Anger| Probability of review that it will show anger emotion|\n|Anticipation|Probability of review that it will show anticipation emotion |\n|Disgust|Probability of review that it will show disgust emotion|\n|Fear|Probability of review that it will show fear emotion|\n|Joy|Probability of review that it will show joy emotion|\n|Sadness|Probability of review that it will show sadness emotion|\n|Surprise|Probability of review that it will show surprise emotion|\n|Stars_1|Probability of review that it will get 1 star|\n|Stars_2|Probability of review that it will get 2 stars|\n|Stars_3|Probability of review that it will get 3 stars|\n|Stars_4|Probability of review that it will get 4 stars|\n|Stars_5|Probability of review that it will get 5 stars|\n\nThe entity relationship diagram! \n\n![erd](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1251020\/2455759\/erd.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210803%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210803T000105Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=53e7abea86abbc9efd45a0a7bdc40e950ba972565fd08ecd354f04e7ccb7ac6f7cc216f6e82d147b55ea3ab32badfb95501549291dfa783968246633b73dad6fb25206696d134b8a2352f7741c3002fa427d12b0320bfaedaeed7a1f8de0658804931e99708899ef95ad44a1bbbafc612f3a2e110ef4442f623003d79d12d8ae572988e8271c038d31d436ee2406e7da07a503548f443d24f65769324034c4ab63efc081a653e86f93f98e36c95198feb925515e487319f07009446d462ee71befeab29e3ef33c7bb48c9bc950974ee34c9fea648f9c3b0d125168035b75d6d6b2de5d09749b1cfce30208c65fe90653713461aa7acd5a1002dbbb4f50ecf44e)\n\n## Usage\n**Text classification:** The main topic in this types of datasets. Vectorize the reviews and define a predictive model. Identify strong and weak points of each restaurant.\n\n**Find patterns:** Compare districts (or restaurants) along the time. What is the common words in an excellent restaurant? Why these restaurants are better? \n\n**Reduction of dimention:** Detect similarities and then, clustering the reviews.\n\n## Acknowledgements\nThanks to Kaggle and its community. In general, thanks to the learners and teachers in machine learning, deep learning, natural language processing and computer vision.\n\n## Inspiration\nNatural language processing is a great tool. One application that I'm interested is detect bullies messages in any social network. I know that exist many notebooks and papers, but I'd like to build a bot that detect **all** possible cases and surely, there exist!\n","224":"The Bahama Breeze Menu Nutrition Data dataset provides detailed nutritional information for various items available at Bahama Breeze, a restaurant known for its Caribbean-inspired cuisine. The dataset is organized into the following columns:\n\n- Item: The name of the food or beverage item offered on the Bahama Breeze menu.\n- Category: The type or classification of the item, such as appetizers, entrees, desserts, or beverages.\n- Calories: The total number of calories contained in one serving of the item.\n- Total_Carbs_g: The total amount of carbohydrates in grams present in one serving of the item.\n- Sodium_mg: The amount of sodium in milligrams found in one serving of the item.\n- Weight_Watchers: The Weight Watchers points value assigned to one serving of the item, useful for individuals following the Weight Watchers diet program.\n\nThis dataset can be used to analyze the nutritional content of Bahama Breeze's offerings, helping customers make informed dietary choices based on their nutritional needs and preferences.","225":"This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes.","226":"## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n","227":"# Natural Language Understanding benchmark\n\nThis repository contains the results of three benchmarks that compare natural language understanding services offering:\n1. **built-in intents** (Apple\u2019s SiriKit, Amazon\u2019s Alexa, Microsoft\u2019s Luis,\nGoogle\u2019s API.ai, and [Snips.ai](https:\/\/snips.ai\/)) on a selection of\nvarious intents. This benchmark was performed in December 2016. Its results\nare described in length in the [following post](https:\/\/medium.com\/snips-ai\/benchmarking-natural-language-understanding-systems-d35be6ce568d).\n2. **custom intent engines** (Google's API.ai, Facebook's Wit, Microsoft's Luis, Amazon's Alexa, and Snips' NLU) for seven chosen intents. This benchmark was performed in June 2017. Its results are described in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/@alicecoucke\/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19).\n3. **extension of Braun et al., 2017** (Google's API.AI, Microsoft's Luis, IBM's Watson, Rasa)\nThis experiment replicates the analysis made by Braun et al., 2017, published in Evaluating Natural Language Understanding Services for Conversational Question Answering Systems as part of SIGDIAL 2017 proceedings. Snips and Rasa are added. Details are available in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/snips-ai\/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a).\n\nThe data is provided for each benchmark and more details about the methods are available in the README file in each folder.\n\n**Any publication based on these datasets must include a full citation to the following paper in which the results were published by the Snips Team:** \n\n[Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system \nfor private-by-design voice interfaces.\" 2018,](https:\/\/arxiv.org\/abs\/1805.10190)\n\naccepted for a spotlight presentation at the [Privacy in Machine Learning and Artificial Intelligence workshop](https:\/\/pimlai.github.io\/pimlai18\/#papers) colocated with ICML 2018.\n\n\n\n *The Snips team has joined Sonos in November 2019. These open datasets remain available and their access is now managed by the Sonos Voice Experience Team. Please email sve-research@sonos.com with any question.*","228":"The dataset roughly include 750 question answering pairs in json format. With three name component (instruction , input and output).Data set was collected via college level intro cs class in China.","229":"### Dataset Description for Filtered Sigma Dolphin Dataset\n\n#### Overview\nThis dataset is a cleaned and filtered version of the Sigma Dolphin dataset (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin), designed to aid in solving maths word problems using AI techniques. This was used as an effort towards taking part in the AI Mathematical Olympiad - Progress Prize 1 (https:\/\/www.kaggle.com\/competitions\/ai-mathematical-olympiad-prize\/overview). The dataset was processed using TF-IDF vectorisation and K-means clustering, specifically targeting questions relevant to the AIME (American Invitational Mathematics Examination) and AMC 12 (American Mathematics Competitions).\n\n#### Context\nThe Sigma Dolphin dataset is a project initiated by Microsoft Research Asia, aimed at building an intelligent system with natural language understanding and reasoning capacities to automatically solve maths word problems written in natural language. This project began in early 2013, and the dataset includes maths word problems from various sources, including community question-answering sites like Yahoo! Answers.\n\n#### Source and Original Dataset Details\n- **Original Dataset:** Sigma Dolphin (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin)\n- **Original Source:** https:\/\/msropendata.com\/datasets\/f0e63bb3-717a-4a53-aa79-da339b0d7992\n- **Project Page:** http:\/\/research.microsoft.com\/en-us\/projects\/dolphin\/\n- **References:**\n  - Shuming Shi, et al. \"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning.\" EMNLP 2015.\n  - Danqing Huang, et al. \"How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation.\" ACL 2016.\n  - JSON: http:\/\/json.org\/\n\n#### Content\nThe filtered dataset includes problems that are relevant for preparing for maths competitions such as AIME and AMC. The data is structured to facilitate the training and evaluation of AI models aimed at solving these types of problems. \n\n#### Datasets:\nThere are several filtered versions of the dataset based on different similarity thresholds (0.3 and 0.5). These thresholds were used to determine the relevance of problems from the original Sigma Dolphin dataset to the AIME and AMC problems.\n\n1. **Number Word Problems Filtered at 0.3 Threshold:**\n   - File: `number_word_test_filtered_0.3_Threshold.csv`\n   - Description: Contains problems filtered with a similarity threshold of 0.3, ensuring moderate relevance to AIME and AMC 12 problems.\n\n2. **Number Word Problems Filtered at 0.5 Threshold:**\n   - File: `number_word_std.test_filtered_0.5_Threshold.csv`\n   - Description: Contains problems filtered with a higher similarity threshold of 0.5, ensuring higher relevance to AIME and AMC 12 problems.\n\n3. **Filtered Number Word Problems 2 at 0.3 Threshold:**\n   - File: `filtered_number_word_problems2_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.3 similarity threshold.\n\n4. **Filtered Number Word Problems 2 at 0.5 Threshold:**\n   - File: `filtered_number_word_problems_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.5 similarity threshold.\n\n#### Why Different Similarity Thresholds?\nDifferent similarity thresholds (0.3 and 0.5) are used to provide flexibility in selecting problems based on their relevance to AIME and AMC problems. A lower threshold (0.3) includes a broader range of problems, ensuring a diverse set of questions, while a higher threshold (0.5) focuses on problems with stronger relevance, offering a more targeted and precise dataset. This allows users to choose the level of specificity that best fits their needs.\n\nFor a detailed explanation of the preprocessing and filtering process, please refer to the [Sigma Dolphin Filtered & Cleaned Notebook](https:\/\/www.kaggle.com\/code\/ryanmutiga\/sigma-dolphin-filtered-cleaned-notebook).\n\n#### Acknowledgements\nWe extend our gratitude to all the original authors of the Sigma Dolphin dataset and the creators of the AIME and AMC problems. This project leverages the work of numerous researchers and datasets to build a comprehensive resource for AI-based problem solving in mathematics.\n\n#### Usage\nThis dataset is intended for research and educational purposes. It can be used to train AI models for natural language processing and problem-solving tasks, specifically targeting maths word problems in competitive environments like AIME and AMC.\n\n#### Licensing\nThis dataset is shared under the Computational Use of Data Agreement v1.0.\n\n---\n\nThis description provides an extensive overview of the dataset, its sources, contents, and usage. If any specific details or additional sections are needed, please let me know!","230":"This dataset, titled \"Financial-QA-10k\", contains 10,000 question-answer pairs derived from company financial reports, specifically the 10-K filings. The questions are designed to cover a wide range of topics relevant to financial analysis, company operations, and strategic insights, making it a valuable resource for researchers, data scientists, and finance professionals. Each entry includes the question, the corresponding answer, the context from which the answer is derived, the company's stock ticker, and the specific filing year. The dataset aims to facilitate the development and evaluation of natural language processing models in the financial domain.\n\nAbout the Dataset\nDataset Structure:\n\n- **Rows**: 7000\n- **Columns**: 5\n- **question**: The financial or operational question asked.\n- **answer**: The specific answer to the question.\n- **context**: The textual context extracted from the 10-K filing, providing additional information.\n- **ticker**: The stock ticker symbol of the company.\n- **filing**: The year of the 10-K filing from which the question and answer are derived.\n\n\n**Sample Data:**\n\nQuestion: What area did NVIDIA initially focus on before expanding into other markets?\nAnswer: NVIDIA initially focused on PC graphics.\nContext: Since our original focus on PC graphics, we have expanded into various markets.\nTicker: NVDA\nFiling: 2023_10K\n\nPotential Uses:\n\n**Natural Language Processing (NLP):** Develop and test NLP models for question answering, context understanding, and information retrieval.\n**Financial Analysis:** Extract and analyze specific financial and operational insights from large volumes of textual data.\n**Educational Purposes:** Serve as a training and testing resource for students and researchers in finance and data science.","231":"Embark on a captivating exploration of Neymar, the revered Brazilian football sensation, with this meticulously curated dataset. Derived from a diverse array of dialogues between users and an interactive chatbot, this dataset provides an immersive journey into Neymar's multifaceted world, sourced directly from the wealth of information available on Wikipedia (https:\/\/en.wikipedia.org\/wiki\/Neymar)  . \n\nDelve into the intricacies of Neymar's illustrious career, from his groundbreaking exploits on the football field to his profound impact beyond it. Each conversation within this dataset offers a nuanced glimpse into Neymar's professional milestones, statistical feats, and personal anecdotes, carefully selected and distilled from the extensive discourse surrounding his life and legacy.\n\nThis dataset is well-suited for Natural Language Processing (NLP) tasks and deep learning applications. With its rich collection of conversational data, it provides an ideal resource for training NLP models, exploring sentiment analysis, question answering systems, and other advanced deep learning techniques.\n\nWhether you're a passionate football aficionado, a data enthusiast, or simply curious about Neymar's remarkable journey, this dataset promises to ignite your imagination and deepen your appreciation for one of football's most iconic figures.\n\nLicense: Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)\n\n","232":"## **Source Dataset:**\n\nThis data is a processed version of [https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa](https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa) dataset. \n\n## **Fields are mapped as below:**\n\n1. data -&gt; questions -&gt; input_text to **question**\n2. data -&gt; answers -&gt; input_text to **answer**\n3. data -&gt; answers -&gt; span_text to **ground_truth**\n4. data -&gt; story to **context**\n5. data -&gt; id to **data_id**\n6. **question_id** is generated sequentially. \n\n\n## **Processed using the notebook:**\n\n[https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth](https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth)\n\n\n## **Dataset Logo Credit:**\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@wesleyphotography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Wesley Tingey<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/yellow-and-white-10-card-FIq7K_wD4jM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n  \n\n","233":"# AstroMCQA Dataset\n\n## Purpose and scope\n\nThe primary purpose of AstroMCQA is for application developers in the domain of space engineering to be able to comparatively assess LLM performances on the specific task of multiple-choice question-answering\n\n## Intended Usage\n\nComparative assessement of differents LLMs, Model evaluation, audit, and model selection. Assessment of different quantization levels, different prompting strategies, and assessing effectiveness of domain adaptation or domain-specific fine-tuning.\n\n## Quickstart\n\n- Explore the dataset here:  https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/viewer\/default\/train\n- Evaluate an LLM (Mistral-7b) on AstroMCQA on collab here:<a target=\"_blank\" href=\"https:\/\/colab.research.google.com\/github\/patrickfleith\/astro-llms-notebooks\/blob\/main\/Evaluate_an_HuggingFace_LLM_on_a_Domain_Specific_Benchmark_Dataset.ipynb\">\n  <img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\">\n<\/a>\n\n## What is AstroMCQA GOOD for?\n\nWhat is AstroMCQA good for?\nThe primary purpose of AstroMCQA is for application developers in the domain of space mission design and operations to be able to address some questions such as: which LLM to use and how does it perform in the different subdomains? It enables to benchmark different models, different size, quantization methods, prompt engineering strategies, effectiveness of fine-tuning on the specific task of multiple-choice question-answering in space engineering.\n\n## What is AstroMCQA NOT GOOD for?\n\nIt is not suitable for training \/ fine-tuning LLM due to the very limited size of the dataset even if it could be combined with other tasks and science dataset for meta-learning.\n\n# DATASET DESCRIPTION\n### Access\n- Manual download from Hugging face hub: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\n- Or with python:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"patrickfleith\/Astro-mcqa\")\n```\n\n### Structure\n200 expert-created Multiple Choice Questions and Answers, one question per row in a comma separated file. Each instance is made of the following field (column):\n- **question**: a string.\n- **propositions**: a list of string. Each item in the list is one choice. At least one of the propositions correctly answer the question, but there can be multiple correct propositions. Even all propositions can be correct.\n- **labels**: list of integer (0\/1). Each element in the labels list correspond to proposition at the same position within the proposition list. A label of 0 means that the proposition is incorrect. A label of 1 means that the proposition is a correct choice to answer the question.\n- **justification**: Optional string. An optional field which may provide a justification of the answer.\n- **answerable**: A boolean, whether the question is answerable or not. At the moment, AstroMCQA only includes answerable questions.\n- **uid**: A unique identifier for the MCQA instance. May be useful for traceability in further processing tasks.\n\n### Metadata\nDataset is version controlled and commits history is available here: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/commits\/main\n \n### Languages\nAll instances in the dataset are in english\n \n### Size\n200 expert-created Multiple Choice Questions and Answers\n \n### Types of Questions\n- Some questions request expected generic knowledge in the field of space science and engineering.\n- Some questions require reasoning capabilities\n- Some questions require mathematical operations since a numerical result is expected (exam-style questions)\n\n### Topics Covered\nDifferent subdomains of space engineering are covered, including propulsion, operations, human spaceflight, space environment and effects, space project lifecycle, communication and link analysis, and more.\n\n# USAGE AND GUIDELINES\n#### License\nAstroMCQA \u00a9 2024 by Patrick Fleith is licensed under Creative Commons Attribution 4.0 International\n\n#### Restrictions\nNo restriction. Please provide the correct attribution following the license terms.\n \n#### Citation\nP. Fleith, AstroMCQA \u2013 Astronautics multiple choice questions and answers benchmark dataset for domain of Space Mission Engineering for LLM Evaluation, (2024). \n \n#### Update Frequency\nMay be updated based on feedbacks. If you want to become a contributor, let me know.\n \n#### Have a feedback or spot an error?\nUse the community discussion tab directly on the huggingface Astro-mcqa dataset page.\n \n#### Contact Information\nReach me here on the community tab or on LinkedIn (Patrick Fleith) with a Note.\n\n#### Current Limitations and future work\n- Only 200 multiple choice questions and answers. This makes it useless for fine-tuning purpose, although it could be integrated as part of a larger pool of datasets compiled for a larger fine-tuning.\n- While being a descent size enabling LLM evaluation, the space engineering expert time is scarce and expensive. On average it takes 8 minutes to create one MCQA example. Having more examples would be much better for robustness.\n- The dataset might be biased toward the very low number of annotators.\n- The dataset might be biased toward European Space Programs.\n- The dataset might not cover all subsystems or subdomain of astronautics although we tried to do our best covering the annotator\u2019s domains of expertise.\n- No peer-reviewing. Ideally we would like to have a Quality Control process to ensure high quality, and correctness of each example in the dataset. Given the limited resources, this is not yet possible. Feel free to come and contribute if you feel that is an issue","234":"EVJVQA, the first multilingual Visual Question Answering dataset with three languages: English, Vietnamese, and Japanese, is released in this task. UIT-EVJVQA includes question-answer pairs created by humans on a set of images taken in Vietnam, with the answer created from the input question and the corresponding image. EVJVQA consists of 33,000+ question-answer pairs for evaluating the mQA models.","235":"Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to fill this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new resource to design and evaluate personalized models related to the two tasks of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with both questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization improves remarkably the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.\n\nMore details can be found here: https:\/\/github.com\/pkasela\/SE-PQA\n\n```\n@inproceedings{10.1145\/3589335.3651445,\nauthor = {Kasela, Pranav and Braga, Marco and Pasi, Gabriella and Perego, Raffaele},\ntitle = {SE-PQA: Personalized Community Question Answering},\nyear = {2024},\nisbn = {9798400701726},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https:\/\/doi.org\/10.1145\/3589335.3651445},\ndoi = {10.1145\/3589335.3651445},\nbooktitle = {Companion Proceedings of the ACM on Web Conference 2024},\npages = {1095\u20131098},\nnumpages = {4},\nkeywords = {personalization, question answering, user model},\nlocation = {},\nseries = {WWW '24}\n}\n```","236":"Dataset Summary\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n\nThese problems take between 2 and 8 steps to solve.\nSolutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ \u2212 \u00d7\u00f7) to reach the final answer.\nA bright middle school student should be able to solve every problem: from the paper, \"Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable.\"\nSolutions are provided in natural language, as opposed to pure math expressions. From the paper: \"We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models\u2019 internal monologues\"","237":"## Discerption :\nThis dataset is a .parquet file prepared on instruction Question Answer dataset in Arabic language on multiple different categories.\n\n## Usage :\nbest used for instruct fine-tuning large language models to perform better on Arabic language.\n\n## License and Usage :  \nThis dataset is a Kaggle version of generated Arabic dataset is created by Yasbok from hugging face. due to issues extracting it directly from \"HF\" using the `Dataset` library, i downloaded it and uploaded it here on kaggle so it can be used easily.\n[Hugging face dataset link](https:\/\/huggingface.co\/datasets\/Yasbok\/Alpaca_arabic_instruct)\n","238":"","239":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8442057%2Ff7c62f9d14b5620b8452f39690a24568%2Ffig1.png?generation=1715577026848120&alt=media)\n\nAbstract: FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). This repository contains an open source sample of 150 annotated examples used in the evaluation and analysis of models assessed in the FinanceBench paper. FinanceBench comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.\n\n**Contact:** To evaluate your models on the full FinanceBench dataset, or if you have questions about this work, you can email us at contact@patronus.ai\n\n**Dataset Overview:** The provided open-source dataset (n=150) consists of the following attributes:\n\n```python\n- financebench_id:  unique question identifier  \n    - question:         question of interest\n    - answer:           gold answer\n    - question_type:    type of the question {domain-relevant, metrics-generated, novel-generated}\n    - doc_name:         name of the relevant financial document to answer the question\n    - doc_link:         url to retrieve the relevant financial document\n    - doc_period:       period of the relevant financial document\n    - evidence_text:    extracted evidence text\n    - page_number       page number(s) of evidence text\n```\n\n**Citation:** If you use our open-source dataset or refer to our result, please use the following citation:\n\n```python\n@misc{islam2023financebench,\n      title={FinanceBench: A New Benchmark for Financial Question Answering}, \n      author={Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},\n      year={2023},\n      eprint={2311.11944},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n","240":"VQA, or Visual Question Answering, is a multifaceted challenge that tasks models with providing coherent responses to natural language questions posed about images. This intricate task demands a deep understanding of both the visual content and the semantic nuances embedded within the questions. By bridging the realms of Computer Vision and Natural Language Processing, VQA addresses a plethora of sub-problems, including object detection, scene classification, and more, rendering it a quintessential benchmark in the realm of AI","241":"**This work has been accepted to ACL 2024.** You can find the pre-print related to this work https:\/\/arxiv.org\/abs\/2309.08902\n\n**Dataset Summary:**\n\n**GenAssocBias** is a dataset that measures **stereotype bias in LLMs**. GenAssocBias consists of 11,940 sentences that measure model preferences across **ageism, beauty, beauty_profession, nationality, and institutional bias.**\n\nSupported Tasks and Leaderboards\n\nmultiple-choice question answering\n\nLanguages\n\nEnglish (en)\n\nDataset Descriptions:\n\nThere are 8 columns in our dataset. The description of each column is given below:\n\nbias_type: This column indicates different types of biases including ageism, beauty, beauty_profession, nationality, and institutional.\n\ntarget_gender: This column indicates the particular gender type. There are three unique gender types namely 'male', 'female', and 'not_specified'.\n\ncontext: This column indicates different sentences. These are the context sentences.\n\nitem_category: This column is either 'positive' or 'negative'. When the attribute or stimulus in the context sentence is positive, we named it as 'positive' and when the attribute or stimulus is negative, then we named it as 'negative'.\n\ntype_category: This column tells us, which direction the data is. There are two different types of direction, namely SAI and ASA.\n\nanti_stereotype: When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is positive among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is negative among the options.\n\nstereotype: This column is the opposite of the 'anti_stereotype' column. When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is negative among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is positive.\n\nunrelated: This column contains the neutral attributes or stimuli.\n\nCitation Information:\n\n@article{kamruzzaman2023investigating, title={Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models}, author={Kamruzzaman, Mahammed and Shovon, Md Minul Islam and Kim, Gene Louis}, journal={arXiv preprint arXiv:2309.08902}, year={2023} }","242":"PDFs of different boardgames rulebooks\n- Root\n- Lords of Waterdeep\n- Troyes","243":"","244":"","245":"","246":"","247":"","248":"This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes.","249":"this graph was created in Loocker studio,PowerBi and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Ff35dd0bdb2f87ea2badf1ea1d57086bc%2Fgraph1.jpg?generation=1718398785729815&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F52689afbe7380af61e578767366ac925%2Fgraph2.jpg?generation=1718398791128270&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F992bd2cc851499b2f8f0bcb34732fab7%2Fgraph3.png?generation=1718398796130065&alt=media)\n\nAbstract\nWe propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.\n\nIntroduction\nOnce viewed as a luxury good, nowadays wine is increasingly enjoyed by a wider range of consumers. Portugal is a top ten wine exporting country, with 3.17% of the market share in 2005 [11]. Exports of its vinho verde wine (from the northwest region) have increased by 36% from 1997 to 2007 [8]. To support its growth, the wine industry is investing in new technologies for both wine making and selling processes. Wine certification and quality assessment are key elements within this context. Certification prevents the illegal adulteration of wines (to safeguard human health) and assures quality for the wine market. Quality evaluation is often part of the certification process and can be used to improve wine making (by identifying the most influential factors) and to stratify wines such as premium brands (useful for setting prices).","250":"","251":"","252":"  Available at: [@Elsevier] http:\/\/dx.doi.org\/10.1016\/j.dss.2009.05.016\n                [Pre-press (pdf)] http:\/\/www3.dsi.uminho.pt\/pcortez\/winequality09.pdf\n                [bib] http:\/\/www3.dsi.uminho.pt\/pcortez\/dss09.bib\n\n1. Title: Wine Quality \n\n2. Sources\n   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n   \n3. Past Usage:\n\n  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n  Modeling wine preferences by data mining from physicochemical properties.\n  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n\n  In the above reference, two datasets were created, using red and white wine samples.\n  The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n  between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n  these datasets under a regression approach. The support vector machine model achieved the\n  best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n  etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n  analysis procedure).\n \n4. Relevant Information:\n\n   The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n   For more details, consult: http:\/\/www.vinhoverde.pt\/en\/ or the reference [Cortez et al., 2009].\n   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\n   These datasets can be viewed as classification or regression tasks.\n   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n   or poor wines. Also, we are not sure if all input variables are relevant. So\n   it could be interesting to test feature selection methods. ","253":"","254":"","255":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","256":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","257":"This dataset contains detailed information on residential property prices in India. It includes features such as the number of bedrooms and bathrooms, living and lot area, number of floors, waterfront presence, house condition and grade, construction and renovation years, and geographic coordinates. Additional features include the number of nearby schools, distance from the airport, and property price. This dataset is ideal for real estate market analysis and predictive modeling, providing insights into factors influencing property prices across different regions in India.","258":"The Bengaluru House Prediction project is a machine learning-based solution designed to help potential homebuyers in Bangalore predict home prices accurately. It addresses a common problem faced by homebuyers who struggle to make informed decisions due to the complexity of factors that influence property prices.\n\nOverall, the Bengaluru House Prediction project is a valuable tool that empowers users with the right information to make informed decisions and demonstrates the potential of machine learning in the real estate industry.","259":"This house costing data is transformed for data modelling and is the updated version of my previous dataset 'Transformed Housing Data'. The updation is the creation of dummy variables of certain categorical features for modelling purpose. There are a total of 31 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","260":"This house costing data is the transformed version of my previous dataset titled 'Raw Housing Cost Data (imputed)'; it's imputed and the variables in it are transformed (for modelling). Same as the earlier data, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","261":"This house costing data has 21609 rows of information about different Houses. It's the imputed version of my previous dataset titled Raw Housing Cost Data. Same as earlier, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment.","262":"This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting.","263":"Dataset includes house sale prices for King County in USA. \nHomes that are sold in the time period: May, 2014 and May, 2015.\n\nColumns:\n    - ida: notation for a house\n    - date: Date house was sold\n    - price: Price is prediction target\n    - bedrooms: Number of Bedrooms\/House\n    - bathrooms: Number of bathrooms\/House\n    - sqft_living: square footage of the home\n    - sqft_lot: square footage of the lot\n    - floors: Total floors (levels) in house\n    - waterfront: House which has a view to a waterfront\n    - view: Has been viewed\n    - condition: How good the condition is ( Overall )\n    - grade: overall grade given to the housing unit, based on King County grading system\n    - sqft_abovesquare: footage of house apart from basement\n    - sqft_basement: square footage of the basement\n    - yr_built: Built Year\n    - yr_renovated: Year when house was renovated\n    - zipcode: zip\n    - lat: Latitude coordinate\n    - long: Longitude coordinate\n    - sqft_living15: Living room area in 2015(implies-- some renovations) \n    - sqft_lot15: lotSize area in 2015(implies-- some renovations)","264":"This dataset contains information related to housing sales, in the form of individual properties. Here's a breakdown of the columns:\n\n| Column Name    | Description                                         |\n|----------------|-----------------------------------------------------|\n| Lot_Frontage   | Linear feet of street connected to the property    |\n| Lot_Area       | Lot size in square feet                             |\n| Bldg_Type      | Type of building|\n| House_Style    | Style of the house      |\n| Overall_Cond   | Overall condition rating of the house               |\n| Year_Built     | Year the house was built                            |\n| Exter_Cond     | Exterior condition rating of the house               |\n| Total_Bsmt_SF  | Total square feet of basement area                  |\n| First_Flr_SF   | First-floor square feet                             |\n| Second_Flr_SF  | Second-floor square feet                            |\n| Full_Bath      | Number of full bathrooms                            |\n| Half_Bath      | Number of half bathrooms                            |\n| Bedroom_AbvGr  | Number of bedrooms above ground                     |\n| Kitchen_AbvGr  | Number of kitchens above ground                     |\n| Fireplaces     | Number of fireplaces                                |\n| Longitude      | Longitude coordinates of the property location      |\n| Latitude       | Latitude coordinates of the property location       |\n| Sale_Price     | Sale price of the property                          |\n\nThe dataset contains 2413 entries and has a mixture of numerical and categorical data. It's likely used for analyzing various factors influencing housing sale prices, such as location, size, condition, and amenities.","265":"House price prediction\nPredicting house prices is a common task in data science and machine learning. Here's a high-level overview of how you might approach it:\n\nData Collection:\nGather a dataset containing features of houses (e.g., size, number of bedrooms, location, amenities) and their corresponding prices. Websites like Zillow, Kaggle, or government housing datasets are good sources.\n\nData Preprocessing:\nClean the data by handling missing values, encoding categorical variables, and scaling numerical features if necessary. This step ensures that the data is in a suitable format for training a model. Feature Selection\/Engineering: Choose relevant features that are likely to influence house prices. You may also create new features based on domain knowledge or data analysis.\n\nModel Selection:\nSelect a regression model suitable for predicting continuous target variables like house prices. Common choices include Linear Regression, Decision Trees, Random Forests, Gradient Boosting, and Neural Networks.\n\nModel Training:\nSplit your dataset into training and testing sets to train and evaluate the performance of your model. You can further split the training set for validation purposes or use cross-validation techniques.\n\nModel Evaluation:\nAssess the performance of your model using appropriate evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or Root Mean Squared Error (RMSE).\n\nHyperparameter Tuning:\nFine-tune your model's hyperparameters to improve its performance. Techniques like grid search or random search can be employed for this purpose.\n\nDeployment:\nOnce satisfied with your model's performance, deploy it to make predictions on new data. This could be as simple as saving the trained model and creating an interface for users to input house features.","266":"Real Estate Prices Dataset\nThis dataset comprises information on 4,600 real estate transactions, providing a detailed snapshot of the housing market in various locations. Each record captures the characteristics of a house, its surroundings, and transaction details from transactions that occurred around May 2, 2014. The dataset includes the following fields:\n\ndate: The date of the transaction.\nprice: The sale price of the property (in USD).\nbedrooms: The number of bedrooms.\nbathrooms: The number of bathrooms, represented in half-baths (e.g., 1.5 indicates one full bath and one half bath).\nsqft_living: The square footage of the home's living area.\nsqft_lot: The square footage of the lot.\nfloors: The number of floors.\nwaterfront: A binary indicator for whether the property is on the waterfront (1) or not (0).\nview: An index from 0 to 4 indicating the quality of the view.\ncondition: An index from 1 to 5 on the condition of the property.\nsqft_above: The square footage of the house apart from the basement.\nsqft_basement: The square footage of the basement.\nyr_built: The year the property was built.\nyr_renovated: The year of the last renovation.\nstreet: The street address of the property.\ncity: The city in which the property is located.\nstatezip: The state and ZIP code.\ncountry: The country of the property.\n\n\nThis dataset can be particularly useful for projects involving real estate market analysis, price prediction models, and economic research related to housing trends. Researchers and enthusiasts can explore aspects such as the impact of property characteristics on price, trends over time, and geographical price variations.","267":"# General Information and License\n- HDB Resale Price data made available by SG government through the Open Data License.\n    1. https:\/\/beta.data.gov.sg\/open-data-license\n    2. https:\/\/beta.data.gov.sg\/collections\/189\/datasets\/d_ebc5ab87086db484f88045b47411ebc5\/view\n- Geodata made available by SG government through the OneMap\n    1. https:\/\/www.onemap.gov.sg\/apidocs\/apidocs\n- I claim NO rights whatsoever to this data. This data has been uploaded here for educational purposes.\n- There is no personally identifiable information in this data.\n\n# Data Availability\n- 1990 - 02-2012 (based on registration date)\n- 03-2012 - 2023-09 (based on approval date)\n\nContains resale prices only because most of Singapore's population lives in (very well-made and maintained!) public housing. \n\nDoes not contain condo\/landed house prices.\n\n# Special Notes\n- Public housing in Singapore is generally sold on a leasehold (typically 99-year) basis.\n- Prices tend to decrease as the remaining lease period decreases (see Bala's Curve).\n\n# Data Representation\nEach row represents 1 transaction.\n\n# Data Dictionary\n\n| Column | Description  |\n| --- | --- |\n| latitude | Latitude coordinate.  |\n| longitude | Longitude coordinate. |\n| postal_code | 6-digit postal code. |\n| address | Street address. |\n| closest_mrt | Name of closest Mass Rapid Transit (MRT) station. |\n| closest_mrt_dist| Distance (metres) to closest MRT station. |\n| cbd_dist | Distance (metres) to Central Business District (1.2830, 103.8513 -- Raffles Place Station) | \n| month| YYYY-MM representation of the transaction date. |\n| town | Name of the neighbourhood in which the flat is located. |\n| flat_type | Generic type of flat, e.g. \"3 ROOM\", \"4 ROOM\". |\n| block | Block number. |\n| street name | Street name. |\n| storey_range | Height of the flat. Specific floors are never given (as of 2024). |\n| floor_area | Floor area in Square Metres. |\n| flat_model | Model\/subtype of flat. Each flat type has certain specifications and configurations. |\n| lease_commence_date | The year that the leasehold contract of the flat began. Most (not all) flats have 99-year leases.|\n| remaining_lease| Remaining lease of the contract in years and months. Not all files have this column. It only starts appearing in the file with data from Jan 2015 to Dec 2016.|\n| resale_price| Target column. This is the ***transacted*** resale price as agreed between buyer and seller. |\n|years remaining| Years remaining on the lease. |\n\n# Update Frequency\n- This dataset may be updated every year or so as new transaction data becomes available.","268":"# **About Dataset**\nThis project is all about taking a closer look at the books available on Amazon. We've collected information on different types of genres, sub-genres, and individual books like their titles, authors, prices, and ratings. By digging into this data, we hope to learn interesting things about what kinds of books are popular, how they're priced, and what people like to read. This can help us understand more about the world of books and what makes them tick on Amazon.\n\n### **Dataset 1: Genre**\n- **Title**: This column contains the main genres of books available on Amazon.\n- **Number of Sub-genres:** Indicates the count of sub-genres associated with each main genre.\n- **URL:** Provides the link to the page on Amazon where books of this genre are listed.\n### **Dataset 2: SubGenre**\n- **Title:** Lists the specific sub-genres within each main genre.\n- **Main Genre:** Indicates the overarching genre to which each sub-genre belongs.\n- **No. of Books:** Shows the count of books categorized under each sub-genre.\n- **URL:** Provides the link to the page on Amazon where books of this sub-genre are listed.\n### **Dataset 3: Books_df**\n- **Title:** The title of the book.\n- **Author:** Name of the author or publication house.\n- **Main Genre:** The main genre the book belongs to.\n- **Sub Genre:** The specific sub-genre of the book.\n- **Type:** Indicates the format of the book, such as paperback, Kindle, audiobook, or hardcover.\n- **Price:** The price of the book.\n- **Rating:** The average rating of the book given by users.\n- **No. of People Rated:** Indicates the count of users who have rated the book.\n- **URLs:** Provides the link to the book's page on Amazon for further details and purchase options.","269":"The dataset provides information about apartments available for sale in Portugal, gathered from Imovirtual. Each entry in the dataset represents an apartment and includes the following attributes:\n\nIndex: A unique identifier for each entry in the dataset.\nName: The apartment type, indicating the number of bedrooms.\nPrice: The selling price of the apartment.\nArea: The area of the apartment in square meters.\nLocation: The district where the apartment is situated.\n\nThe provided data sample encompasses a variety of apartment types, prices, areas, and locations in different districts of Portugal, covering both new and used apartments. These data are valuable for real estate market analysis and price predictions.","270":"This dataset provides detailed information about residential properties, including their prices, area sizes, number of bedrooms, and several other key features. The dataset aims to assist in the analysis and prediction of housing prices based on various property attributes.\n\n","271":"This dataset provides comprehensive information about rental house prices across various locations in India. It includes details such as house type, size, location, city, latitude, longitude, price, currency, number of bathrooms, number of balconies, negotiability of price, price per square foot, verification date, description of the property, security deposit, and status of furnishing (furnished, unfurnished, semi-furnished).\n\n`Note: This is Recently scraped data of April 2024.`\n\n### **Dataset Glossary (Column-Wise)**\n- **House Type**: Type of house (e.g., apartment, villa, duplex).\n- **House Size**: Size of the house in square feet or square meters.\n- **Location**: Specific area or neighborhood where the property is located.\n- **City**: City in India where the property is situated.\n- **Latitude**: Geographic latitude coordinates of the property location.\n- **Longitude**: Geographic longitude coordinates of the property location.\n- **Price**: Rental price of the house.\n- **Currency**: Currency in which the price is denoted (e.g., INR - Indian Rupees).\n- **Number of Bathrooms**: Total number of bathrooms in the house.\n- **Number of Balconies**: Total number of balconies in the house.\n- **Negotiability**: Indicates whether the price is negotiable (Yes\/No).\n- **Price per Square Foot**: Price of the house per square foot.\n- **Verification Date**: Date when the rental information was verified.\n- **Description**: Additional description or details about the property.\n- **Security Deposit**: Amount of security deposit required for renting the property.\n- **Status**: Indicates the furnishing status of the property (furnished, unfurnished, semi-furnished).\n\n### **Usage**\nThis dataset aims to provide valuable insights into the rental housing market in India, enabling analysis of rental trends, comparison of prices across different locations and property types, and understanding the impact of various factors on rental prices. Researchers, analysts, and policymakers can utilize this dataset for a wide range of applications, including real estate market analysis, urban planning, and economic research.\n\n### **Acknowledgement**\nThis Dataset is created from [https:\/\/www.makaan.com\/](https:\/\/www.makaan.com\/). If you want to learn more, you can visit the Website.\n\nCover Photo by: [Playground.ai](https:\/\/playground.com\/post\/a-dynamic-and-lively-shot-of-a-daman--diu-haveli-of-two-flo-clrrmmk6a039gs6019ez0xpjm)","272":"The California Housing dataset contains information about various factors affecting housing prices in the California area. It includes features such as the per capita crime rate, average number of rooms per dwelling, proportion of residential land zoned for lots over 25,000 square feet, and more.\n# Datasource: The data were derived from information collected by the U.S. Census Service concerning housing in the area of California.\n\nColumns:\nLocation: Region\nAddress: Address of the house\nPrice: Price in millions\nCRIMS per capita crime rate by town\nZN proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS proportion of non-retail business acres per town\nCHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nNOX nitric oxides concentration (parts per 10 million)\nRM average number of rooms per dwelling\nAGE proportion of owner-occupied units built prior to 1940\nDIS weighted distances to five Boston employment centres\nRAD index of accessibility to radial highways\nTAX full-value property-tax rate per 10,000usd\nPTRATIO pupil-teacher ratio by town\nB 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT % lower status of the population\n","273":"Welcome to the Gurgaon Real Estate Dataset tailored specifically for training predictive models to forecast house prices in one of India's most dynamic urban centers. This dataset is meticulously curated, merged, and cleaned to facilitate seamless model training and accurate predictions.\n\nEncompassing a diverse array of attributes crucial for predictive modeling, this dataset provides an unparalleled opportunity for data scientists, machine learning enthusiasts, and real estate professionals to develop robust pricing models. Each entry in the dataset represents a unique property, meticulously curated and standardized to ensure consistency and reliability.\n\nKey attributes included in the dataset comprise property type, location, area, number of bedrooms and bathrooms, amenities, parking facilities, and additional features. Additionally, historical transaction data, such as previous sale prices and dates, are provided to enrich the predictive modeling process.\n\nBy leveraging this dataset, users can embark on a journey to:\n\n1. Develop sophisticated machine learning models to accurately predict house prices in Gurgaon.\n2. Identify significant predictors and drivers of house prices, aiding in market analysis and decision-making.\n3. Explore feature engineering techniques to enhance model performance and interpretability.\n4. Evaluate the impact of location, amenities, and other property attributes on pricing dynamics.\n5. Assess model performance through rigorous evaluation metrics and validation techniques.\n\nThis dataset serves as a catalyst for innovation and collaboration within the data science community, empowering practitioners to unlock insights and drive value in the real estate domain. Whether you're a seasoned data scientist or an aspiring enthusiast, this dataset offers an exciting opportunity to delve into the complexities of real estate pricing dynamics and make meaningful contributions to the field.\n\nWe encourage users to explore, analyze, and experiment with this dataset, pushing the boundaries of predictive modeling and advancing our understanding of house price dynamics in Gurgaon. Together, let's harness the power of data to unravel the mysteries of real estate pricing and pave the way for informed decision-making in one of India's most vibrant urban landscapes.","274":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","275":"# **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).","276":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","277":"","278":"This dataset contains 284 images of human chest X-ray belonging to 2 classes (Covid-19 Positive and Negative). The dataset has been divided into train and validation splits with 112 and 30 images respectively. The dataset shall be used to train deep learning models such as CNN.\n\n**Dataset Hierarchy**\n\nDataset.zip\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 normal\n\u2502   \u2514\u2500\u2500 infected\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 normal\n    \u2514\u2500\u2500 infected\n\n**Citations** : \n- Covid-19 Positive Patient Chest X-ray images (Source : [ https:\/\/github.com\/ieee8023\/covid-chestxray-dataset\/tree\/master](url))\n- Kaggle Human Lung X-ray Image Dataset (Extracted only \"Normal\") (Source : [https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia](url))","279":"# Overview\nThe Chest X-ray 8 Subset dataset is a curated collection of chest radiographs specifically designed for the development and evaluation of object detection models focused on thoracic diseases. This subset consists of 790 images and 984 annotated bounding boxes, capturing various abnormalities within the lungs and surrounding structures. The annotations are provided in both YOLO and Pascal VOC formats, facilitating their use in a wide range of machine learning frameworks.\n\n# Data Composition\n- Total Images: 790\n- Total Bounding Boxes: 984\n- Image Format: PNG\n- Annotation Formats: YOLO and Pascal VOC\n- All Images are resized to 512x512 pixels.\n\nTest set: 631 images\nVal set: 159 images\n\n# Classes and Labels\nThe dataset includes annotations for 14 different classes of thoracic diseases:\n\n1. Atelectasis,\n2. Cardiomegaly,\n3. Effusion,\n4. Infiltrate,\n5. Nodule,\n6. Mass,\n7. Pneumonia,\n8. Pneumothorax\n\n","280":"","281":"### Content\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Normal\/Covid-19\/Tuberculosis). A total of 7135 x-ray images are present.\n\n### Acknowledgements\nSources:\nhttps:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia\nhttps:\/\/www.kaggle.com\/tawsifurrahman\/tuberculosis-tb-chest-xray-dataset\nhttps:\/\/www.kaggle.com\/prashant268\/chest-xray-covid19-pneumonia\nhttps:\/\/github.com\/ieee8023\/covid-chestxray-dataset\n\n### Inspiration\nApplication of Deep Learning techniques to detect and classify lung diseases from x-ray images","282":"<div><div><div style=\"min-height: 80px\"><div><h1>Content and context<\/h1>\n<p>Tuberculosis is a disease that affects many people in developing countries. While treatment is possible, it requires an accurate diagnosis first. In these countries projects there are in many cases available X-ray machines (through low-cost projects and donations), but often the radiological expertise is missing for accurately assessing the images. An algorithm that could perform this task quickly and cheaply could drastically improve the ability to diagnose and ultimately treat the disease.<\/p>\n<p>In more developed countries, X-ray radiography is often used for screening new arrivals and determining eligibility for a work-permit. The task of manually examining images is time consuming and an algorithm could increase efficiency, improve performance and ultimately reduce cost of this screening. <\/p>\n<p>This dataset contains over 500 x-rays scans with clinical labels collected by radiologists.<\/p>\n<h1>Acknowledgements<\/h1>\n<p>The two datasets were published together in an analysis here: <a target=\"_blank\" href=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/\">https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/<\/a>.<br>\nThe datasets come from Shenzhen and Montgomery respectively.<\/p>\n<h3>China Set - The Shenzhen set - Chest X-ray Database<\/h3>\n<p>The standard digital image database for Tuberculosis is created by the National Library of Medicine, Maryland, USA in collaboration with Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China. The Chest X-rays are from out-patient clinics, and were captured as part of the daily routine using Philips DR Digital Diagnose systems. <br>\nNumber of X-rays: <\/p>\n<ul>\n<li>336 cases with manifestation of tuberculosis, and <\/li>\n<li>326 normal cases.<\/li>\n<\/ul>\n<p>It is requested that publications resulting from the use of this data attribute the source (National Library of Medicine, National Institutes of Health, Bethesda, MD, USA and Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China) and cite the following publications:  <\/p>\n<ul>\n<li>Jaeger S, Karargyris A, Candemir S, Folio L, Siegelman J, Callaghan F, Xue Z, Palaniappan K, Singh RK, Antani S, Thoma G, Wang YX, Lu PX, McDonald CJ.  Automatic tuberculosis screening using chest radiographs. IEEE Trans Med Imaging. 2014 Feb;33(2):233-45. doi: 10.1109\/TMI.2013.2284099. PMID: 24108713<\/li>\n<li>Candemir S, Jaeger S, Palaniappan K, Musco JP, Singh RK, Xue Z, Karargyris A, Antani S, Thoma G, McDonald CJ. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Trans Med Imaging. 2014 Feb;33(2):577-90. doi: 10.1109\/TMI.2013.2290491. PMID: 24239990<\/li>\n<\/ul>\n<h3>Montgomery County X-ray Set<\/h3>\n<p>X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays are abnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities, including effusions and miliary patterns. The data set includes radiology readings available as a text file.<\/p>\n<h1>Ideas<\/h1>\n<ul>\n<li>Experiment with lung segmentation<\/li>\n<li>Build disease classifiers for various conditions<\/li>\n<li>Test models on data across different manufacturers <\/li>\n<li>Build GANs that are able to make the datasets indistinguishable (Adversarial Discriminative Domain Adaptation: <a target=\"_blank\" href=\"https:\/\/arxiv.org\/abs\/1702.05464\">https:\/\/arxiv.org\/abs\/1702.05464<\/a>)<\/li><\/ul><\/div><\/div><\/div><\/div>","283":"Please cite: **Enhancing COVID-19 prediction using transfer learning from Chest X-ray images**\n\nP. -H. Huynh, T. -N. Tran and V. H. Nguyen, \"Enhancing COVID-19 prediction using transfer learning from Chest X-ray images,\" 2021 8th NAFOSTED Conference on Information and Computer Science (NICS), Hanoi, Vietnam, 2021, pp. 398-403, doi: 10.1109\/NICS54270.2021.9701516.\n\n&gt;@INPROCEEDINGS{9701516,\n  author={Huynh, Phuoc-Hai and Tran, Trung-Nguyen and Nguyen, Van Hoa},\n  booktitle={2021 8th NAFOSTED Conference on Information and Computer Science (NICS)}, \n  title={Enhancing COVID-19 prediction using transfer learning from Chest X-ray images}, \n  year={2021},\n  volume={},\n  number={},\n  pages={398-403},\n  keywords={COVID-19;Training;Pandemics;Pulmonary diseases;Computational modeling;Transfer learning;Predictive models;COVID-19;transfer learning;imbalanced dataset;X-Ray images},\n  doi={10.1109\/NICS54270.2021.9701516}}\n\n\n","284":"In this case study, you are hired by a hospital in Toronto as a deep learning consultant and tasked with automating the detection and classification process of pulmonary diseases.\n\nThe team collected extensive X-Ray chest data and asked you to develop a model that could detect and classify diseases in less than a minute.\n\nThey provided a dataset consisting of 133 images and divided into 4 classes:\n\n0 - Covid-19\n1 - Healty X-ray\n2 - Viral Pneumonia X-ray\n3 - Bacterial Pneumonia X-ray\n\nHowever, it's crucial to note that the use of AI in healthcare raises some ethical and social concerns. For instance, there are worries that AI systems may be biased and result in misdiagnoses. Additionally, there are concerns that AI may replace doctors and other healthcare professionals.","285":"","286":"In this dataset we have 3 classes.\n\n1. Pneumonia\n2. TB\n3. Normal\n\n\nA brief idea about the diseases ,\nPneumonia:\nPneumonia is an infection that inflames the air sacs  in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. Pneumonia can range in seriousness from mild to  life-threatening. It is most serious for infants and young children, people older than age 65, and people with weakened immune systems. Common causes  of pneumonia include bacteria, viruses, and fungi. Treatment depends on the type and severity of pneumonia, but typically involves antibiotics for bacterial pneumonia and antiviral medications for viral pneumonia. Vaccines  are available to prevent some types of pneumonia, such as pneumococcal pneumonia and influenza pneumonia.\n\nTuberculosis (TB):\nTuberculosis (TB) is a bacterial infection primarily affecting the lungs but can  also impact other parts of the body. It spreads through the air when an infected person coughs or sneezes, releasing infectious particles. Symptoms include persistent cough, chest pain,  coughing up blood or sputum, weakness, weight loss, lack of appetite,  chills, fever, and night sweats. TB can be latent, without symptoms and not contagious, but it can become active if untreated. Active TB is  contagious and requires several  months of antibiotic treatment. Drug-resistant strains may need stronger medications and longer treatment. Prevention involves screening, early  detection, treatment, and vaccination with the BCG vaccine, though  its effectiveness varies.","287":"This dataset is what I used to train my model. I made this dataset by combining two different datasets which are available for use in Kaggle. So I would like to cite them here. I am also thanking them for this. \n\nThis dataset combines chest X-ray images from two sources for pneumonia detection. The primary dataset, sourced from Kaggle Chest X-Ray Images (Pneumonia) [1], includes 5,863 images (1,583 normal, 4,273 pneumonia). Supplementary data from the COVID-19 Radiography Database [2], contributes 1,345 pneumonia images and 4,053 normal images. Thus making the number of pneumonia and the normal images the same that is 5618 images and in total, the dataset comprises 11,236 images. \n\nThe two sources from which I took the images are trust worthy as you can see in the source dataset pages.\n\n[1]. Chest X-Ray Images (Pneumonia) :\n https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n[2]. COVID-19 Radiography Database\nhttps:\/\/www.kaggle.com\/datasets\/tawsifurrahman\/covid19-radiography-database?rvi=1","288":"","289":"Context\n\n  The whole world is suffering from COVID-19 disease and its proper and timely diagnosis is the need of the hour. So for building an efficient AI-based diagnostic system we have collected Chest ray images from different sources so that we can train our CNN model for automating the whole diagnosis method. Therefore we have collected Chest ray images from different sources and research papers and combined them to create one comprehensive dataset that can be used by research community.\n\n  This dataset is also used in COVID Lite paper which has shown significant results by building novel CNN based solution.\n\nContent\n\n  This dataset consists of a posteroanterior (PA) view of chest X-ray images comprising Normal, Viral, and CVOID-19 affected patients. There are total 1709 CXR images.","290":"This dataset is shared by Lecturer **Mr. Wahyono, Ph. D.** as a base for his Computer Vision course at Universitas Gadjah Mada. This is the provided dataset for individual assignment COVID-19 Classification. \n\nThe source as described in the `Sources.txt` file is this GitHub link: \n\n[https:\/\/github.com\/ieee8023\/covid-chestxray-dataset](url)","291":"","292":"### pneumonia CXR dataset\nThe dataset is organized into 3 folders (train, validation, test) and contains subfolders for each image category (Normal\/COVID-19\/Bacteria\/Viral). There are 6,126 chest X-ray (CXR) images and 4 categories (Normal\/COVID-19\/Bacteria\/Viral).\n- 1,535 Normal\n- 1,708 COVID-19\n- 1,390 Bacteria pneumonia \n- 1,493 Viral pneumonia \nGround-truth lung segmentation masks are provided for the entire dataset. \n\n### References\nIn pneumonia CXR dataset, the X-ray images are collected from the following repositories:\n1. Chest X-Ray Images (Pneumonia): https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n2. COVID-QU-Ex Dataset: https:\/\/www.kaggle.com\/datasets\/cf77495622971312010dd5934ee91f07ccbcfdea8e2f7778977ea8485c1914df\n","293":"**Description**\n\nLung disease encompasses a wide range of conditions that affect the lungs and their ability to function effectively. These conditions can be caused by various factors, including infections, environmental factors, genetic predispositions, and lifestyle choices. Lung diseases can result in symptoms such as coughing, shortness of breath, chest pain, and reduced lung function. Detecting and diagnosing lung diseases is crucial for patient care, as they can have a significant impact on an individual's health and quality of life.\n\n**Global Impact:**\n\nLung diseases have a substantial global impact. According to the World Health Organization (WHO), respiratory diseases, including lung diseases, are responsible for a significant portion of global mortality. In 2016, respiratory diseases were the fourth leading cause of death worldwide, with an estimated 3.0 million deaths attributed to them. Conditions like pneumonia, chronic obstructive pulmonary disease (COPD), and lung cancer contribute to this high mortality rate. Early detection and accurate diagnosis are essential for reducing the burden of lung diseases on public health.\n\n**The Need to Detect Lung Diseases:**\n\n**Detecting lung diseases is vital for several reasons:**\n\nEarly Intervention: Early detection allows for timely medical intervention and treatment, increasing the chances of successful management and recovery.\n\n**Disease Classification:** Differentiating between various lung diseases, such as pneumonia, tuberculosis, and lung cancer, is crucial for appropriate treatment planning.\n\n**Public Health:** Effective disease detection and management can have a positive impact on public health by reducing the overall disease burden.\n\n**Lung X-Ray Image Dataset:**\n\nThe \"Lung X-Ray Image Dataset\" is a comprehensive collection of X-ray images that plays a pivotal role in the detection and diagnosis of lung diseases. This dataset contains a large number of high-quality X-ray images, meticulously collected from diverse sources, including hospitals, clinics, and healthcare institutions.\n\n**Dataset Contents:**\n\nTotal Number of Images: The dataset comprises a total of 3,475 X-ray images.\nClasses within the Dataset:\n\nNormal (1250 Images): These images represent healthy lung conditions, serving as a reference for comparison in diagnostic procedures.\n\nLung Opacity (1125 Images): This class includes X-ray images depicting various degrees of lung abnormalities, providing a diverse set of cases for analysis.\n\nViral Pneumonia (1100 Images): Images in this category are associated with viral pneumonia cases, contributing to the understanding and identification of this specific lung infection.\n\nIn conclusion, the \"Lung X-Ray Image Dataset\" plays a crucial role in the healthcare sector by providing a diverse and well-documented collection of X-ray images that support the detection, classification, and understanding of lung diseases. This resource is instrumental in advancing the field of respiratory medicine and improving patient outcomes.","294":"# Context\n\"A Song of Ice and Fire\" is an epic series of fantasy novels by George R.R. Martin. The series is set in the fictional continents of Westeros and Essos and chronicles the power struggles among noble families as they vie for control of the Iron Throne of the Seven Kingdoms. The series is renowned for its complex characters, intricate political plots, and the interweaving of multiple storylines.\n\nThe first book, \"A Game of Thrones,\" was published in 1996, and the series has since expanded to include five published novels with two more planned. The books have inspired the highly successful HBO television series \"Game of Thrones,\" which has brought the world of Westeros to a global audience.\n\n# Description of the Dataset\nThis dataset contains dialogues from the \"A Song of Ice and Fire\" book series. Each entry corresponds to a line of dialogue, along with metadata about the context in which the dialogue occurs. This dataset is valuable for those studying narrative structure, character interactions, and dialogue patterns in literature.\n\n##Column Descriptions\n- **Release Date:** The date when the book was released.\n- **Book:** The title of the book from which the dialogue is taken.\n- **Chapter Number:** The sequential number of the chapter within the book.\n- **Chapter Name:** The name of the chapter.\n- **Speaker(s):** The character(s) who is\/are speaking the line of dialogue.\n- **Dialogue:** The actual line of dialogue spoken by the character(s).\n- **Addressee(s):** The character(s) to whom the dialogue is directed.\n- **Listener(s):** The character(s) who hear the dialogue, regardless of whether it is directed to them.\n- **Mentioned Character(s):** Characters mentioned in the dialogue but not necessarily present.\n- **Present:** A boolean indicating whether the dialogue is in the present or in the past.\n\n# Acknowledgment\nThis dataset was created using a combination of artificial intelligence and human corrections to ensure accuracy and comprehensiveness. We acknowledge the effort and time invested by both AI and human editors in compiling this dataset.\n\n# Usage\nThe original work belongs to the author, George R.R. Martin. This dataset is provided for educational and research purposes only. No commercial use is allowed or will be conducted with this dataset. Users must adhere to these terms and respect the intellectual property rights of the original author.","295":"## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n","296":"","297":"### Description:\n\n**Percy Jackson Book Series NLP Dataset**\n\nThis dataset contains the raw text from the first five books of the Percy Jackson series by Rick Riordan. It provides a rich corpus for various Natural Language Processing (NLP) tasks and is ideal for researchers, educators, and developers interested in text analysis, sentiment analysis, named entity recognition, and more.\n\n**Contents:**\n- The Lightning Thief\n- The Sea of Monsters\n- The Titan's Curse\n- The Battle of the Labyrinth\n- The Last Olympian\n\n**Key Features:**\n- **Raw Text:** The dataset includes unprocessed text from the books, providing an authentic and comprehensive source for a variety of NLP experiments.\n- **Rich in Entities:** With a plethora of characters, places, and mythical references, this dataset is perfect for named entity recognition and entity linking tasks.\n- **Diverse Sentiments:** The narrative includes a wide range of emotional expressions, making it suitable for sentiment analysis projects.\n- **Complex Sentences:** The books contain varying sentence structures and vocabularies, beneficial for language modeling and text generation.\n\n**Potential Use Cases:**\n- Sentiment Analysis\n- Named Entity Recognition\n- Text Classification\n- Language Modeling\n- Text Generation\n- Tokenization and Lemmatization Practice\n\n**Notes:**\n- **Raw Data:** The text is presented as-is from the books, with no preprocessing performed. Users may need to clean and preprocess the text according to their specific project requirements.\n- **Ethical Use:** Please ensure the dataset is used in compliance with copyright laws and is intended for educational and research purposes only.\n\nUnlock the potential of Rick Riordan's engaging narratives to enhance your NLP models and algorithms!\n","298":"","299":"**Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases.","300":"**Context:**\nThis dataset was created through web scraping to compile a comprehensive collection of book information, including titles, prices, and star ratings. The purpose of creating this dataset was to analyze and utilize the book data available on the Books to Scrape website, which offers a wide range of books with various attributes that can be beneficial for data analysis and project development.\n**Source:**\nThis dataset was created by web scraping this website (https:\/\/books.toscrape.com). The source code used for web scraping can be found in this GitHub repository: https:\/\/github.com\/NoorFatimaAfzal\/web-scraping\n**Inspiration:**\nThis dataset can be used for a variety of projects, including but not limited to:\n\nData Analysis and Visualization: Analyzing trends in book prices, and ratings. Creating visual representations of the data to identify patterns and insights.\nRecommendation Systems: Building a book recommendation system based on star ratings and other book attributes.\nInventory Management: Developing a system to manage and track book inventory, helping bookstores optimize their stock levels.\nPrice Comparison Tools: Creating tools to compare book prices across different platforms or over time.\nMachine Learning Projects: Using the dataset to train models for predicting book ratings or prices based on other attributes.","301":"The main dataset  on which all the operations are performed are **books.tsv** containing around 406000+ rows. The file contains two columns, **bookurl** and **data** column. Rest of the csv files are extracted files after running the **jupyter** **code**(book0-20.csv, nbook0-20.csv and book_details.csv with pkl files like tags, simi, tfidf.\nWe have built an **UI** on **Streamlit** with a file name **App1.py** and all the subsidary files required for UI are given in dataset section.\n ","302":"This is a database taken from the open library api. It can be used to perform different types of analysis such as: visual analysis; recommendation analysis; machine learning; among others. \n\nThe initial objective of taking this data is to use it in the application I am developing called LitRecs (literature recommendations). Now that I have obtained this data, and given that it is something initial, I have decided to share because it is very difficult to find databases related to books that are complete in information.","303":"","304":"","305":"##Dataset Description:\n* The dataset represents retail transactional data. It contains information about customers, their purchases, products, and transaction details. The data includes various attributes such as customer ID, name, email, phone, address, city, state, zipcode, country, age, gender, income, customer segment, last purchase date, total purchases, amount spent, product category, product brand, product type, feedback, shipping method, payment method, and order status.\n\n##Key Points:\n##Customer Information:\n* Includes customer details like ID, name, email, phone, address, city, state, zipcode, country, age, and gender.\nCustomer segments are categorized into Premium, Regular, and New.\n##Transaction Details:\n* Transaction-specific data such as transaction ID, last purchase date, total purchases, amount spent, total purchase amount, feedback, shipping method, payment method, and order status.\n##Product Information:\n* Contains product-related details such as product category, brand, and type.\nProducts are categorized into electronics, clothing, grocery, books, and home decor.\n##Geographic Information:\n* Contains location details including city, state, and country.\nAvailable for various countries including USA, UK, Canada, Australia, and Germany.\n##Temporal Information:\n* Last purchase date is provided along with separate columns for year, month, date, and time.\nAllows analysis based on temporal patterns and trends.\n##Data Quality:\n* Some rows contain null values, and others are duplicates, which may need to be handled during data preprocessing.\nNull values are randomly distributed across rows.\nDuplicate rows are available at different parts of the dataset.\n##Potential Analysis:\n* Customer segmentation analysis based on demographics, purchase behavior, and feedback.\nSales trend analysis over time to identify peak seasons or trends.\nProduct performance analysis to determine popular categories, brands, or types.\nGeographic analysis to understand regional preferences and trends.\nPayment and shipping method analysis to optimize services.\nCustomer satisfaction analysis based on feedback and order status.\n##Data Preprocessing:\n* Handling null values and duplicates.\nParsing and formatting temporal data.\nEncoding categorical variables.\nScaling numerical variables if required.\nSplitting data into training and testing sets for modeling.","306":"This describes creating an inscription image database for the ancient Marathi inscription image enhancement system. The database was created from inscription images of carved ancient Marathi text. Thus, the database provides the basis for developing practical Marathi inscription enhancement systems. It provides a standard benchmark for comparing different algorithms for Ancient Marathi Inscription Image text and helps in the research and development of ancient inscription image enhancement and recognition systems. The database contains more than 1000 sample inscription images with different scales. It is consisting of more than 10,000 characters from ancient times. Ancient Marathi Inscription Database servers like MNIST are a dataset for handwritten inscription image enhancement and recognition for Marathi carved characters. Augmentation techniques play a pivotal role in enhancing image processing models' performance and generalization capabilities. \nData Collection\nData collection is a significant step in constructing the database. Various methods are used to collect data from different sources:\n1. Utilizing research papers, archaeological books, and relevant materials.\n2. Consulting with archaeological experts.\n3. Conducting field visits and making observations.\nResearch papers, Archaeology related Books and Material:\n Both quantitative and qualitative data are collected from various archaeological books, including \"Kumbhar Anand Sanshodhan Tarang, Navbharat Prakashan, Mumbai\", \"Madhugin Itihaasaachi Sadhne Bharat Itihas Sanshodhak Mandal Khand 2\", \"Prachin Marathi Koriv Lekh, Pune Vidyapeeth Prakashan\", \"Punashya Pandharpur Shilalekh Mahathi Sahitya Patrika Ank\", and \"Maharashtra va Goa Shilalekh: Tamrapatachi Varnanatmak Suchi Mumbai.\"\nFrom these books, the following activities are conducted:\n1. Collection of Marathi inscription images.\n2. Compilation of details about Marathi inscriptions.\n3. Retrieval of information providing various ancient Marathi inscription sites.\n4. Gathering information on the location and specifics of ancient Marathi inscriptions, among other relevant details.\n Archaeology Experts:\nEngaging in communication with experts in the field of archaeology is a crucial approach to gathering data. These experts possess deep knowledge of ancient inscriptions and various ancient languages. Discussions with these experts help collect images and analyze image data. They also consulted with various archaeology Professors to find inscription reading methods, ancient Marathi characters, writing styles, inscription site locations, and relevant literature. During these discussions with experts, variations in each character were identified, which were documented for future research work. \nField Visits and Observations:\nObtaining reliable evidence through field visits and observation is very useful. The authors conducted field visits to Tuljapur, Pandharpur, Nashik, and other locations in the Solapur district, capturing original photos of relevant inscriptions. Additionally, they engaged with various tourists to gather data.  A DSLR camera was utilized to capture photos of inscriptions. The authors noted that identifying suitable inscriptions was challenging throughout these visits.\nInscription image selection:\n            In the preliminary investigation, an experimental study involved collecting and analyzing three distinct types of inscription images. These types include stone, document, and metal plate inscriptions. The inscription font, style, and size are not standard. However, a document uses standard font, style, and size. In stone inscriptions, characters are carved by different individuals, leading to font, style, and character variations. Binarization of documents proves more suitable for enhancing document images. Initially, enhancement based on binarization is applied to all three types of inscriptions, and various metrics are used to check accuracy. The findings reveal that the accuracy of stone inscription images is lower than the other types. For this research, stone inscription images are specifically chosen.\nImages are captured by using a digital camera NIKON D7200 (Focal Length -140 mm, Horizontal and vertical resolution -300*300 dpi, Bit depth-24, Max Aperture- 5, Metering mode- Pattern, Flash Mode \u2013 No flash)\n","307":"Texts related to Ayurveda in English scraped from 21 Books and 2000+ Articles from the internet.\n\nCan be used for finetuning or RAG applications.\n\nThe dataset contains two directories:\nayurveda_books contains book texts and pdfs both\nayurveda_texts contains only texts","308":"# Project Geutenberg Books\n\n1. The Life And Adventures of Robinson Crusoe\n1. Frankenstein, Or The Modern Prometheus\n\nI wanted to begin work towards literature as data. Thus, I shall begin adding books as rapidly as possible to this dataset.\n\n\n**Original Credit and License:**\n```\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n```","309":"","310":"The vast growth of social media platforms, online news outlets, and digital communication has increased user-generated content exponentially in recent years. This unprecedented surge in online discourse has sparked an urgent need to develop automated tools and techniques to effectively analyze the opinions and attitudes expressed within these expansive streams of text. Stance detection, a critical task within the field of Natural Language Processing (NLP), aims to identify the position or perspective of a writer towards a specific topic or entity by analyzing their written text and\/or social media activity, such as preferences and connections. The applications of stance detection are diverse and encompass domains such as politics, marketing, and social media analysis.\n\n## **Classes**\nThe possible stance labels are:\n\n**FAVOR** means that we can infer from the post that the author supports the target (e.g., explicitly supporting the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is in favor of the target).\n\n**AGAINST** means that we can infer from the tweet that the author is against the target (e.g., explicitly opposing the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is against the target).\n\n**NONE** means that the tweet provides no hint as to the author's stance toward the target (e.g., there is no evidence in the tweet to judge the author's stance, such as inquiries, or news that does not express any positive or negative position).\n\n# **Dataset**\nMawqif comprises 4,121 entries distributed across \"COVID-19 vaccine\" (1,373 entries), \"digital transformation\" (1,348 entries), and \"women empowerment\" (1,400 entries). \n\nIt is structured as a multi-label dataset with labels including  stance (Favor, Against, None), sentiment (Positive, Negative, Neutral), and sarcasm (Sarcastic and Non-sarcastic).","311":"","312":"","313":"","314":"# YouTube Comments Sentiment Analysis Dataset\n\n###Description\nThis dataset comprises a collection of YouTube comments extracted from videos of a specific channels, \"BidonWaraq\", \"mmr_sa1\", \"POWR-Esports\" and \"thmanyahPodcasts\". It provides a rich source of data for text analysis, sentiment analysis, sarcasm detection, and speech act recognition tasks. Each comment is annotated with several features, making it suitable for a variety of natural language processing (NLP) applications.\n\n### Dataset Content\n1. **title:** The title of the YouTube video.\n2. **comment:** The text of the comment posted on the YouTube video.\n3. **video_id:** The unique identifier for the YouTube video.\n4. **channel_id:** The unique identifier for the YouTube channel.\n5. **sentiment:** The sentiment of the comment (e.g., Positive, Neutral, Negative).\n6. **sarcasm:** A boolean indicating whether the comment is sarcastic.\n7. **speech_act:** The type of speech act (e.g., Assertion, Expression).\n8. **dangerous:** A boolean indicating whether the comment contains dangerous content.\n9. **sentiment_reasoning:** Explanation for the sentiment annotation.\n10. **sarcasm_reasoning:** Explanation for the sarcasm annotation.\n11. **speech_act_reasoning:** Explanation for the speech act annotation.\n12. **channel_name:** The name of the YouTube channel.\n\n### Potential Uses\nThis dataset can be used for:\n\nSentiment analysis to understand the general sentiment of comments towards the video content.\nSarcasm detection to identify sarcastic remarks within the comments.\nSpeech act recognition to categorize comments according to their communicative intent.\nDangerous content detection to filter out or study comments that may contain harmful content.\n\n### Challenges\nThe dataset poses several challenges including but not limited to:\n\nDealing with multilingual text, as comments might be in different languages.\nUnderstanding context for accurate sentiment and sarcasm detection.\nManaging imbalanced classes, especially for less common annotations like sarcasm or dangerous content.\n\n### Acknowledgments\nThis dataset has been collected and annotated by **Khalaya**, aiming to facilitate research and development in natural language processing and machine learning fields.","315":"Titre du Projet&nbsp;: La d\u00e9tection du sarcasme en arabe\nObjectifs du Projet :\nLa d\u00e9tection du sarcasme est le processus visant \u00e0 d\u00e9terminer si un morceau de texte est sarcastique ou non. Le sarcasme repr\u00e9sente l'un des principaux d\u00e9fis pour les syst\u00e8mes d'analyse de sentiment. La raison en est qu'une phrase sarcastique porte g\u00e9n\u00e9ralement un sentiment implicite n\u00e9gatif, bien qu'il soit exprim\u00e9 \u00e0 l'aide d'expressions positives. Cette contradiction entre le sentiment apparent et celui intentionnel cr\u00e9e un d\u00e9fi complexe pour les syst\u00e8mes d'analyse de sentiment.\nLa d\u00e9tection du sarcasme a attir\u00e9 l'attention dans d'autres langues, mais l'arabe est encore en retard \u00e0 cet \u00e9gard. Il y a eu peu d'efforts pour la d\u00e9tection du sarcasme en arabe, tels que les travaux de (Karoui et al., 2017 ; Ghanem et al., 2020) et la t\u00e2che partag\u00e9e organis\u00e9e par (Ghanem et al., 2019). Des efforts r\u00e9cents ont \u00e9t\u00e9 d\u00e9ploy\u00e9s pour construire des ensembles de donn\u00e9es standard pour cette t\u00e2che, tels que (Abbes et al., 2020 ; Abu Farha et Magdy, 2020). La t\u00e2che partag\u00e9e sur la d\u00e9tection du sarcasme et du sentiment en arabe se tiendra avec WANLP@EACL2021. Cette t\u00e2che partag\u00e9e se concentrera sur l'analyse des tweets et l'identification de leur sentiment ainsi que la d\u00e9termination de leur caract\u00e8re sarcastique ou non.\nM\u00e9thodologie&nbsp;:\n1. Collecte de Donn\u00e9es : Collecter des donn\u00e9es \u00e0 partir de diff\u00e9rentes plateformes de m\u00e9dias sociaux, en se concentrant sur des sujets sp\u00e9cifiques pertinents.\n2. Pr\u00e9traitement des Donn\u00e9es : Nettoyer et pr\u00e9traiter les donn\u00e9es textuelles pour les rendre compatibles avec les mod\u00e8les de Machine Learning.\n3. Construction d'un Mod\u00e8le de Machine Learning : Mettre en \u0153uvre un mod\u00e8le de Machine Learning adapt\u00e9 \u00e0 l'analyse de sentiments, tel qu'un mod\u00e8le de classification.\n4. Entra\u00eenement du Mod\u00e8le : Entra\u00eener le mod\u00e8le sur un ensemble de donn\u00e9es annot\u00e9 pour la classification des sentiments.\n5. \u00c9valuation du Mod\u00e8le : \u00c9valuer les performances du mod\u00e8le sur un ensemble de test et ajuster les hyper param\u00e8tres si n\u00e9cessaire.\nLivrables Attendus :\n1. Rapport Technique : Document d\u00e9taill\u00e9 d\u00e9crivant chaque \u00e9tape du projet, y compris les choix de conception et les r\u00e9sultats obtenus.\n2. Code Source : Ensemble de scripts, modules, et fichiers n\u00e9cessaires \u00e0 la reproduction du projet.\nCalendrier Pr\u00e9visionnel :\nSemaines 0 : T\u00e9l\u00e9chargement des donn\u00e9es.\n Semaines 1-2 : Uploader les donn\u00e9es T\u00e9l\u00e9charg\u00e9es puis effectuer les pr\u00e9traitements n\u00e9cessaires suivant l\u2019application\n2. Semaines 3-4 : Construction du mod\u00e8le et entra\u00eenement initial.\n3. Semaines 3-4 : \u00c9valuation du mod\u00e8le et ajustements.\n4. Semaines 5-6 : D\u00e9veloppement de l'interface utilisateur.\n5. Semaines 9-10 : Finalisation, r\u00e9daction du rapport, et le d\u00e9p\u00f4t de rapport.\nCrit\u00e8res d'\u00c9valuation :\nLe projet sera \u00e9valu\u00e9 en fonction de la qualit\u00e9 du mod\u00e8le de Machine Learning, de la pr\u00e9cision de l'analyse de sentiments, de la convivialit\u00e9 de l'interface utilisateur, et de la clart\u00e9 du rapport final.\nBaseline_score=33.71\n","316":"","317":"","318":"","319":"ArSarcasm-v2 is provided in a CSV format, we provide the same split that was used for the shared task. The training set contains 12,548 tweets, while the test set contains 3,000 tweets.\n\nThe dataset contains the following fields:\n\n- tweet: the original tweet text.\n- sarcasm: boolean that indicates whether a tweet is sarcastic or not.\n- sentiment: the sentiment of the tweet (positive, negative, neutral).\n- dialect: the dialect used in the tweet, we used the 5 main regions in the Arab world, follows the labels and their - meanings:\n- msa: modern standard Arabic.\n- egypt: the dialect of Egypt and Sudan.\n- levant: the Levantine dialect including Palestine, Jordan, Syria and Lebanon.\n- gulf: the Gulf countries including Saudi Arabia, UAE, Qatar, Bahrain, Yemen, Oman, Iraq and Kuwait.\n- magreb: the North African Arab countries including Algeria, Libya, Tunisia and Morocco.\n\nSource: https:\/\/github.com\/iabufarha\/ArSarcasm-v2\n\n@inproceedings{abufarha-etal-2021-arsarcasm-v2,\ntitle = \"Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic\",\n    author = \"Abu Farha, Ibrahim  and\n    Zaghouani, Wajdi  and\n    Magdy, Walid\",\n    booktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\n    month = april,\n    year = \"2021\",\n    }\n","320":"","321":"This dataset is part of the research \"Towards Computational Sarcastic Tweets identification: An open-Source Dataset and Developmental Framework\"\n\nIt contains 19955 rows and columns (features), that are, 'urdu_tweets' & 'is_sarcastic' (labels with 0 and 1, where 1 denotes sarcastic tweet ). This dataset can be used for sentiment analysis, specifically for sarcasm detection in urdu tweets.  ","322":"# Table of Contents\n\n[Objective](#1)\n\n[Title](#2)\n\n[Description](#3)\n\n[Team Members](#4)\n\n[Websites](#5)\n\n[Pandas Read](#6)\n\n[Notes](#7)\n\n[Egyptian Datasets Description](#8)\n\n- [Format](#9)\n\n- [1.Arabic Online Commentary (AOC)](#10)\n\n- [2.Arabic Egyptian Tweets](#11)\n\n- [3.TaghreedT](#12)\n\n- [4.Topic Extraction Data](#13)\n\n- [5.Habibi Lyrics Corpus](#14)\n\n- [6.Arabic Political Tweets](#15)\n\n- [7.ArabicReddit](#16)\n\n- [8.ar_arz_wiki_corpus](#17)\n\n- [9.QCRI](#18)\n\n- [10.SADID Benchmark Dataset](#19)\n\n- [11.DART](#20)\n\n- [12.Callhome Corpus](#21)\n\n[Other 40+ Arabic Datasets Links](#22)\n\n- [Contact](#23)\n\n- [Papers](#24)\n\n- [License](#25)\n\n# <a name=\"1\">Objective<\/a>\n\nThe datasets were part of our AI & Data Science Master's graduation project sponsored by Microsoft.\n\nCheck all the projects here: [Link](https:\/\/github.com\/Mostafanofal453)\n\n# <a name=\"2\">Title<\/a>\n\nAutomating novel terms and usage detection in Egyptian Arabic dialect\n\n# <a name=\"3\">Description<\/a>\n\nThe project aims at identifying unfamiliar terms relatively new in the Egyptian Arabic dialect. In addition to words that their meaning was changed over time with another context or misspelled to enhance the translation corpus.\n\n# <a name=\"4\">Team Members<\/a>\n\n[Khaled Elsaka](https:\/\/www.linkedin.com\/in\/khaled-el-saka-962700161\/)\n\n[Nada Montasser](https:\/\/www.linkedin.com\/in\/nada-montasser-014b1616b\/)\n\n[Shaimaa Mamdouh](https:\/\/www.linkedin.com\/in\/shaimaa-mamdouh-2616a9159\/)\n\n[Aya Reda](https:\/\/www.linkedin.com\/in\/aya-reda-5202b2191\/)\n\n[Mostafa Nofal](https:\/\/www.linkedin.com\/in\/mostafa-nofal-772625194\/)\n\n---\n\n# <a name=\"5\">Websites<\/a>\n\n We searched for the datasets in the related work papers and the most well-known websites like:\n\n\n* Kaggle\n\n* Github\n\n* Huggingface\n\n* Google Datasets\n\n* ScienceDirect\n\n* ResearchGate\n\n* IEEE Xplore\n\nThe datasets had different formats like XML, JSON, CSV and text so we unified them all to excel files format.\n\n\n\nWe also worked on labeling the datasets with the year label and filtered them to combine our final output.\n\n---\n\n# <a name=\"6\">Pandas Read<\/a>\n\n`import pandas as pd`\n\n`data = pd.read_excel(\"Name.xlsx\")`\n\n# <a name=\"7\">Notes<\/a>\n\n\u2022 All Datasets have 3 columns (Text, Year, Source)\n\n\u2022 Maximum Rows for excel file is 1,048,576 rows\n\n\u2022 If Arabic is encoded in excel:\n           - Create new excel file\n           \n           - In the Data tab, click on (From Text\/CSV) button\n           \n           - Browse and select the excel file\n           \n           - Change the File_origin to \"Unicode (UTF-8)\"\n           \n           - Save\n\n---\n\n# <a name=\"8\">Egyptian Datasets Description<\/a>\n\n## <a name=\"9\">Format<\/a>\n\nEvery dataset has the following format\n\n\u2022 Source Name\n\n\u2022 Year\n\n\u2022 Information\n\n\u2022 Rows Number\n\n\u2022 File Name\n\n---\n\n## <a name=\"10\">1.Arabic Online Commentary (AOC)<\/a>\n\n2010\n\nOnline version of Arabic newspaper: Al-Youm Al-Sabe\u2019.\n\n688,550 rows AOC_youm7_articles (ar)\n\n1,048,576 rows AOC_youm7_comments\n\n333,225 rows RestOf_AOC_youm7_comments\n\n564,854 rows AOC_subtitles_authors\n\n---\n\n## <a name=\"11\">2.Arabic Egyptian Tweets<\/a>\n\n2019\n\nEgyptian tweets cover a blend of different general topics (sentiment analysis)\n\n40,000 rows\n\nEgyptian Tweets\n\n---\n\n## <a name=\"12\">3.TaghreedT<\/a>\n\n2021\n\nEgyptian Dialect Corpus (EDC) from Facebook\n\n13,740 rows\n\nTaghreedT\n\n---\n\n## <a name=\"13\">4.Topic Extraction Data<\/a>\n\n2019\n\nEgyptian dialect tweets used for topic extraction and topic modelling research\n\n2,256 rows TE_News\n\n2,052 rows TE_Sports\n\n2,358 rows TE_Telecom\n\n2,065 rows TE_Tweets\n\n---\n\n## <a name=\"14\">5.Habibi Lyrics Corpus<\/a>\n\n2019\n\nArabic Song Lyrics corpus\n\n139,162 rows\n\nHabibi\n\n---\n\n## <a name=\"15\">6.Arabic Political Tweets<\/a>\n\n2019\n\nTwitter hashtag\n\n431,452 rows\n\nPolitical Tweets\n\n---\n\n## <a name=\"16\">7.ArabicReddit<\/a>\n\n2021\n\nArabic dataset of Reddit titles and comments from Arab and Egypt subreddits\n\n10,129 rows\n\nReddit\n\n---\n\n## <a name=\"17\">8.ar_arz_wiki_corpus<\/a>\n\n2017\n\nArabic (Modern Standard) and Egyptian Arabic dialect comparable documents from Wikipedia\n\n9126 rows\n\nArabic_Egyptian_Wikipedia (ar)\n\n---\n\n## <a name=\"18\">9.QCRI<\/a>\n\n2018\n\nManually segmented and POS tagged tweets\n\n350 rows\n\nQCRI\n\n---\n\n## <a name=\"19\">10.SADID Benchmark Dataset<\/a>\n\n2020\n\nSADID Evaluation Datasets for Low-Resource Spoken Language Machine Translation of Arabic Dialects\n\n8,989 rows\n\nSADID\n\n---\n\n## <a name=\"20\">11.DART<\/a>\n\n2018\n\nA Large Dataset of Dialectal Arabic Tweets (c) 2018 Qatar University\n\n5,889 rows\n\nDART\n\n---\n\n## <a name=\"21\">12.Callhome Corpus<\/a>\n\n2014\n\nEgyptian Arabic Speech Translation Corpus\n\n9,637 rows\n\nCallhome\n\n---\n\n# <a name=\"22\">Other 40+ Arabic Datasets Links<\/a>\n\n## <a name=\"23\">Contact<\/a>\n\nTEAD:\n\nTwo million  Egyptian tweets for sentiment analysis\n\nhttps:\/\/github.com\/HSMAabdellaoui\/TEAD\n\n\nQADI\n\n67,783 egyptian tweets\n\nhttps:\/\/alt.qcri.org\/resources\/qadi\n\n\n2016\n\n5963 words\n\nEgyptian Arabic and Modern Standard Arabic sentiment words and their polarity\n\nhttps:\/\/github.com\/NileTMRG\/NileULex\n\n\nEgyptian-Dialect-Gender-Annotated-Dataset\n\nhttps:\/\/github.com\/shery91\/Egyptian-Dialect-Gender-Annotated-Dataset\n\n\n2020\n\nar_cov19\n\n1M tweets of COVID-19 pandemic include both retweets and conversational threads\n\nhttps:\/\/huggingface.co\/datasets\/ar_cov19\n\n\nArabic Poetry\n\n 55K Arabic poem from different Categories with poets from different countries and era\n\nhttps:\/\/www.kaggle.com\/datasets\/ahmedabelal\/arabic-poetry\n\n\n2016\n\n93,700 hotel reviews in Modern Standard Arabic as well as dialectal Arabic from Booking.com\n\nhttps:\/\/huggingface.co\/datasets\/hard\n\n\n2015\n\n8364 restaurant reviews from qaym.com in Arabic for sentiment analysis\n\nhttps:\/\/huggingface.co\/datasets\/ar_res_reviews\n\n\n2019\n\nArabic Flood Twitter Dataset\n\nhttps:\/\/github.com\/alaa-a-a\/Arabic-Twitter-Corpus-for-Flood-Detection\n\n\n2021\n\nArabizi transliteration\n\nhttps:\/\/github.com\/bashartalafha\/Arabizi-Transliteration\n\n\n2018\n\n15,050 labelled YouTube comments in Arabic\n\nAnti-Social Behaviour in Online Communication\n\nhttps:\/\/onedrive.live.com\/?authkey=!ACDXj_ZNcZPqzy0&id=6EF6951FBF8217F9!105&cid=6EF6951FBF8217F9\n\n\narabic-hatespeech-data\n\nhttps:\/\/github.com\/motazsaad\/arabic-hatespeech-data\/blob\/master\/OSACT4\/README.md\n\n\nArabic Offensive Comments dataset from Multiple Social Media Platforms\n\nhttps:\/\/github.com\/shammur\/Arabic-Offensive-Multi-Platform-SocialMedia-Comment-Dataset\n\n\n2017\n\n75 million of fully vocalized words mainly 97 books from classical and modern Arabic language\n\nhttps:\/\/huggingface.co\/datasets\/tashkeela\n\n\nNADiA:\n\nNews Articles Dataset in Arabic for Multi-Label Text Categorization\nSkyNewsArabia will be referred to as NADiA1, while the latter would be NADiA2. NADiA1 is a big dataset containing 37,445 files, while NADiA2 is a huge dataset that contains 678,563 files. However, after filtering and cleaning we reduced the numbers to 35,416 and 451,230 for NADiA 1 and 2, respectively.\nNews, North Africa, Levant, Middle East, The Americas, Research, Finance & Economy, War & Terrorism, Gulf, Europe, Political Figures, Iran, Technology, Russia, Sports, Tennis, Football, English League, Arabian Sports, Spanish League, Health, East Asia, Environment, Other Countries\n\nhttps:\/\/data.mendeley.com\/datasets\/hhrb7phdyx\/1\n\n\nArabic dialect dictionary  (Django)\nArabic to English or English to Arabic definitions and see results in Levantine, Gulf, or Egyptian\nhttps:\/\/github.com\/moraesc\/arabic-dialect-dict\n\n\n2018\n\nArabic POS Dialect\n\n350 manually segmented and POS tagged tweets for each of four dialects: Egyptian, Levantine, Gulf, and Maghrebi\n\nhttps:\/\/huggingface.co\/datasets\/arabic_pos_dialect\n\n\n2017\n\n10,547 tweets, 1,682 (16%) of which are sarcastic\nArabic sentiment analysis datasets (SemEval 2017 and ASTD) and adds sarcasm and dialect labels to them\n\n{ \"Name\": \"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"2,383\", \"Unit\": \"sentences\" }  and others\n\nhttps:\/\/huggingface.co\/datasets\/ar_sarcasm\n\n\n2021\n\n50K posts\n\nSentiment Analysis for social media posts in Arabic dialect\n\n{ \"Name\":\"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"7,519\", \"Unit\": \"sentences\" }\n\nhttps:\/\/msda.um6p.ma\/msda_datasets\n\n\n2018\n\n { \"Name\": \"EGY\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"4,061\", \"Unit\": \"sentences\" }\n\nhttps:\/\/www.lancaster.ac.uk\/staff\/elhaj\/corpora.html\n\n\n2020 (Register)\n\n{ \"Name\": \"Egypt\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"6,635\", \"Unit\": \"sentences\" }\n\nhttps:\/\/sites.google.com\/view\/nadi-shared-task\n\n\n2015\n\nASTD: Arabic Sentiment Tweets Dataset\n\n10k Arabic sentiment tweets classified into four classes subjective positive, subjective negative, subjective mixed, and objective \n\nhttps:\/\/github.com\/mahmoudnabil\/ASTD\n\n\n2013\n\nA Large Scale Arabic Book Reviews Dataset\n\n63,000 book reviews in Arabic\n\nhttps:\/\/huggingface.co\/datasets\/labr\n\n\n2017\n\nEmotional-Tone\n\n10065 tweets in Arabic for Emotion detection in Arabic text\n\nhttps:\/\/huggingface.co\/datasets\/emotone_ar\n\n\n2016\n\nBRAD: Books Reviews in Arabic Dataset\n\n510,600 book reviews in Arabic language\n\nhttps:\/\/github.com\/elnagara\/BRAD-Arabic-Dataset\n\n\nDialectal system \n\nhttps:\/\/mt.qcri.org\/api\n\nhttps:\/\/alt.qcri.org\/resources1\/mt\/arabench\n\n2020\n\nCOVID-19-Arabic-Tweets-Dataset\n\n6 milllion\n\nhttps:\/\/github.com\/SarahAlqurashi\/COVID-19-Arabic-Tweets-Dataset\n\n\n2014\n\neducational video subtitles\n\nMachine learning translation\n\nEnglish and arabic\n\nhttps:\/\/huggingface.co\/datasets\/qed_amara\n\n\n2020\n\nQCRI Parallel Tweets\n\nhttps:\/\/huggingface.co\/datasets\/tweets_ar_en_parallel\n\n\n2015\n\nArabic News Article Classification\n\n14 million words with 15,891,729 tokens contained in 57,827 different articles\n\nhttps:\/\/github.com\/saidziani\/Arabic-News-Article-Classification\n\n\n2015\n\nLarge Multi-Domain Resources for Arabic Sentiment Analysis\n\nhttps:\/\/github.com\/hadyelsahar\/large-arabic-sentiment-analysis-resouces\n\n## <a name=\"24\">Papers <\/a>\n\nFreely Available Arabic Corpora: A Scoping Review  (various)\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666990022000015#bib0039\n\n\ncombination of subsets of five corpora: DART, SHAMI, TSAC, PADIC and AOC\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340921010519\n\nNileULex: A Phrase and Word Level Sentiment Lexicon for Egyptian and Modern Standard Arabic\n\nhttps:\/\/aclanthology.org\/L16-1463.pdf\n\nDART: A Large Dataset of Dialectal Arabic Tweets\nL18-1579.pdf\n\nhttps:\/\/aclanthology.org\/L18-1579.pdf\n\n\nColloquial Arabic Tweets: Collection, Automatic Annotation, and Classification \n\nhttps:\/\/ieeexplore.ieee.org\/abstract\/document\/9310507\n\n\nA Multidialectal Parallel Corpus of Arabic\n\nhttps:\/\/aclanthology.org\/L14-1435\/#:~:text=The%20daily%20spoken%20variety%20of,within%20other%20Arabic%20speaking%20communities\n\n\nAutomatic Building of Arabic Multi Dialect Text Corpora by Bootstrapping Dialect Words\n\nhttps:\/\/www.researchgate.net\/publication\/261489194_Automatic_building_of_Arabic_multi_dialect_text_corpora_by_bootstrapping_dialect_words\n\n## <a name=\"25\">License<\/a>\n\n2021\n\n10,828 Arabic tweets annotated with 10 different labels\nmulti-label Arabic COVID-19 fake news and hate speech detection dataset\n\nhttps:\/\/github.com\/MohamedHadjAmeur\/AraCOVID19-MFH\n\n\n2016\n\nArabic Web16\n\n150,211,934 Arabic Web pages with high coverage of dialectal Arabic\n\nhttps:\/\/sites.google.com\/view\/arabicweb16\/\n\n\nCamel Resources\n\nhttps:\/\/docs.google.com\/forms\/d\/e1FAIpQLSfQqhxslVSkBN5ScQ2bvvM0xUVCUnjXxtvkAjupvxm3SSeZGw\/viewform\n\nhttps:\/\/camel.abudhabi.nyu.edu\/madar-parallel-corpus\/\n\n\nBOLT Egyptian Arabic PropBank and Sense -- Discussion Forum, SMS\/Chat, and Conversational Telephone Speech\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/YS81IR\n\n\nBOLT Egyptian-English Word Alignment -- Discussion Forum Training\n400,448 words of Egyptian Arabic and English parallel text enhanced with linguistic tags to indicate word relations\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/AR1QCS\n\n\nBOLT Egyptian Arabic SMS\/Chat Parallel Training Data\n723,000 tokens of Egyptian Arabic SMS\/Chat data collected for the DARPA BOLT program along with their corresponding English translations.\n\nhttps:\/\/datasetsearch.research.google.com\/search?src=0&query=egyptian%20dialect&docid=L2cvMTFwNjZfMXRieg%3D%3D\n","323":"This is MUStARD (Multimodal Sarcasm Detection Dataset).\nThese are sarcasm data taken from various famous TV Series.\n\nYou can find paper attached with dataset here:\nhttps:\/\/github.com\/soujanyaporia\/MUStARD","324":"This dataset contains a number of tweets classified into sarcastic and further into sub-categories of sarcastic.\n\nIt is very helpful for learning purposes if you are new to NLP you can use this dataset to train and test your models.","325":"### Context\n\nWe present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https:\/\/www.politifact.com\/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n\nCover photo credits: https:\/\/blog.condati.com\/5-claims-retail-marketing-analytics-fact-check\n\n### Content\nEach record consists of 8 attributes:\n\n* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n* ```statement_originator```: the person who made the statement being fact checked\n* ```statement```:  statement being fact checked\n* ```statement_date```:  the date when statement being fact checked was made\n* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n* ```factchecker```: name of the person who fact checked the claim\n* ```factcheck_date```:  date when the fact checked article was published\n* ```factcheck_analysis_link```:  link to the fact checked analysis article\n\nThe cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https:\/\/gist.github.com\/rishabhmisra\/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n\n### Citation\n\nIf you're using this dataset for your work, please cite the following articles:\n\nCitation in text format:\n```\n1. Misra, Rishabh and Jigyasa Grover. \"Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140\/RG.2.2.29923.22566 (2022).\n```\n\nCitation in BibTex format:\n```\n@incollection{misra2022not,\n  title={Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n  author={Misra, Rishabh and Grover, Jigyasa},\n  booktitle={Deep Learning for Social Media Data Analytics},\n  pages={213--235},\n  year={2022},\n  publisher={Springer}\n}\n@dataset{misra2022politifact,\n  author = {Misra, Rishabh},\n  year = {2022},\n  month = {09},\n  pages = {},\n  title = {Politifact Fact Check Dataset},\n  doi = {10.13140\/RG.2.2.29923.22566}\n}\n```\n\nPlease link to [rishabhmisra.github.io\/publications](https:\/\/rishabhmisra.github.io\/publications\/) as the source of this dataset. Thanks!\n\n### Acknowledgements\n\nThis dataset was collected from [PolitiFact](https:\/\/www.politifact.com\/). \n\n### Inspiration\n\n* Can you categorize facts as true or false?  \n\n* Does the sources of false facts have a temporal pattern?\n\n* Is there a linguistic pattern in false facts?\n\n### Want to contribute your own datasets?\n\nIf you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https:\/\/www.amazon.com\/dp\/B08RN47C5T).\n\n### Other datasets\nPlease also checkout the following datasets collected by me:\n\n* [News Headlines Dataset For Sarcasm Detection](https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection)\n\n* [News Category Dataset](https:\/\/www.kaggle.com\/rmisra\/news-category-dataset)\n\n* [Clothing Fit Dataset for Size Recommendation](https:\/\/www.kaggle.com\/rmisra\/clothing-fit-dataset-for-size-recommendation)\n\n* [IMDB Spoiler Dataset](https:\/\/www.kaggle.com\/rmisra\/imdb-spoiler-dataset)\n","326":"## Data Article: [BanglaSarc: A Dataset for Sarcasm Detection](https:\/\/arxiv.org\/abs\/2209.13461)\n## Implementation Article: [Interpretable Bangla Sarcasm Detection using BERT and Explainable AI](https:\/\/arxiv.org\/abs\/2303.12772)\n\n## Implementation: [Sarcasm Detection](https:\/\/github.com\/Sakibapon\/SarcasmDetection)\n\n#### **Citation** \n@article{apon2022banglasarc,\n  title={BanglaSarc: A Dataset for Sarcasm Detection},\n  author={Apon, Tasnim Sakib and Anan, Ramisa and Modhu, Elizabeth Antora and Suter, Arjun and Sneha, Ifrit Jamal and Alam, MD},\n  journal={arXiv preprint arXiv:2209.13461},\n  year={2022}\n}","327":"","328":"","329":"","330":"Three classes of lesions:\n\n- Malignant Melanoma: Melanoma, also known as malignant melanoma, is the most common type of skin cancer and arises from pigment-producing cells known as melanocytes. Melanomas typically appear on the skin and rarely in other locations such as the mouth, intestines, or eye.\n\n- Seborrheic Keratosis: Seborrheic keratosis is a non-cancerous (benign) skin tumor that originates in the cells of the outer layer of the skin (keratinocytes), making it a non-melanocytic lesion.\n\n- Benign Nevus (Mole): A benign skin tumor originating from melanocytes (melanocytic).\n\n# Dataset Description\nThe dataset has been obtained from the 'International Skin Imaging Collaboration' (ISIC) archive. It contains 2750 images divided into 3 sets:\n\nTraining Set: 2000 images\n\nValidation Set: 150 images\n\nTest Set: 600 images\n\nFor each clinical case, we have two images available:\n\nDermoscopic image of the lesion (in the 'images' folder).\n\nBinary mask with segmentation between lesion (mole) and skin (in the 'masks' folder).\n\nAdditionally, there is a CSV file for each dataset (training, validation, and test), where each row corresponds to a clinical case, defined with two fields separated by commas:\n\nThe numeric id of the lesion: which allows defining the paths to the files containing the image and the mask.\n\nThe label of the lesion: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).","331":"# Multi-Races Human Body Semantic Segmentation Data\n\n\n## Description\n602 People \u20133,010 Images Multi-Races Human Body Semantic Segmentation Data,The data diversity includes headphones, body, background,and glasses.In terms of annotation, we adpoted segmentation annotations on headphones, body, background and glasses.The data can be used for tasks such as human body segmentation and the behavior detection of Video conference.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1181?source=Kaggle\n\n## Data size\n602 people, 5 images for each person\n## Collection environment\nOffice, coffee shop, supermarket, apartment\n## Race distribution\n151 Asian people, 151 black people, 150 Caucasians people, 150  brown people ,ranging from teenager to middle-aged people, (Aged between 16 and 60)\n## Gender distribution\n301 males, 301 females\n## Data diversity\ndifferent poses, different ages, different races, different collection backgrounds\n## Device\ncomputer, cellphone\n## Collecting angles\neye-level angle\n## Data format\nthe image data format is .jpg, the annotation file (mask) format is .png\n## Annotation content\nsegmentation annotation of headphones, body, background, glasses\n## Accuracy\nbased on the accuracy of the actions, the accuracy  is more than 97%; Accuracy of semantic segmentation annotation: for each object, the mask edge location errors in x and y directions are less than 5 pixels, and the category label was correctly labeled, which were considered as a qualified annotation; Annotation accuracy: each object is regarded as the unit, annotation accuracy is more than 97%\n# Licensing Information\nCommercial License\n","332":"","333":"CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 .","334":"Overview\nThis dataset is designed for the task of detecting and counting empty and occupied parking spots in a parking area. It includes images, a mask image, a video, and a utility file (util.py) that provides functions to process these images and video frames.\n\nContents\nImages: The dataset includes images of a parking area under different conditions (e.g., various times of the day). Each image captures the entire parking area and shows the status of each parking spot (empty or occupied).\n\nMask Image: A mask image is provided to identify and locate the parking spots within the parking area. This mask is a binary image where the white regions correspond to the parking spots.\n\nVideo: A video file is included to facilitate real-time detection of parking spot occupancy. The video captures the parking area over a period, showing changes in parking spot status.\n\nUtility File: The util.py file contains helper functions:\n\nempty_or_not: Determines if a given parking spot is empty or occupied.\nget_parking_spots_bboxes: Extracts bounding boxes for each parking spot based on the mask image.\nFiles\nImages Directory:\n\nContains images of the parking area.\nExamples:\nimage_001.jpg\nimage_002.jpg\n...\nMask Image:\n\nmask_1920_1080.png: A binary mask image of size 1920x1080 pixels. The white regions indicate the parking spots.\nVideo File:\n\nparking_1920_1080_loop.mp4: A video file capturing the parking area over a period. This video is used for detecting changes in the occupancy status of parking spots in real-time.\nModel File:\n\nmodel.p: A pre-trained machine learning model used by the empty_or_not function to classify parking spots as empty or occupied.\nUtility File:\n\nutil.py: Contains the utility functions for processing the images and detecting parking spots.\nUsage\nThis dataset can be used for training and testing machine learning models aimed at automating the detection of empty and occupied parking spots. The provided utility functions and pre-trained model facilitate quick experimentation and validation of results.\n\nExample Workflow\nLoad Images: Read the images and the mask image.\nExtract Parking Spots: Use the mask image and get_parking_spots_bboxes function to locate and extract parking spots from each image.\nClassify Spots: Use the empty_or_not function to classify each parking spot as empty or occupied.\nCount Available Spots: Count the number of empty parking spots and display the result.\n\nPotential Applications\nAutomated parking management systems.\nSmart city infrastructure to monitor parking lot occupancy.\nReal-time parking guidance systems for drivers.\n\nInstructions for Kaggle\nTo run the provided code on Kaggle, ensure that all necessary files (images, mask, model, and utility file) are correctly uploaded and paths are set appropriately. Use the provided functions and example code snippets to implement the parking spot detection and counting logic.\n\nBy using this dataset, researchers and developers can develop and test sophisticated parking lot detection algorithms, contributing to more efficient and automated parking management solutions.","335":"# Silicone Mask Biometric Attack Dataset\n\n## This is a demo version, full dataset is coming soon. Share with us your feedback and recieve additional samples for free!\ud83d\ude0a\n\n##Introduction\nThe Silicone Mask Attack Dataset is designed to address security challenges in liveness detection systems through 3D silicone mask attacks. These presentation attacks are key for testing Passive Liveness Detection systems crucial for iBeta Level 2 certification. This dataset significantly enhances the capabilities of liveness detection models \n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2Fe70f55bdfc175f2d96b3dfcfabe4eb11%2FIllustration.jpg?generation=1715795985085319&alt=media)\n\n##Why Silicone Mask Data?\nThis dataset is crucial for companies preparing to comply with iBeta Level 2 certification which requires anti-spoofing technologies. In today's digital security landscape, the Silicone Mask Dataset serves as a critical resource for training Machine Learning (ML) models and advanced biometric techniques to detect spoofing attempts. \n\n##Dataset Features\n\u2022\t**Variety of Masks:** Encompasses 8 unique silicone masks (male and female, Caucasian ans Asian ethnicity)\n\u2022\t**Video Collection:** There are roughly 7,000 videos that showcase detailed spoofing detection scenarios.\n\u2022\t**Helpful for Active liveness:** Head movements and blinking are included in the dataset\n\u2022\t**5 Different shooting angles:** Front Selfie and far\/ close back camera, two side shots available\u200b\n\u2022\t**Capture Devices:** Three different recording devices in selfie mode to mirror real-life conditions (Modern iPhone, Xiaomi and Samsung)\n\u2022\t **Environmental Conditions:** Captures videos across diverse lighting and background settings to ensure robustness.\n\n\u2022 **Additional Flexibility:** We can recreate this dataset using both RGB and USB camera inputs to accommodate various research needs.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n##Technical Specifications\n\u2022\t**File Format:** Videos are formatted to be compatible with mainstream ML frameworks.\n\u2022\t**Resolution and Frame Rate:** Tailored for high-resolution and optimal frame rates to capture quick mask placements.\n\n##Best Uses\nThis dataset is ideal for entities striving to meet or exceed iBeta Level 2 certification. By integrating this dataset, organizations can greatly enhance the training effectiveness of anti-spoofing algorithms, ensuring a robust and accurate performance in practical scenarios.\n\n##Conclusion\nWith its comprehensive features and simulation of real attack scenarios, the Silicone Mask Biometric Attack Dataset is indispensable for anyone involved in developing and certifying facial recognition and liveness detection technologies. Utilize this dataset to strengthen your systems against the most deceptive digital security threats.\n\n","336":"","337":"## 3D Mannequin Face Dataset for Liveness Detection (1K+ pictures)\n\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nOur 3D Mannequin Anti-Spoofing Dataset provides a comprehensive collection of mannequin images, optimized for enhancing liveness detection models in face anti-spoofing. Utilizing retail mannequins, this dataset simulates 3D faces, presenting a realistic challenge for spoofing scenarios. By incorporating 3D textures, it significantly improves the capability of anti-spoofing algorithms\n\n## Some Liveness detection SDK do not recognize this attack - here is an example\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2F484daafbdd3b29fa87822fb7d54405c1%2FTest%20example.png?generation=1715709935878279&alt=media)\n\n**Why Using Mannequin Data?**\n\nIntegrating 3D mannequin images into training scenarios boosts the effectiveness of anti-spoofing models. By training with varied 3D representations of masks, algorithms enhance their ability to distinguish between genuine and spoofed faces. This training is crucial for increasing the security and reliability of biometric systems.\n\n**Dataset Description:**\n- Scope: Features over 100 mannequins from retail environments.\n- Diversity: Includes female, male, and children mannequins, some sporting natural hair.\n- Image Capture: Utilizes both selfie and frontal camera perspectives.\n- Variations: Encompasses accessories such as glasses, sunglasses, scarves, and hats.\n- Lighting Conditions: Offers a range of lighting scenarios for well-rounded algorithm training.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n**Best Used For Anti-Spoofing Training:** The dataset\u2019s 3D characteristics elevate the training efficiency of anti-spoofing algorithms, ensuring a more robust learning experience for detection models.\n\n\nKeywords: 3D Mannequin Face Dataset, Liveness Detection Models, Anti-Spoofing, Comprehensive Dataset, Realistic Features, Detection of Genuine Faces, Sophisticated Spoofing Scenarios, Diverse Lighting, Retail Mannequins, Variability in Facial Accessories, Frontal Camera Usage, Enhanced Anti-Spoofing Algorithms, Security in Biometric Systems, Comprehensive Exposure, Facial Recognition Training, Mannequin-Based Anti-Spoofing, 3D Mask Simulation.\n","338":"## Source:\n\n- Derived off the original \"[Construction Site Safety Image Dataset Roboflow](https:\/\/www.kaggle.com\/datasets\/snehilsanyal\/construction-site-safety-image-dataset-roboflow\/data)\" dataset.\n\n## Modifications:\n\n- The original dataset had 10 different classes namely `{0: 'Hardhat', 1: 'Mask', 2: 'NO-Hardhat', 3: 'NO-Mask', 4: 'NO-Safety Vest', 5: 'Person', 6: 'Safety Cone', 7: 'Safety Vest', 8: 'machinery', 9: 'vehicle'}` for object classification after detection.\n- This modified dataset has a total of 6 classes namely `{0: 'Hardhat', 1: 'NO-Hardhat', 2: 'NO-Safety Vest', 3: 'Person', 4: 'Safety Vest', 5: 'NOT-Person'}`.\n- It combines the classes `{6: 'Safety Cone', 8: 'machinery', 9: 'vehicle'}` into a single class named 'NOT-Person' and completely removes the classes `{1: 'Mask', 3: 'NO-Mask'}`.","339":"","340":"It provides a brief overview of the recent developments in tumor classification approaches, tumor shape and size detection techniques. The aim is to generate imposed images for future classification by fusing the tumor mask with image pixels.\n\n**Refer:** M. Rana and M. Bhushan, \"Effective Tumor Diagnosis based on Shape and Size of Tumor,\" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-6, doi: [10.1109\/ICCCNT56998.2023.10308022](url).","341":"## Cutout Photo Print attack dataset (1,5K individuals+) for Presentation Attack Detection level 1 (PAD)\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nThis dataset focuses on cutout photo print attacks which might be used by iBeta and NIST FATE to assess liveness detection algorithms. This dataset is tailored for training AI models to identify a variation of cutout 2D print attack. \n\n\n**Dataset Description**\n- 1,500+ Participants: Engaged in the project\n- Diverse Representation: Balanced mix of genders and ethnicities\n- 1,500+ Cutout Mask Attacks on the participants\n\n**Photo Print attack description**\n- Each attack comprises of 10-15 sec. video \n- High-quality cutout photos with realistic colors\n- Paper attacks conducted on flat photos with a straight view on the camera (not bent or skewed)\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n## Potential Use Cases:\n\nLiveness detection: This dataset is ideal for training and evaluating liveness detection models, enabling researchers to distinguish between selfies and photo cutout print mask with high accuracy\n\nKeywords: \nCutout Print photo attack dataset, Antispoofing for AI, Liveness Detection dataset for AI, Spoof Detection dataset, Facial Recognition dataset, Biometric Authentication dataset, AI Dataset, PAD Attack Dataset, Anti-Spoofing Technology, Facial Biometrics, Machine Learning Dataset, Deep Learning\n\n","342":"CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 .","343":"**Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing)","344":"This is a collection of 1698 images featuring medical face masks. The images in this dataset could depict a variety of scenarios, such as individuals wearing masks, masks on different surfaces, or masks in various environments. This dataset could be useful for projects related to public health, medical studies, or machine learning models aimed at mask detection. Please note that the actual content of the images may vary, and a thorough review of the dataset is recommended to understand its specifics and potential applications.","345":"**Dataset Overview:**\nWelcome to the Retina Blood Vessel Segmentation dataset, a valuable resource for advancing the field of medical image analysis and enhancing the diagnosis of retinal vascular diseases. This dataset contains a comprehensive collection of retinal fundus images, meticulously annotated for blood vessel segmentation. Accurate segmentation of blood vessels is a critical task in ophthalmology as it aids in the early detection and management of various retinal pathologies, such as diabetic retinopathy and macular degeneration.\n\n**Content:**\nThe dataset comprises a total of X high-resolution retinal fundus images captured using state-of-the-art imaging equipment. Each image comes with corresponding pixel-level ground truth annotations indicating the exact location of blood vessels. These annotations facilitate the development and evaluation of advanced segmentation algorithms.\n\n**Key Features:**\n\nImage Size: The images in the dataset are of varying dimensions, ranging from XXX pixels to XXX pixels, mimicking the real-world diversity of retinal images.\nAnnotations: For each image, corresponding pixel-wise annotations in a binary mask format are provided. Blood vessel pixels are marked as 1, while background pixels are labeled as 0.\nPathological Variation: The dataset encompasses a spectrum of retinal conditions, including varying vessel widths, branching patterns, and presence of anomalies, making it suitable for evaluating the robustness of segmentation models.\nUse Cases:\nResearchers and practitioners in the fields of medical image analysis, computer vision, and artificial intelligence will find this dataset invaluable for several applications:\n\nAlgorithm Development: Use the dataset to train and test innovative segmentation algorithms, leveraging the precise annotations to achieve accurate and reliable results.\nDisease Detection: Create models that can assist in the early detection of retinal pathologies, contributing to timely medical interventions.\nEducation: The dataset can be used for educational purposes to help students and professionals understand the complexities of retinal blood vessel structure.\nEvaluation Metrics:\nPerformance evaluation will primarily involve measuring the segmentation accuracy against the ground truth annotations. Common metrics such as Intersection over Union (IoU), Dice Coefficient, and pixel-wise accuracy can be employed to quantify the model's performance.","346":"# Brain segmentation with mateuszbuda brain segmentation pytorch unet\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n# Introduction\n\nThe RSNA-MICCAI Brain Tumor Radiogenomic Classification Contest is a multi-class classification problem, giving MRIs based on radiomic features, where the goal is to predict the presence of MGMT promoter methylation.\n\nProject: [RSNA-MICCAI Brain Tumor Classification Project](https:\/\/www.kaggle.com\/code\/yannicksteph\/rsna-miccai-brain-tumor-classification)\n\nThere are three classes: \n- LGG (low-grade glioma)\n- HGG (high-grade glioma) \n- WT (hemangioblastoma)\n\nThe dataset we will be working with consists of MRI datasets provided by the Radiological Society of North America (RSNA\u00ae) and the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society). The images are provided in DICOM format and are accompanied by a CSV file containing radiomic features extracted from the images.\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n#### Why\nThe **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was utilized to obtain data for our brain dataset. This library is based on the Unet neural network architecture and specifically designed for brain segmentation from medical images.\n\nBy leveraging the capabilities of this library, accurate brain segmentation was achieved on the images within our dataset. Segmentation is a critical task in medical imaging as it enables the extraction of precise information about different regions or classes, in this case, brain structures.\n\nThe selection of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was based on its exceptional performance and user-friendly nature. It provides an efficient implementation of the Unet architecture, renowned for its success in biomedical image segmentation. Consequently, our project was able to deliver reliable and accurate results for brain segmentation.\n\nIn conclusion, the utilization of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library played a pivotal role in acquiring accurate brain segmentation data for our dataset. By leveraging this library, we efficiently segmented medical images and extracted valuable information to further our project's objectives.\n\nSource: [mateuszbuda_brain-segmentation-pytorch_unet on PyTorch Hub](https:\/\/pytorch.org\/hub\/mateuszbuda_brain-segmentation-pytorch_unet\/)\n\n#### Library\n\nTo achieve tumor segmentation, the U-Net for Brain MRI model will be employed.\n\nU-Net for Brain MRI is a convolutional neural network model specifically designed for segmenting brain MRI images. It features a U-shaped architecture with branch connections, comprising four levels of blocks. Each block consists of two convolution layers with batch normalization, ReLU activation function, and an encoding part with a max pooling layer, while the decoding part utilizes up-convolution. The number of convolution filters varies across the model's levels, ranging from 32 to 256.\n\nTo utilize the model, an input brain MRI image with three channels corresponding to pre-contrast, FLAIR, and post-contrast sequences should be provided. The image should be scaled to a size of 256x256 pixels and normalized using the z-score method per volume.\n\nThe pre-trained U-Net model produces a single-channel probability map indicating anomalous regions in the input image. By applying an appropriate threshold, this probability map can be converted into a binary segmentation mask.\n\nIn summary, U-Net for Brain MRI is a pre-trained model capable of automatically segmenting abnormalities in brain MRI images. Its application extends to various medical imaging tasks, including brain tumor detection and analysis.\n\n\nTo perform shape analysis and extract relevant features, the **2D and 3D** class will be utilized.\n\nData from Source: [rsna-miccai-brain-tumor-radiogenomic-classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification)\n","347":"This dataset is the processed version of the original dataset available at [Computed Tomography Images for Intracranial Hemorrhage Detection and Segmentation](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/). The `split_raw_data.py` provided by the original dataset author has been modified for updated libraries and Python 3.11. `ct_ich.yml` is also updated to reflect the same to get the environment where the data was processed.\n\nThe data present in `image` and `label` folders are in 512 x 512 grayscale PNG files. There are 2814 pairs of image and segmentation mask respectively.\n\nOriginal DOI for this data: https:\/\/doi.org\/10.13026\/4nae-zg36\n\nLicense: The PhysioNet Restricted Health Data License\nVersion 1.5.0\nMake sure to sign in to your Physionet account and agree to the the terms and conditions of the [original dataset](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/) to use this medical dataset.","348":"This Dataset contains the ***JPG*** images of ***Breast Cancer*** taken from the ***CBIS-DDSM***.\n\n![Breast Cancer Images](https:\/\/i.imgur.com\/rz4rtQI.png)\n\n## Descripton\nThis dataset contains ***JPG*** format images (2.49 GB) of the original ***CBIS-DDSM dataset*** (163 GB) which are in ***DICOM*** format and by maintaining the same resolution of the images as it was in the original dataset.\n\nThe original dataset was split into train and test by having two cases one is ***Mass*** and another is ***Calcification(Calc)*** i.e. ***calc_case_description_test_set.csv, calc_case_description_train_set.csv,  mass_case_description_test_set.csv, mass_case_description_train_set.csv, and metadata.csv*** but in here this dataset is made by *converting the images from DICOM to JPG format, removing the unnecessary columns by Data Cleaning and concatenating both the Mass and Calcification(Calc) cases train test into one* i.e. ***calc_case(with_jpg_img).csv, mass_case(with_jpg_img).csv, and metadata(with_jpg_img).csv***.\n\n<br>\n| Collection | |\n| --- | --- |\n| Number of Studies | 6775 |\n| Number of Series | 6775 |\n| Number of Participants | 1,566(NB) |\n| Number of Images | 10239 |\n| Modalities | MG |\n| Image Size (GB) | 6(.jpg) |\n\n\n<br>\n\n**NB**: *The image data for this collection is structured such that each participant has multiple patient IDs. For example, pat_id 00038 has 10 separate patient IDs which provide information about the scans within the IDs (e.g. Calc-Test_P_00038_LEFT_CC, Calc-Test_P_00038_RIGHT_CC_1) This makes it appear as though there are 6,671 participants according to the DICOM metadata, but there are only 1,566 actual participants in the cohort.*\n<br>\n\n##File Description\n###1. JPG image folder file structure\n![Cancer Image file structure](https:\/\/i.imgur.com\/KtBMlVm.png)\n**File naming:** \n- Folder name: `Subject ID &gt; Study UID &gt; Series UID`\n- File name:  `Series Description &gt; img_0 &gt; 1.jpg`\n\n<br>\n###2. CSV files description\n| CSV File | Description |\n| --- | --- | \n| calc_case(with_jpg_img).csv | This file contains the ***Calcification cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| mass_case(with_jpg_img).csv | This file contains the ***Mass cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| metadata(with_jpg_img).csv | This file contains both of the ***Mass and Calcification(Calc) patients*** with their *Series UID, Subject ID, Study UID, Series Description, Modality, SOP Class Name, SOP Class UID, Number of Images, jpg_folder_path* |\n\n<br><br>\n\n##Summary\nThe ***CBIS-DDSM (Curated Breast Imaging Subset of DDSM)*** is an updated and standardized version of the  ***Digital Database for Screening Mammography (DDSM)***.  The DDSM is a database of ***2,620 scanned film mammography studies***. It contains ***normal, benign, and malignant*** cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer.  The images have been decompressed and converted to DICOM format.  Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included.  A manuscript describing how to use this dataset in detail is available at https:\/\/www.nature.com\/articles\/sdata2017177.\n\nPublished research results from work in developing decision support systems in mammography are difficult to replicate due to the lack of a standard evaluation data set; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. Few well-curated public datasets have been provided for the mammography community. These include the DDSM, the Mammographic Imaging Analysis Society (MIAS) database, and the Image Retrieval in Medical Applications (IRMA) project. Although these public data sets are useful, they are limited in terms of data set size and accessibility.\n\nFor example, most researchers using the DDSM do not leverage all its images for a variety of historical reasons. When the database was released in 1997, computational resources to process hundreds or thousands of images were not widely available. Additionally, the DDSM images are saved in non-standard compression files that require the use of decompression code that has not been updated or maintained for modern computers. Finally, the ROI annotations for the abnormalities in the DDSM were provided to indicate a general position of lesions, but not a precise segmentation for them. Therefore, many researchers must implement segmentation algorithms for accurate feature extraction. This causes an inability to directly compare the performance of methods or to replicate prior results. The CBIS-DDSM collection addresses that challenge by publicly releasing a curated and standardized version of the DDSM for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography.\n<br><br>\n\n##Source of the Original dataset\nhttps:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n<br><br>\n\n##Citations & Data Usage Policy \nUsers must abide by the TCIA Data Usage Policy and Restrictions. Attribution should include references to the following citations:\n\n<br>\n###CBIS-DDSM Citation\n&gt;Sawyer-Lee, R., Gimenez, F., Hoogi, A., & Rubin, D. (2016). Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) (Version 1) [Data set]. The Cancer Imaging Archive. https:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n\n<br>\n###Publication Citation\n&gt;Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M., & Rubin, D. L. (2017). A curated mammography data set for use in computer-aided detection and diagnosis research. In Scientific Data (Vol. 4, Issue 1). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1038\/sdata.2017.177\n\n<br>\n###TCIA Citation\n&gt;Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., & Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. In Journal of Digital Imaging (Vol. 26, Issue 6, pp. 1045\u20131057). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1007\/s10278-013-9622-7\n\n\n<br><br>\nNOTE: ***The CBIS-DDSM dataset lack DICOM image for cropped and ROI images, so there are some cropped and ROI images that may not be found corresponding to their given path, so it is recommended to use the full mammography images from this dataset (only)***","349":"# The Portrait and 26 Photos (`272` people), faces dataset\n\nEach set includes 27 photos of people. Each person provided two types of photos: one photo in profile (portrait_1), and 26 photos from their life (photo_1, photo_2, ..., photo_26).\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 7,300+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, \u2026, photo_26_extension, portrait_1_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, \u2026, photo_26_resolution, portrait_1_resolution** - photo resolution in the dataset\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test) to discuss your requirements, learn about the price and buy the dataset**\n\n## The Portrait\nThe portrait photo is a photo that shows a person in profile. Mandatory conditions for the photo are:\n- The person is pictured alone;\n- Shoulder-length photo;\n- No sunglasses or medical mask on the face;\n- The face is calm, with no smiling or gesturing.\n\n## 26 Photos\nThe rest of the photos are completely different, with one exception being that they show a person from The Portrait. There may be different people in it, taken at different times of life and in different locations. The person may be laughing, wearing a mask, and surrounded by friends.\n \n*keywords: biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, supervised learning dataset, person re-identification, person re-identification dataset, person re-Identification by photo, person re-ID dataset*","350":"# Paper\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html\n\n# Abstract\nThe previous image synthesis research for surgical vision had limited results for real-world applications with simple simulators, including only a few organs and surgical tools and outdated segmentation models to evaluate the quality of the image. Furthermore, none of the research released complete datasets to the public enabling open research. Therefore, we release a new dataset to encourage further study and provide novel methods with extensive experiments for surgical scene segmentation using semantic image synthesis with a more complex virtual surgery environment. First, we created three cross-validation sets of real image data considering demographic and clinical information from 40 cases of real surgical videos of gastrectomy with the da Vinci Surgical System (dVSS). Second, we created a virtual surgery environment in the Unity engine with \ufb01ve organs from real patient CT data and 22 the da Vinci surgical instruments from actual measurements. Third, We converted this environment photo-realistically with representative semantic image synthesis models, SEAN and SPADE. Lastly, we evaluated it with various state-of-the-art instance and semantic segmentation models. We succeeded in highly improving our segmentation models with the help of synthetic training data. More methods, statistics, and visualizations on https:\/\/sisvse.github.io\/.\n\n# The contribution of our work\n- We release the \ufb01rst large-scale instance and semantic segmentation dataset, including both real and synthetic data that can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS\n- We systematically analyzed surgical scene segmentation using semantic image synthesis with state-of-the-art models with ten combinations of real and synthetic data.\n- We found exciting results that synthetic data improved low-performance classes and was very e\ufb00ective for Mask AP improvement while improving the segmentation models overall.\n\n# Data generation\nWe collected 40 cases of real surgical videos of distal gastrectomy for gastric cancer with the da Vinci Surgical System (dVSS), approved by an institutional review board at the medical institution. In order to evaluate generalization performance, we created three cross-validation datasets considering demographic and clinical variations such as gender, age, BMI, operation time, and patient bleeding. Each cross-validation set consists of 30 cases for train\/validation and 10 cases for test data. You can find the overall statistics and demographic and clinical information details [in the paper](https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html).\n\n# Object categories\nWe list \ufb01ve organs (Gallbladder, Liver, Pancreas, Spleen, and Stomach) and 13 surgical instruments that commonly appear from surgeries (Hamonic Ace; HA, Stapler, Cadiere Forceps; CF, Maryland Bipolar Forceps; MBF, Medium-large Clip Applier; MCA, Small Sclip Applier; SCA, Curved Atraumatic Graspers; CAG, Suction, Drain Tube; DT, Endotip, Needle, Specimenbag, Gauze). We classify some rare organs and instruments as \u201cother tissues\u201d and \u201cother instruments\u201d classes. The surgical instruments consist of robotic and laparoscopic instruments and auxiliary tools mainly used for robotic subtotal gastrectomy. In addition, we divide some surgical instruments according to their head, H, wrist; W, and body; B structures, which leads to 24 classes for instruments in total.\n\n# Virtual Surgery Environment and Synthetic Data\nAbdominal computed tomography (CT) DICOM data of a patient and actual measurements of each surgical instrument are used to build a virtual surgery environment. We aim to generate meaningful synthetic data from a sample patient. We annotated \ufb01ve organs listed for real data and reconstructed 3D models by using VTK. In addition, we precisely measured the actual size of each instrument commonly used for laparoscopic and robotic surgery with dVSS. We built 3D models with commercial software such as 3DMax, Zbrush, and Substance Painter. After that, we integrated 3D organ and instrument models into the unity environment for virtual surgery. A user can control a camera and two surgical instruments like actual robotic surgery through a keyboard and mouse in this environment. To reproduce the same camera viewpoint as dVSS, we set the exact parameters of an endoscope used in the surgery. While the user simulates a surgery, a snapshot function projects a 3D scene into a 2D image. According to the projected 2D image, the environment automatically generates corresponding segmentation masks.\n\n# Qualified annotations\nSeven annotators trained for surgical tools and organs annotated six organs and 14 surgical instruments divided into 24 instruments according to head, wrist, and body structures with a web-based computer vision annotation tool (CVAT). We call this real data (R). After that, three medical professionals with clinical experience inspected the annotations to ensure their quality. The three medical professionals also manually simulated virtual surgeries to generate virtual surgical scenes. We call this manual synthetic data (MS). On the other hand, we also use an automatic data generation method called domain randomization, a technique to put objects randomly in a scene to cover the variability in real-world data. We call this domain randomized synthetic data (DRS).\n\nIf the dataset is helpful for your research, [please cite the research](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-031-16449-1_53).\n\n#Contact\n- Email to yjh2020@hutom.io , mkchoi@hutom.io\n- Discussion on https:\/\/www.kaggle.com\/datasets\/yjh4374\/sisvse-dataset\n- Issues on https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n\n# Links\n- https:\/\/sisvse.github.io\n- https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html","351":"The dataset introduced in the paper [\"MFVT Dataset for Real-time Masked Facial Detection\"](https:\/\/drive.google.com\/file\/d\/1ZDmdQHI9KFFFDYbhKxun_44QBhsRDNt5\/view)\n\nNowadays, there has been quite a lot of research done on the Masked Facial Detection problem, including the introduction of new datasets and improvements to existing baseline models. However, many of the datasets that have been introduced have some limitations.\n- Some datasets, such as **MAFA**, **Moxa3K**, and **AIZOOTech**, only consider whether a person is wearing a mask or not. The detail that a person is wearing a mask in a proper way as recommended by medical experts has been ignored.\n- For that point, there are also some datasets that take it into account as well, such as **PWMFD** and **FMLD**. However, the way the authors annotate those datasets has some unreasonableness about the material of the mask and the items used alternatively for masks (scarfs, hijabs for example). Furthermore, the bounding boxes of the objects in those datasets are drawn quite carelessly.\n\nTherefore, we introduce the MFVT dataset in order to address those shortcomings of the datasets used for the Masked Facial Detection problem. To build the MFVT dataset, we selected images from previously published datasets and carefully re-annotated the entire dataset with our mask-wearing conventions.\n\nThe MFVT dataset has total of 13507 images. The overall statistics for this dataset are given in the below table:\n<img src=\"https:\/\/i.imgur.com\/V5c1Ipb.jpeg\">\n\nWe provide the COCO annotation format version of this dataset. For the meaning of class indexes, **0**, **1** and **2** correspond to **OK**, **NONE** and **WRONG**.","352":"The dataset contains the Optical Coherence Tomography (OCT) images and their masks of Cystoid Macular Edema (CME) ocular disease for image segmentation purposes.\n\nAcquiring OCT data from Diabetic Macular Edema (DME) patients is difficult due to the lack of an open database from any ophthalmic hospital. Therefore, we obtained a database of retinal images available at [Retinal OCT Images](https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/kermany2018) in which we took a dataset of only DME subjects. From this dataset, only 1000 images were chosen by the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, that are trained to identify Cystoid Macular Edema (CME) and its progression and provide a confirmatory diagnosis of CME. Whereas 200 OCT images are taken with the consent of patients from the Institute of Ophthalmology, Jamshoro for the testing set. These OCT images were explicitly selected to contain several CME regions and a wide variety in their shapes and sizes which is utilized for both training and test sets to get better results.\n\nIn OCT images, the approach of semantic segmentation is that each pixel of an image is labeled with a corresponding clinical class representing a particular disease. The manual binary image masking was performed accordingly using the Image Segmentation application in MATLAB software offers a variety of tools (flood fill\/draw freehand) for manually selecting an image region of interest (ROI). This was necessary for the training of our model to identify and isolate the CME and to validate the accuracy of digitally picked ROIs (CME). The pixels whose value is \u201c1\u201d in the mask image indicate the foreground, that is CME, and the pixels whose value is \u201c0\u201d in the mask image indicate the background.\nThese are the actual ground truth label image and for the given input DME image, the model must predict the labels.\n\nThis dataset has been arranged and categorized under the supervision of the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, Pakistan, that are trained to identify CME and its progression and provide a confirmatory diagnosis of CME.\n\n\n## **Cite Our Work as:** <br>\n#### **Research Paper**\n* Ahmed Z, Panhwar SQ, Baqai A, Umrani FA, Ahmed M, Khan A. Deep learning based automated detection of intraretinal cystoid fluid. Int J Imaging Syst Technol. 2021;1-16. https:\/\/doi.org\/10.1002\/ima.22662 \n\n#### **Dataset**\n* Zeeshan Ahmed, Munawar Ahmed, Attiya Baqai, & Fahim Aziz Umrani. (2022). <i>Intraretinal Cystoid Fluid<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DS\/2277068","353":"","354":"# Face Mask Detection - Faces Dataset\nDataset includes  376 000+ images, 4 types of mask worn on 94 000 unique faces. All images were collected by **[TrainingData.pro](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)**\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 376 000+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, photo_3_extension, photo_4_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, photo_3_extension, photo_4_resolution** - photo resolution in the dataset\n\n# Content\n\nFile with the extension **.csv** includes variables:\n- ID - image id\n- TYPE - image type\n- USER_ID - user id\n- GENDER - gender of a person\n- AGE - person's age\n- name - file name\n- size_mb - image size\n\n## Types of images:\n- **TYPE 1** - There is no mask on the face. \n- **TYPE 2**  - The mask is on, but does not cover the nose or mouth.\n- **TYPE 3** - The mask covers the mouth, but does not cover the nose.\n- **TYPE 4** - The mask is worn correctly, covers the nose and mouth.\n\n![](https:\/\/sun9-10.userapi.com\/impg\/qn0W_s_C3xVYUc_5_IUNEJ6a3xQexHj8GSLlHg\/breQf6Qthzo.jpg?size=2560x988&quality=96&sign=1d633a32909adb9c95eeb5e781e17490&type=album)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1) to learn about the price and buy the dataset**\n\n# **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** provides high-quality data annotation tailored to your needs.\n\n*keywords: facial mask detection, face masks detection, face masks classification, face masks recognition, covid-19, re-identification, public safety, health, automatic face mask detection, biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, object detection dataset, deep learning datasets, computer vision datset, human images dataset, human faces dataset*","355":"# Kvasir-Instrument\nGastrointestinal (GI) tract pathologies are screened, biopsied, and resected (if needed) periodically using surgical tools. However, these biopsied and\/or resected areas are not tracked due to which the video analysis for assessing disease burden or the amount of pathology resection remains unknown. To tackle such issues, we have released the novel \u201cKvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy\u201d dataset, which consists of 590 annotated frames comprising of GI procedure tools such as snares, balloons, biopsy forceps, etc. By adding segmentation masks and bounding boxes information to this dataset, we enable computer vision and GI endoscopy researchers to contribute to the field of automated tool segmentation.\n\n# Dataset Details\nThe Kvasir-Instrument dataset (size 170 MB) contains 590 endoscopic tool images and their ground truth mask. The resolution of the image in the dataset varies from 720x576 to 1280x1024. The image file is encoded using jpeg compression. To the best of our knowledge, this is the first attempt to provide the GI tract organ tools dataset. The open-access dataset can be easily downloaded for research and educational purposes. To facilitate the training and testing on the same dataset, we also provide a train-test split so that researchers can build the methods and improve the results using the same dataset. The bounding box information (box coordinates (x, y, width, height)) for the corresponding images are stored in a JSON file. The dataset is designed to push the state-of-the-art solution for the automatic tool segmentation in gastrointestinal endoscopy.\n\n# Applications of the Dataset\nThe Kvasir-Instrument is intended to be used for researching and developing new algorithms for image segmentation, detection, and localization tasks. We have provided a separate file for training and validation which will assist in the development of approaches across the same dataset.\n\n# Annotation Protocol\nWe took a three-step strategy during annotation. First, the selected dataset samples were labeled by two experienced research assistants. These annotations were sent to the expert gastroenterologist for the verification. Finally, the suggested changes were incorporated, and the images were validated for those samples.\n\n# Suggested Metrics for Segmentation\nWe suggest calculating the dice similarity coefficient (DSC) and Jaccard index or Intersection over Union (IoU) for the segmentation task. The other standard metrics for medical image segmentation are precision, recall, and overall accuracy. We also suggest calculating these metrics.\n# \nSuggested Metrics for Detection and Localization\nFor the detection and localization task, we suggest calculating the average precision at different IoU thresholds. Usually, AP at IoU threshold of 50 is taken for evaluation of this dataset. We also recommend calculating overall IoU.\n\n\nFor more information:\nhttps:\/\/datasets.simula.no\/kvasir-instrument\/","356":"The \u201cMedico automatic polyp segmentation challenge\u201d aims to develop computer-aided diagnosis systems for automatic polyp segmentation to detect all types of polyps (for example, irregular polyp, smaller or flat polyps) with high efficiency and accuracy. The main goal of the challenge is to benchmark semantic segmentation algorithms on a publicly available dataset, emphasizing robustness, speed, and generalization.\n\nParticipants will get access to a dataset consisting of 1,000 segmented polyp images from the gastrointestinal tract and a separate testing dataset. The challenge consists of two mandatory tasks, each focused on a different requirement for efficient polyp detection. We hope that this task encourages multimedia researchers to apply their vast knowledge to the medical field and make an impact that may affect real lives.\n\n# Data\nThe dataset contains 1,000 polyp images and their corresponding ground truth mask. The datasets were collected from real routine clinical examinations at Vestre Viken Health Trust (VV) in Norway by expert gastroenterologists. The VV is the collaboration of the four hospitals that provide healthcare service to 470,000 peoples. The resolution of images varies from 332\u2715487 to 1920\u27151072 pixels. Some of the images contain green thumbnail in the lower-left corner of the images showing the position marking from the ScopeGuide (Olympus). The training dataset can be downloaded from https:\/\/datasets.simula.no\/kvasir-seg\/.\n\nThe test dataset is now released. It can be downloaded from https:\/\/drive.google.com\/file\/d\/1uP2W2g0iCCS3T6Cf7TPmNdSX4gayOrv2\/view?usp=sharing.","357":"Dataset Details\nLabeled Images. In total, the dataset contains 10,662 labeled images stored using the JPEG format. The images can be found in the images folder. The classes, which each of the images belongto, correspond to the folder they are stored in (e.g., the \"polyp\" folder contains all polyp images, the \"barretts\" folder contains all images of Barrett\u2019s esophagus, etc.). The number of images per class are not balanced, which is a general challenge in the medical field due to the fact that some findings occur more often than others. This adds an additional challenge for researchers, since methods applied to the data should also be able to learn from a small amount of training data. The labeled images represent 23 different classes of findings.\n\nUnlabeled Images. In total, the dataset contains 99,417 unlabeled images. The unlabeled images can be found in the unlabeled folder which is a subfolder in the image folder, together with the other labeled image folders. In addition to the unlabeled image files, we also provide the extracted global features and cluster assignments in the Hyper-Kvasir Github repository as Attribute-Relation File Format (ARFF) files. ARFF files can be opened and processed using, for example, the WEKA machine learning library, or they can easily be converted into comma-separated values (CSV) files.\n\nSegmented Images. We provide the original image, a segmentation mask and a bounding box for 1,000 images from the polyp class. In the mask, the pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain polyp pixels. The bounding box is defined as the outermost pixels of the found polyp. For this segmentation set, we have two folders, one for images and one for masks, each containing 1,000 JPEG-compressed images. The bounding boxes for the corresponding images are stored in a JavaScript Object Notation (JSON) file. The image and its corresponding mask have the same filename. The images and files are stored in the segmented images folder. It is important to point out that the segmented images have duplicates in the images folder of polyps since the images were taken from there.\n\nAnnotated Videos. The dataset contains a total of 373 videos containing different findings and landmarks. This corresponds to approximately 11.62 hours of videos and 1,059,519 video frames that can be converted to images if needed. Each video has been manually assessed by a medical professional working in the field of gastroenterology and resulted in a total of 171 annotated findings.\n\nDevelopment Dataset\nThe dataset can be split into four distinct parts; Labeled image data, unlabeled image data, segmented image data, and annotated video data. Each part is further described below. In total, the dataset contains 110,079 images and 373 videos where it captures anatomical landmarks and pathological and normal findings. The results is more than 1.1 million images and video frames all together.\n\nTest Dataset\nThe dataset is split into two distinct parts; the classification dataset and segmentation dataset. The classification dataset should be used to perform the detection and speed tasks, while the segmentation part should be used for the segmentation task.\n\nMore details can be found here:\nhttps:\/\/endotect.com\/\n","358":"# Kvasir-SEG information:\n\nThe Kvasir-SEG dataset (size 46.2 MB) contains 1000 polyp images and their corresponding ground truth from the Kvasir Dataset v2. The images' resolution in Kvasir-SEG varies from 332x487 to 1920x1072 pixels. The images and its corresponding masks are stored in two separate folders with the same filename. The image files are encoded using JPEG compression, facilitating online browsing. The open-access dataset can be easily downloaded for research and educational purposes.\n\n# Applications of the Dataset\nThe Kvasir-SEG dataset is intended to be used for researching and developing new and improved methods for segmentation, detection, localization, and classification of polyps. Multiple datasets are prerequisites for comparing computer vision-based algorithms, and this dataset is useful both as a training dataset or as a validation dataset. These datasets can assist the development of state-of-the-art solutions for images captured by colonoscopes from different manufacturers. Further research in this field has the potential to help reduce the polyp miss rate and thus improve examination quality. The Kvasir-SEG dataset is also suitable for general segmentation and bounding box detection research. In this context, the datasets can accompany several other datasets from a wide range of fields, both medical and otherwise.\n\n# Ground Truth Extraction\nWe uploaded the entire Kvasir polyp class to Labelbox and created all the segmentations using this application. The Labelbox is a tool used for labeling the region of interest (ROI) in image frames, i.e., the polyp regions for our case. We manually annotated and labeled all of the 1000 images with the help of medical experts. After annotation, we exported the files to generate masks for each annotation. The exported JSON file contained all the information about the image and the coordinate points for generating the mask. To create a mask, we used ROI coordinates to draw contours on an empty black image and fill the contours with white color. The generated masks are a 1-bit color depth images. The pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain positive pixels. Some of the original images contain the image of the endoscope position marking probe, ScopeGuide TM, Olympus Tokyo Japan, located in one of the bottom corners, seen as a small green box. As this information is superfluous for the segmentation task, we have replaced these with black boxes in the Kvasir-SEG dataset.\n\n# Suggested Metrics\nThere are different metrics for evaluating the performance of the architectures on the image segmentation dataset. For medical image segmentation task, the most commonly used ones are Dice coefficient and Intersection over Union (IOU). Based on related work in this field, we have used these metrics for the evaluation of the algorithms. In future work, we encourage the use of these metrics for evaluating the performance of the model. In the future, it might be even better to include as many as possible metrics for the fair comparison of the models.\n\nThe bounding box (coordinate points) for the corresponding images are stored in a JSON file. This dataset is designed to push the state-of-the-art solution for the polyp detection task. Some examples of the dataset.","359":"### Context\n\nHumans in the Loop is publishing an open access dataset annotated as a contribution to the worldwide fight against COVID-19. \n\n### Content\n\nThe dataset consists of 6k images acquired from the public domain with an extreme attention to diversity, featuring people of all ethnicities, ages, and regions. In addition, the dataset covers 20 classes of different accessories as well as a classification of faces with a mask, without a mask, or with an incorrectly worn mask (class list is available in meta.json).\n\n### Acknowledgements\n\nThe images were collected and annotated by the refugee workforce of Humans in the Loop in Bulgaria. The platform used for annotation is Supervise.ly.","360":"### COVID-QU-Ex Dataset\nThe researchers of Qatar University have compiled the COVID-QU-Ex dataset, which consists of 33,920 chest X-ray (CXR) images including:\n-\t11,956 COVID-19\n-\t11,263 Non-COVID infections (Viral or Bacterial Pneumonia)\n-\t10,701 Normal\nGround-truth lung segmentation masks are provided for the entire dataset. This is the largest ever created lung mask dataset. \n\n**If you use  COVID-QU-Ex Dataset in your research, please consider to cite the publications\/dataset below:**\n[1] A. M. Tahir, M. E. H. Chowdhury, A. Khandakar, Y. Qiblawey, U. Khurshid, S. Kiranyaz, N. Ibtehaz, M. S. Rahman, S. Al-Madeed, S. Mahmud, M. Ezeddin, K. Hameed, and T. Hamid, \u201cCOVID-19 Infection Localization and Severity Grading from Chest X-ray Images\u201d, Computers in Biology and Medicine, vol. 139, p. 105002, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.105002.\n[2] Anas M. Tahir, Muhammad E. H. Chowdhury, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Serkan Kiranyaz, Uzair Khurshid, Nabil Ibtehaz, Sakib Mahmud, and Maymouna Ezeddin, \u201cCOVID-QU-Ex .\u201d Kaggle, 2021, https:\/\/doi.org\/10.34740\/kaggle\/dsv\/3122958.\n[3] T. Rahman, A. Khandakar, Y. Qiblawey A. Tahir S. Kiranyaz, S. Abul Kashem, M. Islam, S. Al Maadeed, S. Zughaier, M. Khan, M. Chowdhury, \"Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-rays Images,\" Computers in Biology and Medicine, p. 104319, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.104319.\n[4] A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. H. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj, \"Covid-19 infection map generation and detection from chest X-ray images,\" Health Inf Sci Syst 9, 15 (2021), https:\/\/doi.org\/10.1007\/s13755-021-00146-8.\n[5] M. E. H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir, Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. A. Emadi, M. B. I. Reaz, M. T. Islam, \"Can AI Help in Screening Viral and COVID-19 Pneumonia?,\" IEEE Access, vol. 8, pp. 132665-132676, 2020, https:\/\/doi.org\/10.1109\/ACCESS.2020.3010287.\n\nTo the best of our knowledge, this is the first study that utilizes both lung and infection segmentation to detect, localize and quantify COVID-19 infection from X-ray images. Therefore, it can assist the medical doctors to better diagnose the severity of COVID-19 pneumonia and follow up the progression of the disease easily.\n\nThe experiments were conducted on two CXR sets, where each set is divided into train, validation and test sets: \n1)\tLung Segmentation Data\n        Entire COVID-QU-Ex dataset (33,920 CXR images with corresponding ground-truth lung masks)\n2)\tCOVID-19 Infection Segmentation Data\n         A subset of COVID-QU-Ex dataset (1,456 Normal and 1,457 Non-COVID-19 CXRs with corresponding lung mask, plus 2,913 COVID-19 CXRs with \n         corresponding lung mask from COVID-QU-Ex dataset and corresponding infections masks from QaTaCov19 dataset).\n### References\nIn COVID-QU-Ex, the X-ray images are collected from the following repositories and studies:\n\u2022\tCOVID-19 Samples: [1- 7].\n\u2022\tNon-COVID Samples: [8- 10].\n\u2022\tNormal Samples: [8- 10].\n\n[1] QaTa-COV19 Database. https:\/\/www.kaggle.com\/aysendegerli\/qatacov19-dataset. Accessed 14 March 2021.\n[2] Covid-19-image-repository. Available: https:\/\/github.com\/ml-workgroup\/covid-19-image-repository\/tree\/master\/png. Accessed 14 March 2021.\n[3] Eurorad. Available: https:\/\/www.eurorad.org\/. Accessed 14 March 2021.\n[4] Covid-chestxray-dataset. Available: https:\/\/github.com\/ieee8023\/covid-chestxray-dataset. Accessed 14 March 2021.\n[5] COVID-19 DATABASE. Available: https:\/\/www.sirm.org\/category\/senza-categoria\/covid-19\/. Accessed 14 March 2021.\n[6] Kaggle. (2020). COVID-19 Radiography Database. Available: https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database. Accessed 14 March 2021.\n[7] GitHub. (2020). COVID-CXNet. Available: https:\/\/github.com\/armiro\/COVID-CXNet. Accessed 14 March 2021.\n[8] RSNA Pneumonia Detection Challenge. Available: https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge\/data. Accessed 14 March 2021.\n[9] Chest X-Ray Images (Pneumonia). Available: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia. Accessed 14 March 2021.\n[10] Medical Imaging Databank of the Valencia Region. PadChest: A large chest x-ray image dataset with multi-label annotated reports. Available: https:\/\/bimcv.cipf.es\/bimcv-projects\/padchest\/. Accessed 14 March 2021.","361":"Please read our paper:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\n### Abstract\n\nDeep learning has shown tremendous potential in many real-life applications in different domains. One of these potentials is object detection. Recent object detection which is based on deep learning models has achieved promising results concerning the finding of an object in images. The objective of this paper is to annotate and localize the medical face mask objects in real-life images. Wearing a medical face mask in public areas, protect people from COVID-19 transmission among them. The proposed model consists of two components. The first component is designed for the feature extraction process based on the ResNet-50 deep transfer learning model. While the second component is designed for the detection of medical face masks based on YOLO v2. Two medical face masks datasets have been combined in one dataset to be investigated through this research. To improve the object detection process, mean IoU has been used to estimate the best number of anchor boxes. The achieved results concluded that the adam optimizer achieved the highest average precision percentage of 81% as a detector. Finally, a comparative result with related work has been presented at the end of the research. The proposed detector achieved higher accuracy and precision than the related work.\n\n\n### Context\n\nThis Dataset conducted its experiments based on two public medical face mask datasets. The first dataset is Medical Masks Dataset (MMD)published by Mikolaj Witkowski (https:\/\/www.kaggle.com\/vtech6\/me dical-masks-dataset). The MMD dataset consists of 682 pictures with over 3k medical masked faces wearing masks. The second public masked face dataset is a Face Mask Dataset (FMD)in (https:\/\/www.kaggle.com\/andrewmvd\/face-mask-detection). The FMD dataset consists of 853 images. We created a new dataset by combining MMD and FMD. The merged dataset contains 1415 images by removing bad quality images and redundancy\n\n\n### Acknowledgements\n\nCite our papers:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\nLoey, M., Manogaran, G. & Khalifa, N.E.M. A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images. Neural Comput & Applic (2020). https:\/\/doi.org\/10.1007\/s00521-020-05437-x\nLoey, Mohamed; Smarandache, Florentin; M. Khalifa, Nour E. 2020. \"Within the Lack of Chest COVID-19 X-ray Dataset: A Novel Detection Model Based on GAN and Deep Transfer Learning\" Symmetry 12, no. 4: 651. https:\/\/doi.org\/10.3390\/sym12040651\nKhalifa, N.E.M., Smarandache, F., Manogaran, G. et al. A Study of the Neutrosophic Set Significance on Deep Transfer Learning Models: an Experimental Case on a Limited COVID-19 Chest X-ray Dataset. Cogn Comput (2021). https:\/\/doi.org\/10.1007\/s12559-020-09802-9\n\n### Inspiration\n\nCreating the proposed database presents more challenges\nBenha University\nhttp:\/\/bu.edu.eg\/staff\/mloey\nhttps:\/\/mloey.github.io\/\nhttps:\/\/orcid.org\/0000-0002-3849-4566\nArabic Handwritten Characters Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahcd1\nArabic Handwritten Digits Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahdd1","362":"Welcome to the \"Playing Cards Dataset\" designed for YOLO Object Detection! This dataset contains high-quality images of playing cards, annotated and ready for training object detection models using the YOLO (You Only Look Once) framework. Whether you're developing an AI for card game analysis, magic tricks detection, or any other application involving playing cards, this dataset will provide the necessary data to build and fine-tune your models.\n\nLabels was generated in source (1):\n`['10', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'J', 'K', 'Q']\n['Clubs', 'Diamonds', 'Hearts', 'Spades']\n['10_Clubs', '10_Diamonds', '10_Hearts', '10_Spades', '2_Clubs', '2_Diamonds', '2_Hearts', '2_Spades', '3_Clubs', '3_Diamonds', '3_Hearts', '3_Spades', '4_Clubs', '4_Diamonds', '4_Hearts', '4_Spades', '5_Clubs', '5_Diamonds', '5_Hearts', '5_Spades', '6_Clubs', '6_Diamonds', '6_Hearts', '6_Spades', '7_Clubs', '7_Diamonds', '7_Hearts', '7_Spades', '8_Clubs', '8_Diamonds', '8_Hearts', '8_Spades', '9_Clubs', '9_Diamonds', '9_Hearts', '9_Spades', 'A_Clubs', 'A_Diamonds', 'A_Hearts', 'A_Spades', 'J_Clubs', 'J_Diamonds', 'J_Hearts', 'J_Spades', 'K_Clubs', 'K_Diamonds', 'K_Hearts', 'K_Spades', 'Q_Clubs', 'Q_Diamonds', 'Q_Hearts', 'Q_Spades'] 52`\n\nFeel free to leave comments on things I should fix or improve","363":"## **Overview**\n\nThe success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset\u2019s average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset\u2019s effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union(IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8436749%2F67e47a3479c81c63a09ab4e3deed6f9c%2FDataset.jpg?generation=1716177287419838&alt=media)\n\n\n**Dataset Details**\nTotal Images: 17,326\nInstances: 81,542\nBBox Per Image - 4.7036\nVehicle Categories: 1. Bicycle 2. Bus 3. Bhotbhoti 4. Car 5. CNG 6. Easybike 7. Leguna 8. Motorbike 9. MPV 10. Pedestrian 11. Pickup 12. PowerTiller 13. Rickshaw 14. ShoppingVan 15. Truck 16. Van 17. Wheelbarrow\n\n## Model Testing Results\nThe dataset has been rigorously tested with YOLO v5-v8 models. The mean Average Precision at 50% Intersection over Union (IoU) is an impressive 84.8%.\n\n| Model        | Dataset          | mAP0.5     | mAP 0.5:0.95   | Precision  | Recall     | Weight                                                                                                                                                                   |\n| --------     | ---------------- | ---------- | -------------- | ---------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **YOLOv5**   | CARL-D           | 0.437      | 0.328          | 0.633      | 0.423      | |   \n|              | DhakaAI          | 0.416      | 0.255          | 0.640      | 0.393      | |\n|              | P2 Dhaka         | 0.655      | 0.400          | 0.804      | 0.581      | |\n|              | PoribohonBD      | 0.981      | 0.743          | 0.939      | 0.948      | |\n|              | **BNVD**         | **0.826**  | **0.609**      | **0.836**  | **0.762**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V5\/weights\/best.pt)      |\n| **YOLOv6**   | CARL-D           | 0.479      | 0.372          | 0.58       | 0.453      | |\n|              | DhakaAI          | 0.420      | 0.262          | 0.311      | 0.548      | |\n|              | P2 Dhaka         | 0.775      | 0.494          | 0.762      | 0.71       | |\n|              | PoribohonBD      | 0.899      | 0.648          | 0.899      | 0.81       | |\n|              | **BNVD**         | **0.837**  | **0.624**      | **0.805**  | **0.76**   | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V6\/weights\/best_ckpt.pt) |\n| **YOLOv7**   | CARL-D           | 0.478      | 0.369          | 0.619      | 0.459      | |\n|              | DhakaAI          | 0.464      | 0.284          | 0.692      | 0.438      | |\n|              | P2 Dhaka         | 0.743      | 0.462          | 0.816      | 0.688      | |\n|              | PoribohonBD      | 0.907      | 0.656          | 0.914      | 0.841      | |\n|              | **BNVD**         | **0.841**  | **0.623**      | **0.83**  | **0.779**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V7\/weights\/best.pt)      |\n| **YOLOv8**   | CARL-D           | 0.478      | 0.359          | 0.602      | 0.446      | |\n|              | DhakaAI          | 0.435      | 0.276          | 0.694      | 0.446      | |\n|              | P2 Dhaka         | 0.69       | 0.449          | 0.798      | 0.604      | | \n|              | PoribohonBD      | 0.889      | 0.658          | 0.898      | 0.823      | |\n|              | **BNVD**         | **0.848**  | **0.643**      | **0.841**  | **0.774**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V8\/weights\/yolov8_ods_new_100e_best.pt)      |\n\n\n\n## Checkpoints\nPre-trained model weight files can be found in the \"Checkpoints\" folder of this repository.\n\n## License\nThis dataset is released under the [insert license type] license.\n\n## Citation\nIf you use this dataset in your work, please consider citing:\n\n[Insert citation information here]\n\n## Contributors\nWe would like to thank the following contributors for their valuable contributions to the development of this dataset.\n\n- Bipin Saha (bipinsaha.bd@gmail.com)\n- Md. Johirul Islam* (johirul@phy.ruet.ac.bd)\n- Shaikh Khaled Mostaque* (misha@ru.ac.bd)\n- Aditya Bhowmik (bhowmik.aditya0@gmail.com)\n- Tapodhir Karmakar Taton (tapodhirtaton@gmail.com)\n- Md Nakib Hayat Chowdhury (nakib2025@gmail.com)\n- Mamun Ibne Bin Reaz* (mamun.reaz@iub.edu.bd)","364":"","365":"Modified Version of Indian Driving Dataset to work for SSD and YOLO. \n\nWhat is Indian Driving Dataset?\n\nWhile several datasets for autonomous navigation have become available in recent years, they have tended to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in object or background appearance and strong adherence to traffic rules. We propose a novel dataset for road scene understanding in unstructured environments where the above assumptions are largely not satisfied. It consists of 10,000 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. The label set is expanded in comparison to popular benchmarks such as Cityscapes, to account for new classes.\n\nThe dataset consists of images obtained from a front facing camera attached to a car. The car was driven around Hyderabad, Bangalore cities and their outskirts. The images are mostly of 1080p resolution, but there is also some images with 720p and other resolutions.","366":"## Dataset for the paper entitled \"Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models\" (CVPR 2024) \n\nThe Natural Denoising Diffusion Attack (NDDA) dataset is designed to systematically evaluate the natural attack capability in diffusion models. The latest NDDA dataset consists of the following 15 classes: stop sign, car, dog, hot dog, traffic light, zebra, fire hydrant, frog, horse, bird, boat, air plane, bicycle, cat, and carrot with 6 diffusion models (Dall-E 2, Dall-E 3, Stable Diffusion 2, Deepfloyd IF, Stable Diffusion 1.5, MidJourney, and Google Duet). \n\nThe NDSS dataset is organized into `diffusion parent folder` that separate each diffusion model's set of images, which in turn contains multiple folders for each of the 15 object classes from COCO. Each object class folder then contains multiple subfolders that hold the NDD attack images, and these subfolders' names are the text prompts used to generate the set of NDD attack images. For example, if images were generated using Stable Diffusion 2 with the text prompt, `blue dog`, the path to this prompt subfolder would be `diffusion_output\\_diffusion\\_2\/blue\\_dog`. \n\nMore details are on our website: https:\/\/sites.google.com\/view\/cav-sec\/ndd-attack?authuser=0\n\n\n```bibtex\n@InProceedings{sato2022towards,\n  author    = {Takami Sato and Qi Alfred Chen},\n  title     = {{Towards Driving-Oriented Metric for Lane Detection Models}},\n  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2022}\n}\n```\n","367":"Thermal Object Detection for Autonomous Vehicles\nThis dataset provides real-world thermal imagery for training and evaluating object detection algorithms in autonomous vehicles. It  focuses on identifying moving objects crucial for self-driving cars, including:\n\nCars\nBicycles\nPeople\nDogs\nData Summary:\n\nSize: Over 12,000 thermal images\nFocus: Primarily annotated for cars\nApplications:\nTraining object detection models for autonomous vehicles\nResearch on thermal image analysis for self-driving applications\nIf you're working on self-driving car perception systems, this dataset can be valuable for:\n\nDeveloping and refining thermal object detection algorithms\nEvaluating the performance of object detection models in low-light or adverse weather conditions","368":"This meticulously curated dataset is a comprehensive collection of annotated images featuring Egyptian car license plates, tailored specifically for object detection tasks. With a focus on facilitating advanced computer vision research and development, this dataset is an invaluable resource for training, testing, and benchmarking state-of-the-art algorithms in license plate detection.","369":"This dataset is created from the data available at https:\/\/www.kaggle.com\/datasets\/javiersanchezsoriano\/roundabout-aerial-images-for-vehicle-detection?select=roundabouts.csv .\n\n\nclasses:\n0-car\n1-cycle\n2-bus\n3-truck\n4-van\n\n\n\n","370":"#Carla Object Detection Dataset\n\n##Description:\n\nThe Carla Object Detection Dataset is a labeled dataset designed for object detection tasks within the Carla Simulator environment. The images were captured using the autopilot mode in Carla Simulator across various environments, including Town01, Town02, Town03, Town04, and Town05. Frames were saved at regular intervals during simulation, providing diverse scenes for training and evaluation.\n\n##Labels:\n\nEach image in the dataset is accompanied by annotation files in three different formats:\n\nPascal VOC Format: XML files containing labels are stored in the labels folder.\nYOLO Format: YOLO-formatted label files can be found in the labels_yolo_format folder.\nMS COCO Format: JSON files with annotations in MS COCO format are located in the annotations folder.\nAvailable Classes:\n\nThe dataset includes the following object classes:\n\nVehicle (Car, Truck)\nBike\nMotorbike\nTraffic Light\nTraffic Sign","371":"This dataset contains car images for object detection task. The dataset is split into two folders, namely train and test. However, for training purposes, it should be split into three sets necessary for Machine Learning and Deep Learning tasks, namely train, validation, and test splits. The structure of the data is as follows:\n\n- ROOT\n   - train:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - test:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - meta_deta.csv.\n\nFor the object detection task, the bounding box coordinates can be obtained using meta_deta.csv file .","372":"Diverse License Plate Detection - A combination of images from multiple LPD datasets along with negative samples to make the dataset training-ready.\nThe images are from a large number of datasets from Kaggle as well as general internet sources, as will be listed below.\n\nPurpose:\nTraining a YOLOv8 object detection model for license-plate detection.\nSee https:\/\/github.com\/ultralytics\/ultralytics for pre-trained models.\n\nData:\nThere are about 3100 images of cars with tagged license plates, on the conditions that the plate has at least one recognizable character and is not seen through the glass of another car (or reflection for that matter).\nThere are also about 1500 images which either do not contain cars at all, or contain cars but in such an angle or resolution that their license plate is not detectable. This is to address false positives which I have encountered when using some of the open-source models found on huggingface (ones that were only trained on images with at least one license-plate in them)\n*Update 24\/03\/2024: Dataset contains 5500 images, see Updates for specifications.*\n\nUpdates:\n**Update 24\/03\/2024:**\n- After using a different open-source model from huggingface trained on a dataset from roboflow on yolo5 (dataset in list here) we encountered two items of note.\n  - A. The system yielded much better results when using a Yolo8 model to first identify vehicles, and then detect plates on them\n  - B. The dataset on which the yolov5 model was trained on contained almost no negative examples, and contained a lot of duplications.\n- Added some images from a potholes dataset (listed in Sources) to help with disassociate roads with plates, and also get a few more car images.\n- Added images from the roboflow dataset which included images from a parking garage, allowing for a different angle.\n- Added images from a parking sign dataset, few with car images but mostly to disassociate street and road signs from plates found on actual cars.\n- Trained a Yolo8 model on the dataset, but did not run in production and was only really for testing. However the class loss decreased at a much faster rate than when trained with a much larger dataset consisting of very similar (quality, size, locality) positive images, even by epoch 10.\n- *Current plan moving forward: Run transfer learning on Yolo9 with this dataset for somewhere between 100 and 200 epochs (yay for Programmable Gradient Information) and post some of the model results here.*\n\n**Update 25\/03\/2024** Training update\nSplit dataset into Train, Validation, Test by ratio of 80, 10, 10.\nTrained using the ultralytics library with imgsz=640, batch=16, dropout=0.2 for 150 epochs\nValidation plateaued after about 110 epochs. Out of 100 test photos from real data there were about 5 false positives and 5 false negatives when using this model only, and 2 false positives + 1 false negative when first processing the image through YOLOv8x and filtering for [car, truck, motorcycle, bus]. Currently runs in production on Torchserve.\nFalse positives: Rectangles with lines (like grates)\nFalse negatives: Very low quality images, or images in which the plate was already very small and so disappeared when image was resized to 640.\n\nDiversity:\nThe datasets sampled for this \"diverse\" one contain plates from a variety of countries, but mainly focusing on the ones from this site which are:\n- European (+Russian)\n- American\n- Middle Eastern\n- Indian\n\nSources: (In no particular order)\n- https:\/\/www.kaggle.com\/datasets\/amriteshtiwari20\/truck-licenseplate-dataset\n- https:\/\/www.kaggle.com\/datasets\/gaelcohen\/license-plate-israel\n- https:\/\/www.kaggle.com\/datasets\/kedarsai\/indian-license-plates-with-labels\n- https:\/\/www.kaggle.com\/datasets\/mohamedalitrabelsi\/tunisiania-license-plate-detection\n- https:\/\/www.kaggle.com\/datasets\/mrabduqayum\/license-plate-detection-yolov5\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert\n- https:\/\/www.kaggle.com\/datasets\/balraj98\/stanford-background-dataset\n- https:\/\/www.kaggle.com\/datasets\/mikhailma\/house-rooms-streets-image-dataset\n- https:\/\/www.kaggle.com\/datasets\/paulchambaz\/google-street-view\n- https:\/\/www.kaggle.com\/datasets\/pcmill\/license-plates-on-vehicles\n- https:\/\/www.kaggle.com\/datasets\/psvishnu\/pennfudan-database-for-pedestrian-detection-zip\n- https:\/\/www.kaggle.com\/datasets\/rayanechibani\/dataset\n- https:\/\/www.kaggle.com\/datasets\/yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel\n- https:\/\/www.kaggle.com\/datasets\/amirhoseinahmadnejad\/car-license-plate-detection-iran\n- https:\/\/www.kaggle.com\/datasets\/aritrag\/license\n- https:\/\/www.kaggle.com\/datasets\/marshah\/license-plate-persian-coco\n- https:\/\/www.kaggle.com\/datasets\/narimanjabbar\/dataset-iraq-license-plate\n- https:\/\/www.kaggle.com\/datasets\/nuralitileuov\/car-license-plate\n- https:\/\/www.kaggle.com\/datasets\/saisirishan\/indian-vehicle-dataset\n- https:\/\/www.kaggle.com\/datasets\/sushankghimire\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/trainingdatapro\/license-plates-1-209-438-ocr-plates\n- https:\/\/www.kaggle.com\/datasets\/truongthanh081203\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/andrewmvd\/pothole-detection\n- https:\/\/universe.roboflow.com\/augmented-startups\/vehicle-registration-plates-trudk\/dataset\/1\n- https:\/\/www.kaggle.com\/datasets\/mathurinache\/san-francisco-parking-sign-detection\n- Internet\n\nIt should be noted, with the size of some of these datasets, not all of the images in the datasets were used, depending on the size and variety in each dataset.\n\nLicense:\nAs specified by the datasets (those that did specify a License and did not specify \"Unknown\")\n- amriteshtiwari20\/truck-licenseplate-dataset: MIT\n- kedarsai\/indian-license-plates-with-labels: CC0: Public Domain\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert: CC BY-NC-SA 4.0\n- balraj98\/stanford-background-dataset: The dataset is derived from Stanford DAGS Lab's Stanford Background Dataset from their Scene Understanding Datasets, see the Acknowledgements section for credits\n- mikhailma\/house-rooms-streets-image-dataset: CC0: Public Domain\n- paulchambaz\/google-street-view: GNU Lesser General Public License\n- pcmill\/license-plates-on-vehicles: CC0 Public Domain\n- psvishnu\/pennfudan-database-for-pedestrian-detection-zip: Acknowledgements\n- yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel: CC0 Public Domain\n- amirhoseinahmadnejad\/car-license-plate-detection-iran: Database Contents License (DbCL) v1.0\n- aritrag\/license: CC0 Public Domain\n- marshah\/license-plate-persian-coco: Attribution-NonCommercial 4.0 International\n- trainingdatapro\/license-plates-1-209-438-ocr-plates: Attribution-NonCommercial 4.0 International\n- truongthanh081203\/license-plate: Apache 2.0\n\nNotes on Licenses:\nIf your dataset is included in this list and has a license that could be changed (like CC BY-NC-SA 4.0) please contact me if you'd like your photos removed from the dataset.\n\nAcknowledgements:\nS. Gould, R. Fulton, D. Koller. Decomposing a Scene into Geometric and Semantically Consistent Regions. Proceedings International Conference on Computer Vision (ICCV), 2009.\nhttps:\/\/www.cis.upenn.edu\/~jshi\/ped_html\/","373":"YOLO (You Only Look Once) is a popular object detection algorithm that processes images in a single pass through a neural network to simultaneously predict bounding boxes and class probabilities for objects within those boxes. YOLOv3 is one of the versions of this algorithm, known for its balance between speed and accuracy.\n\nFor training and testing YOLOv3, you can use a variety of datasets depending on your application. Some commonly used datasets for object detection tasks, including those suitable for YOLOv3, are:\n\nCOCO (Common Objects in Context): This dataset is widely used for object detection, segmentation, and captioning tasks. It contains over 200,000 labeled images with 80 object categories.\n\nPascal VOC (Visual Object Classes): This dataset consists of images annotated with object bounding boxes and class labels. It includes 20 object categories such as person, car, dog, etc.\n\nOpen Images Dataset: This is a large-scale dataset with millions of images annotated with object bounding boxes and class labels. It covers a wide range of object categories.\n\nKITTI Vision Benchmark Suite: Primarily used for autonomous driving research, this dataset contains images captured from a moving vehicle with annotations for object detection, tracking, and scene understanding.\n\nImageNet: Although primarily known for image classification, ImageNet also contains bounding box annotations for object detection tasks. It consists of millions of images across thousands of categories.\n\nThese datasets provide a diverse range of objects in various contexts, making them suitable for training and evaluating object detection models like YOLOv3.","374":"\nThe \"26 Class Object Detection Dataset\" comprises a comprehensive collection of images annotated with objects belonging to 26 distinct classes. Each class represents a common urban or outdoor element encountered in various scenarios. The dataset includes the following classes:\n\nBench\nBicycle\nBranch\nBus\nBushes\nCar\nCrosswalk\nDoor\nElevator\nFire Hydrant\nGreen Light\nGun\nMotorcycle\nPerson\nPothole\nRat\nRed Light\nScooter\nStairs\nStop Sign\nTraffic Cone\nTrain\nTree\nTruck\nUmbrella\nYellow Light\nThese classes encompass a wide range of objects commonly encountered in urban and outdoor environments, including transportation vehicles, traffic signs, pedestrian-related elements, and natural features. The dataset serves as a valuable resource for training and evaluating object detection models, particularly those focused on urban scene understanding and safety applications.","375":"## RSUD20K\n\n**Concordia University**\n\nHasib Zunair, Shakib Khan, A. Ben Hamza\n\n[[`Paper`](https:\/\/arxiv.org\/abs\/2401.07322)] [[`Dataset`](https:\/\/www.kaggle.com\/datasets\/hasibzunair\/rsud20k-bangladesh-road-scene-understanding)]\n\n**Here's a collage of outputs of RSUD20K trained model in action! Play in 4K for best results.**\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/pdRXa10SrAc\/0.jpg)](https:\/\/www.youtube.com\/watch?v=pdRXa10SrAc)\n\nThis is official code for our **paper under review at ICIP 2024**:<br>\n[RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving](https:\/\/arxiv.org\/abs\/2401.07322)\n<br>\n\nRSUD20K is a new object detection dataset for road scene understanding, comprised of over **20K** high-resolution images from the driving perspective on Bangladesh roads, and includes **130K** bounding box annotations for **13** objects. RSUD20K consists of the following classes for multi-class object detection:\n\n```bash\n# classes.txt\nperson\nrickshaw\nrickshaw van\nauto rickshaw\ntruck\npickup truck\nprivate car\nmotorcycle\nbicycle\nbus\nmicro bus\ncovered van\nhuman hauler\n```\n\nFor details on format, see [here](https:\/\/github.com\/meituan\/YOLOv6\/blob\/main\/docs\/Train_custom_data.md#1-prepare-your-own-dataset).\n\n**Dataset statistics**:\n\n| Name  | Number of images\/label pairs |\n| ------------- | ------------- |\n| `train`  | 3985 |\n| `val`  | 1004 |\n| `test`  | 649 |\n| `pseudo` (used for training)  | 14696 |\n\n\nAll code and pre-trained models for reproducibility are available in [GitHub](https:\/\/github.com\/hasibzunair\/RSUD20K).\n","376":"The dataset comprises a diverse collection of images, focusing specifically on Nepali motor vehicles, including motorcycles and scooters, along with annotations to facilitate object detection tasks. The dataset is designed to support computer vision and machine learning applications that involve the identification and localization of vehicles, contributing to the development and training of object detection models.\n\nKey features of the dataset include a comprehensive representation of various motorcycle and scooter models commonly found in Nepal, reflecting the unique characteristics and specifications of vehicles used in the region. Additionally, the dataset includes images of cars, providing a broader scope for understanding and detecting different types of motor vehicles on the road.\n\nEach image in the dataset is accompanied by annotations, which typically include information about the bounding boxes around the vehicles and associated class labels. This annotation data is essential for training object detection algorithms, allowing the models to learn and generalize patterns for accurate detection and localization of bikes and cars within images.\n\nResearchers, developers, and data scientists can leverage this dataset to enhance the capabilities of their object detection models, particularly those focused on recognizing and analyzing Nepali motor vehicles. The dataset's specificity to the Nepali context makes it valuable for applications related to traffic monitoring, vehicle counting, and overall transportation-related analysis in the context of Nepal.","377":"The \"Image Obstacles in Public Spaces\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. The description of this dataset includes several key points:\n\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (JPG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces. It is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset.","378":"The \"Obstacles in Public Spaces for Dist-YOLO\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. This dataset has been curated and annotated with the aim of supporting the development of the Dist-YOLO (You Only Look Once) model for object detection.\nThe description of this dataset includes several key points:\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (e.g., JPG or PNG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing Object Detection models, especially models like Dist-YOLO, in recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces.\nIt is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset.","379":"This dataset contains car images with disabled people signs. The dataset includes 1241 images.\nCars are annotated in YOLOv8 format.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 416x416 (Stretch)\n\nThe following augmentation was applied to create 2 versions of each source image:\n\nThe following transformations were applied to the bounding boxes of each image:\n* 50% probability of horizontal flip\n* Random rotation of between -15 and +15 degrees","380":"CLASSES =10 \nbike, \nmotobike,\n person,\n traffic_light_green, \ntraffic_light_orange, \ntraffic_light_red, \ntraffic_sign_30,\n traffic_sign_60, \ntraffic_sign_90,\n vehicle\n\nCarla Self-Driving Car Simulator\n","381":"This data was taken from VOC 2012 competition dataset. I have extracted the data from the .xml files into .txt files in the correct directories as required for yolo. There are 20 classes \n--&gt; labels_dictionary = {'person':0, 'car':1, 'chair':2, 'bottle':3, 'pottedplant':4, 'bird':5, 'dog':6,\n'sofa':7, 'bicycle':8, 'horse':9, 'boat':10, 'motorbike':11, 'cat':12, 'tvmonitor':13,\n'cow':14, 'sheep':15, 'aeroplane':16, 'train':17, 'diningtable':18, 'bus':19}. \n","382":"Dataset Name: Celeb-DF Faces Dataset\n\nDescription:\nThe Celeb-DF Faces Dataset is a curated collection of facial images extracted from the Celeb-DF dataset. This dataset focuses on providing a comprehensive set of facial images for research and analysis in the field of deepfake detection and facial image analysis. The images are categorized into two classes: \"Fake\" and \"Real,\" based on the source of the videos.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\n\nSource Folders:\n\nceleb-df-v2\/Celeb-real: Contains authentic facial videos.\nceleb-df-v2\/Celeb-synthesis: Contains synthesized (fake) facial videos.\nceleb-df-v2\/YouTube-real: Contains additional authentic facial videos from YouTube.\nOutput Folder:\n\nceleb_faces_224\/: Contains the extracted and resized facial images.\nMetadata File:\n\nmetadata_celebs.csv: A CSV file storing metadata information for each extracted image with the following columns:\nName: The filename of the extracted image.\nLabel: The label indicating whether the image is \"Fake\" or \"Real.\"\nCreation Process:\n\nVideo Frame Extraction:\n\nThe first frame from each video in the source folders is extracted.\nImage Resizing:\n\nThe extracted frames are resized to 224x224 pixels to ensure uniformity and compatibility with common machine learning models.\nImage Storage:\n\nThe resized images are saved in the celeb_faces_224\/ folder with filenames corresponding to the original video names.\nMetadata Compilation:\n\nA metadata CSV file (metadata_celebs.csv) is created to store the filenames and labels of the images, indicating whether they are from \"Fake\" or \"Real\" videos.\nIntended Use:\nThe dataset is ideal for tasks such as:\n\nDeepfake detection and analysis\nTraining and evaluation of machine learning models for facial image classification\nImage forensics research and development\nNote: This dataset is derived from the Celeb-DF dataset and is intended for research and educational purposes only.","383":"Dataset Name: FaceForensics++ Faces_224\nDescription:\nThe FaceForensics++ Faces Dataset is a collection of images extracted from the FaceForensics++ dataset, focusing specifically on facial imagery. It comprises two main categories: \"Fake\" and \"Real,\" representing manipulated and authentic facial images, respectively.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\nFolders:\nFF++_Faces_224\/\nContains the extracted and resized images from the FaceForensics++ dataset.\nmetadata_FF++.csv\nA CSV file storing metadata information for each image, including filename (Name) and label (Label) indicating whether the image is \"Fake\" or \"Real.\"\nIntended Use:\nThe dataset is suitable for tasks such as:\n\nFacial image analysis\nDeep learning-based manipulation detection\nImage forensics research and development\nNote: This dataset is derived from the FaceForensics++ dataset and is intended for research and educational purposes only.\n\n","384":"**Contacts Dataset**\nThe Contacts dataset simulates a list of individual contacts with detailed personal and contact information, useful for representing a customer database:\n\nID: A unique identifier for each contact, generated as a UUID.\nEmail: A randomly generated realistic email address.\nCountry: Randomly chosen from a list of six countries (France, US, UK, Germany, Portugal, China), represented by their respective codes.\nCity: A random city, ideally from the chosen country, although this script uses a random city generator without specific country linkage for simplicity.\nPhone: A randomly generated phone number, which should ideally include the country prefix (not implemented in this simple version).\nFirstname: A randomly generated first name.\nBirthdate: A randomly generated birth date for each contact, ranging from 1930 to 2008, making the age of contacts between 15 and 93.\nPostal Code: A postal code corresponding to the randomly chosen city.\nAcquisition Source: The source through which the contact was acquired, chosen randomly from options like Facebook ad, Google ad, promotional email, or a birthday mail campaign.\nCreated At: The timestamp when the contact record was created, ranging from 2019 to the present.\nUpdated At: The timestamp of the last update to the contact record, which is any time between the creation date and the current date.\n\n\n\n**Products Dataset**\nThe Products dataset simulates an inventory of items that might be sold by a company:\n\nID: A unique identifier for each product, generated as a UUID.\nSKU: A unique Stock Keeping Unit code for each product.\nCategories: A list of categories assigned to each product, chosen from predefined options like Electronics, Clothing, Home, Toys, Books. Each product can belong to multiple categories.\nPrice: A randomly generated price for each product, ranging from $10 to $500.\nName: A randomly generated name for the product.\nDescription: A short description for the product, generated randomly.\nParent ID: The ID of a parent product if the current product is a variant; this is left blank in the script for simplicity.\nURL: A fake URL simulating a product page on an e-commerce site.\nImage URL: A fake URL for the product\u2019s image.\nBrand: The brand of the product, either randomly selected from a list or generated.\nCreated At: The timestamp when the product record was created, within the last two years.\nModified At: The timestamp of the last update to the product record.\n\n\n**Stores Dataset**\nThe Stores dataset represents physical or conceptual locations where the company operates:\n\nID: A unique identifier for each store, generated as a UUID.\nName: The name of the store, generated by appending \"Store\" to a randomly generated company name.\nCity: The city where the store is located, chosen randomly.\nCountry: The country where the store is located, chosen randomly.\nPostal Code: A postal code for the store, generated randomly.\nCreated At: The timestamp when the store record was created, within the last two years.\nModified At: The timestamp of the last update to the store record.\n\n\nThese datasets can be used individually or combined to simulate real-world business applications like customer relationship management, inventory tracking, and retail operations. If you need these datasets linked (e.g., linking products to stores, or contacts to purchases), additional scripting would be needed to establish these relationships within the data.","385":"# Biometric Attack Dataset, Asian People\n\n# The similar dataset that includes all ethnicities - [Anti Spoofing Real Dataset](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)\n\nThe dataset for face anti spoofing and face recognition includes images and videos of asian people. **30,600**+ photos & video of **15,300** people from **32** countries. All people presented in the dataset are  **South Asian, East Asian or Middle Asian**.  The dataset helps in enchancing the performance of the model by providing wider range of data for a specific ethnic group.\n\nThe videos were gathered by capturing faces of genuine individuals presenting spoofs, using facial presentations. Our dataset proposes a novel approach that learns and detects spoofing techniques, extracting features from the genuine facial images to prevent the capturing of such information by fake users. \n\nThe dataset contains images and videos of real humans with various **resolutions, views, and colors**, making it a comprehensive resource for researchers working on anti-spoofing technologies.\n\n### People in the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff545aa561432738d251c09f09e1f5e92%2FFrame%20104.png?generation=1713356643038606&alt=media)\n\n### Types of files in the dataset:\n- **photo** - selfie of the person \n- **video** - real video of the person \n\nOur dataset also explores the use of neural architectures, such as deep neural networks, to facilitate the identification of distinguishing patterns and textures in different regions of the face, increasing the accuracy and generalizability of the anti-spoofing models. \n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 30,600 files, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **video_extension** - video extensions in the dataset\n- **video_resolution** - video resolution in the dataset\n- **video_duration** - video duration in the dataset\n- **video_fps** - frames per second for video in the dataset\n- **photo_extension** - photo extensions in the dataset\n- **photo_resolution** - photo resolution in the dataset\n\n### Statistics for the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2F6de78d350a9213d8437f766b085d4551%2Fasian_video_liveness.png?generation=1713356627116331&alt=media)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection) to learn about the price and buy the dataset**\n\n# Content\nThe dataset consists of:\n- **files** - includes 10 folders corresponding to each person and including 1 image and 1 video,\n-  **.csv file** -  contains information about the files and people in the dataset\n\n### File with the extension .csv\n- **id**: id of the person,\n- **selfie_link**: link to access the photo,\n- **video_link**: link to access the video,\n- **age**: age of the person,\n- **country**: country of the person,\n- **gender**: gender of the person,\n- **video_extension**: video extension,\n- **video_resolution**: video resolution,\n- **video_duration**: video duration,\n- **video_fps**: frames per second for video,\n- **photo_extension**: photo extension,\n- **photo_resolution**: photo resolution\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** provides high-quality data annotation tailored to your needs\n\n*keywords: liveness detection systems, liveness detection dataset, biometric dataset, biometric data dataset, biometric system attacks, anti-spoofing dataset, face liveness detection, deep learning dataset, face spoofing database, face anti-spoofing, ibeta dataset, face anti spoofing, large-scale face anti spoofing, rich annotations anti spoofing dataset, asian people, asian classification, asian image dataset*","386":"","387":"Dataset Name: **Banknote Authentication Dataset**\n\n**Description:**\n\nThis dataset contains a collection of features extracted from images of genuine and counterfeit banknotes. It's commonly used to train and evaluate machine learning models for automated banknote authentication, aiming to distinguish real banknotes from forgeries.\n\n**Features:**\n\nEach data point represents a single banknote and includes the following features:\nVariance of Wavelet Transformed image (continuous): Measures image texture variation.\nSkewness of Wavelet Transformed image (continuous): Quantifies image asymmetry.\nCurtosis of Wavelet Transformed image (continuous): Captures image tailedness.\nEntropy of image (continuous): Reflects image randomness or information content.\nClass (categorical): Indicates whether the banknote is genuine (1) or counterfeit (0).\nNumber of Instances:\n\nThe dataset typically contains several hundred banknote images, with approximately equal proportions of genuine and counterfeit examples.\nSource:\n\nThe dataset was originally collected by researchers at the University of Applied Sciences, Ostwestfalen-Lippe, Germany.\nApplications:\n\nDevelop and evaluate machine learning models for banknote authentication.\nCompare the performance of different classification algorithms in this domain.\nExplore feature engineering techniques to improve model accuracy.\nInvestigate the effectiveness of various feature selection methods for identifying the most informative features for authentication.\nAdditional Notes:\n\nThe dataset is often used as a benchmark for classification tasks due to its balanced class distribution and relatively simple feature set.\nIt's essential to consider data preprocessing techniques (e.g., normalization, handling missing values) before model training.\nModel evaluation should involve metrics suitable for imbalanced classes if the distribution of genuine and counterfeit notes is skewed.","388":"This datasets(205k images) will be used for GAN models training.\n\nGenerative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.\n\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n\nGANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.","389":"# GENERATED Vietnamese Passports Dataset\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's *robustness and generalization*.\n\nThe dataset contains GENERATED Vietnamese passports, which are replicas of official passports but with randomly generated details, such as *name, date of birth etc*. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to buy the dataset\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to *develop and refine models using simulated passport data without risking privacy leaks*.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff2778d432611db436f18b9284daec240%2F666.png?generation=1691945421899877&alt=media)\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset\n\n# Passports might be generated in accordance with your requirements.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems*","390":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3032492%2Ff4201da2a12cae17fed7a8f5a242c78e%2F2023-07-12%20%208.33.10.png?generation=1689161628737196&alt=media)\n\n2023 Fake or Real: AI-generated Image Discrimination Competition dataset is now available on Kaggle!\n---\n\n\nHello, Kagglers\ud83d\udd90\ufe0f\n\nWe are excited to announce the release of the dataset for the 2023 Fake or Real: AI-generated Image Discrimination Competition. The competition was held on AI CONNECT(https:\/\/aiconnect.kr\/) from June 26th to July 6th, 2023, with 768 participants.\n\nIf you're interested in evaluating the performance of your model on the test dataset, we encourage you to visit the [competition page](https:\/\/aiconnect.kr\/competition\/detail\/227\/task\/295\/taskInfo) on AI CONNECT and submit your results. Please note that it supports only Korean yet. Of course we data scientists can always use Chrome translate, and\/or even better translation models\ud83e\udd73. Plus, multilingual service will be provided in the (hopefully near) future, so please stay tuned!\n\n\n# Background\nAs the advancement of generative AI technology has enabled the easy creation of indistinguishable fake information from genuine content, concerns regarding its misuse have surfaced. Image generation AI, in particular, has raised significant alarm due to its potential risks such as identity theft, revenge porn, and political manipulation. In response, it has become imperative to develop technologies that can effectively discern between real and AI-generated fake images.\n\nThe training dataset consists of diffusiondb (https:\/\/huggingface.co\/datasets\/poloclub\/diffusiondb) and Flickr images, with the inclusion of some low-quality fake images. For the test dataset, we took measures to construct it in a manner that closely resembles real-world scenarios involving image misuse. We utilized multiple generative AI models, fine-tuned on diverse photorealistic datasets, and applied negative prompt keywords like 'cartoon' and 'too many fingers' to generate realistic images.\n\nWe hope this dataset encourages the development of robust solutions and stimulates discussions on tackling the challenges associated with AI-generated fake images. \n\nBest Regards,\nAI CONNECT\n\n","391":"# GENERATED USA Passports Dataset\n\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's robustness and generalization.\n\nData augmentation techniques involve applying various transformations to existing data samples to create new ones. These transformations include: *random rotations, translations, scaling, flips, and more*. Augmentation helps in increasing the dataset size, introducing natural variations, and improving model performance by making it more invariant to specific transformations.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to buy the dataset\n\nThe dataset contains **GENERATED** USA passports, which are replicas of official passports but with randomly generated details, such as name, date of birth etc. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to develop and refine models using simulated passport data without risking privacy leaks.\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F618942%2F30c6650541e63733f9ea0fcdc3bfc2cb%2FMacBook%20Air%20-%201%20(2).png?generation=1688719414649908&alt=media)\n\n# Content\n\n### Folders\n- **original**: includes original generated images of USA passports\n- **augmentation**: contains subfolders, corresponding to the original photos and including 3 black and white generated passport scans with different photo editing.\n\nThe augmentated photos are presented with random rotations, noise and brightness. Augmentation varies depending on the amount of noise and blur in the passport images, from slight (**us_pass_augmentated_1**) to significant (**us_pass_augmentated_3**).\n\n### File with the extension .csv\n\nincludes the following information for each media file:\n\n- **original**: link to access the image of the generated passport,\n- **us_pass_augmentated_1**: link to the first augmentated image, \n- **us_pass_augmentated_2**: link to the second augmentated image, \n- **us_pass_augmentated_3**: link to the third augmentated image\n\n# USA Passeport Photos might be generated in accordance with your requirements.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems, augmentation dataset, data augmentation, augmented images, image augmentation*","392":" ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F8cdd2e3fbfb1a3e97a9ff5733068564c%2Fvending%20machine.jpg?generation=1683391775041836&alt=media)\nVending machines and money changers deal with taking in cash form a user, identifying the denomination of the cash, and detect cash's validity ( i.e is it a real or a counterfeit (fake)). Coins seem to be a major type of cash that are exchanged in these machines. The current methods of identifying the denomination of coins by the these machines commonly involves measuring the weight, size, and thickness of the coin using techniques such as light sensing, and detecting weather the coin is real or fake involves obtaining the composition of the coin to detect if it is metallic or not using techniques such as electromagnetic field transmission. These methods do work, except there are two problems:\n1. In many countries (like India), the versions of coins keep changing overtime and consequently the dimensions and compositions of the coins change too making it difficult to identify and detect coins using the afore mentioned methods.\n2. Fake coins can be made of metallic plates that resemble the size and composition of real coins, when such plates are inserted into the machine, they might get falsely detect as valid coins. \n\nOne potential way to solve these problems would be to use Machine Learning to identify the coin denomination and detect it's validity. But how? An ML model can be trained on a dataset of images of both real and fake coins of various denominations. If the images were captured with a flash, the amount of light reflected off of the surface of metallic coins would be higher compared to that of non-metallic fake coins. Also the higher reflection from metallic surface would produce regions of bright spots only in images of metallic coins.\n\nThe figure below shows some images captured with flash, taken from the dataset, showing some differences present between the images of real and fake coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F5bf4e9119046d51e7c9258eada047ae0%2FRealVsFakeCoins.jpg?generation=1683396232330130&alt=media)\n(a) & (b) are images of a real coin, showing reflected light as bright spots on the coin.\n(c) is the image of a fake non-metallic coin, not showing any apparent bright spots of light.\n(d) is fake too but is a metallic plate and thus showing spots of brightness, but lacks the presence of imprints of denomination, the rupee symbol and artistry.\n(e) is fake, made by printing the image of a real coin on a piece of paper and pasted on a plastic plate, and thus the image contains both the spots of brightness and the information and artistry similar to that of a real coin but lacks the lustrous appearance of a real coin. Also the paper surface appears rougher compared to a real coin.\n\nKeeping in mind these differences, this dataset was created with 1750 images of various real and fake coins captured with flash and the images are classified into:\n1. The front face of real\/valid Indian coins consisting the imprint of common denominations-\nRs 1, Rs 2, and Rs 5.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F3cbee0087a974d872ff3684709e9ad2d%2Freal_coins_front.png?generation=1683302664681760&alt=media)\nThe images of front face of 1 rupee coins are contained in the folder named '1_rupee', images of 2 rupees in folder named '2_rupee' and 5 rupees in '5_rupee'.\n\n2. The back or reverse face of real\/valid coins that do not show their denomination values.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F23601ab0ffde0cf1465058ea94e03f80%2Freal_coins_back.png?generation=1683302796939179&alt=media)\nThese images are contained in the folder named 'reverse'\n\n3. Fake\/Invalid coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fb4adf4b82266250237fcf61722732aa5%2Ffake_coins.png?generation=1683302878176987&alt=media)\nThese images are contained in the folder named 'invalid'\n\n\nThe images were originally captured with a 12MP camera with a resolution of around 2000x4000 pixels and were processed in OpenCV to detect the region of the coin within the image, crop it along the region and resize it to a resolution of 256x256 pixels. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fa9b0997b7704c2d679eb12db880e5f5f%2FImageProcessing.png?generation=1683394637379764&alt=media)\nThis preprocessing was done to make it easier for ML models to learn important features. The code for processing and the steps involved in the processing are described in detail in this Github repo.\n\nThus a model trained on this dataset potentially can:\n1. Classify coin denominations based on the features present in the coin images such as the denomination value and designs imprinted on the face of the coin.\n2. Further classify the coin as valid(real) or invalid(fake) based on the features such as presence or absence of information and artistry, the amount of light present, the region of bright spots present or absent in the image, among other factors.\n\n&gt;I have created a notebook in which I have built a ConvNet model to classify the coins in this dataset, you can check it out [here](https:\/\/www.kaggle.com\/code\/mssomanna\/coin-classification-using-cnn\/).\n\n&gt;I have also hosted a project on GitHub with the same dataset, notebook and other relevant items, you can check it out [here](https:\/\/github.com\/ms-somanna\/Coin-Denomination-and-Validity-Detection-using-Neural-Network-and-Coin-Images).","393":"Simplified version of Handwritten Signature datasets where it has 5 classes (PersonA...PersonE) splited into training\/ testing folders. and a `CSV files` folder containing whether its real or forged for each class in training \/ test.\nThe goal here is to train 2 models (1 for identifying the person sig. and 2 for validating that it is a real one).\nThis dataset is part of the Computer Vision course by FCIS-ASU divided into 2 main tasks :\n&gt; 1. Signature Identification : Building a classical vision or DL techniques to identify the person of this signature.\n&gt; 2. Signature Verification : Build a 5 different models to identify for each class if it is a real or fake OR build a 1 siamese network that verifies the signature of the person whether it is a real one or fake one.","394":"# CIFAKE: Real and AI-Generated Synthetic Images\nThe quality of AI-generated images has rapidly increased, leading to concerns of authenticity and trustworthiness.\n\nCIFAKE is a dataset that contains 60,000 synthetically-generated images and 60,000 real images (collected from CIFAR-10). Can computer vision techniques be used to detect when an image is real or has been generated by AI?\n\nFurther information on this dataset can be found here: [Bird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290)\n\n## Dataset details\nThe dataset contains two classes - REAL and FAKE. \n\nFor REAL, we collected the images from Krizhevsky & Hinton's [CIFAR-10 dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)\n\nFor the FAKE images, we generated the equivalent of CIFAR-10 with Stable Diffusion version 1.4\n\nThere are 100,000 images for training (50k per class) and 20,000 for testing (10k per class)\n\n## Papers with Code\nThe dataset and all studies using it are linked using [Papers with Code](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n[https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n\n\n## References\nIf you use this dataset, you **must** cite the following sources\n\n[Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images.](https:\/\/www.cs.toronto.edu\/~kriz\/learning-features-2009-TR.pdfl)\n\nBird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.\n\nReal images are from Krizhevsky & Hinton (2009), fake images are from Bird & Lotfi (2024). The Bird & Lotfi study is available [here](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290).\n\n## Notes\n\nThe updates to the dataset on the 28th of March 2023 did not change anything; the file formats \".jpeg\" were renamed \".jpg\" and the root folder was uploaded to meet Kaggle's usability requirements.\n\n## License\nThis dataset is published under the [same MIT license as CIFAR-10](https:\/\/github.com\/wichtounet\/cifar-10\/blob\/master\/LICENSE):\n\n*Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:*\n\n*The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.*\n\n*THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.*","395":"A Dataset comprised of two parts, images generated by AI image generation models such as DALL-E and Midjourney, and real images known to be made by humans. The majority of AI generated images are artistic works of some type and not photorealistic because it was found that having more artistic works than photos in the human generated set yielded better test results. One major issue found when trying to train classifiers on this set is while a test accuracy as high as 94% was achieved, if the image (regardless of source AI or human) contained noise such as a film grain or fur there was a higher error rate and the image was more likely to be mislabeled as AI generated. My theory is because diffusion image generation models (DALL-E etc.) start with random noise and turn it into an image based on the prompt, so the classifier could be using the noise of the image as a way to detect Ai generated art and by adding noise the model is getting confused. One possible solution to this is using image denoising on the image or edge detection however I have yet to test either. \n\n\n## Benefits over other datasets\n\nThe benefit of this dataset compared to other artificially generated image datasets (such as CIFAKE) is that all images are in there original size and aspect ratio.\n\n","396":"Problem statement\n\nIn verification services related to face recognition (such as eKYC and face access control), the key question is whether the input face video is real (from a live person present at the point of capture), or fake (from a spoof artifact or lifeless body). Liveness detection is the AI problem to answer that question.\n\nIn this challenge, participants will build a liveness detection model to classify if a given facial video is real or spoofed.\n\n- Input: a video of selfie\/portrait face with a length of 1-5 seconds (you can use any frames you like).\n\n- Output: Liveness score in [0...1] (0 = Fake, 1 = Real).\n\nExample:\n\n- Input: VideoID.mp4\n\n- Output: Predict.csv\n\nfname, liveness_score\nVideoID.mp4, 0.10372\n...\t...\n...\t...","397":"Dataset is consist of 3 different kinds of gemstones\n1. Ruby\n2. Turquoise\n3. Emerald\n\nThe purpose to collect the dataset was to make a model with Convolutional Neural Network Algorithm to identify real or fake gemstones.\nThe Dataset is split into three folders\n1. Train for training the model \n2. Test for Testing the model\n3. Validation for Validating the model\nEach Folders have six subfolders:\n1. Emerald ~ 507 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n2. Fake Emerald ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n3. Ruby ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n4. Fake Ruby ~ 536 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n5. Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n6. Fake Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n\nTotal of 6043 Images are in this dataset. It can be used for Classification Problem (Binary and Multiple). I didn't used any prebuild model I suggest you to use pre-trained model with it to increase the accuracy. The highest Accuracy I achieved is 95% and Best of luck for you...","398":"**About Dataset**\nThis dataset contains real and fake images of human faces.\nReal and Fake Face Detection\nFake Face Photos by Photoshop Experts\nIntroduction\nWhen using social networks, have you ever encountered a 'fake identity'?\nAnyone can create a fake profile image using image editing tools, or even using deep learning based generators.\nIf you are interested in making the world wide web a better place by recognizing such fake faces, you should check this dataset.","399":"### Context\n\nData collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.\n\nWith high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. \n\nAs an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.\n\n### Content\n\nThe dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.\n\nThe data has been generated using the Python scripts and .blend file present in [this repository](https:\/\/github.com\/TheFamousRat\/ChessR). The chess board and pieces models that have been used for those renders are not provided with the code.\n\nData characteristics :\n\n- Images : 1280x1280 JPEG images representing pictures of chess game boards.\n- Annotations : JSON files containing two variables : \n    - \"config\", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.\n    - \"corners\", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.\n- config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : \n    - \"cellsCoordinates\", a dictionary associating a cell name to its coordinates on the board. We have for example {\"A1\" : [0,0], \"A2\" : [1,0], ...}\n    - \"piecesTypes\", a list of strings containing the types of pieces present in the renders.\n\nNo distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.\nA proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.\n\n### Acknowledgements\n\nI would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. \n\n### Inspiration\n\nTwo interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : \n\nErroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt (2021) *Fake It Till You Make It: Face analysis in the wild using synthetic data alone* https:\/\/arxiv.org\/abs\/2109.15102\nSalehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) *PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision* https:\/\/arxiv.org\/abs\/2112.09290","400":"This dataset contains manipulated images and real images. The manipulated images are the faces which are created by various means. The source for this dataset was [https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd](https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd)\nthis dataset was processed as our will to get maximum outcome out of these images. Each image is a 256 X 256 jpg image of human face either real or fake","401":"### Context\n\nData on the Israeli Knesset from the first Knesset to the 24th. \n\n\n\n### Content\n\nFor Knesset 1-24 this contains:\n- Names of all politicians\n- The Party they are affiliated with\n- The number(s) of parliaments each politician was active in \n- Gender\n- Place of Birth\n- Place of Death\n- image link\n- date of birth\n- primary language \netc.\n\n\nSource:\nhttps:\/\/main.knesset.gov.il\/mk\/Pages\/current.aspx?pg=mklist\nWikipedia\nWikidata\n\n### Inspiration\nDuring 2019-2020 isreal was stuck in a political deadlock with election following election. This contribution was done in the hope to allow easier access to political data which will enable verifying news as fake or real news. ","402":"This dataset is extracted from https:\/\/www.indiansuperleague.com on 6\/25\/2024 at 5:15:00 PM  It contains data of Indian players under the age of 25 for the ISL 2023-2024 season. This dataset aims to find players who can play for India in the upcoming years to qualify for the World Cup.","403":"This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures","404":"This dataset contains detailed statistics on soccer players for the 2023-2024 season, sourced from FBRef. FBRef is a renowned source for soccer statistics and analysis, providing comprehensive and accurate data on players and teams from various leagues and international competitions. The data was extracted using web scraping techniques. \nFor more information, please refer to the GitHub [repository](https:\/\/github.com\/dyomed93\/Scraping-Analysis-FbRef).","405":"#**Description**\nThis dataset contains detailed information on players participating in the Super Lig for the 2023-2024 season. It includes players' numbers, teams, positions, preferred foot, attacking (ATT), technique (TEC), tactical (TAC), defense (DEF), creativity (CRE) skill ratings, market values, and nationalities. With a total of 630 rows, this dataset is a valuable resource for analysis and research purposes.\n\n#**Subtitles**\n**Number:**Player's jersey number\n**Team:** The team the player is playing for\n**Position:** The position the player is playing (e.g., forward, midfielder, defender)\n**Preferred Foot:** The player's dominant foot (right, left)\n**ATT:** Attacking skill rating\n**TEC:** Technique skill rating\n**TAC:** Tactical skill rating\n**DEF:** Defense skill rating\n**CRE:** Creativity skill rating\n**Market Value:**The market value of the player (in million \u20ac)\n**Nationality:** The player's nationality\n\n#**File Information**\n**File Name:** Super Lig 2023-2024.csv\n**File Size:** 39.68KB\n**Format:** CSV (Comma-Separated Values)\n\n#**Author**\nThis dataset has been compiled to provide detailed information on players in the Super Lig for the 2023-2024 season for analysis and research purposes.","406":"**Comprehensive Football Player Statistics: 2023-2024 Season**\nThis dataset contains detailed player statistics from top football leagues for the 2023-2024 season. Sourced from FBref, the dataset includes a wide range of metrics covering various aspects of player performance, such as defense, goalkeeping, passing, and shooting.\n\n**Key Features**\nDetailed Player Metrics: Statistics for individual players across multiple performance areas.\nStructured Data: Organized into tables focusing on different aspects of the game for easy analysis.\nTop Leagues: Includes data from prominent leagues that provide comprehensive detailed stats.\n\nGithub Repository link of the project : https:\/\/github.com\/GuechtouliAnis\/Football-Data-Scraping\n\nBy: Guechtouli Anis","407":"Dataset of all the players that are in the squad of the teams participating in the UEFA EURO 2024. Contains info about clubs, age, height, market value etc. which can be very good for EDA and Data Visualizations.","408":"All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions","409":"**Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n","410":"Context-\nThe data was created from the IPL auction stats displaying funds and players\nSource-\nThe data was scrapped from https:\/\/www.iplt20.com\/auction\/2021 \nAbout-\nEight franchises buy 57 players at IPL 2021 Player Auction The VIVO IPL 2021 Player Auction concluded in Chennai with 57 players filling up the 61 available slots. The eight franchises went for some of the finest talent available, giving them a chance to unleash their potential and make an impact in the upcoming edition of the tournament.","411":"This dataset contains detailed information on 3,060 unique football\/soccer players. Each entry provides a comprehensive profile of a player, including personal details, overall performance ratings, specific skills, and various attributes. The dataset is structured to offer insights into player capabilities, potential, and market value. \n\nBelow is a detailed description of the columns included in the dataset:\n\n**ID:** Unique identifier for each player.\n**Age:** Age of the player in years.\n**Height:** Height of the player in centimeters.\n**Weight:** Weight of the player in kilograms.\n**Preferred foot:** The preferred foot of the player (e.g., Left, Right).\n**Overall rating:** The current overall rating of the player, reflecting their performance level.\n**Potential:** The potential rating indicating the highest possible overall rating a player can achieve.\n**Best overall:** The highest overall rating achieved in any position by the player.\n**Best position:** The position where the player performs best.\n**Growth:** The difference between the player's current overall rating and their potential.\n**Value:** The market value of the player, usually in currency (e.g., Euros).\n**Wage:** The weekly wage of the player.\n**Release clause:** The release clause value in the player's contract.\n**Total attacking:** The total score of all attacking attributes combined.\n**Crossing:** Ability to accurately deliver the ball from wide areas.\n**Finishing:** Ability to score goals in one-on-one situations.\n**Heading accuracy:** Accuracy of headers when trying to score or pass the ball.\n**Short passing:** Precision and accuracy of short-distance passes.\n**Volleys:** Technique and power in volley shots.\n**Total skill:** The combined score of all skill-related attributes.\n**Dribbling:** Ability to maintain control of the ball while maneuvering around opponents.\n**Curve:** Ability to curve the ball, typically used in shots and passes.\n**FK Accuracy:** Accuracy in taking free kicks.\n**Long passing:** Precision and accuracy of long-distance passes.\n**Ball control:** Ability to control and manipulate the ball effectively.\n**Total movement:** The combined score of all movement-related attributes.\n**Acceleration:** Quickness of reaching top speed.\n**Sprint speed:** Top speed a player can achieve.\n**Agility:** Ability to quickly change direction and position.\n**Reactions:** Quickness of responding to situations during the game.\n**Balance:** Stability and coordination while moving or under pressure.\n**Total power:** The combined score of all power-related attributes.\n**Shot power:** Strength and power in shots on goal.\n**Jumping:** Ability to jump vertically.\n**Stamina:** Endurance and ability to sustain performance over time.\n**Strength:** Physical strength and ability to win physical duels.\n**Long shots:** Accuracy and power in long-distance shots.\n**Total mentality:** The combined score of all mentality-related attributes.\n**Aggression:** Level of intensity and physicality in play.\n**Interceptions:** Ability to intercept passes and disrupt the opposition's play.\n**Att. Position:** Positioning intelligence when attacking.\n**Vision:** Ability to see and execute key passes and plays.\n**Penalties:** Ability to score from penalty kicks.\n**Composure:** Calmness and performance under pressure.\n**Total defending:** The combined score of all defending-related attributes.\n**Defensive awareness:** Understanding and anticipation of defensive responsibilities.\n**Standing tackle:** Effectiveness in making standing tackles.\n**Sliding tackle:** Effectiveness in making sliding tackles.\n**Total goalkeeping:** The combined score of all goalkeeping attributes.\n**GK Diving:** Ability to dive and save shots.\n**GK Handling:** Ability to catch and control the ball.\n**GK Kicking:** Accuracy and power of goal kicks.\n**GK Positioning:** Positioning intelligence in goalkeeping situations.\n**GK Reflexes:** Quickness of reflexes to make saves.\n**Total stats:** The overall total of all individual attribute scores.\n**Base stats:** The base statistics without modifiers or bonuses.\n**International reputation:** Reputation of the player on the international stage (e.g., 1 to 5 stars).\n**Pace \/ Diving:** Combined score of Pace attributes for outfield players or Diving for goalkeepers.\n**Shooting \/ Handling:** Combined score of Shooting attributes for outfield players or Handling for goalkeepers.\n**Passing \/ Kicking:** Combined score of Passing attributes for outfield players or Kicking for goalkeepers.\n**Dribbling \/ Reflexes:** Combined score of Dribbling attributes for outfield players or Reflexes for goalkeepers.\n**Defending \/ Pace:** Combined score of Defending attributes for outfield players or Pace for goalkeepers.\n\nThis dataset offers a rich resource for analyzing player performance, potential, and market value in the world of football\/soccer. It can be utilized for various purposes, including player scouting, performance analysis, and strategic planning for team management. ","412":"All statistics sourced from FBRef and Transfermarkt and scraped using Ruby on Rails. This dataset contains 3 CSVs, \"Teams\" with basic info (just name and FBRef link), \"Players\" with name and summary statistics for the 2023-2024 season as well as their age and value from Transfermarkt, and \"Match Logs\" which has in-depth information for every Premier League game the player's team took part in over the past 2 seasons (or whatever length of time the player was on said team).","413":"**Dataset will be updated during EURO 2024**<br>\n**You may support this project via the upvotes button**\n\n## Description \nThe dataset contains all players & coaches, all matches & results, and main match events in Football\/Soccer UEFA European Championship\/EURO (1960-2024), and Nations League (2019-2023).\n\n* `matches`:\n * `euro`: `1960.csv` ...  `2024.csv` -  all EURO matches.\n * `nations`: `2019.csv`, `2021.csv`, `2023.csv` - all Nations League matches.\n * `friendly_2021-2024.csv` - all friendly matches from 2021 to 2024.\n * `qualifying_1960-2024.csv` - all qualifying matches from 1960 to 2024.\n* `logos` - contains flags of associations\n*  `euro_coaches.csv` - all coaches from 1960 to 2024\n* `euro_lineups.csv` - all players from 1960 to 2024\n* `euro_summary.csv` - table includes basic information about each EURO.\n\n## Related Datasets\n- [Football - FIFA Men's World Cup, 1930 - 2022](https:\/\/www.kaggle.com\/datasets\/piterfm\/fifa-football-world-cup)\n- [Football - FIFA Women's World Cup, 1991 - 2023](https:\/\/www.kaggle.com\/datasets\/piterfm\/football-fifa-womens-world-cup-1991-2023)\n\n## Table columns\n\n## Updates\n2024-06-16 - add lineups & coaches; add associans flags folder.<br>\n2024-06-09 - add friendly, qualifying & Nations League matches.","414":"","415":"","416":"This dataset is collected from a simulated save file of the game Football Manager 23. \n\nNorthampton FC has clawed their way upto the second tier of English football, the Championship in 2030-31 season. After a rough season and nearly escaping relagation, they are focusing heavily on their player recruitment to become an established Championship team. \n\nNorthampton finished 21st in the table, securing only 46 points with a goal difference of -21. Their lack of goalscoring ability upfront and leaky defense at the back has made the club rethink their recruitment strategy.\n\nFrom the list of 422 scouted players, they're actively looking for :\n\n- A clinical striker with a keen eye for goal.\n- A creative midfielder to provide high-quality chances\n- A hard working, physical midfielder to provide cover for the creative midfielder.\n- An aerially dominant Center-back who is also sound in defense. \n- A wingback who covers a lot of ground and dishes out quality crosses inside the final third.\n\nTo accomodate the signings, the Northampton FC board has allowed an extra wage budget of 60k\/week.\n\nYour job as a manager is to find the best recruitments within the wage budget.","417":"2024 NWSL (so far) complete player stats. The dataset will be updated monthly throughout the season as the season progresses. \n\nThe data is mostly scraped from NWSL.com. It has been cleaned, and additional information has been added, such as position.\n\nThe current file is scraped for the current 2024 season through May 25, 2024, and will be updated monthly with updated information as NWSL posts updates.\n\nThere is currently data for 344 players across 40 variables. \n\nHere is the type of information included:\n\n- team\n- player_name\n- position\n- games_played\n- games_started\n- minutes_played\n- goals\n- accurate_pass_percentage\n- assists\n- total_scoring_attempts\n- on_target_scoring_attempts\n- total_attacking_assists\n- tackles\n- fouls_committed\n- fouls_suffered\n- total_offside\n- yellow_cards\n- red_cards\n- accurate_passes\n- total_passes\n- crosses\n- assists_avg_over_90_mins\n- long_balls\n- successful_short_passes\n- turnovers\n- goals_avg_over_90_mins\n- penalty_kick_goals\n- penalty_kick_taken\n- penalty_kick_percentage\n- accurate_shooting_percentage\n- successful_dribble\n- dribble_percentage\n- goals_and_assists\n- tackles\n- tackles_percentage\n- interceptions\n- headed_duel\n- gk_saves\n- gk_long_ball_percentage\n- gk_total_clearance\n\nUse the data to make interesting visualizations, make your own predictions for the season, power your app, do your own analysis, have fun! Just an NWSL superfan here making information available for you. Please message me for any other ideas or questions.\n","418":"For the Pakistan Super League (PSL) Season 9 dataset, you have a couple of good options:\n\n1. Kaggle: You can find a comprehensive dataset for PSL Season 9 on Kaggle. This dataset includes detailed information on matches, players, scores, and other relevant statistics. It's a great resource for analysis and research related to the 2024 season of the PSL. You can access it [here](https:\/\/www.kaggle.com\/datasets\/umerhaddii\/psl-season-9-complete-dataset-2024)\u30105\u2020source\u3011.\n\n2. Open Data Pakistan: Another source for PSL data, including historical data from previous seasons, is available on Open Data Pakistan. This dataset contains extensive records collected from various seasons, including detailed performance metrics. It's available [here](https:\/\/opendata.com.pk\/dataset\/pakistan-super-league-datasets)\u30106\u2020source\u3011\u30107\u2020source\u3011.\n\n","419":"**# Background**\n\n**The Pakistan Super League (PSL)** is a professional Twenty20 cricket league in Pakistan, founded by the Pakistan Cricket Board (PCB) in 2015. The league was conceived to enhance the cricketing talent pool in Pakistan and provide a platform for local players to compete with international stars.\n**Inception and Early Years (2016-2017):**\nThe inaugural season took place in February 2016, with five franchises representing major cities: Islamabad United, Karachi Kings, Lahore Qalandars, Peshawar Zalmi, and Quetta Gladiators. Matches were held in the United Arab Emirates due to security concerns in Pakistan. Islamabad United won the first season, defeating Quetta Gladiators in the final.\n**Expansion and Domestic Growth (2018-2019):**\nThe PSL expanded in 2018 with the addition of a sixth team, Multan Sultans. This period marked a significant effort by the PCB to bring matches back to Pakistan. In 2018, the final was held in Karachi, and by 2019, more matches, including the playoffs and the final, were held in various Pakistani cities, signaling the return of international cricket to the country.\n**Consolidation and Popularity (2020-Present):**\nBy 2020, the entire PSL season was held in Pakistan for the first time. This move bolstered local support and further integrated the league into the national sports culture. The league continued to attract international players, enhancing its competitiveness and entertainment value.\n**Future Expansion (Post PSL-10):**\nThe PSL has continued to thrive, and after the tenth season, the league is set to expand further. Plans are in place to introduce two additional franchises, increasing the number of teams from six to eight. This expansion aims to bring in more regional representation, further develop cricketing talent, and enhance the league's competitiveness.\n**Impact and Future:**\nThe PSL has significantly impacted Pakistan's cricket, offering a lucrative career path for players and improving the country's cricketing infrastructure. It has also played a crucial role in the revival of international cricket in Pakistan. The league's success has paved the way for potential future expansions and innovations.\n\nOverall, the PSL has grown rapidly since its inception, becoming a prominent fixture in the global cricket calendar and contributing to the resurgence of cricket in Pakistan. The planned expansion after PSL-10 underscores the league's commitment to growth and development in the sport.\nOne of the hallmarks of the PSL is its competitive nature, featuring a dynamic mix of local Pakistani players and international cricket stars. This blend of talent not only raises the level of competition but also enhances the global appeal of the league. Over the years, the PSL has grown into a significant fixture on the cricketing calendar, celebrated for its thrilling matches and the high-quality cricket on display.\n\u2003\n## Gratified\nSeason 8 and Season 9 of the Pakistan Super League (PSL) were captivating cricketing spectacles that highlighted the blend of local and international talent. \n**Season 8 of the Pakistan Super League** played from February 13 to March 19, 2023, captivating fans with its intense matches and high-level performances. The tournament was hosted across multiple cities in Pakistan, showcasing the country's passion for cricket. Lahore Qalandars emerged victorious, clinching their second title by defeating Multan Sultans in a thrilling final. The season was marked by remarkable individual performances, with standout players like Fakhar Zaman and Rashid Khan making significant contributions.\n**Season 9 of the Pakistan Super League** followed, held from February 17 to March 18, 2024. This season continued to build on the excitement and competitive spirit of the PSL, with matches hosted in four cities across Pakistan. Islamabad United secured their third title by triumphing over Multan Sultans in a nail-biting final, underscoring their dominance in the league. The tournament featured 34 matches filled with outstanding performances from both local and international stars. Notable players like Babar Azam and Usama Mir grabbed headlines with their exceptional skills and match-winning contributions.\nFor Season 9, a comprehensive dataset is available, containing detailed summaries of each match as well as ball-by-ball information. This data provides a granular view of the performances and dynamics of the season, offering valuable insights into the intricacies of the game.\n\n**Venues: **Karachi, Lahore, Multan and Rawalpindi\n**Time-period:** 9 February to 19 March 2023 (Season 8) and 17 February to 18 March 2024  (Season 9)  \u2003\n\n## T20 Dataset from Cricsheet.org\nThe T20 dataset available on Cricsheet.org is a comprehensive and detailed collection of cricket match data, focusing specifically on the T20 format. This dataset is a valuable resource for cricket analysts, data scientists, and enthusiasts interested in exploring the intricacies of T20 cricket. The data encompasses a wide range of match details, including player performances, match outcomes, and ball-by-ball events, offering a granular view of the dynamics of each game.\nThis dataset opens up numerous possibilities for data-driven insights and advanced cricket analytics, making it an essential asset for anyone looking to explore the fast-paced world of T20 cricket through a data-centric lens.\nThis dataset is also good source for Data Science students to practice.\n\n","420":"## Context##\n\nThis dataset is collected from the simulated season of 2030-31 in Championship in the game Football Manager 23. \n\nWebscrapping is cool and all. But most of the time, you don't get much from the extracted data from sites like FBref, Opta, and so on. \n\nBut if you're a Football Manager player, you can generate your own data by simulating a full season in the game. And that's exactly what i did.\n\n---\n\nP.S Note: This dataset consists of players in the 2030-31 season. So most of the older players have already retired by now and you'll also stumble upon a lot of newly generated young players in the teams.\n\nHere's all the columns:\n\n---\n\nName  --- Player Name\nNat --- Nationality\nClub --- Playing Club\nPosition --- Playing Position\nAge --- Player Age\nHeight --- Player Height(Feet'inches\")\nPreferred Foot --- Strong foot of the player\nApps --- Appearances\/ Matches played\nStarts --- Starting appearance\nMins --- Minuttes\nGls --- Goals\nAst --- Assists\nWage --- Player Wages\nTransfer_Value --- Estimated transfer value of player\nPens --- Penalties Attempted\nPens_S --- Penalties Scored\nPas_A --- Passes Attempted\nPas_% --- Pass completion rate\nPr_Passes --- Progressive Passes\nPres_A --- Press attempted\nPress_C --- Press completed\nBlk --- Block\nShts Blckd --- Shots Blocked\nClear --- Clearance\nHdrs --- Headers\nItc --- Interception\nTck_A --- Tackles attempted\nTck_W --- Tackled Won\nOff --- Offsides\nGl_Mst --- Mistakes leading to Goal\nK_Tck --- Key Tackles\nDistance --- Distance Covered\nDrb --- Dribbles made\nCr_A --- Crosses attempted\nCr_C% --- Cross Completion Ratio\nShots --- Shots taken\nShts_on_target --- Shots on target\nK_Pas --- Key Passes\nYel --- Yellow Cards\nRed --- Red Cards\nxG --- Expected Goals\nxA --- Expected Assists\n\n---\n**Some of you might get the UTF-8 error(UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte) while defining the dataframe. \n\nIn that case, simply add ''encoding =('ISO-8859-1'),low_memory =False'', after calling the location.\n\nExample:\n\ndf = pd.read_csv('\/kaggle\/input\/football-manager-23-championship-3031-dataset\/Championship_2030-31 Simulation data.csv', encoding =('ISO-8859-1'),low_memory =False)\ndf.head()**\n\n---\n\nThe dataset is not cleaned AT ALL. I'll try to clean it up asap. Until then, it's going to be a bit messy to deal with it. Cheers!","421":"2023 complete and comprehensive player stats for 347 NWSL athletes\/players.\nData comes from NWSL.com. Data fields have been cleaned, and additional fields, like player position, have been added for NWSL data science fun.\n\nComplete stats include:\n- Attacking\n- Passing\n- Goals\n- Defending\n- PKs\n- Goalkeeping\n\nand more. 38 variables included. \n\nI have left NaNs in the dataset instead of filling them with 0s because 0 is a meaningful stat in this dataset. NaNs means there was no data available. For example, goalkeeping saves, 0 is a meaningful statistical number. But you can drop and clean NaNs for your own purposes on your own.\n\nPercentages are in whole numbers.\n\n2024 season stats and stadium attendance stats will be added soon. \n\nMessage me for other women's sports ideas and variables to include. ","422":"This dataset contains football match data from the English Premier League for the seasons 2008-2024. Each file includes detailed match statistics, including home and away scores, halftime scores, and various other match details.\n","423":"This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures","424":"These files contain data about football players in Europe's most popular leagues between 2017-2024. There are a wide variety of attributes, the names of teams and players have been properly formatted. If you are unsure what a column is, please visit fbref.com and find the relevant section for a better description of each column. \n\nI used this data to build a Euro 2024 Match predictor (Spoiler: Germany and France Final) using Random Forest Regressor, a machine learning algorithm and I would greatly appreciate any feedback on the project.\nProject link:\nhttps:\/\/github.com\/GurpreetSDeol\/Euro-2024-Match-Predictor-\/tree\/main","425":"","426":"The English Women's Football (EWF) Database is an open database of matches played in the top tiers of women's football in England. It covers all matches played since the 2011 season for the highest division (the Women's Super League) and since the 2014 season for the second-highest division (the Women's Championship).\n\nThe database contains three datasets:\n\n- ewf_matches contains all matches that have been played and has one observation per match per season.\n- ewf_appearances contains all appearances by a team and has one observation per team per match per season.\n- ewf_standings contains all end-of-the-season division tables and has one observation per team per season.\n\nAll three datasets will be updated with the latest information at the end of each season.\n\nSource: The English Women's Football (EWF) Database, May 2024, https:\/\/github.com\/probjects\/ewf-database.\n\nInspiration has been taken from the Fjelstul English Football Database, a similarly structured dataset that covers men's professional football since 1888.\n\n### Data dictionary\n\n#### ewf_matches\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| home_team_id| The unique ID for the home team. Has the format `T-###-T`.|\n| home_team_name| The name of the home team at the match.|\n| away_team_id| The unique ID for the away team. Has the format `T-###-T`.|\n| away_team_name| The name of the away team at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team_score| The score of the home team.|\n| away_team_score| The score of the away team.|\n| home_team_score_margin| The score margin for the home team, equal to home_team_score minus away_team_score.|\n| away_team_score_margin| The score margin for the away team, equal to away_team_score minus home_team_score.|\n| home_team_win| Whether the home team won the match. Possible values are `1` if the home team won the match and `0` otherwise.|\n| away_team_win| Whether the away team won the match. Possible values are `1` if the away team won the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. Possible values are `1` if the match ended in a draw and `0` otherwise.|\n| result| The result of the match. Possible values are `Home team win`, `Away team win`, and `Draw`.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n\n#### ewf_appearances\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team. References `match_id` in the `ewf_matches` dataset.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team at the match.|\n| opponent_id| The unique ID for the team\u2019s opponent. Has the format `T-###-T`.|\n| opponent_name| The name of the team\u2019s opponent at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team| Whether the team was the home team. Possible value are `1` if the team was the home team and `0` otherwise.|\n| away_team| Whether the team was the away team. Possible value are `1` if the team was the away team and `0` otherwise.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| result| The result of the match. Possible values are `Win`, `Loss`, and `Draw`.|\n| win| Whether the team won the match. The possible values are `1` if the team won the match and `0` otherwise.|\n| loss| Whether the team lost the match. The possible values are `1` if the team lost the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. The possible values are `1` if the match ended in a draw and `0` otherwise.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n| points| The number of points the team earned from the match. A team earns `0` points for a loss, `1` point for a draw, or `3` points for a win.|\n\n#### ewf_standings\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| position | The team's final position in the season.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team during the season.|\n| played| The number of matches that the team played.|\n| wins| The number of matches that the team won.|\n| draws| The number of matches that the team drew.|\n| losses| The number of matches that the team lost.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| points| The number of points that the team earned over the whole season (after applying `point_adjustment`).|\n| point_adjustment| The number of points that were deducted by the league due to violations of rules or added by the league due to forfeits.|\n| season_outcome| The outcome for the team following the season. This variable is included to track the movement of teams across seasons more easily. Possible values are `Club folded`, `No change` for when the team remains in their current tier, `Promoted to tier 1` for when the team moves into tier 1 from a lower tier, `Relegated to tier 2` for when the team moves into tier 2 from a higher tier, and `Relegated to tier 3` for when the team moves into tier 3 from a higher tier.|","427":"All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions","428":"**Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n","429":"This dataset contains all the event-by-event actions of World Cup 2022 final match between Argentina vs France. Collected from Statsbomb open data : [https:\/\/github.com\/statsbomb\/open-data](url)\n\nThe dataset contains a total of 4407 recorded events(rows) and 120 columns.\n","430":"The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data.","431":"This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information","432":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","433":"### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv)","434":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","435":"Dataset of Job descriptions and relevant IT skills, soft skills, education and experience required for the job. \nHard skills include IT keywords from job descriptions. It can be used to train LLM models to extract skills from various IT specific job descriptions","436":"Contains a small dataset of labeled IT skills, IT Tools, IT technologies to train named entity recognition (NER) model for extracting skills from job description using spaCy. it will be updated weekly to increase the number of rows.","437":"This Dataset is a collection of popular dark patterns from different E-Commerce and Online store sites. It can be utilized by creating accurate ML Model to detect Dark Patterns in Online Stores and E-Commerces.","438":"## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n```","439":"Did We Solve the Problem?\nThe objective of this analysis was to predict high streaming counts on Spotify and perform a detailed cluster analysis to understand user behavior. Here\u2019s a summary of how we addressed each part of the objective:\n\nPrediction of High Streaming Counts:\n\nImplemented Multiple Models: We utilized several machine learning models including Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN).\nComparison and Evaluation: These models were evaluated based on classification metrics like accuracy, precision, recall, and F1-score. The Gradient Boosting and Random Forest models were found to be the most effective in predicting high streaming counts.\nCluster Analysis:\n\nK-means Clustering: We applied K-means clustering to segment users into three clusters based on their listening behavior.\nDetailed Characterization: Each cluster was analyzed to understand the distinct characteristics, such as average playtime, skip rate, offline usage, and shuffle usage.\nVisualizations: Histograms and scatter plots were used to visualize the distributions and relationships within each cluster.\nResults and Insights\nEffective Models: The Gradient Boosting and Random Forest models provided the highest accuracy and balanced performance for predicting high streaming counts.\nUser Segmentation: The cluster analysis revealed three distinct user segments:\nCluster 1: Users with longer playtimes and lower skip rates.\nCluster 2: Users with moderate playtimes and skip rates.\nCluster 3: Users with shorter playtimes and higher skip rates.\nThese insights can be leveraged for targeted marketing, personalized recommendations, and improving user engagement on Spotify.\n\nConclusion\nYes, we solved the problem. We successfully predicted high streaming counts using effective machine learning models and provided a detailed cluster analysis to understand user behavior. The analysis offers valuable insights for enhancing Spotify\u2019s recommendation system and user experience.","440":"The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data.","441":"This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information","442":"**Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation},","443":"I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n","444":"## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media)","445":"### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv)","446":"","447":"A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user. ","448":"**ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","449":"**ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","450":"**ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |","451":"**ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n ","452":"This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit.","453":"","454":"Dataset created for training a neural network that can recognize four-player chess pieces. The dataset consists of photos taken based on the expected design, created with a 3D printer, for easy and straightforward recognition.","455":"","456":"The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\""},"text":{"0":" IMDB Top 1000 Movies Dataset \"Release Year, Duration, Ratings, Metascores, Vote Counts, and Plot Summaries\" This dataset provides comprehensive information about top-rated movies from IMDB. It includes the title of each movie, the year it was released, and its duration in hours and minutes. The dataset also provides the IMDB rating and the number of votes the movie has received on the platform. Additionally, it features the Metascore from Metacritic, indicating the critical reception of each movie. The content rating, which indicates the appropriate audience for the movie, is also included. Finally, a brief plot summary is provided for each movie, offering an overview of the storyline. This dataset is ideal for movie enthusiasts, researchers, and data analysts interested in exploring and analyzing the characteristics of top-rated movies.\n\n\n\n\n\n\n movies and tv shows","1":" IMDB top 1000 dataset   movies and tv shows","2":" Imdb Movie Dataset from year 1893 to 2020  The IMDb movie dataset and IMDb rating dataset provide comprehensive information about movies listed in the IMDb database along with their ratings. Here's a brief description of each:\n\n1. IMDb Movie Dataset:\n   - This dataset contains detailed information about movies, including their titles, original titles, release years, publication dates, genres, durations, countries of production, languages, directors, writers, production companies, actors, descriptions, average votes, total votes, budgets, gross incomes (both in the USA and worldwide), metascores, and reviews from users and critics.\n   - Each row represents a unique movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset offers insights into various aspects of movies, such as their production details, cast and crew, plot summaries, and reception through ratings and reviews.\n\n2. IMDb Rating Dataset:\n   - This dataset provides ratings and voting statistics for movies listed in the IMDb database.\n   - It includes information such as IMDb title IDs, weighted average votes, total votes, mean and median votes, distribution of votes across different rating levels (from 1 to 10), and average votes and votes counts segmented by gender and age groups.\n   - Additionally, it offers ratings from specific voter groups, such as top 1000 voters, US voters, and non-US voters.\n   - Each row corresponds to a movie entry in the IMDb database, identified by its IMDb title ID.\n   - The dataset enables analyses of movie ratings across different demographics and provides insights into the audience's perception and reception of movies listed on IMDb.\n\nIMDB Movie Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. title: Title of the movie.\n3. original_title: Original title of the movie (may differ from the localized title).\n4. year: Year of release of the movie.\n5. date_published: Date when the movie was published or released.\n6. genre: Genre(s) to which the movie belongs.\n7. duration: Duration of the movie in minutes.\n8. country: Country or countries where the movie was produced or filmed.\n9. language: Language(s) spoken in the movie.\n10. director: Director(s) of the movie.\n11. writer: Writer(s) of the screenplay or story for the movie.\n12. production_company: Production company or companies involved in producing the movie.\n13. actors: Main actors or cast members of the movie.\n14. description: Brief description or summary of the movie's plot or storyline.\n15. avg_vote: Average rating or vote score given to the movie by IMDb users.\n16. votes: Total number of votes received by the movie on IMDb.\n17. budget: Budget allocated for producing the movie.\n18. usa_gross_income: Gross income or revenue generated from the movie's release in the United States.\n19. worlwide_gross_income: Gross income or revenue generated from the movie's release worldwide.\n20. metascore: Metascore rating assigned to the movie by critics (if available).\n21. reviews_from_users: Number of user reviews or ratings submitted for the movie.\n22. reviews_from_critics: Number of reviews or ratings given by critics for the movie.\n\nIMDB Rating Dataset Description:\n1. imdb_title_id: Unique identifier for each movie in the IMDb database.\n2. weighted_average_vote: Weighted average rating or vote score given to the movie.\n3. total_votes: Total number of votes received by the movie.\n4. mean_vote: Mean or average vote score given to the movie.\n5. median_vote: Median vote score given to the movie.\n6. votes_10: Number of votes rating the movie as 10.\n7. votes_9: Number of votes rating the movie as 9.\n8. votes_8: Number of votes rating the movie as 8.\n9. votes_7: Number of votes rating the movie as 7.\n10. votes_6: Number of votes rating the movie as 6.\n11. votes_5: Number of votes rating the movie as 5.\n12. votes_4: Number of votes rating the movie as 4.\n13. votes_3: Number of votes rating the movie as 3.\n14. votes_2: Number of votes rating the movie as 2.\n15. votes_1: Number of votes rating the movie as 1.\n16. allgenders_0age_avg_vote: Average vote score given by viewers of all genders in the 0-18 age group.\n17. allgenders_0age_votes: Number of votes from viewers of all genders in the 0-18 age group.\n18. allgenders_18age_avg_vote: Average vote score given by viewers of all genders in the 18-30 age group.\n19. allgenders_18age_votes: Number of votes from viewers of all genders in the 18-30 age group.\n20. allgenders_30age_avg_vote: Average vote score given by viewers of all genders in the 30-45 age group.\n21. allgenders_30age_votes: Number of votes from viewers of all genders in the 30-45 age group.\n22. allgenders_45age_avg_vote: Average vote score given by viewers of all genders in the 45+ age group.\n23. allgenders_45age_votes: Number of votes from viewers of all genders in the 45+ age group.\n24. males_allages_avg_vote: Average vote score given by male viewers of all ages.\n25. males_allages_votes: Number of votes from male viewers of all ages.\n26. females_allages_avg_vote: Average vote score given by female viewers of all ages.\n27. females_allages_votes: Number of votes from female viewers of all ages.\n28. top1000_voters_rating: Average rating given by the top 1000 voters.\n29. top1000_voters_votes: Number of votes from the top 1000 voters.\n30. us_voters_rating: Average rating given by voters from the United States.\n31. us_voters_votes: Number of votes from voters from the United States.\n32. non_us_voters_rating: Average rating given by voters from outside the United States.\n33. non_us_voters_votes: Number of votes from voters from outside the United States. movies and tv shows","3":" TMDB TOP 10000 (10K) Movies Dataset This dataset contains the TOP 10K movies fetched from TMDB API as of March 2024.  movies and tv shows beginner intermediate recommender systems tabular","4":" Google Data Analytics Capstone Project: Netflix Insights from historical data Introduction:\n\nIn this case study the skills that I acquired from Google Data Analytics Professional Certificate Course is demonstrated. These skills will be used to complete the imagined task which was given by Netflix. The analysis process of this task will be consisted of following steps. Ask, Prepare, Process, Analyze, Share and Act.\n\n\nScenario:\n\nThe Netflix Chief Content Officer, Bela Bajaria, believes that companies success depends on to provide the customers what they want. Bajaria stated that the goal of this task is to find most wanted contents of the movies which will be added to the portfolio. Most of the movie contracts are signed before they come to the theaters, and it is hard to know if the customers really want to watch that movie and if the movie will be successful. There for my team wants to understand what type of content a movies success depends on. From these insights my team will design an investment strategy to choose the most popular movies that are expected to be in theaters in the near future. But first, Netflix executives must approve our recommendations. To be able to do that we must provide satisfying data insights along with professional data visualizations. \n\n\nAbout the Company:\n\n\nAt Netflix, we want to entertain the world. Whatever your taste, and no matter where you live, we give you access to best-in-class TV series, documentaries, feature films and games. Our members control what they want to watch, when they want it, in one simple subscription. We\u2019re streaming in more than 30 languages and 190 countries, because great stories can come from anywhere and be loved everywhere. We are the world\u2019s biggest fans of entertainment, and we\u2019re always looking to help you find your next favorite story.\n\nAs a company Netflix knows that it is important to acquire or produce movies that people want to watch. \n\nThere for Bajaria has set a clear goal: Define an investment strategy that will allow Netflix to provide customers the movies what they want to watch which will maximize the Sales.\n\n\n\nAsk:\n\nBusiness Task: \nTo find out what kind of movie customers wants to watch and if the content type really has a correlation with the movie success.\nStakeholders: \n\nBela Bajaria: She joined Netflix in 2016 to oversee unscripted and scripted series. Bajaria also responsible from the content selection and strategy for different regions. \n\nNetflix content analytics team: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Netflix content strategy.\n\nNetflix executive team: The notoriously detail-oriented executive team will decide whether to approve the recommended content program.\n\n\nPrepare:\n\nI start my preparation procedure by downloading every piece of data I'll need for the study. Top 1000 Highest-Grossing Movies of All Time.csv will be used. Additionally, 15 Lowest-Grossing Movies of All Time.csv was found during the data research and this dataset will be analyst as well. The data has been made available by IMDB and shared this two following URL addresses: https:\/\/www.imdb.com\/list\/ls098063263\/ and https:\/\/www.imdb.com\/list\/ls069238222\/ .\n\n\nProcess:\n\nData Cleaning: \n\nSQL: To begin the data cleaning process, I opened both csv file in SQL and conducted following operations: \n\n\u2022\tChecked for and removed any duplicates. \n\u2022\tChecked if there any null values. \n\u2022\tRemoved the columns that are not necessary.\n\u2022\tTrim the Description column to have only gross profit in it. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\n\u2022\tRenamed the Description column as Gross_Profit. (This cleaning procedure only used for 1000 Highest-Grossing Movies of All Time.csv dataset.)\n\nFollwing SQL codes were used during the data cleaning:\n#SQL CODE used for Highest Grossing Movies DATASET\n\nSELECT \nPosition,\nSUBSTR(Description,34,12) as Gross_Profit,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\n FROM `even-electron-400301.Highest_Gross_Movies.1` \n\n#SQL CODE used for Lowest Grossing Movies DATASET\n\nSELECT \nPosition,\nTitle, \nIMDb_Rating,\nRuntime__mins_, \nYear, \nGenres, \nNum_Votes, \nRelease_Date\nFROM `even-electron-400301.Lowest_Grossing_Movies.2` \nOrder By Position\n\n\n\n\nAnalyze:\n\nAs a starter, I want to reemphasize the business task once again. Is content has a big impact on a movie\u2019s success?\n\nTo answer this question, there were a few information that I projected that I could pull of and use it during my analysis. \n\n\u2022\tAverage gross profit \n\u2022\tNumber of Genres\n\u2022\tTotal Gross Profit of the most popular genres\n\u2022\tThe distribution of the Gross income on Genres\n\n\nI used Microsoft Excel for the bullet points above. The operations to achieve the values above are as follows:\n \n\u2022\tAverage function for Average Gross profit in 1000 Highest-Grossing Movies of All Time.\n\u2022\tCreated a pivot table to work on Genres and Gross_Profit\n\u2022\tAnalyzed all genre which consist different categories (exp: Action, Adventure, Horrror ...) in it by using filtering for specific categories like, action, adventure etc. \n\u2022\tThe content analysis mentioned in the last bullet point implemented also for 15 Lowest-Grossing Movies of All Time dataset. \n\nShare:\nThe dashboard I created for the project is also posteed on tableau, you can find the dashboard can be found here\n\nFor the visualizations, it was aimed to emphasize the only the data which demonstrates the key differences between Genre and Gross Profit. \nFollowing visualizations were included into the dashboard;\n\n\u2022\tGross income distributions on movie genres\n\u2022\tGenre\u2019s category distribution for highest grossing movies\n\u2022\tGenre\u2019s category distribution for lowest grossing movies\n\n\n\nHere is a summary of the most meaningful insights for content and movie success:\n\nHighest Grossing Movies: \n\n\u2022\tAmong the 1000 most profited movies the most common genre is \u201cAction, Adventure, Sci-Fi\u201d, 64 movies have this genre type in the list.\n\u2022\t The following genre type is \u201cAction, Adventure, Thriller\u201d with 33 movies. \n\u2022\tEach Genre has several categories in it. During analysis it was observed that some categories has occurred more than other categories. \n\u2022\tAmong the highest grossing movies there were clear difference between different categories that is used in genres. According to my analysis among the 1000 movies 520 movies possessed adventure category and 484 movies had action category in it. \n\u2022\tMovies with horror genre is one of the least sucessfull ones in the 1000 highest grossing movies list in terms of gross profit.\n\nLowest Grossing Movies: \n\u2022\tThere is no movie in the lowest gross income movies with action or adventure genres.\n\u2022\t30% of the 15 least grossing movies has horror genre which is the most common genre in this list.\n\nAbout the analysis: \n\nThe data collected from secondary resources was limited to come up with a detailed analysis. Since the analysis has conducted with the limited data, it might affect the insights and might not demonstrate us the real-life distribution which will lead to wrong strategies especially when it comes to different regions. \n\nAct: \n\nBased on my analysis, following actions are recommended which I believe that it will help content department to create an effective strategy for content creation and selection. \n\n\n\u2022\tAction\/Adventure base strategy: When Netflix signing with a new movie or creating a new one, we can add more movies with action and adventure category to our portfolio. \n\u2022\tHorror Movies: Netflix should give less priority to horror movies. Statistically, horror movies are the least successful movies among all. And to increase the number of movies which will be invested it will increase the probability to fail.\n\u2022\tCombination of the forces: Netflix can try to combine the most successful categories in terms of gross profit. For instance, comedy with action or animation and action to increase the attraction. \n\n\nBy implementing these strategies, Netflix can make more meaningful decision on content creation and sign more precise movie agreements which are demanded by the audience. \n movies and tv shows business computer science programming","5":" Movie Identification Dataset [800 Movies] Frames from 800 top-rated movies for building movie identification models Google's GEMINI can identify a movie if you show a few frames of it. Cool stuff! Can you build a model that can do the same thing?\n\nThe dataset contains frames from approximately 800 top-rated movies. Each movie has around 1000 frames, totaling up to 800,000 frames in the dataset. \n\n&gt; If you find this dataset valuable for building movie identification models, don't forget to hit the upvote button! \ud83d\ude0a\ud83d\udc9d \n\nTop rated movies were identified from my [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) dataset\n\n### Checkout my top datasets\n- [Top Spotify Songs in 73 Countries](https:\/\/www.kaggle.com\/datasets\/asaniczka\/top-spotify-songs-in-73-countries-daily-updated)\n- [Wages by Education in the USA](https:\/\/www.kaggle.com\/datasets\/asaniczka\/wages-by-education-in-the-usa-1973-2022)\n- [Amazon Products Dataset (1.4M Products)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-products-dataset-2023-1-4m-products)\n- [TMDB 950K Movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies)\n- [Amazon Kindle Books (130K Books)](https:\/\/www.kaggle.com\/datasets\/asaniczka\/amazon-kindle-books-dataset-2023-130k-books)\n\n\n## Interesting Task Ideas:\n\n1. Create a movie identification system that can predict the title of a movie based on extracted frames.\n2. Train an AI model to identify similar scenes or genres using movie frames.\n3. Develop a content-based recommendation system that suggests movies based on visual similarity.\n4. Explore the correlation between movie ratings and the visual elements captured in the frames. Use [TMDB Full movies](https:\/\/www.kaggle.com\/datasets\/asaniczka\/tmdb-movies-dataset-2023-930k-movies) for ratings\n5. Train deep learning models like VIT or ResNet using the dataset to develop powerful movie identification systems.\n\n## Specs:\n\n- Each frame has a height of 256px.\n- Width will vary since different movies have different aspect ratios.\n- Roughly 1000 frames were extracted from each movie.\n- All frames for a movie will exist under the folder with the movie's name.\n- Movie names are in the format of `Movie name (Year of release)`.\n- To get more details on movies, use my TMDB movies dataset. Movie names are not a 100% match. Filter by release year and try semantic similarity or vector similarity.\n\n---\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@jakobowens1?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Jakob Owens<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/clap-board-roadside-jakob-and-ryan-CiUR8zISX60?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n   popular culture arts and entertainment movies and tv shows image classification","6":" IMDb top rated 1000 movies  The dataset have seven features : \n1) Movie Name\n2) Category\n3) Release year\n4) Duration\n5) Rating\n6) Meta score\n7) Votes . movies and tv shows","7":" Top 1000 IMDb movies Dataset ( Web Scraping )   movies and tv shows","8":" IMDB TOP 1000 MOVIES DATASET   movies and tv shows","9":" IMDb Top 1000 Movies Dataset Web Scraped data from IMDb Website This dataset contains information about top 1000 IMDB movies, including their titles, certificates, durations, genres, IMDb ratings, Metascores, directors, cast members, the number of votes they received, grossed earnings, and plot summaries. The data is a curated list of highly acclaimed and popular movies.\n\nColumns\/Variables:\n\nMovie Name: The title of the movie.\nCertificate: The certificate or rating assigned to the movie.\nDuration: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nIMDb Rating: The IMDb rating of the movie.\nMetascore: The Metascore rating of the movie.\nDirector: The director of the movie.\nStars: The main cast members of the movie.\nVotes: The number of user votes\/ratings the movie has received.\nGrossed in $: The gross earnings in dollars (if available).\nPlot: A brief summary or plot description of the movie.\nSize: The dataset contains 1000 rows and 11 columns.\n\n\nData Quality: The dataset appears to be well-structured and complete. There are no missing values, and it seems to be ready for analysis.\n\nUse Cases: This dataset can be used for various analyses, such as exploring the relationship between IMDb ratings and Metascores, identifying top-rated directors, or understanding the distribution of movie ratings across genres. arts and entertainment movies and tv shows","10":" IMDB Anime Data Most Popular Animes of all Time on IMDB This Dataset displays only the top 1000 Anime of all time on IMDB. It is in the same order as displayed on the IMDB website which is filtered by - \"Most Popular Movies and TV Shows tagged with keyword \"anime\". [**Source**](https:\/\/www.imdb.com\/search\/keyword\/?keywords=anime&mode=detail&page=1&ref_=kw_nxt&sort=moviemeter,asc)\n\n\n**DATA DICTIONARY:**<br>\n`Title`: The name of the Anime.\n`Release Year`: The year the anime was released and till which year it is Ended, The OA in the values tells that the anime is still airing.\n`Genre`: Categories where the anime belongs.\n`Duration`: Anime running time in minutes.\n`Rating`: Ratings given by IMDb registered users (on a scale of 1 to 10)\n`Description`: Summary of the Anime which gives a brief description what it is about.\n`No. of Votes`: Number of votes cast by IMDb registered users.\n\n\n*Note: There are NA values present in most of the columns, because the value is not available on the source* anime and manga","11":" Top 10000 Voted IMDB Movies Dataset   movies and tv shows","12":" Dataset   ","13":" IMDB top 1000 movies This dataset contains all the IMDb top 1000 movies  The IMDb Top 1000 Movies dataset is a treasure trove of information about the most esteemed films in cinematic history. It includes a comprehensive collection of attributes that can fuel numerous insightful analyses and visualizations.\n\nTitle: The title of the movie.\nYear: The year the movie was released.\nRating: The IMDb rating of the movie, reflecting its overall popularity and reception.\nDuration: The runtime of the movie in minutes.\nGenre: The genre(s) to which the movie belongs (e.g., Drama, Action, Adventure).\nIMDb Rank: The rank of the movie within the IMDb Top 1000 list.\nMetascore: The Metascore assigned to the movie, offering a critical review of its quality.\nDirector: The director(s) responsible for bringing the movie to life.\nStars: The lead actors and actresses who play pivotal roles in the movie.\nVotes: The number of user votes the movie has received on IMDb, reflecting its popularity.\nGross: The gross earnings of the movie at the box office or through various distribution channels.\nDescription: A brief synopsis or overview of the movie's plot, theme, or significance. movies and tv shows","14":" Top 1000 IMDb Movies Dataset Discover the Greatest Movies of All Time - IMDb's Top 1000 Movie Rankings Welcome to the \"Top 1000 IMDb Movies Dataset\" - a treasure trove of cinematic excellence! This comprehensive collection presents the most celebrated and beloved movies, as rated and ranked by IMDb users. Whether you're a movie enthusiast, a data scientist, or a filmmaker seeking inspiration, this dataset promises a thrilling journey through the world of cinema's finest creations.\n\n**Content**:\n\nThis dataset boasts a rich array of movie attributes to help you delve into the realm of top-rated films:\n\nMovie Name: The title of each movie, representing iconic masterpieces that have left an indelible mark on the film industry and audiences worldwide.\n\nYear of Release: The year when each movie was released, providing valuable context for historical and chronological analyses.\n\nWatch Time: The duration of each movie, allowing you to identify captivating films for various viewing preferences.\n\nMovie Rating: IMDb's user ratings, serving as a benchmark for gauging audience reception and appreciation.\n\nMetascore of Movie: Metascores from renowned critics, offering insights into the films' critical acclaim and recognition.\n\nGross Earnings: The worldwide box office earnings, reflecting the commercial success and popularity of each movie.\n\nVotes: The number of votes cast by IMDb users, indicating the films' popularity and reach.\n\nDescription: Brief summaries that provide a glimpse into the captivating plots and themes of these cinematic marvels.\n\n**Usage**:\n\nThe \"Top 1000 IMDb Movies Dataset\" opens up a world of possibilities for movie aficionados and data enthusiasts alike:\n\nFilm Analysis and Insights: Dive into data-driven analyses to uncover intriguing patterns, trends, and correlations among highly-rated movies.\n\nRating Predictions: Develop predictive models to estimate movie ratings based on other attributes and gain a deeper understanding of factors influencing user perceptions.\n\nBox Office Performance: Examine the relationship between critical acclaim, ratings, and box office success, providing valuable insights for filmmakers and producers.\n\nGenre Exploration: Investigate the distribution of top-rated movies across different genres to identify trends in audience preferences.\n\nTemporal Trends: Analyze changes in movie ratings, box office performance, and viewer preferences over time to reveal shifts in cinematic taste and trends.\n\n**Caution**:\n\nWhile this dataset is a treasure trove for movie analysis, it is essential to respect copyright and intellectual property rights. Users are encouraged to use the data responsibly and comply with IMDb's terms of use and any applicable legal restrictions.\n\n**Conclusion**:\n\nThe \"Top 1000 IMDb Movies Dataset\" offers an immersive and data-driven exploration of cinema's greatest works, igniting creativity, fostering discussions, and enriching our appreciation for the art of storytelling. Let the magic of these cinematic gems captivate you as you embark on an exciting journey into the world of acclaimed movies! movies and tv shows","15":" imdb top 1000 Dataset consist of top 1000 imdb movies  movies and tv shows","16":" IMDB Top 1000 Movies Exploring what makes movies stand out from the rest Data of the top 1000 movies scraped from the IMDB website. Dataset also duplicates a movie in another row if the movie consists of more than one genre. Missing values present in 'gross(M)' and 'metascore' column represented as 0.00 and 0 respectively.\n\nEdit: Version 3. I have included the raw data from scraping the website for anyone interested in cleaning the data and transforming data types.\nI have also updated the csv file to include the director of the movie\n\nDISCLAIMER:  The grossing values of movies from IMDb only include numbers from US and Canada and are thus not representative of the values on a global scale. movies and tv shows","17":" IMDb Top 10000 Movies Dataset Feature Film, Rating Count at least 10,000 (Sorted by IMDb Rating Descending) This dataset consists of a meticulously collected collection of 10,000 feature films from IMDb, one of the most popular and authoritative sources for movie information. The movies included in this dataset are sorted based on their IMDb ratings in descending order. The dataset covers a wide range of genres, directors, and stars, providing a comprehensive overview of highly regarded films across various categories.The scraping process was performed on June 17, 2023.\n\nDataset Columns:\n\nID: Unique identifier for each movie in the dataset.\nMovie Name: The title of the movie.\nRating: The IMDb rating for the movie.\nRuntime: The duration of the movie in minutes.\nGenre: The genre(s) to which the movie belongs.\nMetascore: The Metascore rating for the movie (if available).\nPlot: A brief summary or description of the movie's plot.\nDirectors: The director(s) of the movie.\nStars: The main cast or actors featured in the movie.\nVotes: The number of votes\/ratings received by the movie.\nGross: The gross revenue generated by the movie (if available).\nLink: The IMDb link to access the full details and additional information about the movie. arts and entertainment movies and tv shows nlp data analytics recommender systems","18":"  IMDB Dataset of Top 1000 Movies and TV Shows   movies and tv shows","19":" IMDB Movies Dataset Top 1000 Movies by IMDB rating  arts and entertainment movies and tv shows","20":" clothing_reviews   ","21":" womens_clothing_reviews   ","22":" womens_clothing_reviews_new   ","23":" Reddit Posts on Borderline Personality Disorder CLEANED almost 6000 posts about borderline personality This dataset contains 5,879 Reddit posts discussing Borderline Personality Disorder (BPD) , posts are about people telling own experiencies with this disorder either they suffer from or have met people or raised by bpd parents \n\nThis dataset provides a valuable resource for analyzing the discourse around BPD on social media, studying patterns in user interactions, and understanding the experiences and challenges faced by individuals with BPD.\n\n**Columns:**\n**Title**: The title of the Reddit post.\n**Score**: The score of the post, reflecting its popularity and engagement.\n**ID**: The unique identifier of the post.\n**Comments**: The number of comments the post received.\n**Creation Time**: The timestamp when the post was created\n**Content**: The main text of the post, detailing the user's thoughts, experiences, and discussions about BPD\n\nThis dataset can be used for various purposes, including sentiment analysis, natural language processing, mental health research mental health philosophy text english","24":" Stress and Anxiety Posts on Reddit 4,000 Reddit posts from people experiencing stress\/anxiety. This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit. psychology text","25":" Stress and Anxiety Posts on Reddit 4,000 Reddit posts from people experiencing stress\/anxiety. This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit. psychology text","26":" Emotion Classification Dataset Emotion-labeled Dataset: Unveiling Sentiments in Textual Content This dataset presents a comprehensive collection of textual content annotated with corresponding sentiments: happy, sad, and angry. Each entry in the dataset contains a piece of text along with its associated sentiment label, providing a rich resource for sentiment analysis and emotion detection tasks. classification text pandas python english","27":" Twitter Financial News Sentiment Dataset A Twitter Financial Dataset for Clarification  # Introduction \nThe Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment.\n\nThe dataset holds 11,932 documents annotated with 3 labels:\n```python\nsentiments = {\n    \"LABEL_0\": \"Bearish\", \n    \"LABEL_1\": \"Bullish\", \n    \"LABEL_2\": \"Neutral\"\n}  \n```\n\nThe data was collected using the Twitter API. The current dataset supports the multi-class classification task.\n\n# Data Splits\nThere are 2 splits: train and validation. Below are the statistics:\n\nDataset Split\tNumber of Instances in Split: \n\nTrain -----------9,938\n\nValidation-------2,486\n\n# Licensing Information\nThe Twitter Financial Dataset (sentiment) is released under the MIT License.\n\n# Source \nhttps:\/\/huggingface.co\/datasets\/zeroshot\/twitter-financial-news-sentiment classification tabular news","28":" Sentiment Analysis of Tweet Reviews Building a predictive model based on positive and negative tweets of twitter **Context**\nThe objective of this task is to build a model based on pre-processed tweets. For the sake of simplicity, we say a tweet contains a negative review if it has a racist or hate sentiment associated with it. So, the task is to predict the labels on the test dataset after building a model.\n\n**Content**\nIn the dataset, a labelled train data is given where label '0' denotes the tweet is positive \ud83d\ude0a and label '1' denotes the tweet is negative \u2639\ufe0f\n\n**Acknowledgements**\nDataset is provided by [Analytics Vidhya](http:\/\/https\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/)\n nlp text mining binary classification social networks text classification","29":" Twitter Customer Reviews of Popular Smart Phone Building a predictive sentiment analysis model using machine learning **Context**\n\nThis dataset is a part of our research work titled \"Opinion Mining of Customer Reviews Using Supervised Learning Algorithms\". If you use this dataset then please cite our work.\nYou can find the article in https:\/\/ieeexplore.ieee.org\/document\/9733435\n\n**Content**\n\nNowadays, a lot of people express their opinions on various topics using social networking sites. Twitter has become a famous social networking site where people can express their opinions to the point and so it has become a great source for opinion mining. In this research, the goal was to train and build a model that can automatically and accurately categorize the opinion of customer tweet reviews about popular cell phone brands. We have used python TextBlob library for getting the polarity values of all the tweet reviews of the dataset. We have also used Support Vector Machine (SVM), Na\u00efve Bayes, Logistic Regression, Decision Tree and Random Forest algorithms along with Bag of Words and TF-IDF vectorizers separately to train and build the model. We have investigated the opinions using five classes which are Strongly Positive, Positive, Neutral, Negative and Strongly Negative.\n\n\n**When referencing this dataset please cite the below paper**\n\n**Bibtex**\n@inproceedings{arif2021opinion,\ntitle={Opinion Mining of Customer Reviews Using Supervised Learning Algorithms},\nauthor={Arif, Shibbir Ahmed and Hossain, Taslima Binte},\nbooktitle={2021 5th International Conference on Electrical Information and Communication Technology (EICT)},\npages={1--6},\nyear={2021},\norganization={IEEE}\n} mobile and wireless exploratory data analysis text mining multiclass classification social networks","30":" Chatgpt DataSet 50000 Tweets from initial month Obtained by Twitter API v2, Best for Topic Modelling and Sentiment Analysis  text classification sentence similarity english","31":" Indian Political Sentiment on Twitter  Unveiling Public Sentiment on Politics: A Comprehensive Analysis of India This dataset provides a comprehensive collection of public sentiment and discourse related to Indian politics. The entries cover a wide range of opinions, news, social media posts, and other forms of public communication. \nEach entry is meticulously labeled with a sentiment score, capturing the polarity of the opinion from strongly negative to strongly positive. \n\nThis dataset is structured to facilitate detailed sentiment analysis and examination of political sentiments in India.\n\nUse Cases\n\nThis dataset is ideal for:\n\nSentiment Analysis: Researchers can use this dataset to train and evaluate sentiment analysis models specifically tailored to the political context in India.\nTrend Analysis: Analysts can track the evolution of public sentiment over time, identifying key events that influenced public opinion.\nPolitical Studies: Scholars can investigate the relationship between public sentiment and political events, figures, and policies in India.\nNatural Language Processing (NLP): NLP practitioners can leverage this dataset for various tasks such as text classification, opinion mining, and more.\n india politics nlp text","32":" Twitter Emotion Dataset Unveiling the Emotional Tapestry of Social Media Title: Twitter Emotion Dataset: Unveiling the Emotional Tapestry of Social Media\n\nDescription:\n\nDive into the intricate world of human emotions expressed through Twitter messages with our meticulously curated dataset. Each entry in this comprehensive collection features a text segment extracted from Twitter, accompanied by a corresponding label denoting the predominant emotion conveyed by the message. The emotions are thoughtfully categorized into six distinct classes: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\nThis dataset is tailor-made for researchers, data scientists, and enthusiasts keen on exploring the dynamic emotional landscape within the realm of social media. Whether you're delving into sentiment analysis, emotion classification, or text mining, our Twitter Emotion Dataset provides a rich foundation for unraveling the subtle nuances that define human emotions in the digital age.\n\n**Dataset Highlights:**\n- Over 416809 Twitter messages covering a diverse range of topics and user demographics.\n- Manually annotated labels ensure high-quality emotional classification.\n- Tweets sourced from a variety of geographies and languages, capturing a global perspective on emotions in social media.\n- A balanced distribution across the six emotion categories for robust model training.\n\n**Potential Applications:**\n- Sentiment Analysis: Explore sentiment trends and patterns to understand how users express emotions over time.\n- Emotion Classification: Develop models to accurately predict and categorize emotional states within tweets.\n- Text Mining: Extract valuable insights from the vast pool of emotional data to inform decision-making and marketing strategies.\n\n**Why Use Our Dataset?**\n- Enrich your research with a diverse and well-labeled dataset, ensuring the reliability and accuracy of your findings.\n- Leverage the global scope of the dataset for cross-cultural and multilingual analyses of emotional expression.\n- Contribute to advancements in emotional intelligence research and its applications in social media analytics.\n\n**Dataset Conclusions:**\nUncover the hidden gems within our Twitter Emotion Dataset and draw meaningful conclusions about the evolving emotional landscape of social media. Whether you're identifying sentiment shifts during significant events or tracking the ebb and flow of joy, anger, and surprise, this dataset provides the tools to delve into the intricacies of human emotion in the digital realm. As you analyze and model, contribute to the collective understanding of emotional intelligence on social platforms and pave the way for innovative applications in communication, mental health, and beyond. Join us in decoding the emotional tapestry of Twitter and shaping the future of emotion-aware technologies. earth and nature nlp social networks","33":" Emotions Dataset Emotions are conscious mental reactions &gt;# Content:\nThis Emotion Classification dataset is designed to facilitate research and experimentation in the field of natural language processing and emotion analysis. It contains a diverse collection of text samples, each labelled with the corresponding emotion it conveys. Emotions can range from happiness and excitement to anger, sadness, and more.\n&gt;# About emotions.csv file\nEach entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into **six categories**: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within social media.\n- **text**: Description of context\n- **label**: The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n\n&gt;# Usecase:\n- **Sentiment Analysis**: Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification**: Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis**: Explore linguistic patterns and expressions associated with different emotional states. mental health lstm tabular multiclass classification text pre-processing","34":" Clean-NLP with Disaster Tweets Dataset NLP with Disaster Tweets: Data Preprocessing  Introduction\n\n Raw tweet data is often messy and requires cleaning and normalization before building an effective NLP model.  Here's a breakdown of common preprocessing steps and their purpose:\n\n 1. Data Gathering\n\nObtaining a suitable dataset of labeled tweets (e.g., from Twitter, existing NLP datasets, or Kaggle competitions).\n 2. Removing HTML Tags\n\nHTML tags can clutter the text and don't contribute to understanding a tweet's content or sentiment.\n 3. Removing URLs\n\nURLs often don't add significant meaning for disaster classification and may introduce unnecessary variability.\n 4. Converting to Lowercase\n\nMakes the text case-insensitive, ensuring that \"Disaster\" and \"disaster\" are treated as the same word, improving word frequency analysis.\n 5. Removing Emojis\n\nWhile emojis can carry sentiment, they require sophisticated techniques to interpret consistently. Basic preprocessing often removes them, although more advanced models might incorporate emoji analysis.\n 6. Removing Punctuation\n\nPunctuation marks rarely contribute to core meaning in disaster classification and can introduce noise.\n 7. Removing Stop Words\n\nRemoving common words like \"the,\" \"and,\" etc., that have little semantic value. This reduces computational load and lets the model focus on more informative words.\n 8. Handling Abbreviations\/Slang\n\nExpanding abbreviations and slang terms (e.g., \"lol\" -&gt; \"laughing out loud\") aids in understanding the full meaning of the text and makes the vocabulary more standardized.\n 9. Stemming\n\nReduces different forms of words to their root (e.g., \"flooding,\" \"flooded\" -&gt; \"flood\"), potentially helping the model generalize better.\n 10. Spelling Correction\n\nFixing typos ensures words are correctly interpreted and can make word frequencies more accurate.\n 11. Tokenization\n\nSplits the text into individual words or meaningful units (e.g., \"New York\" is often better treated as a single token) to prepare the data for further analysis and model input. education computer science","35":" Tweets Dataset Sentiment analysis using 1M+ tweets dataset &gt;## Context\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = negative, 4 = positive) and can be used to detect sentiment.\n## Content\nIt contains the following 6 fields:\n- target: the polarity of the tweet (0 = negative and 4 = positive)\n- ids: The id of the tweet ( 2087)\n- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n- user: the user that tweeted.\n- text: the text of the tweet. intermediate survey analysis classification social networks sentence similarity","36":" Emotions Where Words Paint the Colors of Feelings # Introduction:\n\n&gt;Welcome to the \"Emotions\" dataset \u2013 a collection of English Twitter messages meticulously annotated with six fundamental emotions: anger, fear, joy, love, sadness, and surprise. This dataset serves as a valuable resource for understanding and analyzing the diverse spectrum of emotions expressed in short-form text on social media.\n\n[![DOI](https:\/\/zenodo.org\/badge\/DOI\/10.34740\/KAGGLE\/DSV\/7563141.svg)](https:\/\/www.kaggle.com\/nelgiriyewithana\/datasets)\n\n\n# About the Dataset:\n\n&gt;Each entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within the realm of social media.\n\n# Key Features:\n\n&gt;- **text**: A string feature representing the content of the Twitter message.\n- **label**: A classification label indicating the primary emotion, with values ranging from 0 to 5.\n\n# Potential Use Cases:\n\n&gt;- **Sentiment Analysis:** Uncover the prevailing sentiments in English Twitter messages across various emotions.\n- **Emotion Classification:** Develop models to accurately classify tweets into the six specified emotion categories.\n- **Textual Analysis:** Explore linguistic patterns and expressions associated with different emotional states.\n\n# Sample Data:\n\nHere's a glimpse of the dataset with a few examples:\n\n| text                                                  | label |\n|-------------------------------------------------------|-------|\n| that was what i felt when i was finally accept...      | 1     |\n| i take every day as it comes i'm just focussin...      | 4     |\n| i give you plenty of attention even when i fee...      | 0     |\n\n<br>\nIf you find this dataset useful consider giving it a vote! \ud83d\ude0a\u2764\ufe0f \n arts and entertainment people and society education computer science nlp data visualization","37":" EventClassifier: Twitter Data Set Analyzing Public Opinions: Twitter Data with Labels for Different Categories Explore the diverse landscape of social media with this annotated Twitter dataset. Categorized into 'Political,' 'Positive,' 'Protest,' 'Riot,' 'Terror,' 'Disaster,' and 'Other,' these tweets provide valuable insights into public sentiments and trending topics. Whether you're interested in understanding political discourse, positive expressions, social movements, or crisis events, this dataset offers a comprehensive look into various aspects of online conversations. Use it for sentiment analysis, trend detection, or studying public reactions across a spectrum of categories. beginner nlp text text classification","38":" crypto nft tweets data crypto and nft related tweets and sentiments for NLP, market trends, etc NO MORE UPDATES\n[nitter.net](https:\/\/nitter.net) the twitter mirror we were pulling from has shutdown indefinitely.\n\n* this is a twitter tweet dataset on crypto, nfts, exchanges, and popular forums like wallstreetbets  \n* pulling tweets live from twitter ~3-4 days worth of tweets from the marked date of the files  \n\n\n## search queries in `_q.csv`\nthis file contains all the queries for different cryptocurrencies and exchanges \/ forums that were searching on twitter to gather the data.\n\n## tweets `yymmdd.*` files\n- these are the tweets data files\n- note they are marked sol and binance but they contain results from all queries\n\n## columns\n```\n|-------------------------|-------------------------------------------------------|\n| field                   | explanation                                           |\n|-------------------------|-------------------------------------------------------|\n| username                | who posted the tweet                                  |\n|-------------------------|-------------------------------------------------------|\n| tweet_raw               | unfiltered tweets *have links whitespace special chs. |\n|-------------------------|-------------------------------------------------------|\n| tweet_text              | cleaned tweet text                                    |\n|-------------------------|-------------------------------------------------------|\n| date                    | date tweet was posted                                 |\n|-------------------------|-------------------------------------------------------|\n| happy\/sad\/etc           | these are emotion \/ sentiment scoring                 |\n|-------------------------|-------------------------------------------------------|\n| afinn\/bing\/sid\/bertweet | more sentiment scoring                                |\n|-------------------------|-------------------------------------------------------|\n```\n\n\n text mining tabular text currencies and foreign exchange text classification","39":" Tweets and Engagement Metrics Twitter Data containing temporal, geographical, and general tweet metrics. ### This is the data wrangled and cleaned from an existing dataset, and it is the output of my notebook: [https:\/\/www.kaggle.com\/code\/paakhim10\/analyzing-and-classifying-twitter-sentiments](url)\n\n####You can use this dataset for:\n\n&gt;\u2022 Machine Learning\n1. Sentiment analysis\n2. Spam detection\n3. Text classification\n4. Trend identification (temporal and geographical)\n5. Building recommendation systems\n\n&gt;\u2022 Exploratory Data Analysis\n1. Geospatial or temporal mapping\n2. Hashtags Trend Detection\n3. User Engagement Metrics\n\n##### Credits:\nIf you use this dataset in your research, please credit the original authors.\nIf you use this dataset in your research, please credit [https:\/\/data.world\/kjensen18](url) internet email and messaging","40":" Short Jokes Dataset Humorous Short Jokes _____\n# Short Jokes Dataset\n### Humorous Short Jokes\nBy Fraser Greenlee (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n_____\n\n### About this dataset\n> \n> This dataset offers a valuable resource for various applications such as natural language processing, sentiment analysis, joke generation algorithms, or simply for entertainment purposes. Whether you're a data scientist looking to analyze humor patterns or an individual seeking some quick comedic relief, this dataset has got you covered.\n> \n> By utilizing this dataset, researchers can explore different aspects of humor and study the linguistic features that make these short jokes amusing. Moreover, it provides an opportunity for developing computer models capable of generating similar humorous content based on learned patterns.\n> \n\n### How to use the dataset\n> \n> \n> - **Understanding the Columns:**\n>    - `text`: This column contains the text of the short joke.\n>    - `**text`: No information is provided about this column.\n> \n> - **Exploring the Jokes:**\n>    - Start by exploring the `text` column, which contains the actual jokes. You can read through them and have a good laugh!\n>   \n> - **Analyzing the Jokes:**\n>    - To gain insights from this dataset, you can perform various analyses:\n>      - Sentiment Analysis: Use Natural Language Processing techniques to analyze the sentiment of each joke.\n>      - Categorization: Group jokes based on common themes or subjects, such as animals, professions, etc.\n>      - Length Distribution: Analyze and visualize the distribution of joke lengths.\n>    \n> - **Creating New Content or Applications:**\n>   Since this dataset provides a large collection of short jokes, you can utilize it creatively:\n>     - Generating Random Jokes: Develop an algorithm that generates new jokes based on patterns found in this dataset.\n>     - Humor Classification: Build a model that predicts if a given piece of text is funny or not using machine learning techniques.\n> \n> - **Sharing Your Findings:** \n>   If you make interesting discoveries or create unique applications using this dataset, consider sharing them with others in Kaggle community.\n> \n> Please note that no information regarding dates is available in train.csv; therefore, any temporal analysis or date-based insights won't be feasible with this specific file.\n> \n\n### Research Ideas\n> - Analyzing humor patterns: This dataset can be used to analyze different types of humor and identify patterns or common elements in jokes that make them funny. Researchers and linguists can use this dataset to gain insights into the structure, wordplay, or comedic techniques used in short jokes.\n> - Natural language processing: With the text data available in this dataset, it can be used for training models in natural language processing (NLP) tasks such as sentiment analysis, joke generation, or understanding humor from written text. NLP researchers and developers can utilize this dataset to build and improve algorithms for detecting or generating funny content.\n> - Social media analysis: Short jokes are popular on social media platforms like Twitter or Reddit where users frequently share humorous content. This dataset can be valuable for analyzing the reception and impact of these jokes on social media platforms. By examining trends, engagement metrics, or user reactions to specific jokes from the dataset, marketers or social media analysts can gain insights into what type of humor resonates with different online communities.\n> Overall, this dataset provides a rich resource for exploring various aspects related to humor analysis and NLP tasks while offering opportunities for sociocultural studies related to online comedy culture\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name   | Description                                   |\n|:--------------|:----------------------------------------------|\n| **text**      | The actual content of the short jokes. (Text) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Fraser Greenlee (From Huggingface)](https:\/\/huggingface.co\/datasets\/Fraser\/short-jokes).\n\n popular culture literature nlp","41":" Hate Speech and Offensive Language Detection Hate Speech and Offensive Language Detection on Twitter _____\n# Hate Speech and Offensive Language Detection\n### Hate Speech and Offensive Language Detection on Twitter\nBy hate_speech_offensive (From Huggingface) [[source]](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n_____\n\n### About this dataset\n> This dataset, named hate_speech_offensive, is a meticulously curated collection of annotated tweets with the specific purpose of detecting hate speech and offensive language. The dataset primarily consists of English tweets and is designed to train machine learning models or algorithms in the task of hate speech detection. It should be noted that the dataset has not been divided into multiple subsets, and only the train split is currently available for use.\n> \n> The dataset includes several columns that provide valuable information for understanding each tweet's classification. The column count represents the total number of annotations provided for each tweet, whereas hate_speech_count signifies how many annotations classified a particular tweet as hate speech. On the other hand, offensive_language_count indicates the number of annotations categorizing a tweet as containing offensive language. Additionally, neither_count denotes how many annotations identified a tweet as neither hate speech nor offensive language.\n> \n> For researchers and developers aiming to create effective models or algorithms capable of detecting hate speech and offensive language on Twitter, this comprehensive dataset offers a rich resource for training and evaluation purposes\n\n### How to use the dataset\n> \n> - Introduction:\n> \n> - Dataset Overview:\n>    - The dataset is presented in a CSV file format named 'train.csv'.\n>    - It consists of annotated tweets with information about their classification as hate speech, offensive language, or neither.\n>    - Each row represents a tweet along with the corresponding annotations provided by multiple annotators.\n>    - The main columns that will be essential for your analysis are: count (total number of annotations), hate_speech_count (number of annotations classifying a tweet as hate speech), offensive_language_count (number of annotations classifying a tweet as offensive language), neither_count (number of annotations classifying a tweet as neither hate speech nor offensive language).\n> \n> - Data Collection Methodology:\n>     The data collection methodology used to create this dataset involved obtaining tweets from Twitter's public API using specific search terms related to hate speech and offensive language. These tweets were then manually labeled by multiple annotators who reviewed them for classification purposes.\n> \n> - Data Quality:\n>     Although efforts have been made to ensure the accuracy of the data, it is important to acknowledge that annotations are subjective opinions provided by individual annotators. As such, there may be variations in classifications between annotators.\n> \n> - Preprocessing Techniques:\n>     Prior to training machine learning models or algorithms on this dataset, it is recommended to apply standard preprocessing techniques such as removing URLs, usernames\/handles, special characters\/punctuation marks, stop words removal, tokenization, stemming\/lemmatization etc., depending on your analysis requirements.\n> \n> - Exploratory Data Analysis (EDA):\n>     Conducting EDA on the dataset will help you gain insights and understand the underlying patterns in hate speech and offensive language. Some potential analysis ideas include:\n>     - Distribution of tweet counts per classification category (hate speech, offensive language, neither).\n>     - Most common words\/phrases associated with each class.\n>     - Co-occurrence analysis to identify correlations between hate speech and offensive language.\n> \n> - Building Machine Learning Models:\n>     To train models for automatic detection of hate speech and offensive language, you can follow these steps:\n>    a) Split the dataset into training and testing sets for model evaluation purposes.\n>    b) Choose appropriate features\/\n\n### Research Ideas\n> - Sentiment Analysis: This dataset can be used to train models for sentiment analysis on Twitter data. By classifying tweets as hate speech, offensive language, or neither, the dataset can help in understanding the sentiment behind different tweets and identifying patterns of negative or offensive language.\n> - Hate Speech Detection: The dataset can be used to develop models that automatically detect hate speech on Twitter. By training machine learning algorithms on this annotated dataset, it becomes possible to create systems that can identify and flag hate speech in real-time, making social media platforms safer and more inclusive.\n> - Content Moderation: Social media platforms can use this dataset to improve their content moderation systems. By using machine learning algorithms trained on this data, it becomes easier to automatically detect and remove offensive or hateful content from the platform, reducing the burden on human moderators and improving user experience by keeping online spaces free from toxic behavior\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/huggingface.co\/datasets\/hate_speech_offensive)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/).\n\n### Columns\n\n**File: train.csv**\n| Column name                  | Description                                                                                            |\n|:-----------------------------|:-------------------------------------------------------------------------------------------------------|\n| **count**                    | The total number of annotations for each tweet. (Integer)                                              |\n| **hate_speech_count**        | The number of annotations classifying a tweet as hate speech. (Integer)                                |\n| **offensive_language_count** | The number of annotations classifying a tweet as offensive language. (Integer)                         |\n| **neither_count**            | The number of annotations classifying a tweet as neither hate speech nor offensive language. (Integer) |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [hate_speech_offensive (From Huggingface)](https:\/\/huggingface.co\/datasets\/hate_speech_offensive).\n\n gender computer science nlp text mining social issues and advocacy","42":" indian political tweets  The \"Indian Political Tweets\" dataset is a collection of tweets related to the Indian political party BJP during election campaigns. The tweets were collected using Twitter API and Github sources. The tweets were preprocessed using various techniques to clean the data. This included removing duplicate tweets, removing retweets, removing mentions and URLs, and correcting spelling and grammar errors. The remaining tweets were then tokenized, lemmatized, and stop words were removed to prepare the data for sentiment analysis.\nThe dataset contains two labels - positive and negative - which indicate the sentiment of the tweet towards the political party being mentioned (BJP ). The sentiment analysis was performed using the Vader sentiment analysis tool, which assigned a score to each tweet based on the presence of positive and negative words in the text. Tweets with a positive score were labeled as positive, while tweets with a negative score were labeled as negative.\nThe dataset includes a total of 10210 number of tweets, with an approximately equal number of positive and negative tweets for both BJP . The dataset is suitable for data analysis and machine learning tasks such as text classification, sentiment analysis, and topic modeling. politics","43":" US Airlines Sentiment Analysis in Knime Analytics Made by a group of students from FORE School of Management Project: **Predictive Model for Twitter US Airline Comments Classification**\nThe project was developed by a group of students from FORE School of Management, It was aimed to develop a predictive model for classifying passengers' tweets about different US airlines into three categories: positive, negative, and neutral. The primary objective was to predict a tweet's classification based on its content.\n****The workflow and all the steps in the workflow have been explained in the attached project report with the dataset*** education","44":" Preprocessed Dell Tweets  A collection of tweets about the international computer technology business Dell can be found in the \"Preprocessed Dell Tweets\" dataset. To make sentiment analysis and natural language processing jobs easier, these tweets have undergone meticulous preprocessing. The dataset was first made up of tweets that were scraped from Twitter; preprocessing was done to clean and organize the data.\n\n**Important characteristics:**\n\n **Text:** The preprocessed text of the tweets is contained in this column, making it appropriate for natural language processing and text analysis.\n\n **Sentiment:** To facilitate machine learning activities, the sentiment column has been converted into numeric values. The following is how sentiment labels have been encoded:\n0: Neutral\n1: Positive\n2: Negative\n\n**Possible Applications:**\nThe dataset is perfect for machine learning, sentiment analysis, and sentiment classification tasks. This dataset can be used by researchers and data scientists to test and refine sentiment analysis methods. Efficient sentiment models for prediction can be developed thanks to the numerical sentiment labels.\n\n**Data Preprocessing:**\nTo prepare the dataset for analysis, the following preprocessing steps were applied:\n\n- Punctuation and special characters were removed from the text.\n- URLs and hyperlinks were stripped from the text.\n- Text was converted to lowercase for uniformity.\n- Stopwords (common words with limited analytical value) were removed.\n- Tokenization, stemming, and lemmatization were performed to normalize the text data.\n\n\n computer science","45":" Web Scrapping Twitter Racism Dataset generated and used in the final project of UAA-ICI-S7-M1 As part of my studies at the *Aguascalientes Autonomous University* in the *Intelligent Computing Engineering* program, during our seventh semester, we took a course called \"Metaheuristics 1.\" Taught by Professor **Francisco Javier Luna Rosas**, we were going to undertake a quite ambitious project planned for the entire semester.\n\nThe project consisted of several phases where we would perform web scraping of a social network on a specific sensitive topic on a massive scale, aiming to simulate a Big Data operation for sentiment analysis of this data and classify it as positive or negative. Additionally, we were required to propose a load balancer that would handle the distributed processing, providing us with speed in the project's execution time.\n\nThe teacher said that, since this project was very complex, it needed to be accomplished in teams. So with that, the team I collaborated with was formed. Teaming up with [Andrea Melissa Almeida Ortega](https:\/\/github.com\/Melissa-AO), [\u00d3scar Alonso, Flores Fern\u00e1ndez](https:\/\/github.com\/Dem0n2000), [Dariana G\u00f3mez Garza](https:\/\/github.com\/DariGmz), Fernando Francisco Gonz\u00e1lez Arenas, and [Hiram Efra\u00edn Orocio Garc\u00eda](https:\/\/github.com\/hiram57ef) we began sentiment analysis focusing on the topic of racism using the social network Twitter and conducted web scraping with a developer account using the Python library Tweepy. The procedure took place throughout the second semester of 2021, and at that time, the Twitter API download limit was approximately 900 requests every 15 minutes. Therefore, we developed a program to continuously make these requests, aiming to gather the maximum number of tweets possible and effectively simulate Big Data for the project. The number of tweets obtained after intensive web scraping reached a total of **6,942,021 tweets**, resulting in a **1.14 GB** file. Here it's uploaded separating the total scrapping each of the team members could do.\n\nA simple genetic algorithm procedure was applied, serving as a dynamic load balancer among the six collaborating project computers to perform parallel processing as quickly as possible. The NLTK library in Python was used for lemmatization procedures on the tweets. The Random Forest model was chosen as the classifier for this sentiment analysis, implemented using the sci-kit learn library in Python.\n\nThe classifier achieved an accuracy of **0.9999157** as evaluated through a confusion matrix. The project can be reviewed at this link [here](https:\/\/github.com\/Joul24py\/UAA-ICI\/tree\/main\/23-S7-M1-ClassExercises\/04-FinalProject). text binary classification online communities racial equity english","46":" Twitter Sentiment Analysis - Xitsonga  Xitsonga Twitter Sentiment Analysis Dataset he Xitsonga Twitter Sentiment Analysis Dataset is a carefully curated collection of tweets written in the Xitsonga language, with the primary objective of discerning the emotional tones expressed in these tweets. This dataset is specifically designed to classify tweets into one of three distinct emotional categories: \"Positive,\" \"Neutral,\" or \"Negative.\" or \"irrelevant\"\n\nNotable Features:\nIn this dataset, you will find several key characteristics:\n\nLanguage Focus: All the tweets included in this dataset are composed in the Xitsonga language, ensuring that the sentiment analysis is tailored to the linguistic nuances of this specific language.\n\nEmotion Classification: Each tweet within the dataset has been manually categorized into one of three emotional labels: \"Positive\" for tweets conveying positive sentiments, \"Neutral\" for tweets that exhibit no significant emotional tone, and \"Negative\" for those expressing negative emotions or viewpoints.\n\nComment-Centric: The primary emphasis of this dataset is on the analysis of user comments and conversations on the Twitter platform, making it particularly useful for understanding the sentiments and reactions of Xitsonga-speaking Twitter users.\n\nDiverse Content: The dataset encompasses tweets from a broad spectrum of topics and subject matters, offering a comprehensive representation of discussions and discourse on Twitter in the Xitsonga language.\n\nSubstantial Data Size: With a substantial number of tweets, this dataset provides ample data for the development, training, and evaluation of sentiment analysis models. email and messaging social networks","47":" Urban Mobility Dataset Synthetic data on urban mobility for enhancing urban planning This dataset contains synthetic data generated to model urban mobility patterns. It includes detailed information on public transportation usage, traffic flow, bike-sharing programs, and pedestrian movement, enriched with additional contextual factors like weather conditions, holidays, and events. The dataset is designed to support urban planners and transportation authorities in making data-driven decisions to improve urban mobility and reduce traffic congestion.\n\nColumns:\n1. timestamp: Date and time of the record.\n2. public_transport_usage: Number of public transport users per hour.\n3. traffic_flow: Number of vehicles passing a specific point per hour.\n4. bike_sharing_usage: Number of users utilizing bike-sharing services per hour.\n5. pedestrian_count: Number of pedestrians recorded per hour.\n6. weather_conditions: Weather conditions at the time of the record (e.g., Clear, Rain, Snow, Fog).\n7. day_of_week: Day of the week (e.g., Monday, Tuesday).\n8. holiday: Indicator of whether the day is a holiday (1 if holiday, 0 otherwise).\n9. event: Type of event occurring (e.g., None, Concert, Sports, Festival).\n10. temperature: Temperature in degrees Celsius.\n11. humidity: Humidity percentage.\n12. road_incidents: Number of road incidents reported per hour.\n13. public_transport_delay: Average delay in public transport in minutes.\n14. bike_availability: Number of available bikes at bike-sharing stations per hour.\n15. pedestrian_incidents: Number of incidents involving pedestrians per hour.\n\nThis comprehensive dataset can be utilized for various analyses and modeling efforts, such as predicting traffic patterns, optimizing public transportation schedules, and enhancing the efficiency of bike-sharing programs. social science","48":" Yulu Bike Rental Dataset Rental Trends and User Behavior ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F15074417%2F2d51367540969117110739eb4482da7f%2FWhatsApp-Image-2023-09-26-at-18.35.14.jpg?generation=1717081743433603&alt=media)\n\nYulu is India's premier micro-mobility service provider, offering distinctive vehicles for daily commutes. Initially aimed at eliminating traffic congestion in India, Yulu delivers a safe commuting solution through a user-friendly mobile app, enabling shared, solo, and sustainable travel.\n\nYulu zones are strategically placed at key locations, including metro stations, bus stops, office spaces, residential areas, and corporate offices, ensuring that first and last-mile travel is smooth, affordable, and convenient.\n\n**Business Case**\nRecently, Yulu has experienced significant declines in revenue. To address this, they have hired a consulting firm to analyze the factors influencing the demand for their shared electric cycles in the Indian market. They aim to understand the elements affecting the popularity of these shared electric cycles.\n\n**Potential Usecases**\n1. **Seasonal Demand Forecasting**\n2. **Impact of Weather on Bike Rentals**\n3. **Holiday and Working Day Analysis**\n4. **User Behavior Insights**\n5. **Operational Efficiency**\n6. **Urban Planning and Infrastructure Development**\n7. **Sustainability and Environmental Impact**\n8. **Business Strategy and Expansion**\n9. **User Experience Enhancement**\n10. **Data-Driven Marketing Campaigns** cycling","49":" Natural Language Processing of Chatbot Prompts  This dataset represents work related to the paper LENN. A copy of the evaluation questions for this dataset are as follows: \nHow would you create a KNN model to classify emails as spam or not spam based on their content and metadata?\nHow could you implement a KNN model to classify handwritten digits using the MNIST dataset?\nHow would you use a KNN approach to build a recommendation system for suggesting movies to users based on their ratings and preferences?\nHow could you employ a KNN algorithm to predict the price of a house based on features such as its location, size, and number of bedrooms etc?\nCan you create a KNN model for classifying different species of flowers based on their petal length, petal width, sepal length, and sepal width?\nHow would you utilise a KNN model to predict the sentiment (positive, negative, or neutral) of text reviews or comments?\nCan you create a KNN model for me that could be used in malware classification?\nCan you make me a KNN model that can detect a network intrusion when looking at encrypted network traffic? \nCan you make a KNN model that would predict the stock price of a given stock for the next week?\nCan you create a KNN model that could be used to detect malware when using a dataset relating to certain permissions a piece of software may have access to?\nCan you describe the steps involved in building a decision tree model to classify medical images as malignant or benign for cancer diagnosis and return a model for me?\nHow can you utilise a decision tree approach to develop a model for classifying news articles into different categories (e.g., politics, sports, entertainment) based on their textual content?\nWhat approach would you take to create a decision tree model for recommending personalised university courses to students based on their academic strengths and weaknesses?\nCan you describe how to create a decision tree model for identifying potential fraud in financial transactions based on transaction history, user behaviour, and other relevant data?\nIn what ways might you apply a decision tree model to classify customer complaints into different categories determining the severity of language used?\nCan you create a decision tree classifier for me?\nCan you make me a decision tree model that will help me determine the best course of action across a given set of strategies?\nCan you create a decision tree model for me that can recommend certain cars to customers based on their preferences and budget?\nHow can you make a decision tree model that will predict the movement of star constellations in the sky based on data provided by the NASA website?\nHow do I create a decision tree for time-series forecasting?\nCan you describe the steps involved in building a random forest model to classify different types of anomalies in network traffic data for cybersecurity purposes and return the code for me?\nIn what ways could you implement a random forest model to predict the severity of traffic congestion in urban areas based on historical traffic patterns, weather and time of day data?\nWhat approach would you take to create a random forest model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab results?\nHow could you deploy a random forest algorithm to develop a model for classifying different species of plants based on their botanical features, such as leaf shape, flower colour, and stem length?\nCan you describe how to create a random forest model for predicting customer turnover in a telecommunications company based on customer usage patterns, service plans, and time active?\nIn what ways might you apply a random forest model to analyse and classify sentiment in social media posts or product reviews?\nWhat steps would you take to develop a random forest model for identifying potential leads for sales people based on demographic information, purchasing intent, and online engagement?\nCan you create a Random Forest ensemble algorithm for me that incorporates at least 10 trees?\nCan you design a random forest classifier for me that can be used in cybersecurity for the detection of keyloggers and remote admin tools based on background process metadata?\nHow do you create a random forest algorithm that has tuned hyperparameters?\nCan you describe the steps involved in building an MLP model to classify images of handwritten digits from the MNIST dataset?\nHow can you implement a MLP model to predict the likelihood of a customer clicking on a particular advertisement based on their browsing history, demographics, and past interactions with similar ads?\nHow might you use an MLP approach to develop a model for detecting fraudulent activity in credit card transactions based on transaction history, spending patterns, and geographic locations?\nHow do you create a MLP model for diagnosing medical conditions (e.g., cancer, diabetes) based on patient health records, genetic markers, and test results?\nHow could you use an MLP algorithm to develop a model for forecasting energy consumption of different postcodes based on historical usage data, weather forecasts, and time of day?\nCan you create a MLP model for sentiment analysis of text data, classifying documents or social media posts as positive, negative, or neutral?\nHow would you utilise an MLP model for speech recognition, accurately transcribing spoken words into text for applications like virtual assistants?\nWhat steps would you take to build an MLP model for recommending personalised playlists to music listeners based on their listening history and preferences?\nHow would you use an MLP model to predict the outcome of a football match based on historical player interaction data and team performance?\nCan you create for me a MLP model with a minimum of three layers?\nHow would you create a support vector machine (SVM) model to classify different types of tumours as benign or malignant based on features extracted from medical images?\nCan you build a SVM model to predict whether a loan applicant is likely to default on their loan based on their credit history, income, and other relevant factors?\nHow might you use an SVM model for identifying spam emails by analysing their content, sender information, and email headers?\nCan you create a SVM model for detecting and classifying different types of plant diseases based on images of affected leaves, stems or fruit?\nHow do you create an SVM algorithm to develop a model for sentiment analysis of text data, classifying customer reviews or social media posts as positive, negative, or neutral?\nCan you tell me how to create an SVM model for predicting the outcome of sports matches (e.g. rugby) based on team statistics, player performance, and weather conditions?\nHow do you create an SVM model for speech recognition?\nWhat steps would you take to build an SVM model for predicting the stock market trends based on historical market data, technical indicators, and external factors like the news and economy?\nHow would you create an SVM model for examining the chemical compounds found in vehicle exhaust fumes?\nHow do you create a SVM?\nHow would you create a Naive Bayes model to classify emails as spam or not spam based on their content and metadata?\nCan you build a Naive Bayes model to predict the sentiment (positive, negative, or neutral) of customer reviews for a video streaming platform?\nHow do you create a Naive Bayes model to classify news articles into different categories (e.g., politics, sports, technology) based on their textual content?\nHow can you build a Naive Bayes model for detecting and classifying different types of diseases based on symptoms reported by patients?\nCan you create a Naive Bayes model for predicting whether a loan applicant is likely to default on their loan based on their financial history, credit score and personal information?\nCan you create a Naive Bayes model for classifying images of handwritten numbers from the MNIST dataset into their respective numeric representations?\nHow do you create a Naive Bayes model for predicting the probability of a customer purchasing a certain product based on their browsing history and past purchase behaviour?\nCan you create a Naive Bayes model for diagnosing medical conditions (e.g., diabetes, heart disease) based on patient health records and lab test results?\nHow do you create a Naive Bayes model for predicting the success of new songs, based on their composition and current trending listening patterns on music streaming platforms?\nCan you create a naive bayes model? ","50":" Bike rentals This is Yulu dataset. Yulu is India\u2019s leading micro-mobility service provider. About Yulu\n\nYulu is India\u2019s leading micro-mobility service provider, which offers unique vehicles for the daily commute. Starting off as a mission to eliminate traffic congestion in India, Yulu provides the safest commute solution through a user-friendly mobile app to enable shared, solo and sustainable commuting.\n\nYulu zones are located at all the appropriate locations (including metro stations, bus stands, office spaces, residential areas, corporate offices, etc) to make those first and last miles smooth, affordable, and convenient!\n\nYulu has recently suffered considerable dips in its revenues. They have contracted a consulting company to understand the factors on which the demand for these shared electric cycles depends. Specifically, they want to understand the factors affecting the demand for these shared electric cycles in the Indian market. intermediate retail and shopping pandas scipy seaborn","51":" OptiCom Signal Quality Dataset OptiCom Sig Quality: Signals' perf metrics for optical comms. ## Data Collection Method:\n\n- The data was collected using a combination of laboratory experiments and field measurements in real-world optical communication networks. Various instruments such as optical power meters, spectrum analyzers, and BER testers were utilized to capture relevant parameters.\n\n### Feature Names and Descriptions:\n\n1. Transmit Power Level (Tx): \n   - Description: The power level at which the signal is transmitted into the optical fiber.\n\n2. Receive Power Level (Rx): \n   - Description: The power level of the received signal after propagation through the optical fiber.\n\n3. Power Level at Various Points (Various Points): \n   - Description: Power levels measured at different points along the optical fiber.\n\n4. Signal-to-Noise Ratio (SNR) at Receiver (SNR Receiver): \n   - Description: The ratio of signal power to noise power at the receiver.\n\n5. SNR at Different Stages of Signal Processing (SNR Stages): \n   - Description: Signal-to-noise ratio measured at various stages of signal processing.\n\n6. Bit Error Rate (BER) at Receiver (BER Receiver): \n   - Description: The rate at which bits are received in error at the receiver.\n\n7. BER Under Different Environmental Conditions (BER Environmental): \n   - Description: Bit error rate measured under different environmental conditions.\n\n8. Modulation Format (Modulation Format): \n   - Description: The format used for modulating the optical signal.\n\n9. Modulation Depth or Index (Modulation Depth): \n   - Description: The depth or index of modulation applied to the optical signal.\n\n10. Transmission Distance (Transmission Distance): \n    - Description: The distance over which the optical signal is transmitted.\n\n11. Distance Between Repeaters or Amplifiers (Distance Between):\n    - Description: The distance between repeaters or amplifiers along the optical fiber.\n\n12. Fiber Attenuation Coefficients (Fiber Attenuation):\n    - Description: Coefficients representing the attenuation of the optical signal in the fiber.\n\n13. Splice Losses or Connector Losses (Splice Losses):\n    - Description: Losses incurred at splices or connectors along the optical path.\n\n14. Gain of Optical Amplifiers (Optical Amplifier Gain):\n    - Description: The gain provided by optical amplifiers to amplify the optical signal.\n\n15. Polarization Mode Dispersion (PMD) Coefficient (PMD Coefficient):\n    - Description: Coefficient representing the level of polarization mode dispersion in the system.\n\n16. PMD Compensation Techniques (PMD Compensation):\n    - Description: Techniques employed to compensate for polarization mode dispersion.\n\n17. Chromatic Dispersion (CD) Coefficient (CD Coefficient):\n    - Description: Coefficient representing the level of chromatic dispersion in the system.\n\n18. CD Compensation Techniques (CD Compensation):\n    - Description: Techniques employed to compensate for chromatic dispersion.\n\n19. Temperature at Critical Points (Temperature):\n    - Description: Temperature measurements at critical points in the optical communication system.\n\n20. Humidity Levels (Humidity):\n    - Description: Levels of humidity affecting the performance of the optical communication system.\n\n21. Wavelength Drift or Shift (Wavelength Drift):\n    - Description: Drift or shift in the wavelength of the optical signal.\n\n22. Stability of Optical Source (Optical Source Stability):\n    - Description: Stability characteristics of the optical source used in the system.\n\n23. Nonlinear Effects (Nonlinear Effects):\n    - Description: Effects of nonlinearities in the optical components on signal quality.\n\n24. Equalization Techniques (Equalization Techniques):\n    - Description: Techniques employed for equalizing the optical signal to mitigate distortions.\n\n25. Digital Signal Processing Algorithms (Signal Processing Algorithms):\n    - Description: Algorithms used for digital signal processing in the optical communication system.\n\n26. Optical Crosstalk from Adjacent Channels or Fibers (Optical Crosstalk):\n    - Description: Crosstalk from adjacent channels or fibers affecting signal integrity.\n\n27. Crosstalk Mitigation Techniques (Crosstalk Mitigation):\n    - Description: Techniques employed to mitigate optical crosstalk in the system.\n\n28. Level of Data Traffic in the Network (Network Traffic Load):\n    - Description: The level of data traffic present in the optical communication network.\n\n29. Congestion Levels (Congestion Levels):\n    - Description: Levels of congestion experienced in the optical communication network.\n\n30. Presence of Security Threats Affecting Signal Quality (Security Threats):\n    - Description: Identification of security threats impacting signal integrity in the network.\n\n31. Impact of Security Measures on Signal Integrity (Security Measures Impact):\n    - Description: Assessment of the impact of security measures on signal quality.\n\n32. Signal Quality (Signal Quality):\n    - Description: Measure of the overall quality of the optical signal.\n\n earth and nature business internet electronics beginner tabular binary classification","52":" City Traffic and Vehicle Behavior Dataset \"Urban Mobility Insights: Exploring City Traffic and Vehicle Behavior\" The City Traffic and Vehicle Behavior Dataset is a collection of data regarding various factors related to city traffic and vehicle behavior. Here's a description of each column in the dataset:\n\n**1. City:** The name of the city where the data was collected.\n**2. Vehicle Type:** The type of vehicle involved in the traffic (e.g., car, truck, bus, motorcycle).\n**3. Weather:** The prevailing weather conditions at the time of data collection (e.g., sunny, rainy, snowy).\n**4. Economic Condition:** The economic conditions prevailing in the city (e.g., booming, recession, stable).\n**5. Day Of Week:** The day of the week when the data was collected (e.g., Monday, Tuesday, etc.).\n**6. Hour Of Day:**The hour of the day when the data was collected, typically represented in 24-hour format.\n**7. Speed:** The speed of the vehicles in the traffic, measured in miles per hour (mph) or kilometers per hour (km\/h).\n**8. Is Peak Hour:** A binary indicator (0 or 1) indicating whether the data was collected during peak traffic hours.\n**9. Random Event Occurred:** A binary indicator (0 or 1) indicating whether any random event (e.g., accident, road closure) occurred during the data collection period.\n**10. Energy Consumption:** The energy consumption of vehicles, typically measured in fuel consumption or electricity usage.\n\nThis dataset can be used for various purposes such as analyzing traffic patterns, studying the impact of weather and economic conditions on traffic, evaluating energy consumption trends, and predicting traffic congestion. Researchers and transportation planners may find this dataset valuable for understanding and improving urban mobility. law","53":" US Automatic Traffic Recorder Stations Data Vehicle Traffic Counts and Locations at US ATR Stations _____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n united states automobiles and vehicles data visualization","54":" Economy_Health brazil Contains data from the World Bank's data portal. There is also a consolidated. Contains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Gender equality is a core development objective in its own right. It is also smart development policy and sound business practice. It is integral to economic growth, business growth and good development outcomes. Gender equality can boost productivity, enhance prospects for the next generation, build resilience, and make institutions more representative and effective. In December 2015, the World Bank Group Board discussed our new Gender Equality Strategy 2016-2023, which aims to address persistent gaps and proposed a sharpened focus on more and better gender data. The Bank Group is continually scaling up commitments and expanding partnerships to fill significant gaps in gender data. The database hosts the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Cities can be tremendously efficient. It is easier to provide water and sanitation to people living closer together, while access to health, education, and other social and cultural services is also much more readily available. However, as cities grow, the cost of meeting basic needs increases, as does the strain on the environment and natural resources. Data on urbanization, traffic and congestion, and air pollution are from the United Nations Population Division, World Health Organization, International Road Federation, World Resources Institute, and other sources.\n\nContains data from the World Bank's data portal. There is also a consolidated country dataset on HDX. Data here cover child labor, gender issues, refugees, and asylum seekers. Children in many countries work long hours, often combining studying with work for pay. The data on their paid work are from household surveys conducted by the International Labour Organization (ILO), the United Nations Children's Fund (UNICEF), the World Bank, and national statistical offices. Gender disparities are measured using a compilation of data on key topics such as education, health, labor force participation, and political participation. Data on refugees are from the United Nations High Commissioner for Refugees complemented by statistics on Palestinian refugees under the mandate of the United Nations Relief and Works Agency. brazil banking advanced data analytics graph","55":" US Traffic Congestions (2016-2022) Comprehensive Dataset of 33 Million U.S. Traffic Congestion Events ### Description\nThis is a countrywide traffic congestion dataset that covers __49 states of the USA__. The congestion events data were collected from __February 2016 to September 2022__, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by various entities, including the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road networks. The dataset contains approximately __33 million congestion records__. We also provide a __sampled version__ of data that includes 2 million events for easier processing and handling for those who prefer to work with a smaller amount of data.\n\n### Acknowledgements\nIf you use this dataset, please kindly cite the following paper:\n\n- Moosavi, Sobhan, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan Parthasarathy, and Rajiv Ramnath. [\"Short and long-term pattern discovery over large-scale geo-spatiotemporal data.\"](https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3292500.3330755) In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2905-2913. 2019.\n\n### Inspiration\nThe US Traffic Congestion dataset can be used for numerous applications, such as traffic modeling, simulated routing, identifying traffic hotspot locations, and exploring intrinsic traffic patterns and how they evolve over time. \n\n### Missing Data and Update Policy\nPlease note that the dataset may be missing data for certain days, which could be due to network connectivity issues during data collection. The dataset will not be updated, and this version should be considered the latest.\n\n### Usage Policy and Legal Disclaimer\nThis dataset is being distributed solely for research purposes under the Creative Commons Attribution-Noncommercial-ShareAlike license (CC BY-NC-SA 4.0). By downloading the dataset, you agree to use it only for non-commercial, research, or academic applications. If you use this dataset, it is necessary to cite the paper mentioned above.\n\n### Inquiries or need help?\nFor any inquiries or assistance, please contact Sobhan Moosavi at sobhan.mehr84@gmail.com united states categorical transportation tabular urban planning","56":" Traffic Prediction Dataset Real Traffic Prediction Dataset  Traffic congestion and related problems are a common concern in urban areas. Understanding traffic patterns and analyzing data can provide valuable insights for transportation planning, infrastructure development, and congestion management. \n\n**What exactly is this dataset and how was it created?**\nit is a valuable resource for studying traffic conditions as it contains information collected by a computer vision model. The model detects four classes of vehicles: cars, bikes, buses, and trucks. The dataset is stored in a CSV file and includes additional columns such as time in hours, date, days of the week, and counts for each vehicle type (CarCount, BikeCount, BusCount, TruckCount). The \"Total\" column represents the total count of all vehicle types detected within a 15-minute duration. \n\nThe dataset is updated every 15 minutes, providing a comprehensive view of traffic patterns over the course of one month. Additionally, the dataset includes a column indicating the traffic situation categorized into four classes: 1-Heavy, 2-High, 3-Normal, and 4-Low. This information can help assess the severity of congestion and monitor traffic conditions at different times and days of the week.\n\n**In what cases can it be useful?**\nThe dataset is useful in transportation planning, congestion management, and traffic flow analysis. It helps understand vehicle demand, identify congested areas, and inform infrastructure improvements. The dataset enables targeted interventions like signal optimizations and lane adjustments. It allows researchers to study traffic patterns by hour, day, or specific dates and explore correlations with external factors. It supports transportation research on vehicle relationships and traffic behavior. Urban planners can assess traffic impact for zoning and infrastructure decisions. Overall, the dataset empowers stakeholders to make data-driven decisions, enhance urban mobility, and create efficient and sustainable cities.\n\n**Is there a new update?**\nYes, in the next update, the dataset will be expanded to include the speed of the cars. Additionally, the data will not be limited to a single route; instead, it will encompass a traffic intersection. This expansion aims to provide a more comprehensive understanding of traffic dynamics and enable better analysis and decision-making for traffic management. The inclusion of speed data will offer insights into the flow and efficiency of vehicles, further enhancing the dataset's value for transportation planning and congestion management efforts.\n\nThanks business intermediate advanced data analytics classification feature engineering","57":" Traffic Detection Project This dataset contains various images of traffic. Images mostly taken from Turkey Are you passionate about computer vision, object detection, or traffic monitoring systems? Look no further! This comprehensive dataset, carefully curated and annotated, offers a rich collection of traffic camera images from various countries around the world. Whether you're an AI researcher, a machine learning enthusiast, or a developer working on traffic management solutions, this dataset is a valuable resource for your projects.\n\nKey Features:\n\nDiverse Geographic Coverage: Our dataset encompasses traffic camera images from different countries, providing a global perspective on traffic monitoring and management.\n\nHigh-Quality Annotations: Each image is meticulously annotated using bounding boxes to identify various objects, including vehicles, pedestrians, and traffic signs, making it perfect for object detection tasks.\n\nVaried Environmental Conditions: The dataset includes images captured under different weather conditions, lighting, and traffic scenarios, making it suitable for real-world applications.\n\nTraining-Ready: This dataset has been used successfully to train a YOLOv5 object detection model, achieving an impressive Mean Average Precision (mAP) of 0.89, Precision (P) of 0.88, and Recall (R) of 0.89, ensuring that it's ready for immediate use in your own projects.\n\nOpen for Research and Innovation: We believe in the power of collaboration. By sharing this dataset on Kaggle, we aim to foster innovation in the fields of computer vision and traffic management. Researchers and developers can leverage this resource to develop advanced traffic monitoring and safety solutions.\n\nPotential Use Cases:\n\nObject detection and tracking in traffic camera feeds.\nTraffic analysis and congestion prediction.\nRoad safety and accident prevention.\nUrban planning and smart city development.\nAI-based traffic management systems.\nCitation:\n\nIf you use this dataset in your work, please consider citing it to give credit to the contributors and help others find this valuable resource.\n\nAnd if you want to weights file send me an email!\n people computer vision image public safety image classification","58":" Newcastle's 4 main junctions traffic data  **Context**\n\nTraffic congestion is rising in cities around the world. Contributing factors include expanding urban populations, aging infrastructure, inefficient and uncoordinated traffic signal timing and a lack of real-time data.\n\n**Content**\n\nThis dataset contains 48.1k (48120) observations of the number of vehicles each hour in four different junctions:\n1) DateTime\n2) Juction\n3) Vehicles\n4) ID\n\n**About the data**\n\nThe sensors on each of these junctions were collecting data at different times, hence you will see traffic data from different time periods. Some of the junctions have provided limited or sparse data requiring thoughtfulness when creating future projections.\n\n**Source**\n\n(Confidential Source) - Use only for educational purposes\nIf you use this dataset in your research, please credit the author. ","59":" Cities Skyline Springfield  ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2e9634ea52cb839eb70af650ae0e3285%2F20230719072239_1%20-%20Copy.jpg?generation=1691161469842544&alt=media)\n\n**Welcome to Springfield.** The 1-tile city I created quickly in Cities Skyline. I do not have all DLC, so you will see quite a few 0 in DLC items\n\nI've been playing cities skyline for years and loved it. However, the game is time-consuming, so I haven't touched the game in a while. In light of the announcement of Cities Skyline II, it has sparked my interest in the game again, all I need is a justification to play it. \n\nI have been searching for a dataset to work on as my next project and found out there are a lot of datasets in regard to statistics from a city. The idea dawned on me that if I am analyzing a city's statistics, why not analyse my own city. Hence the creation of Springfield. \n\nThe data is collected using a steam workshop item called [More City Statistics](https:\/\/steamcommunity.com\/sharedfiles\/filedetails\/?id=2685974449) created by [rcav8tr](https:\/\/steamcommunity.com\/profiles\/76561198959572921\/myworkshopfiles\/?appid=255710)\n\nMajor Events\n\nSpringfield has witnessed a series of significant events that have molded its urban landscape:\n\n2024: Discovery of an oil deposit, which laid the foundation for the city's primary source of revenue. This year also saw a one-time grant of 10 million, leading to a substantial cash injection, and the initiation of the city's first bus line.\n2025: Development and commencement of operations in the Oil and farming industries.\n2027: Introduction of the tram line and the inauguration of the Central Park.\n2028: Opening of the city's university.\n2030: Upgrades to utility systems, including nuclear power and water treatment facilities.\n2036: Opening of passenger and cargo airports.\n2040s: Springfield reaches full urbanization.\n2044: The city grapples with the \"Great Congestion,\" primarily due to an upsurge in trucks delivering goods, which the existing infrastructure couldn't support.\n2047: Introduction of the metro system.\n2045-2065: Implementation of extensive road reforms, with a primary focus on the oil-rich areas. This period also saw the introduction of a new highway, toll booths to regulate traffic flow and recover costs, banning of external vehicles in downtown, optimization of bus and tram routes, and the establishment of a more walkable city layout.\n2065-2069: Expansion and optimization of the oil areas, emphasizing refining oil and plastic production over raw crude oil production.\n2069: Implementation of the Tax Reduction Act.\n\n**Gallary**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fbbbaa8d14afd56945eef4b69f499419f%2F20230719072216_1%20-%20Copy.jpg?generation=1691162526803072&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2Fce17e8f8c1b96f92a28d6bc58ee8fac9%2F20230719072411_1%20-%20Copy.jpg?generation=1691162573788287&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F55ca6d902c59c7ff2500670ff76e7c93%2F20230719072609_1%20-%20Copy.jpg?generation=1691162621739170&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13710878%2F2c0fffa2ee428cb2100dfb338ab0b6be%2F20230719072710_1%20-%20Copy.jpg?generation=1691162642523757&alt=media) games cities and urban areas business beginner intermediate advanced","60":" Cars Detection Cars Detection Dataset **Description:**\n\nWelcome to the Cars Object Detection Dataset, a comprehensive and meticulously annotated collection designed to empower researchers and practitioners in the field of computer vision and object detection. This dataset showcases a diverse range of vehicles, comprising five distinct classes: 'Ambulance', 'Bus', 'Car', 'Motorcycle', and 'Truck'.\n\n\n\n**Dataset Overview:**\n\nOur meticulously curated dataset contains a wealth of high-resolution images featuring vehicles captured in a variety of real-world scenarios. Each image has been expertly annotated to facilitate accurate object detection, enabling you to explore and advance cutting-edge object detection algorithms.\n\n\n\n**Key Features:**\n\n**Diverse Vehicle Classes:** With a comprehensive set of five vehicle classes, our dataset enables you to tackle a wide range of real-world challenges in object detection, from the smaller and more agile 'Motorcycle' to the larger and more complex 'Truck'.\n\n**High-Quality Annotations:** Each image is accompanied by precise bounding box annotations for all instances of the target classes, meticulously handcrafted to ensure accuracy and reliability. This facilitates robust model training and evaluation.\n\n**Real-World Scenarios:** The dataset captures vehicles in various environmental conditions, lighting situations, and viewpoints, reflecting the complexities of object detection tasks in real-world applications.\n\n**Large-Scale Collection:** Our dataset encompasses a substantial number of images, providing ample training and testing samples to foster robust model development and comprehensive evaluation.\n\n\n\n**Potential Applications:**\n\nThe Cars Object Detection Dataset offers a plethora of potential applications, including but not limited to:\n\n**Object Detection Research:** Fuel your research endeavors by leveraging our dataset to design, develop, and benchmark state-of-the-art object detection algorithms tailored to automotive scenarios.\n\n**Autonomous Vehicles:** Train and validate object detection models that contribute to the advancement of autonomous driving systems, enhancing their ability to perceive and interact with diverse vehicles on the road.\n\n**Traffic Monitoring and Management:** Harness the power of our dataset to create solutions for traffic monitoring, congestion analysis, and vehicle counting in urban environments.\n\n**Safety and Emergency Services:** Develop object detection models that aid emergency response teams in recognizing and responding to different vehicle types, such as 'Ambulance' and 'Bus', to enhance road safety.\n\n\n\n**Conclusion:**\n\nEmbark on a journey of innovation and discovery with our Cars Object Detection Dataset. Whether you're a seasoned researcher, an aspiring data scientist, or a visionary engineer, this dataset serves as a powerful resource to drive progress in object detection technology within the realm of automotive environments. Explore, experiment, and excel with the wealth of information at your fingertips. computer vision deep learning image yolo object detection","61":" Bike Sharing Dataset Regression and anamoly detection using Bike Sharing Dataset Bike sharing systems represent a modern evolution of traditional bike rentals, where the entire process - from membership to rental and return - has been automated. This innovation allows users to easily rent bikes from one location and conveniently return them at another. With over 500 bike-sharing programs worldwide, encompassing a staggering 500,000 bicycles, these systems have garnered significant attention due to their pivotal role in addressing traffic congestion, environmental concerns, and promoting public health.\n\nThe appeal of bike sharing systems extends beyond their practical applications; they offer a wealth of valuable data for research purposes. Unlike other modes of transport like buses or subways, bike sharing systems record precise details such as travel duration and specific departure and arrival positions. This unique attribute transforms these systems into virtual sensor networks that effectively capture mobility patterns across the city. As a result, these datasets hold the potential to detect and monitor crucial events and trends, contributing to the understanding of urban dynamics and fostering smarter city planning.\n\n###########################################\n###########                 Data Set               ###########\n###########################################\n\n\nBike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http:\/\/capitalbikeshare.com\/system-data. We aggregated the data on two hourly and daily basis and then \nextracted and added the corresponding weather and seasonal information. Weather information are extracted from http:\/\/www.freemeteo.com. \n\n\n###########################################\n###########         Associated tasks         ###########\n###########################################\n1. Regression: \n\tPredication of bike rental count hourly or daily based on the environmental and seasonal settings.\n\t\n2. Event and Anomaly Detection:  \n\tCount of rented bikes are also correlated to some events in the town which easily are traceable via search engines. For instance, query like \"2012-10-30 Washington D.C.\" in Google returns related results to Hurricane Sandy. Therefore the data can be used for validation of anomaly or event detection algorithms as well.\n\n###########################################\n##### Columns Details with their encoded labels: #####\n###########################################\n- instant: record index\n- dteday : date\n- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n- yr : year (0: 2011, 1:2012)\n- mnth : month ( 1 to 12)\n- hr : hour (0 to 23)\n- holiday : weather day is holiday or not (extracted from http:\/\/dchr.dc.gov\/page\/holiday-schedule)\n- weekday : day of the week\n- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n+ weathersit : \n- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of total rental bikes including both casual and registered\n\n\n-Use of this dataset in publications must be cited to the following publication:\n\n[1] Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007\/s13748-013-0040-3.\n\n@article{\n\tyear={2013},\n\tissn={2192-6352},\n\tjournal={Progress in Artificial Intelligence},\n\tdoi={10.1007\/s13748-013-0040-3},\n\ttitle={Event labeling combining ensemble detectors and background knowledge},\n\turl={http:\/\/dx.doi.org\/10.1007\/s13748-013-0040-3},\n\tpublisher={Springer Berlin Heidelberg},\n\tkeywords={Event labeling; Event detection; Ensemble learning; Background knowledge},\n\tauthor={Fanaee-T, Hadi and Gama, Joao},\n\tpages={1-15}\n}\n cycling exploratory data analysis data visualization outlier analysis regression","62":" all traffic signs dataset  This dataset contains images of various traffic signs captured in different conditions to facilitate the training and evaluation of machine learning models for traffic sign recognition. The dataset is designed to be used in computer vision tasks such as object detection and classification. law","63":" Iranian Traffic Sign Detection Annotated Dashcam images to detect traffic signs  **A beginner-friendly object recognition dataset of &gt;6000 images,**\n\nAs part of my thesis, I was required to gather a dataset for Iranian Traffic Signs. I set up a dashcam using my phone and a tape and drove for ~ 1 hour. \n\nNext, I got the recorded video and chopped up the frames. I selected only 3-4 frames per second, otherwise I would end up with many frames looking alike. \n\nThe images were not annotated, I needed supervised labels to actually train my network.  I used CVAT for my annotations. It was a time-consuming progress. \n\nThere are multiple signs on the road and many of them are advertisements and not traffic signs. To make sure my model doesn't mix them together, I annotated advertisement signs but put a different label on them. This way my model acknowledged these signs but would differentiate them from the valid traffic signs we wanted to detect. \n\nIf you're looking for a beginner-friendly dataset to practice object recognition give it a look! beginner advanced computer vision image object detection","64":" Driver Inattention Detection Dataset  # Description:\n\nThis comprehensive gray-scale dataset is a valuable resource for researchers and developers in the field of computer vision, machine learning and deep learning, particularly those focused on driver behavior analysis and driver assistance systems. Comprising over 14,000 labeled images across six distinct classes, it provides a diverse and extensive collection for training, validation, and testing purposes, specifically tailored for gray-scale image processing.\n\n**The dataset is organized into three main directories:**\n\n**Training Set (train):**This directory contains 11,942 gray-scale images, carefully curated and labeled across the six classes.\n\n**Validation Set (validation):** With 1,922 gray-scale images, this subset provides a means for fine-tuning models and evaluating their performance during development.\n\n**Test Set (test):** Comprising 985 gray-scale images, this directory is reserved for final model evaluation and benchmarking.\n\n**The dataset encompasses six classes of driving behaviors:**\n\n**1- Dangerous Driving:** Gray-scale images capturing instances of reckless or hazardous driving behavior, such as speeding or erratic lane changes.\n**2- Distracted Driving:** Instances where the driver's attention is diverted away from the road, possibly due to smartphone usage, eating, or interacting with passengers.\n**3- Drinking:** Gray-scale images depicting drivers consuming alcoholic beverages while behind the wheel, highlighting the dangers of driving under the influence.\n**4- Safe Driving:** Examples of responsible and cautious driving behavior captured in gray-scale, including obeying traffic laws, maintaining safe distances, and using turn signals.\n**5- Sleepy Driving:** Instances where drivers exhibit signs of drowsiness or fatigue, posing a significant risk of accidents due to reduced alertness, depicted in gray-scale.\n**6- Yawn:** Gray-scale images capturing drivers in the act of yawning, often indicative of fatigue or tiredness, which can impair driving performance.\n automobiles and vehicles computer vision deep learning image keras","65":" Traffic-Signs-Dataset Unlock Road Safety: Explore 52 Types of Traffic Signs in High-Resolution Imagery The Traffic-Signs-Dataset is a comprehensive collection of images used for computer vision tasks, particularly in traffic sign detection and recognition. It comprises a diverse range of images showcasing 52 different types of traffic signs commonly encountered in urban and rural settings. Each image is labeled, enabling supervised learning algorithms to be trained effectively. This dataset is instrumental for researchers, developers, and practitioners in the field, facilitating the development and evaluation of machine learning models and computer vision algorithms for accurate interpretation of traffic signs in various real-world conditions. law image image classification image augmentation image generator","66":" Healthy and Unhealthy Goat Images For Educational, Research or Commercial Purpose Collection of images depicting healthy and unhealthy conditions in goats, showcasing visual examples for veterinary diagnostics, research, and educational purposes. The dataset includes diverse images highlighting symptoms, physical traits, and conditions affecting goat health, supporting studies on animal welfare, disease detection, and agricultural management animals education intermediate tabular","67":" Wildlife  We have integrated this dataset through field photography and online collection, which includes seven categories: birds, egrets, deer, sheep, hares, wild cats, and wild boars, and can be used in deep learning target detection. animals deep learning image","68":" Cat Dataset A dataset of 29843 cat pictures (64x64), compiled together for training models. **Context**\n\nA dataset of 29843 cat pictures (64x64), compiled together for training models.\n\n**Description** :\nThe Cat Dataset is a collection of 29,843 color images of cats, intended for training machine learning models for computer vision tasks such as image classification, object detection, and image segmentation. The dataset consists of various breeds, poses, backgrounds, and lighting conditions, providing a diverse representation of cat images.\n\n**Details**\n\nNumber of Images: 29,843\n\nImage Size: 64 x 64 pixels\n\nImage Format: PNG\n\nImage Resolution: RGB (3 channels)\n\n**Data Split**\n\nThe dataset is divided into the following subsets:\n\nTraining Set: 29,843 images (100% of the dataset)\n\nThis dataset can be utilized for a variety of computer vision tasks, including but not limited to:\n\nImage Classification: Training models to classify images as containing a cat or not.\nObject Detection: Detecting and localizing cats within images.\nImage Segmentation: Pixel-level labeling and segmentation of cats in images.\nTransfer Learning: Using pre-trained models on this dataset for related computer vision tasks.\n\n**Acknowledgements**\nOwner: [AnnikaV9](https:\/\/github.com\/AnnikaV9)\n\nThe dataset was originally found at: https:\/\/av9.dev\/cat-dataset\/ earth and nature animals classification","69":" FluNet, Global Influenza Programme - WHO The Importance of Monitoring the Global Uptrend of Influenza. **Introduction:**\n\nIn the face of an ever-present threat of influenza pandemics, the World Health Organisation's FluNet system stands as a critical global guardian. By tracking influenza cases and viral subtypes worldwide, FluNet enables early detection and rapid response to potential outbreaks. This is particularly crucial with the looming threat of avian influenza strains like H5N1, which has recently raised concerns due to its presence in dairy cows, specifically in their udder receptors. This worrying development underscores the importance of vigilant surveillance and swift action to prevent zoonotic transmission and potential pandemics. \n\n*Cows Are Potential Spreaders of Bird Flu to Humans.* [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n\n**FluNet: A Global Guardian Against the Next Flu Pandemic.**\n\nThe threat of a flu pandemic looms large, particularly with the ever-present risk of avian influenza strains like H5N1 making the jump to humans. In this scenario, early detection and rapid response are crucial for mitigating the impact of a potential outbreak. This is where FluNet, a global influenza surveillance system by the World Health Organisation (WHO), steps in as a vital early warning system.\n\n**What is FluNet?**\n\nLaunched in 1997, FluNet is a web-based platform that serves as a central repository for influenza data collected from over 129 countries. National Influenza Centres (NICs) and collaborating laboratories contribute data on:\n- The number of influenza cases detected.\n- The specific influenza virus subtypes identified (e.g., A(H1N1), B).\n- This real-time data allows for a crucial function: Global influenza virological surveillance.\n\n**The importance of Virological Surveillance.**\n\nFluNet goes beyond simply counting flu cases. By tracking the types of influenza viruses circulating, FluNet provides valuable insights into:\n- Viral Spread: Identifying dominant strains and their geographic distribution helps predict potential outbreaks.\n- Mutations: Monitoring changes in viral strains allows for early detection of potentially dangerous mutations that could increase transmissibility or virulence in humans.\n- Vaccine Effectiveness: Understanding circulating strains helps determine if the current influenza vaccine formulation remains effective.\n\n**FluNet and Avian Flu Pandemics.**\n\nAvian influenza viruses, particularly the H5N1 strain, pose a significant pandemic threat. These viruses primarily infect birds, but mutations can enable them to jump to other animals then to humans, as seen in rare cases. \nFluNet plays a critical role in detecting such zoonotic events (transmission from animals to humans) by:\n- Early Warning: Identifying an increase in H5N1 cases in poultry or humans in a specific region can trigger immediate public health responses.\n- Strain Tracking: FluNet allows researchers to track the specific H5N1 strain circulating, aiding in understanding its potential for human-to-human transmission.\n- Seasonal Flu Monitoring: Tracking seasonal influenza activity allows public health officials to target prevention efforts and optimise resource allocation.\n- Vaccine Development: Data on circulating strains informs vaccine development and helps ensure vaccines remain effective.\n- Global Collaboration: FluNet fosters international collaboration in influenza surveillance and response, promoting a unified approach to pandemic preparedness.\n\nWhile invaluable, FluNet has limitations:\n- Data Dependence: FluNet relies on timely and accurate data reporting from participating countries. Delays or inconsistencies can hinder its effectiveness.\n- Focus on Confirmed Cases: FluNet primarily tracks confirmed influenza cases, potentially missing milder or undiagnosed infections.\n\n**The Future of FluNet.**\n\nThe WHO is constantly working to improve FluNet. Potential advancements include:\n- Strengthening Data Collection: Implementing stricter reporting standards and capacity building in developing countries can ensure timely and accurate data.\n- Integration with Other Systems: Linking FluNet with other surveillance systems like respiratory illness tracking can provide a more comprehensive picture.\n- Enhanced Data Analysis: Utilising advanced analytics can help identify emerging threats and predict potential outbreaks more effectively.\n\n**FluNet: Conclusion.**\n\nFluNet stands as a cornerstone of global influenza surveillance. By facilitating early detection of potential pandemics like avian flu, it serves as a critical line of defence for public health. As we move forward, continued investment in strengthening FluNet's capabilities will be essential in ensuring our preparedness for the next influenza challenge.\n\nFluNet: Summary web page (regularly updated). [link](https:\/\/www.who.int\/tools\/flunet\/flunet-summary)\n\nA table with a list of historical influenzas and their impact. [link](https:\/\/docs.google.com\/spreadsheets\/d\/1q7MUgB9h0KJyHZFonkPUxk7HsvszfZMLMQHX7FsPJz4\/edit?usp=sharing)\n\nCurrent H5N1 Bird Flu Situation in Dairy Cows: CDC - [link](https:\/\/www.cdc.gov\/bird-flu\/situation-summary\/mammals.html)\n\n**Data Visualisations:**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F7d09bb3cde653776650cb9c479218174%2FScreenshot%202024-05-22%2017.08.46.png?generation=1716394219323407&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fff8586ac4f1af15abcb7b8cffc3b93b8%2FScreenshot%202024-05-22%2015.31.37.png?generation=1716392814698129&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F99152d8a5fccfb5584cdb2e4644f40a3%2FScreenshot%202024-05-22%2017.06.16.png?generation=1716394324316727&alt=media)\n\nA Markdown document with R code for the above visualisations. [link](http:\/\/rpubs.com\/Paddy_5142\/1187730)\n\nFunctions of the code:\n- Load the required libraries and dataset.\n- Identify numeric and character columns.\n- Replace NA values in numeric columns with 0 and in character columns with an empty string.\n- Print basic and extended summaries of the data.\n- Perform missing values analysis and print the summary.\n- Create and print the following separate plots:\n - Visualisation of missing values: Shows the number of missing values for each variable.Histograms for numeric columns.\n - Histograms for Numeric Columns: Displays the distribution of numeric variables.\n - Boxplot for SPEC_PROCESSED_NB by FLUSEASON: Compares the distributions of SPEC_PROCESSED_NB across different FLUSEASON values.\n - Time series plot for SPEC_PROCESSED_NB over ISO_WEEKSTARTDATE: Plots the trend of SPEC_PROCESSED_NB over time.\n \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd3ad9224156caacf8c91fa1056f248c6%2FScreenshot%202024-05-23%2012.40.58.png?generation=1716465022560161&alt=media)\n\nA Markdown document with R code for the above visualisation. [link](http:\/\/rpubs.com\/Paddy_5142\/1188049)\n\n**Description of the Chart:**\n\nThe above chart is a faceted plot that shows the number of influenza cases over time by region. The data is divided into six regions: Africa, Americas, Eastern Mediterranean, Europe, South-East Asia, and Western Pacific. Each facet (sub-plot) represents one of these regions, displaying the number of cases on the y-axis and the years on the x-axis. The y-axes are labelled with \"Number of Cases\" and are scaled differently for each region to fit the range of their data.\n\n**Observations by Region:**\n- Africa (Top Left).\n  - Displays a sharp increase in cases around 2010 and another notable peak around 2015-2020.\n- Americas (Top Right).\n  - Shows a dramatic spike in cases around 2010.\n- Eastern Mediterranean (Middle Left).\n  - Exhibits a significant rise in cases around 2010 and another increase around 2015-2020.\n- Europe (Middle Right).\n  - Demonstrates a spike around 2010 and consistently higher numbers in the subsequent years.\n- South-East Asia (Bottom Left).\n  - Shows a peak around 2010 and another substantial increase around 2015-2020.\n- Western Pacific (Bottom Right).\n  - Displays a marked increase in cases around 2010 and another spike around 2015-2020.\n\n**General Uptrend in Influenza Cases:**\n- Population Growth:\n  - The increasing global population naturally leads to a higher number of influenza cases over time.\n- Urbanisation:\n  - More people living in densely populated areas facilitates the faster spread of infectious diseases like influenza.\n- Improved Surveillance and Reporting:\n  - Advances in influenza surveillance systems, diagnostic tools, and reporting mechanisms have led to more accurate and higher reporting of influenza cases. Enhanced surveillance efforts have resulted in more cases being detected and recorded.\n- Viral Evolution:\n  - The continuous evolution of the influenza virus results in new strains that can bypass existing immunity in the population, leading to periodic increases in cases.\n\n**Specific Spikes in 2009-2010, 2015, and 2020:**\n- 2009-2010 Spike (H1N1 Pandemic):\n  - The 2009-2010 spike in influenza cases corresponds to the H1N1 influenza pandemic, caused by a novel H1N1 strain that rapidly spread worldwide. The high transmissibility and widespread impact of this novel strain resulted in a significant increase in reported influenza cases during this period.\n- 2015 Spike (H3N2 Influenza Virus):\n  - In 2014-2015, many regions experienced severe influenza seasons driven primarily by the H3N2 strain of the virus. H3N2 is known to cause more severe illness, especially in older adults and young children, leading to higher numbers of reported cases and hospitalisations.\n  - The 2014-2015 flu season saw a mismatch between the circulating H3N2 strains and the strains included in the seasonal flu vaccine. This mismatch reduced the vaccine's effectiveness, contributing to higher case numbers as more people were susceptible to the circulating virus.\n  - Continuous improvements in global influenza surveillance systems, including better diagnostic tools and reporting mechanisms, can also lead to an apparent increase in cases. Enhanced surveillance efforts in some regions might have contributed to the observed spike in 2015.\n- 2020 Spike (COVID-19 Pandemic Effects):\n  - The onset of the COVID-19 pandemic in late 2019 and early 2020 had significant impacts on global health systems and disease reporting. Initially, there was heightened awareness and testing for respiratory illnesses, including influenza, which could have contributed to a spike in reported influenza cases in early 2020.\n  - During the early months of the COVID-19 pandemic, there was a surge in health-seeking behaviour and testing for respiratory symptoms. This increased vigilance likely led to more influenza cases being detected and reported.\n  - Early in the COVID-19 pandemic, distinguishing between COVID-19 and influenza based solely on symptoms was challenging. This difficulty might have led to an increased number of influenza diagnosis, either due to co infections or initial misdiagnosis.\n  - In some regions, the initial stages of the COVID-19 pandemic overlapped with the tail end of the traditional influenza season (late winter to early spring). This overlap could have led to a temporary spike in influenza cases before the full impact of COVID-19 mitigation measures (e.g., lockdowns, social distancing) drastically reduced influenza transmission.\n\n**Chart Conclusion.**\n\nThe chart illustrates the dynamic nature of influenza case trends across different regions and time periods. While specific factors like the H1N1 pandemic in 2009-2010 and the COVID-19 pandemic in 2020 are clear contributors to spikes, the consistent uptrend in influenza cases can be attributed to improved surveillance, population growth, urbanisation, and viral evolution. Each of these elements plays a role in shaping the observed patterns in influenza case data across different regions and time periods.\n\n*Could bird flu in cows lead to a human outbreak? Slow response worries scientists.\nThe H5N1 virus is a long way from becoming adapted to humans, but limited testing and tracking mean we could miss danger signs.* [link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n*Risk assessment of a highly pathogenic H5N1 influenza virus from mink.* [link](https:\/\/www.nature.com\/articles\/s41467-024-48475-y)\n\n**Overall Conclusion:**\n\nFluNet's invaluable role in global influenza surveillance cannot be overstated. By providing real-time data on circulating influenza strains and their geographic distribution, FluNet empowers public health officials to make informed decisions regarding prevention, vaccination, and resource allocation. As we navigate the complex landscape of influenza threats, FluNet's continuous evolution and expansion are crucial for maintaining our preparedness. However, the recent finding of H5N1 receptors in the udders of dairy cows introduces a new and concerning dimension. This discovery highlights the potential for zoonotic transmission and underscores the need for heightened vigilance and intensified research to understand the implications of this new transmission pathway. The risk of H5N1 jumping from animals to humans remains a real concern, necessitating a coordinated global effort to mitigate the risks and safeguard public health.\n\n\n\nPatrick Ford \ud83d\udc2e\n\n------------------------------------------------------------------------------------------------------------------------\n\nA previous project of mine entitled *COVID-19 & the virus that causes it: SARS-CoV-2.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/covid-19)\n\n diseases public health data visualization r","70":" whaleTagData Automated behavioral classification using multisensor data collected from whales These datasets contain the multisensor data from three distinct whales. \n\nThe multisensor data contains signal characteristics that are known to be indicators of unique animal behaviors.\n\nFor a behavior to be detected and analyzed using automated detection methods, a proxy must exist from which the behavior can be identified. Humpback whales feed by lunging towards their intended prey while simultaneously opening their mouths, thus generating peaks in the norm-jerk signal due to produced and incurred changes in acceleration.\n\nThese events were logged and stored in an annotation tabel that links to an event key table, one for each whale dataset. \n\nAn automated detection method to detect the times of the fin whale (Balaenoptera physalus) and humpback whale (Megaptera novaeangliae) foraging events can be derived from the norm-jerk signal. research earth science animals science and technology artificial intelligence r","71":" Local Ocean Conservation Sea Turtle Face Detection Create a model that detects the bounding box around a sea turtle\u2019s face Being able to identify individual animals is a critical aspect of modern conservation. In the case of sea turtle conservation efforts, tracking where and when individuals are spotted can help reveal patterns of movement and residency, and allow more accurate estimates of population. Sea turtles can be identified using their facial scales, which are as unique as a human fingerprint.\n\nLocal Ocean Conservation (LOC) would like to use the unique fingerprint of turtle faces to integrate facial image recognition into their existing turtle applications, which would speed up many routine conservation tasks. As a first step towards such a system, we need to develop a tool that can crop a given image to show only the important facial region, reducing the chances of an accidental match down the line.\n\nThe goal of this competition is to develop an algorithm or model that can take in an image of a sea turtle and output the position of a bounding box around that all-important scale pattern. A labelled training set with bounding box annotations has been provided. earth science animals","72":" CatFLW Cat Facial Landmarks in the Wild The `Cat Facial Landmarks in the Wild (CatFLW)` dataset contains 2079 images of cats' faces in various environments and conditions, annotated with 48 facial landmarks and a bounding box on the cat\u2019s face.\n\nIf you use the `CatFLW`, please cite the [dataset paper](https:\/\/arxiv.org\/abs\/2305.04232):\n\n```\n@article{martvel2023catflw,\n  title={Catflw: Cat facial landmarks in the wild dataset},\n  author={Martvel, George and Farhat, Nareed and Shimshoni, Ilan and Zamansky, Anna},\n  journal={arXiv preprint arXiv:2305.04232},\n  year={2023}\n}\n```\n\nYou can also check out the [landmark detection paper](https:\/\/link.springer.com\/article\/10.1007\/s11263-024-02006-w) and compare the detection performance on the CatFLW:\n\n```\n@article{martvel2024automated,\n  title={Automated Detection of Cat Facial Landmarks},\n  author={Martvel, George and Shimshoni, Ilan and Zamansky, Anna},\n  journal={International Journal of Computer Vision},\n  pages={1--16},\n  year={2024},\n  publisher={Springer}\n}\n``` arts and entertainment animals computer vision image regression","73":" Macaque Monkey Images Macaque Monkey detection (Object Detection) A dataset containing macaque monkey images typically consists of a collection of digital images depicting various aspects of macaque monkeys. These images may include photographs taken in natural habitats or controlled environments such as research facilities or zoos. The dataset may cover a wide range of activities, behaviors, and characteristics of macaques asia animals deep learning image","74":" Reactive Anomaly Synthetic Data Reactive Synthetic Data Augmentation of the UCSD Anomaly Detection Dataset Reactive synthetic data augmentation to the widely used UCSD anomaly dataset based on the paper [Augmenting Anomaly Detection Datasets with Reactive Synthetic Elements](https:\/\/diglib.eg.org\/handle\/10.2312\/cgvc20231204) from Computer Graphics and Visual Computing (CGVC) 2023\n\nThe dataset contains three types of augmentations for the testing data for both the PED1 and PED2 subsets:\n- Synthetic humans that react to real pedestrians and do anomalous actions like falling, jumping, walking on the grass, etc.\n- Synthetic animals that react to real pedestrians - dogs, cats, horses\n- Synthetic bags that are given to random real pedestrians and are dropped after a random period as an anomaly\n\nThe synthetic models are realistically occluded by real pedestrians in front of them and by parts of the foreground.\nThe testing data comes with frame-level labels suggesting an anomaly or normal data in the form of .npy files\n\nIn addition, there is an augmented training dataset with synthetic humans that talk together with real pedestrians.\n\n\n earth and nature","75":" Sri Lankan Fish Species Dataset Top 16 fish species consumed in Sri Lanka Dataset contains images of top 16 fish species consumed in Sri Lanka with it's object detection label and coordinates.\nWe collected 1920 (=120*16) images, After applying the augmentation methods, the number of images increased to 14,400 images for all top 16 fish species fish and aquaria","76":" Guinea Pig Detection Hand-labeled images of guinea pigs for object detection # Guinea Pig Detection\n\nThis hand-labeled dataset contains 1551 images of guinea pigs for object detection task. The dataset is currently  in YOLOv8 format, therefore is splitted into training, validation and testing sets.\nBased on your preferencies, you can export this dataset into any other format in [the Roboflow page of this project](https:\/\/app.roboflow.com\/projects-josub\/guinea-pig-detection-grlwn\/1).\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6825250%2Fbef126a0df6086e6bf6d205720abe1ea%2F2.png?generation=1710515099016870&alt=media) animals computer vision image object detection yolov8","77":" sea-animals-image-dataste   ","78":" COVID-19 & the virus that causes it: SARS-CoV-2. Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ? **Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing) public health data visualization health conditions public safety r data storytelling","79":" Hiking Wildlife Dataset containing Images of Wildlife Animals Welcome to the Wildlife Recognition Dataset, a comprehensive collection of high-quality images featuring diverse wildlife animals. This dataset is curated to support deep learning projects focused on wildlife identification and classification. \"Let's download  data set of wildlife and get some fun training a model to classify them\".  \n**Content:**\n- 17 meticulously organized folders, each dedicated to a specific wildlife species.\n- Rich variety of wildlife, including mammals, birds, reptiles, and more.\n- High-resolution images captured in their natural habitats.\n\n**Categories:**\nHere i create a small data set of 9,638 images in the folder wildlife.zip distributed in 17 classes as follows:\n\nfolder: Horse images: 743\nfolder: bald_eagle images: 732\nfolder: black_bear images: 706\nfolder: bobcat images: 679\nfolder: cheetah images: 342\nfolder: cougar images: 659\nfolder: deer images: 670\nfolder: elk images: 585\nfolder: gray_fox images: 608\nfolder: hyena images: 303\nfolder: lion images: 289\nfolder: raccoon images: 685\nfolder: red_fox images: 697\nfolder: rhino images: 376\nfolder: tiger images: 265\nfolder: wolf images: 927\nfolder: zebra images: 372\n\n**Intended Use:**\nThis dataset is designed for deep learning enthusiasts and researchers aiming to develop models capable of identifying and classifying wildlife animals. Whether you are working on image recognition, object detection, or species classification, this dataset provides a robust foundation for your projects.\n\n\n\n earth and nature animals computer vision deep learning image multiclass classification travel","80":" Birds, Bats, Grasshoppers Audio \"Avian & Insect Harmonies: Recordings from Xeno-Canto\" \"Dive into the world of avian and insect calls with this comprehensive collection from **Xeno-Canto**. Featuring **.mp3** recordings of **birds, bats**, and **grasshoppers**, this dataset offers a rich resource for audio classification and detection tasks. Whether you're exploring machine learning or deep learning techniques, this repository provides a diverse array of sounds to train and test your models. From identifying species to studying behavioral patterns, unlock the potential of audio analysis with this meticulously curated dataset.\"\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fd234991df91ed7957d3116731123ff30%2Fbirds.jpg?generation=1706681903852772&alt=media\">\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13430725%2Fac72f0a856ff4db0498717825ddb765a%2Fbats.jpeg?generation=1706681918141313&alt=media\">\n<img src=\"https:\/\/www.worldatlas.com\/r\/w960-q80\/upload\/43\/d6\/e4\/shutterstock-743534308.jpg\"> global animals deep learning audio audio classification","81":" Zebrafish mirror biting Zebrafish aggressive behaviour in front of mirror This dataset is a collection of video frames from zebrafish biting mirror experiments, designed to facilitate the development and evaluation of machine learning models for target detection and behaviour analysis. The experiment involves observing zebrafish interacting with a mirror, where the fish may exhibit aggressive behaviour by biting the reflective surface. earth and nature animals image","82":" Water Quality -  Every Drop Matters. Water water everywhere, but not a drop to drink! **Introduction:**\n\nThe famous line, \u2018water, water everywhere and not a drop to drink\u2019, in Samuel Taylor Coleridge\u2019s poem, \u2018The Rime of the Ancient Mariner\u2019. \n\n[link](https:\/\/www.poetryfoundation.org\/poems\/43997\/the-rime-of-the-ancient-mariner-text-of-1834) to the poem. \n\n[link](https:\/\/youtu.be\/RGH4p4z4s5A?si=Wzwaww5jbSyywKhY) to a YouTube video where Richard Burton reads the poem.\n\nImagine a world where every crystal-clear drop holds a hidden threat, where thirst gnaws at your throat and every sip can bring sickness. Sadly, for many, this isn't just a dystopian fiction, but a lived reality. Clean water, far from a mere convenience, forms the very bedrock of life on Earth. It's the silent partner in every beat of our hearts, the canvas upon which nature paints its vibrant ecosystems - [link](https:\/\/water.org\/our-impact\/water-crisis\/).\n\nThis delicate equilibrium, however, faces unprecedented challenges. Its history whispers cautionary tales, its present demands immediate action, and its future hangs precariously in the balance. We rise each morning to gulp down this elixir of life, a seemingly mundane act that masks a complex symphony. From the gurgling of glacial rivers to the vast, salty oceans, water orchestrates the dance of life across the planet. Each drop, precious and irreplaceable, holds the secrets of ecosystems, the whispers of ancient rains, and the promise of future sustenance.\n\nBut this vital symphony faces off-key notes. Industrial waste and agricultural runoff stain the canvas, transforming crystalline streams into murky stories of ecological distress. Plastic, an insidious invader, weaves its own dark narrative, choking rivers and poisoning aquatic life. The water we once took for granted now carries a bitter aftertaste of worry, forcing us to confront the fragility of our dependence on it.\n\nOnly 3% of Earth's water is readily available for human consumption. The rest, a vast, shimmering expanse, remains a constant reminder of our vulnerability. Meanwhile, our thirst for progress leaves a trail of contamination on this precious resource. The history of water quality paints a canvas of struggle \u2013 from ancient civilisations grappling with drought to modern societies facing polluted rivers. This narrative, however, is not devoid of hope. The Clean Water Act, a beacon of progress, stands as a testament to our ability to fight for this vital resource - [link](https:\/\/www.nwf.org\/Our-Work\/Waters\/Clean-Water-Act#:~:text=Congress%20passed%20the%20Clean%20Water,public%20health%20and%20wildlife%20habitat.).\n\nToday, we stand at a crossroads. Our water, a fragile treasure, faces the onslaught of overuse and contamination. The future, however, remains unwritten. Through innovation, sustainable practices, and a collective vow to protect this vital resource, we can rewrite the narrative. We can turn the tide of pollution, compose a symphony of clean water, and ensure that every drop continues to sing the sweet song of life.\nThis project dives deeper into this critical story, exploring the intricate dance between water and environment. We delve into the historical echoes, confront the present challenges, and envision a future where every drop is life-sustaining. Join me on this journey, for the water we drink is not merely a resource, but a story waiting to be told, a poem waiting to be penned, and a legacy waiting to be secured.\n\n**The Water Cycle - A Dance Between Ocean, Atmosphere, and Biosphere:**\n- Ocean Evaporation \u2013 From Salty Deep to Gaseous Ascent:\n  - The sun's warmth kisses the vast ocean, and like a magician, transforms liquid water into invisible water vapour. \n  - This process, evaporation, releases freshwater into the atmosphere, leaving behind the ocean slightly saltier as the water cycle starts its journey.\n- Atmospheric Journey \u2013 Riding the Winds of Change:\n  - The rising water vapour hitches a ride on air currents, travelling near and far, forming fluffy clouds, and sometimes swirling in majestic storms. \n  - Temperature and pressure dictate the next act \u2013 when conditions are right, condensation transforms the vapour back into tiny water droplets, paving the way for...\n- Rainfall \u2013 Life-Giving Return to Earth:\n  - Gravity pulls the condensed water, now in the form of rain, snow, or hail, back towards Earth. \n  - These precious drops nourish forests, quench thirsty landscapes, and fill rivers and streams, giving life and sustenance to countless ecosystems.\n- River Run \u2013 Connecting Land and Ocean:\n  - The rivers, swollen with rainwater, become the veins of the land, carrying not just water but dissolved nutrients, minerals, and sometimes, unfortunately, pollutants. \n  - They carve valleys, feed deltas, and eventually, yearn for their aquatic origin.\n- Return to the Ocean \u2013 Completing the Cycle:\n  - And so, the journey ends where it began. Rivers empty into the vast ocean, replenishing its depths and completing the grand cycle. But the story doesn't end there.\n- Enter the Biosphere - A Web of Interdependence:\n  - Plants transpire, releasing water vapour from leaves, contributing to the cycle. \n  - Animals drink, excrete, and decompose, returning water to the Earth and atmosphere. This intricate web of life is fundamental to the cycle's health.\n- The Poisoned Chalice - Why Polluting Water Harms Us All:\n  - When we pollute our water \u2013 with chemicals, fertilisers, sewage \u2013 we poison not just the oceans and rivers, but ourselves. \n  - These toxins accumulate in the food chain, eventually reaching our tables. Contaminated water can also directly threaten our health through pathogens and diseases.\n\nTherefore, protecting the water cycle isn't just about environmental ethics; it's about self-preservation. We must reduce pollution, treat wastewater effectively, and manage water resources sustainably. The health of the water cycle is inextricably linked to our own. \n\n**Data Visualisation:**\nThe below chart shows the 42 best & worst countries in terms of water pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fd81273e722743ca51b7fa3bfad69cd10%2FScreenshot%202024-01-13%2022.22.40.png?generation=1705184678294788&alt=media)\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138061).\n\n**Visualisation Conclusion:**\n\nIn regions marred by conflict like Sierra Leone and the Central African Republic, water pollution is a sombre reality, casting a shadow on both lives and ecosystems. Conversely, in Micronesia, industrial progress brings both advancements and environmental challenges, evidenced by algal blooms and plastic pollution.\n\nDespite these challenges, the narrative is not one of despair but a call to action. Just as the fjords of Norway and the lakes of Finland exemplify pristine environments, there is hope for troubled lands. The vision is a harmonious dance of progress, where industry aligns with nature, and sanitation orchestrates a vibrant symphony.\n\nEnvision a future where Sierra Leone reverberates with laughter over rejuvenated rivers, and the Central African Republic is adorned with thriving forests, where clean water symbolises peace. This vision is not merely a dream but an achievable destiny. To realise this destiny, collective efforts are needed. Governments must embrace stability, citizens must advocate for a cleaner future, and together, we can usher in a tide of change, cleansing the stains of conflict and leaving behind crystal waters and lush shores. Always remember, a world immersed in clean water is a world where everyone can thrive. \n\n**Water Historically:**\n- Hunter-Gatherers and Water:\n  - Nomadic existence for 200,000 years: hunter-gatherers constantly on the move.\n  - Early aversion to unpleasant water (unpleasant tastes, odours, or appearances) due to potential pathogens.\n  - Waterborne health risks are likely minimal due to nomadic lifestyle.\n  - Reconstruction of water use relies on analogies with later societies.\n- Shift to Agriculture:\n  - 10,000 years ago: transition to permanent settlements and agriculture.\n  - Population growth tied to water resources, forming villages, cities, states.\n  - Increased vulnerability to waterborne pathogens, necessitating pure water.\n- Early Water Management:\n  - Jericho (8000-7000 B.C.): strategic location near water sources.\n  - Ancient wells, water pipes, and toilets in Egypt, Mesopotamia, Crete.\n  - Groundwater from springs and wells: vital and reliable sources.\n- Cultural Significance of Water:\n  - Myths in ancient cultures emphasise cleanliness and water's sacredness.\n  - Ancient Greeks recognised water's importance for public health.\n  - Enduring awareness of the connection between clean water and human well-being.\n\n**Medieval Water and the Fight for Health:**\n- Black Death: This plague, carried by rodents on trade routes, ravaged Europe for centuries. Stagnant water in medieval cities became breeding grounds, highlighting the link between water and disease - [link](https:\/\/en.wikipedia.org\/wiki\/Black_Death#:~:text=In%201348%2C%20the%20disease%20spread,population%20of%20100%2C000%20people%20died.).\n- Early Control: Venice's 40-day quarantine set the stage for international cooperation. Plague boards, later sharing information, laid the groundwork for future global health efforts.\n- Beyond Plague: Smallpox, diphtheria, and even dancing mania also plagued communities, showcasing the broader waterborne threat.\n- Shifting Sands: Stricter measures during outbreaks, combined with neo-Hippocratic ideas and the printing press, spurred a gradual shift towards cleaner water and sanitation - [link](https:\/\/ehne.fr\/en\/encyclopedia\/themes\/ecology-and-environment-in-europe\/health-and-environment\/heritage-neo-hippocratism-in-environmental-thought-sixteenth-nineteenth-century#:~:text=While%20historians%20do%20not%20agree,have%20an%20impact%20on%20health.).\n- Environmental Engineering: By the 18th century, proactive approaches like improved ventilation, drainage, and water maintenance emerged, paving the way for a healthier future.\n- Legacy: The medieval struggle against disease, while fraught with limitations, planted the seeds of international cooperation and scientific advancement, shaping our fight against disease even today.\n\n**Buxton Water - A Legacy of Purity from Plague Times to Modern Hydration:**\n\nBuxton Water is indeed a brand steeped in history, with its source, St. Ann's Well, boasting a reputation for clean water that stretches back centuries. In fact, during the devastating bubonic plague that swept across Europe in the 14th century, Buxton's spring was renowned as a safe haven for those seeking uncontaminated water, a precious commodity when many wells were tainted with disease:\n- A History of Healing:\n  - Legends abound about the curative properties of Buxton's water. Romans, who named the town Aquae Arnemetiae (\"Waters of the Goddess Arnemeia\"), built baths around the spring, believing its warmth and minerals held healing powers. This belief persisted throughout the centuries, attracting royalty and nobility to Buxton for its \"spas\" and purported health benefits.\n- The Science Behind the Purity:\n  - The secret to Buxton Water's historical reputation and modern popularity lies in its unique geology. Rainwater percolates through layers of limestone, filtering naturally for thousands of years before emerging at St. Ann's Well. This filtration process removes impurities and enriches the water with minerals like calcium, magnesium, and sodium, contributing to its distinctive taste and purported health benefits.\n- Beyond History:\n  - Today, Buxton Water remains a popular choice for those seeking a refreshing and mineral-rich beverage. Bottled at the source, it retains its natural purity and distinctive taste. Beyond its historical significance, Buxton Water is also a champion of sustainability, using renewable energy to power its bottling facilities and minimising its environmental impact.\n- Other Historical \"Plague Waters\":\n  - While Buxton stands out for its enduring reputation, other water sources also gained recognition during the plague for their perceived purity. Some notable examples include:\n  - Holywell Spring in Wales: Believed to have healing properties due to its association with St. Winefride, this spring attracted pilgrims seeking protection from the plague.\n  - Aqua Virgo in Rome: Built by the Romans to bring clean water to the city, this aqueduct remained a vital source during outbreaks of disease.\n  - The Zamzam Well in Mecca: Considered sacred by Muslims, this well provided water to pilgrims during the Hajj, and some believed it offered protection from illness.\n\nThese examples highlight the historical importance of clean water during times of crisis and the enduring human quest for safe and healthy hydration.\n\n**The Brewing Industry and the Black Death:**\n\nThe Black Death, a devastating pandemic that swept across Europe in the mid-14th century, was a time of immense hardship and suffering. While the brewing industry couldn't directly cure the plague, it played a surprisingly significant role in the lives of those living through it, in several ways:  \n- Safer than water: \n   - In an era where clean water was often scarce and contaminated, beer, with its boiled ingredients and alcohol content, offered a safer alternative. While not a guaranteed protection against the plague, drinking beer instead of water likely reduced the risk of contracting waterborne diseases that could exacerbate the plague's effects.\n- Nutritional boost: \n   - Beer, particularly unfiltered brews, was a valuable source of calories, carbohydrates, and B vitamins, which were often deficient in medieval diets. This additional nourishment was crucial for those weakened by the plague or facing food shortages.\n- Psychological comfort: \n   - In the face of widespread death and fear, taverns and alehouses offered a rare space for socialisation and solace. Sharing a tankard of ale could provide a temporary escape from the grim realities of the time and foster a sense of community and shared experience.\n- Economic impact: \n   - The increased demand for beer during and after the plague boosted the brewing industry, creating jobs and supporting local economies. This economic activity helped communities recover from the devastating losses caused by the pandemic.\n- Evolution of pub culture: \n   - The Black Death's impact on the brewing industry is arguably still felt today. The rise of commercialised taverns and alehouses in this period laid the foundation for the vibrant pub culture that thrives in many parts of the world.\n\nIt's important to remember that the Black Death was a complex event with far-reaching consequences. While the brewing industry wasn't a cure-all, it offered several crucial benefits that helped people cope with the immense challenges of the time. So, while raising a glass of beer today won't ward off any plagues, but we can appreciate the historical role this industry played in offering a glimmer of hope and nourishment during a dark period in human history.\n\n**The Global Symphony of Brewers, Water, and Purity: From Reinheitsgebot to Burton's Global Brews.**\n\nImagine savouring a perfectly crisp German pilsner brewed not in Bavaria, but amidst the canals of England. Or raising a glass of citrusy American IPA, crafted not on the West Coast, but nestled in the heart of Burton-on-Trent. Thanks to the remarkable adaptability of Burton's water, the ingenuity of modern brewing technology, and the enduring legacy of purity laws, such global beer journeys are now a reality:\n- Purity Through Time: The Reinheitsgebot's Lasting Influence.\n  - While not solely focused on water purity, the German Reinheitsgebot of 1516, declaring beer ingredients could only be barley, hops, water, and yeast, indirectly emphasised the crucial role of this \"liquid canvas.\" German brewers traditionally favour soft water, low in minerals, allowing the nuances of malt and hops to shine through. This focus on purity and precision in water selection laid the foundation for the meticulous control brewing water receives today.\n- Burton's Mineral Masterclass: A Canvas for Global Brews.\n  - But not all beer thrives on soft water. The English town of Burton-on-Trent boasts hard water, rich in calcium sulphate, lending distinct maltiness and subtle bitterness to its iconic pale ales. This unique profile, however, presents an opportunity \u2013 a chance to replicate the brewing conditions of other countries, even without crossing borders.\n- Modern Alchemy: From Burton to Bavaria and Beyond.\n  - Enter the reverse osmosis plant, a water sculptor of the modern age. This technology meticulously removes unwanted elements, creating a blank canvas for brewers to paint with specific mineral additions. For a German pilsner, softening Burton's water takes centre stage. Reverse osmosis paves the way for precise additions of calcium carbonate and magnesium sulphate, mimicking the soft water profile of traditional German pilsner regions. The result? A faithful Burton-brewed pilsner, where malt sweetness and hop bitterness dance in perfect harmony, just like their Bavarian counterparts.\nBut the symphony of water extends beyond Europe. Crafting an American IPA in Burton requires a different melody. Here, brewers retain the hardness of Burton's water while adjusting the mineral composition. Gypsum enhances malt sweetness, while carefully controlled hop additions bring out the signature citrusy aromas and sharp bitterness of American IPAs. Burton's water, once again, transforms, becoming a stage for American brewing tradition to flourish.\n- A Toast to Water: The Unsung Hero of Global Brewing.\n  - From the Reinheitsgebot's emphasis on quality to the art of Burtonisation and the precision of reverse osmosis, water emerges as the unsung hero of global brewing. Its purity, mineral symphony, and adaptability allow brewers to craft beers that transcend borders, carrying the soul of one country within the heart of another. So, raise a glass to water, the foundation upon which every great beer, whether born in Burton or beyond, is built.\n\n**War and Water:**\n- Water as a Weapon:\n - Deprivation: Cutting off the besieged city's water supply was a common siege tactic. Armies would divert rivers, contaminate wells, or build dams, forcing surrender through dehydration and disease.\n  - Flooding: Flooding enemy fortifications could weaken walls, create temporary bridges, or flush out defenders.\n  - Contamination: Poisoning water sources with animal carcasses, diseased bodies, or even industrial waste was a cruel but effective way to spread illness and demoralise the enemy.\n- Protecting Water Sources:\n  - Fortification: Cities often prioritised building strong defences around wells, springs, and aqueducts, knowing their critical importance.\n  - Hidden Reserves: Secret cisterns and wells within city walls allowed defenders to access water even if external sources were compromised.\n  - Purification Tactics: Boiling, filtering through sand or charcoal, and exposing water to sunlight were early methods used to combat contaminated water.\n- Historical Examples:\n  - The Siege of Masada (73 AD): Roman forces cut off Jewish rebels' access to water, contributing to their ultimate defeat.\n  - The Siege of Constantinople (1453): Ottomans diverted the city's main water supply, forcing Byzantines to rely on less secure sources and weakening their defences.\n  - The Dutch Revolt (1568-1648): Spanish forces flooded Dutch lowlands in an attempt to drive out rebels, leading to strategic shifts and ecological changes.\n- Modern Concerns:\n  - Weaponization of water: Concerns persist about the potential for deliberate contamination of water sources during modern conflicts.\n  - Climate change and water security: Increased droughts and flooding threaten water access in conflict zones, potentially exacerbating tensions and humanitarian crises.\n  - Protecting water infrastructure: The vulnerability of critical water systems to cyber attacks and sabotage poses new challenges for military and civilian authorities.\n\n**The Two Sides of the Dam - Benefits and Damage of Large Dams:**\n\nLarge dams, like the Hoover Dam, are complex structures with a mixed bag of consequences. They offer undeniable benefits, while also raising concerns about environmental and social impacts. Let's dive into both sides of the coin:\n- Benefits:\n  - Hydro-power: Dams harness the power of flowing water to generate electricity, often considered a clean and renewable energy source. For example, the Hoover Dam supplies power to millions in the American Southwest.\n  - Flood Control: By regulating river flows, dams can mitigate flooding, protecting lives and infrastructure. The Hoover Dam helps prevent flooding in the Grand Canyon and downstream communities.\n  - Irrigation: Stored water from dams can be used for agricultural irrigation, improving crop yields and food security in arid regions. The Hoover Dam provides crucial water for farms across California, Arizona, and Nevada.\n  - Navigation: Controlled water levels can improve river navigation, facilitating transportation and trade. The Mississippi River system heavily relies on dams for this purpose.\n- Damage:\n  - Ecosystem Disruption: Dams disrupt the natural flow of rivers, impacting downstream ecosystems. They can block fish migration, reduce nutrient-rich sediment flow, and harm riverine habitats. The Hoover Dam has significantly reduced the Colorado River's flow into Mexico, impacting its ecology and agriculture. [link](https:\/\/www.theguardian.com\/environment\/2019\/oct\/21\/the-lost-river-mexicans-fight-for-mighty-waterway-taken-by-the-us)\n  - Displacement and Resettlement: Dam construction often displaces communities, particularly indigenous populations, from their ancestral lands. The Three Gorges Dam in China displaced at least 1.3 million people. [link](https:\/\/www.britannica.com\/topic\/Three-Gorges-Dam)\n  - Loss of Cultural Heritage: Archaeological sites and cultural artefacts can be submerged or destroyed during dam construction.\n  - Siltation and Sedimentation: Dams trap sediment, reducing its availability downstream and leading to erosion and loss of fertile land. This is a significant concern for the Nile Delta.\n  - Safety Concerns: Dam failures can have catastrophic consequences, causing flooding and loss of life. The Brumadinho dam disaster in Brazil in 2019 is a tragic example. [link](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0303243420300192#:~:text=On%2025th%20January%202019%2C%20the,missing%20as%20of%20January%202020.)\n- The Hoover Dam Case:\n  - The Hoover Dam exemplifies both the benefits and challenges of large dams. While it provides vital water and electricity to millions, its impact on the Colorado River's flow has caused ecological and social problems in Mexico. Balancing these issues requires careful planning, environmental mitigation measures, and international cooperation.\n- Moving Forward:\n  - Large dams should be considered with caution, only after exhaustive cost-benefit analyses and comprehensive environmental impact assessments. Sustainable alternatives like solar and wind power should be explored whenever feasible. Moreover, transparency and community engagement are crucial to ensure dams do not exacerbate social injustice or environmental degradation.\nUltimately, the decision to build a large dam should be made with a nuanced understanding of its potential benefits and damage, weighing them carefully against alternative solutions and prioritising the long-term well-being of both humans and the environment.\nRemember, the water cycle and the ecosystems it supports are delicate. We must tread carefully and make informed decisions about how we interact with them, ensuring their health and our own for generations to come.\n\n**Modern Water Contamination:**\n- **Antibiotic Resistance:**\n  - Sources: Wastewater from hospitals, farms using antibiotics in animal feed, and improper disposal of unused medications contribute to the presence of antibiotics in water.\n  - Problem: Bacteria can develop resistance to antibiotics, making it harder to treat infections and potentially leading to super-bugs, becoming untreatable by existing antibiotics. This can increase the risk of serious illnesses, longer hospital stays, and even death.\n  - Impact: The World Health Organisation has called antibiotic resistance one of the \"ten greatest threats to global health in today's world.\"\n  - Antibiotic stewardship: Promoting responsible use of antibiotics in healthcare and agriculture can help reduce their presence in the environment.\n- **Increased Oestrogen:**\n  - Sources: Birth control pills, hormone replacement therapy medications, and agricultural runoff containing livestock hormones can contribute to elevated oestrogen levels in water.\n  - Problem: Oestrogen in water can disrupt the endocrine system of aquatic life, leading to feminization in male fish, reduced fertility, and population decline. In humans, exposure to oestrogen in water through drinking or swimming may be linked to certain cancers and reproductive problems.\n  - Impact: The widespread presence of oestrogen in water poses a threat to both ecological and human health, requiring improved wastewater treatment and responsible use of hormones.\n  - Improved wastewater treatment: Upgrading wastewater treatment plants to remove antibiotics, hormones, and other contaminants is crucial.\n  - Public awareness: Raising public awareness about these issues and encouraging responsible disposal of medications and personal care products can contribute to reducing pollution.\n  - Microplastics: These tiny plastic fragments can adsorb antibiotics and other contaminants, potentially increasing their bio-availability and posing additional risks to aquatic life and human health. \n- **Glyphosate:** The most widely used herbicide globally, primarily found in Roundup weed killer. While its effects on land are well-studied, its presence in water raises growing concerns.\n  - Minimal direct use: Glyphosate isn't applied directly to water bodies for weed control. However, it can reach water indirectly.\n  - Runoff from agricultural fields, lawns, and gardens. Rainfall and irrigation can carry glyphosate residues into rivers, lakes, and groundwater.\n  - Glyphosate is routinely used pre-harvest to help ripen or dry out crops. [link](https:\/\/ahdb.org.uk\/pre-harvest-glyphosate#:~:text=Pre%2Dharvest%20glyphosate%3A%20best%20practice,particularly%20valuable%20in%20wet%20seasons.)\n  - Industrial discharge: Factories producing or using glyphosate may release wastewater containing the herbicide.\n  - Atmospheric deposition: Glyphosate can travel through the air and eventually settle on water surfaces.\n- Dangers and Concerns:\n  - Impact on aquatic life: While glyphosate itself is relatively non-toxic to fish and invertebrates at current levels, concerns exist about potential indirect effects:\n  - Disruption of aquatic ecosystems: Glyphosate can harm algae, the base of the food chain, impacting entire ecosystems.\n  - Increased susceptibility to disease: Glyphosate may weaken aquatic organisms, making them more vulnerable to infections and environmental stressors.\n- **Human health concerns:** Though the EPA considers glyphosate safe at current levels in drinking water, some studies suggest potential risks.\n  - Cancer: The International Agency for Research on Cancer classified glyphosate as \"probably carcinogenic to humans,\" though evidence remains inconclusive.\n  - Developmental effects: Some studies suggest potential harm to fetal development at high exposure levels, requiring further research.\n  - Contamination of human breast milk: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9322831\/)  \n  - Glyphosate use, toxicity and occurrence in food: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8622992\/#:~:text=Glyphosate%20is%20applied%20intensively%20in,in%20human%20urine%20%5B6%5D.)\n  - Antibiotic resistance: Some concerns exist about potential links between glyphosate use and the development of antibiotic resistance in bacteria.\n- Regulations: Different countries have varying regulations regarding glyphosate levels in drinking water. \n  - Detection and treatment: Glyphosate can be detected in water through sophisticated testing methods. However, removing it from drinking water remains a challenge, requiring advanced treatment technologies.\n  - Ongoing Research: More research is needed to fully understand the long-term impacts of glyphosate in water, especially on human health and aquatic ecosystems.\n  - Raising awareness: Public education is crucial to understand the potential risks of glyphosate in water and promote responsible herbicide use.\n  - Supporting research: Continued research on glyphosate's environmental and health impacts is essential for informing regulatory decisions and protecting public health.\n  - Exploring alternatives: Investigating and promoting safer alternatives to glyphosate-based herbicides can help minimize potential risks to water quality and ecosystems.\n- Emerging contaminants: New and poorly understood chemicals, such as PFAS (per- and polyfluoroalkyl substances), are increasingly being detected in water, raising concerns about their potential health effects.\n  - A document entitled - PFAS: The 'Forever Chemicals' With a Toxic Legacy: [link](https:\/\/docs.google.com\/document\/d\/1dKyWDo2BGMwqpOnWbRqxBbZ5_jdccbKlAZKENm3Hn4c\/edit?usp=sharing)\n\n**Data Visualisation:** The below chart shows the 42 best & worst countries in terms of fine particulate matter, from the data (WHO_PM.csv) for the years 2014 - 2019. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F365fd85dc0eccd344966dc3a7fe48a02%2FScreenshot%202024-01-13%2023.08.00.png?generation=1705187441671576&alt=media)\nA Markdown document with the R code for the above plot. The code analyses air pollution and air quality data and calculates average values for each location - [link](http:\/\/rpubs.com\/Paddy_5142\/1138053).\n\nParticle pollution can transport contaminants, such as toxic heavy metals and organic compounds, which can accumulate in fish tissues and be ingested by humans. These pollutants can harm aquatic life through various processes, such as physical damage, ingestion, bio-accumulation, light attenuation, and toxicity. [link](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S1532045623001011#:~:text=Particle%20pollution%20can%20transport%20contaminants,%2C%20light%20attenuation%2C%20and%20toxicity.)\n\n**See current wind, weather, ocean, and pollution conditions, as forecast by supercomputers, on an interactive global animated map:**\n- Earth Nullschool.\n- Updated every three hours.  \n- The following link will take you to: **Wind & Particulate Matter &lt; 2.5 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm2.5\/orthographic)\n- The following link will take you to: **Wind & Particulate Matter &lt; 10 \u00b5m at Surface Level.** [link](https:\/\/earth.nullschool.net\/#current\/particulates\/surface\/level\/overlay=pm10\/orthographic=-354.03,-1.14,182)\n- Click anywhere on the global map to see particulate matter in \u00b5g\/m3 (concentration of an air pollutant, given in micrograms (one-millionth of a gram) per cubic meter of air).\n\n**Data Visualisation:** The below chart shows the 25 best & worst countries in terms of water and air pollution, from the data (Cities1.csv), for 2020.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fbe8a7a353496b2c9503e53ea615f0ad8%2FScreenshot%202024-01-14%2014.25.31.png?generation=1705243641434813&alt=media)\n\nA Markdown document with the R code for the above plot - [link](http:\/\/rpubs.com\/Paddy_5142\/1138105).\n\nFrom the above visualisation: \n- Palau is the cleanest country for both air and water quality. [link](https:\/\/pristineparadisepalau.com\/)\n- Palau, the Federated States of Micronesia, and Eritrea are tied in first place for air quality.\n- Palau and Liechtenstein are tied in first place for water quality.\n- Contrary to its water pollution, the Federated States of Micronesia is in second place for air quality.\n\n*A project of mine entitled 'What about the Wind?' which explores its origins, its role in a changing climate, the ways we harness its strength, and its impact on human lives:* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind)\n\n**Plastic in Water: A Ubiquitous Threat from Land to Sky.**\n\nPlastic, once a revolutionary material, has become a pervasive pollutant, infiltrating every corner of our environment, including our water sources. From the vast oceans to the raindrops falling from the sky, plastic has become a ubiquitous threat.\n\n- Plastic in Rainwater:\n  - Microplastics: Tiny plastic fragments, less than 5 millimetres in size, are now routinely found in rainwater around the world, even in remote locations like the Himalayas and the Pyrenees. These microplastics can originate from various sources, including car tires, synthetic clothing, and the breakdown of larger plastic debris.\n  - Microplastics in rain: Studies have shown that microplastics can be incorporated into raindrops as they form in clouds. This means that even pristine rainwater can now be contaminated with plastic particles.\n  - Potential health risks: The long-term health effects of ingesting microplastics are still unknown, but concerns exist about their potential to harm human health. Microplastics can act as sponges for pollutants and may release harmful chemicals into the body.\n- Atomised Plastic from the Ocean:\n  - Ocean plastic breakdown: The vast gyres of plastic waste in our oceans are constantly breaking down into smaller and smaller pieces under the influence of sunlight, waves, and chemical reactions.\n  - Sea spray and atomisation: Wind and waves create sea spray, which can send tiny plastic particles, known as atomised plastic, into the atmosphere. These particles are even smaller than microplastics, ranging from 1 to 10 micrometers in size.\n  - Plastic rain: Atomised plastic can be carried by air currents for long distances and eventually fall back to Earth as rain, contaminating land and water resources.\n- The Impact of Plastic in Water:\n  - Harm to aquatic life: Plastic pollution in water can entangle and suffocate marine animals, disrupt their feeding habits, and introduce harmful chemicals into the food chain.\n  - Threats to human health: Microplastics and atomised plastic can be ingested by humans through drinking water and eating seafood, potentially posing health risks.\n  - Environmental damage: Plastic pollution can disrupt ecosystems, reduce biodiversity, and impact water quality.\n- What We Can Do:\n  - Reduce plastic use: Making conscious choices to avoid single-use plastics and opting for reusable alternatives can help stem the tide of plastic pollution.\n  - Proper waste management: Ensuring proper disposal and recycling of plastic waste can prevent it from ending up in our environment.\n  - Support research: Investing in research to understand the full impact of plastic pollution and develop solutions for its mitigation is crucial.\n  - Policy changes: Advocating for stricter regulations on plastic production and consumption can help address this global challenge.\n\n**Rain and Contaminants:**\n\nWhile some contaminants can reach the atmosphere and potentially return to Earth through rain or other forms of precipitation, several factors prevent all contaminants from following this cycle: \n- Chemical properties: Not all contaminants are easily evaporated or transported through the air. Heavy metals, for example, tend to remain dissolved in water and are unlikely to become part of raindrops.\n- Biological degradation: Some contaminants can be broken down by microorganisms in the ocean before reaching the atmosphere. This breakdown process prevents them from joining the water cycle.\n- Deep-sea sequestration: Certain pollutants, like heavy metals or persistent organic pollutants (POPs), can sink down to the ocean floor and become trapped in sediments, effectively removed from the cycle.\n- Human intervention: We can mitigate the presence of some contaminants through wastewater treatment and pollution control measures, preventing them from reaching the ocean in the first place.\n\nTherefore, while rain can be a pathway for transporting certain contaminants, it's not a guaranteed endpoint for all pollutants present in seawater. Understanding the specific properties and fate of different contaminants is crucial for assessing their potential environmental impact and developing effective mitigation strategies.\n\n**Modern Water Treatment: Bristol, UK.** [link](https:\/\/www.bristolwater.co.uk\/)\n\nThe water treatment process in Bristol by Bristol Water is a fascinating journey, transforming raw river water into the fresh, clean tap water we all rely on. Here's a detailed breakdown:\n- Sources:\n  - Surface Water: Primarily sourced from reservoirs in the Mendip Hills, fed by rivers like the Chew and the Yeo.\n  - Groundwater: Used in drier periods, sourced from boreholes in limestone aquifers.\n- Treatment stages:\n  - Screening: Removing large debris like leaves and branches.\n  -  Pre-treatment: Adding a coagulant (aluminum sulfate) to bind small particles together.\n  - Sedimentation: Allowing the large clumps to settle out in tanks.\n  - Filtration: Passing the water through sand filters to remove remaining particles.\n  - Activated Carbon Filtration: Removing dissolved contaminants like pesticides and organic matter using highly absorbent activated carbon.\n  - Disinfection: Adding chlorine to kill bacteria and viruses.\n  - pH Adjustment: Balancing the water's acidity for corrosion control and improved taste.\n  - Quality Control: Continuous monitoring and testing at various stages to ensure safe, high-quality water.\n- Bristol Water's \"Wicked Water Treatment\" Initiative: [link](https:\/\/www.bristolwater.co.uk\/wicked-water-treatment#:~:text=We%20use%20wicked%20water%20treatment,can%20leak%20into%20our%20rivers.)\n  - A public education program explains the treatment process in a fun and engaging way, particularly for children.\n  - Emphasises sustainable practices like energy efficiency and waste reduction.\n  - Offers tours of treatment works for a firsthand look at the process.\n- Additional notes:\n  - Bristol Water uses a multi-barrier approach, meaning each stage contributes to overall water quality and safety.\n  - Treatments may vary depending on the specific source and water quality.\n  - The company invests heavily in research and development to improve efficiency and adapt to new challenges.\n\nWastewater in Bristol, and indeed most places, typically undergoes one treatment cycle before being discharged back into the environment, not being recycled in the traditional sense. However, the water used in the treatment process itself is often recycled!\n\n- Here's a breakdown of the water usage in Bristol's wastewater treatment:\n  - Raw water intake: Around 180 million litres of water are drawn from reservoirs and aquifers daily for both drinking water and wastewater treatment.\n  - Treatment process: Only a small portion, around 1%, of this water is actually used in the various treatment stages like screening, sedimentation, and filtration. The majority of the water simply carries the wastewater through the system.\n  - Treated water discharge: The treated wastewater, now significantly cleaner, is released back into rivers or the Severn Estuary.\n  - Treatment plant water recycling: The water used within the treatment plant for tasks like back-washing filters and machinery is often treated and recycled internally. This reduces the overall freshwater intake needed for the process.\n\nSo, while the wastewater itself isn't directly recycled for drinking or other purposes, there are efforts to minimise freshwater use within the treatment plants themselves. Additionally, research into wastewater reuse for non-potable applications like irrigation or industrial processes is ongoing, and Bristol Water is exploring these possibilities as well. [link](https:\/\/corporate.wessexwater.co.uk\/our-purpose\/investment-schemes\/bristol-water-recycling-centre)\n\n**The Rise of Ocean Radioactivity: A Shadow of Fukushima.**\n\nThe vast expanse of our oceans, once considered pristine and boundless, now faces a growing threat: increased radioactivity. This rise is largely attributed to the 2011 Fukushima Daiichi nuclear power plant disaster in Japan, where meltdowns and explosions released significant amounts of radioactive material into the Pacific Ocean.\n\n- Fukushima's Radioactive Footprint:\n  - Cesium-137: The primary radioactive isotope released from Fukushima, cesium-137, has a half-life of 30 years, meaning it will persist in the environment for decades to come.\n  - Ocean Contamination: Cesium-137 has been detected in Pacific Ocean waters thousands of miles from the accident site, contaminating marine life and raising concerns about long-term impacts on the food chain.\n  - Pacific Currents: Ocean currents play a crucial role in dispersing radioactive material, carrying it across vast distances and potentially exposing marine ecosystems far beyond the immediate vicinity of the accident.\n- Impacts on Marine Life:\n  - Bio-accumulation: Radioactive isotopes can accumulate in the tissues of marine organisms, increasing their radiation exposure and potentially posing risks to predators higher up the food chain, including fish consumed by humans.\n  - Disruption of Ecosystems: Radiation can harm the reproduction and development of marine life, potentially disrupting the delicate balance of ocean ecosystems.\n  - Long-Term Uncertainty: The full extent of the long-term consequences of Fukushima's radioactive releases on ocean health and human health through seafood consumption is still being studied and debated. - [link](https:\/\/www.globalresearch.ca\/28-signs-that-the-west-coast-is-being-absolutely-fried-with-nuclear-radiation-from-fukushima\/5355280)\n  - Possible Immune System Compromise: Cesium-137. - [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Beyond Fukushima:\n  - Other Contributors: While Fukushima is a major source of ocean radioactivity, it's not the only one. Nuclear testing, accidents at other nuclear power plants, and even naturally occurring radioactive elements contribute to the overall burden of radioactivity in the oceans.\n- Global Concern: The increasing presence of radioactive materials in our oceans raises concerns about the long-term health of marine ecosystems and the potential risks to human health through seafood consumption. This issue demands international cooperation and continued research to mitigate the risks and protect our oceans.\n- What We Can Do:\n  - Support Research: Continued research on the impacts of ocean radioactivity and the development of monitoring and mitigation strategies is crucial.\n  - Sustainable Seafood Practices: Choosing sustainably sourced seafood and supporting responsible fishing practices can help minimise exposure to radioactive contaminants.\n  - Advocate for Change: Raising awareness about ocean radioactivity and its potential consequences can encourage policymakers to implement stricter regulations on nuclear activities and invest in cleaner energy sources.\n\n**A Future of Abundant and Healthy Oceans: A Collective Responsibility.**\n\nOur journey through the history of water revealed its profound connection to human health and the fragility of its purity. From the challenges of hunter-gatherers to the complexities of modern water management, one truth remains constant: clean water is the lifeblood of our planet and all its inhabitants.\n\nHowever, our oceans, once vast and seemingly limitless, now face a multitude of threats. Increased radioactivity from nuclear accidents like Fukushima casts a long shadow, while the invisible menace of antibiotic resistance, oestrogen contamination, glyphosate residues, and the ubiquitous presence of plastic paint a worrying picture of a polluted sea.\n\nYet, amidst these challenges lies a powerful opportunity. By acknowledging the gravity of these threats and taking collective action, we can forge a future where our oceans are brimming with vibrant life, free from the burden of modern contaminants. This necessitates a multifaceted approach:\n- Research and monitoring: Understanding the extent and long-term impacts of these threats through robust research and continuous monitoring is crucial to informing effective response strategies.\n- Policy and regulation: Implementing stricter regulations on pollution sources, such as nuclear activities, agricultural practices, and plastic production, is essential to curbing the tide of contamination.\n- Technological innovation: Developing innovative solutions like advanced water treatment technologies and cleaner energy sources can offer sustainable pathways towards ocean health.\n- Individual responsibility: Making conscious choices in our daily lives, from reducing plastic use to supporting responsible seafood practices, can collectively make a significant difference.\n\nThe health of our oceans is not just an environmental concern; it is a matter of human health and the very viability of life on Earth. By standing together in this endeavour, we can ensure that future generations inherit a legacy of abundant and healthy oceans, forever teeming with life, forever a source of wonder and sustenance.\nRemember, clean water is not a luxury, it is a birthright. Let us protect it, cherish it, and ensure its abundance for all.\n\n**Cultivating Without Soil: Hydroponics, Aeroponics, and the Future of Farming.**\n\nTraditional soil-based agriculture has served humanity for millennia, but as we face challenges like limited land, water scarcity, and climate change, innovative methods like hydroponics and aeroponics are emerging as sustainable alternatives. Let's delve into these fascinating techniques and explore their potential for the future of farming, even in the vast expanse of space!\n\nHydroponics: Growing in a Watery Embrace.\n\n- Imagine lush vegetables thriving not in soil, but in a nutrient-rich water solution. That's the magic of hydroponics! Plants' roots are suspended in a variety of mediums like rockwool, perlite, or even just water, while receiving a precisely balanced blend of nutrients and oxygen through the solution. This method offers several advantages:\n  - Water efficiency: Hydroponics uses up to 90% less water than traditional agriculture, making it ideal for arid regions or periods of drought.\n  - Increased yields: Precise control over nutrients and environment leads to faster growth and higher yields compared to soil-based methods.\n  - Reduced pest and disease problems: Soil-borne diseases are less of a concern with hydroponics, leading to fewer pesticides and healthier crops.\n  - However, hydroponics also requires significant upfront investment in equipment and infrastructure. Maintaining the correct pH and nutrient balance in the water solution is crucial, and any technical glitches can have a rapid impact on the plants.\n\nAeroponics: Dancing on Air.\n\nTake hydroponics up a notch, and you have aeroponics! In this method, plant roots are suspended in midair and misted with a nutrient-rich solution at regular intervals. This constant exposure to oxygen fosters even faster growth and eliminates the need for a growing medium altogether.\n\n- Aeroponics boasts all the benefits of hydroponics, with some additional advantages:\nEven better root aeration: Direct exposure to air maximises oxygen uptake, leading to even more vigorous plant growth.\n  - Reduced risk of root disease: Without any growing medium, bacteria and fungal diseases have less chance of taking hold.\n  - However, aeroponics demands even stricter control over the nutrient solution and misting frequency. Any interruption in the misting system can quickly stress or even kill the plants.\n\nSoil vs. Soil-less: Weighing the Options.\n- While hydroponics and aeroponics offer numerous benefits, traditional soil-based agriculture remains dominant for several reasons:\n  - Lower cost: Setting up and maintaining soil-less systems requires significant investment, making soil-based farming more accessible, especially for small-scale operations.\n  - Wider range of crops: Not all plants thrive in soil-less systems, while soil can accommodate a diverse range of crops.\n  - Resilience: Soil-based systems offer inherent buffering against fluctuations in temperature, pH, and nutrient levels, making them more forgiving of minor errors.\n\nUltimately, the choice between soil and soil-less methods depends on factors like the type of crop, climate, resources available, and desired level of control.\n\nPure Water: The Lifeblood of Soil-less Systems.\n\nThe importance of clean water is paramount in hydroponics and aeroponics. Impurities in the water can clog systems, disrupt nutrient balance, and harm the plants. Therefore, using filtered or even reverse osmosis-treated water is crucial for optimal results.\n\nSpace faring Sprouts: Farming Beyond Earth.\n\nThe potential of soil-less agriculture extends beyond Earth's soil. In the context of space exploration and potential long-duration missions, hydroponics and aeroponics offer solutions for growing fresh food in the limited space and harsh conditions of spacecraft and lunar or Martian outposts.\nImagine astronauts enjoying salads and herbs grown right on board, reducing reliance on pre-packaged food and providing psychological benefits of cultivating their own sustenance. Research is already underway to develop closed-loop systems that recycle water and nutrients, minimising waste and maximising efficiency.\n\nThe Future of Farming: A Tapestry of Techniques.\n\nAs the world's population grows and resources become scarcer, innovative agricultural methods like hydroponics and aeroponics will undoubtedly play a crucial role in feeding the future. While traditional soil-based agriculture will remain relevant, these soil-less techniques offer promising solutions for sustainable and efficient food production, both on Earth and potentially beyond. So, the next time you bite into a crisp lettuce or juicy tomato, remember that the future of farming might not involve soil at all, but a carefully orchestrated dance of water, nutrients, and air.\n\n**Quenching the Thirst: Unveiling the Secrets of Desalination and Distilled Water.**\n\nWater, the elixir of life, is a precious resource, and with growing populations and climate change, its scarcity is becoming a pressing concern. Enter desalination, the process of removing salt and other minerals from seawater, making it fit for human consumption and other uses. Let's dive into the fascinating world of desalination and explore the journey towards the purest water, distilled water, through ancient and modern methods.\n\nFrom Ancient Ingenuity to Modern Marvels: Desalination's Journey.\n\nThe quest for fresh water from saline sources is as old as civilisation itself. Ancient Greeks and Romans used simple methods like sun evaporation and filtration through clay pots to desalinate seawater. Fast forward to the 19th century, steam-powered distillation became the dominant method, with the first large-scale desalination plant built in 1872 in Santiago, Chile.\n\nToday, desalination has evolved into a sophisticated field, employing a variety of technologies.\n\n- Thermal Desalination: This method uses heat to evaporate seawater, leaving behind salt and other minerals. The most common thermal desalination technologies are:\n  - Multi-stage flash distillation (MSF): Seawater is heated in a series of chambers, and the vapour condenses on cool surfaces to produce fresh water.\n  - Multi-effect distillation (MED): Similar to MSF, but uses waste heat from one stage to heat the next, making it more energy-efficient.\n  - Membrane Desalination: This method uses semi-permeable membranes that allow water molecules to pass through while rejecting salt and other impurities. The most common membrane desalination technology is:\n  - Reverse osmosis (RO): High pressure forces seawater through a membrane, leaving behind concentrated brine and producing fresh water.\n\nDistilled Water: Purity Reborn.\n\nDistillation, the oldest and arguably the purest method of water purification, involves boiling water and collecting the condensed vapour. This process removes virtually all impurities, including minerals, salts, and even some bacteria and viruses. Distilled water is often used in laboratories, medical applications, and certain industrial processes where ultra-pure water is required:\n- Ancient Echoes in Modern Purity:\n  - The principles of distillation are not lost on modern technology. Vacuum distillation, a more advanced method, operates at lower temperatures and pressures, further reducing the energy consumption and preserving the delicate properties of the water. This makes it a preferred choice for producing high-purity water for sensitive applications.\n- The Future of Desalination and Distilled Water:\n  - As the need for clean water grows, so does the importance of desalination and distilled water production. Advancements in technology are making these processes more efficient and affordable, opening up possibilities for wider adoption. Research is also ongoing in developing sustainable energy sources for desalination, such as solar and wind power, to reduce its environmental footprint.\n- The Choice Between Purity and Practicality:\n  - While distilled water offers unparalleled purity, it also lacks essential minerals that are beneficial for human health. \n   - Additionally, the energy consumption of some desalination methods raises concerns about sustainability. Therefore, the choice between desalinated, distilled, or other types of purified water depends on the specific needs and context.\n\nIn conclusion, desalination and distilled water production are vital tools in our quest for clean water. Understanding their history, methods, and limitations helps us make informed decisions about utilising these technologies to ensure a sustainable future with enough water for all.\n\n**Water, water everywhere and not a drop to drink!**\n\nThe tale of the Robertson family's 38-day ordeal adrift in the Pacific Ocean following their yacht's sinking in 1972 is indeed an incredible testament to human resilience and ingenuity. Their story, echoing Samuel Taylor Coleridge's \"Rime of the Ancient Mariner,\" is one of facing the very element that sustains life while desperately battling for it:\n- The Ordeal Begins:\n  - The Robertsons, a British family of six - Dougal and Lyn, along with their children Douglas, Anne, and twins Neil and Sandy - set sail from Falmouth, England, in 1971 on a voyage around the world aboard their 43-foot wooden schooner, Lucette. \n  - Sadly, their dream adventure took a horrific turn when, 200 miles off the coast of the Galapagos Islands, a pod of orcas attacked their vessel, sinking it within minutes.\n- Adrift in a Tiny Dinghy:\n  - The family grabbed a few items and abandoned ship and managed to board their inflatable rubber life raft and nine-foot fiberglass dinghy Ednamair. \n  - They only had enough water for ten days and emergency rations for three days on the raft.\n  - Lyn had taken their papers, the logbook, and a bag of onions, and they had a kitchen knife, a tin of biscuits, ten oranges, six lemons, half a pound of glucose sweets and flares.   \n  - Faced with the agonising reality of being surrounded by water yet desperately parched, they knew they had to think outside the box to survive.\n- Desperate Measures for Survival:\n  - In their fight for survival, the Robertsons resorted to several unconventional techniques, some bordering on the macabre. \n  - They rationed their food meticulously, collecting rainwater in any way they could, even squeezing moisture from their clothes. \n  - To combat dehydration, they employed saltwater enemas, a risky procedure that, while unpleasant, helped to retain some fluids.\n- A Gruesome Necessity:\n  - Perhaps the most shocking aspect of their survival story is their reliance on turtle blood. Driven by sheer desperation, they captured and killed sea turtles, consuming their flesh and drinking their blood to quench their thirst and stave off starvation. This act, while understandably controversial, was a life-or-death decision made in the face of unimaginable circumstances.\n- A Beacon of Hope:\n  - After 38 gruelling days adrift, their ordeal finally came to an end when they were spotted and rescued by a Japanese fishing vessel. \n  - Emaciated and sunburnt but alive, the Robertsons had cheated death against all odds. \n  - Their story became a global sensation, a testament to the indomitable human spirit and the lengths we will go to for survival.\n- Lessons Learned:\n  - The Robertson family's experience left an indelible mark on them and the world. It serves as a stark reminder of the power and unpredictability of nature, the importance of preparation and resourcefulness in the face of adversity, and the depths of human strength and resilience. It is a story that continues to inspire and captivate, a testament to the will to live even in the face of the most desperate odds.\n  - While their methods may raise eyebrows, their tale underscores the desperate lengths humans will go to for survival. It's a story that serves as a powerful reminder of the human spirit's enduring strength and the importance of hope in the face of seemingly insurmountable challenges. [link](https:\/\/nmmc.co.uk\/2022\/05\/the-50th-anniversary-of-the-robertson-family-rescue\/)\n\n**Navigating the Hydration Highway: Bottled Water, Tap Water, Filters, and Distillers.**\n\nHydration \u2013 the essential fuel for our daily adventures. But when it comes to choosing our drink, we're often greeted by a crossroads: bottled water, shimmering in its plastic shell, or tap water, the familiar flow from our taps\/faucets. Let's explore the advantages and downsides of each, along with their trusty sidekicks \u2013 water filters and home distillers \u2013 to find the perfect lane on the hydration highway.\n\nBottled Water: Convenience with Caveats.\n\n- Advantages:\n  - Convenience: Portable and readily available, whether scaling mountains or stuck in traffic, bottled water keeps us quenched.\n  - Taste: Some prefer the perceived \"cleaner\" taste, especially in areas with chlorine-tinged tap water.\n  - Perception of Purity: Often marketed as purer than tap water, though bottled water quality can vary greatly.\n- Disadvantages:\n  - Cost: Significantly more expensive than tap water, making it an unsustainable daily option.\n  - Environmental Impact: Mountains of plastic bottles generate enormous waste, posing a major environmental threat.\n  - Limited Mineral Content: While often perceived as purer, bottled water may lack essential minerals naturally found in tap water.\n\nTap Water: The Everyday Essential.\n- Advantages:\n  - Cost-Effective: The most affordable hydration option, readily available at the turn of a tap.\n  - Mineral Content: Naturally rich in essential minerals important for health.\n  - Lower Environmental Impact: No plastic waste generated, significantly reducing environmental footprint.\n- Disadvantages:\n  - Taste: Can have a chlorinated taste or be affected by local water quality issues.\n  - Purity Concerns: Depending on location, tap water may contain contaminants or require additional filtration.\n  - Accessibility: Not always readily available in all locations or situations.\n\nWater Filters: On-Demand Purification.\n\nEnter the water filter, the trusty sidekick for tap water warriors. Filter jugs, tap\/faucet attachments, and reverse osmosis systems offer various levels of filtration, removing contaminants like chlorine, sediment, and even some heavy metals.\n\n- Advantages:\n  - Improved Taste: Removes unpleasant odours and tastes, enhancing the palatability of tap water.\n  - Reduces Contaminants: Provides an extra layer of protection against potential contaminants in tap water.\n  - Cost-Effective: A more sustainable and affordable option compared to bottled water.\n- Disadvantages:\n  - Maintenance: Filters require regular cleaning and replacement, adding to the cost and effort.\n  - Limited Removal: May not remove all contaminants, depending on the type of filter and local water quality.\n\nHome Distillers: Crafting Purest H2O.\n\nFor those seeking the ultimate water purity, home distillers offer a fascinating option. By boiling and condensing water, these devices remove virtually all impurities, including minerals, salts, and even some bacteria.\n\n- Advantages:\n  - Purest Water: Produces the purest drinking water available, ideal for sensitive applications.\n  - Removes Contaminants: Eliminates virtually all impurities, including those not addressed by filters.\n  - Flexibility: Allows control over the mineral content of the produced water.\n- Disadvantages:\n  - Cost: Home distillers can be expensive and require energy to operate.\n  - Slow Production: Water production is slower compared to other options.\n  - Mineral Removal: Removes essential minerals naturally found in water, requiring supplementation.\n\nThe Verdict: Hydrate Wisely.\n\nUltimately, the perfect hydration choice depends on individual needs, priorities, and access to clean tap water:\n- Remember:\n  - Choose wisely: Consider convenience, cost, environmental impact, and desired water purity.\n  - Embrace filters: Tap water with a filter can be a cost-effective and sustainable option. [link](https:\/\/www.who.int\/publications\/i\/item\/924156251X)\n  - Distill with caution: While pure, distilled water may require mineral supplementation.\n  - Reduce plastic: Opt for reusable bottles and recycle diligently. [link](https:\/\/bottledwater.org\/packaging\/#:~:text=Plastic%20bottled%20water%20containers%20are,and%20HDPE%20for%209.2%20percent.)\n\n**Conclusion: A Ripple of Responsibility in the Aquatic Symphony.**\n\nWater, the essence of life, stands at a pivotal juncture. While its life-sustaining properties nourish us, it confronts an array of threats. From the subtle encroachment of chemical pollutants to the relentless grasp of microplastics, the aquatic landscape endures a silent siege. Heavy metals clandestinely inhabit its depths, unsettling the fragile equilibrium of ecosystems, while the looming shadows of industrial activities stretch across its pristine purity. The very wellspring of life\u2014its groundwater\u2014stands contaminated, posing risks to health and well-being.\n\nIn this challenging panorama, glimmers of hope persist. Acknowledging the intricate interconnectedness of life, we must craft a nuanced and collective response. Clearer and more stringent regulations should resound like a clarion call, reverberating across landscapes burdened by unsustainable practices. Sustainable agriculture, efficient waste management, and international cooperation must waltz in unison, choreographed by a commitment to shared responsibility.\n\nHowever, our focus cannot be exclusively global; it must also turn inward. Every choice we make, from the type of water we consume to the source we rely on, sends ripples outward, influencing the intricate web of life. Within every decision for a reusable bottle and each conserved drop lies the potential for transformative impact.\n\nNavigating the Aquatic Symphony demands not only personal initiative but also an unwavering awareness of the global tableau we collectively paint. Let us, therefore, stride ahead with a revitalised commitment\u2014toward sustainability, equity, and a future where each sip of water heralds a shared resource accessible to all. Ultimately, the power to generate a wave of change resides within our individual choices, capable of cleansing the stains of pollution and revitalising the lifeblood of our planet.\n\nEmbarking on this journey, guided by a shared aspiration, envisions a world where every human revels in the brilliance of clean, safe water. May each droplet murmur a testament to our collective responsibility\u2014a ripple of hope resonating across the expansive canvas of the Aquatic Symphony.\n\n\nPatrick Ford \ud83c\udf0a\n\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) to my project: *We did not weave the web of life !* Where I concentrate on some important factors that will affect humanity's potential to survive on planet Earth.\n\n-----------------------------------------------------------------------------------------------------------------\n\n[link](https:\/\/www.kaggle.com\/datasets\/patricklford\/what-about-the-wind) to my project: *What about the Wind?*\nFrom a gentle rustle of leaves to the howl of a hurricane, wind is an unseen force that shapes our world. Revered and feared throughout history, wind has inspired myths, driven ships, and fuelled revolutions in energy. We'll explore its origins, its role in a changing climate, the ways we harness its strength, its impact on human lives and the effect on water quality. Our journey will blend science, history, and technology, and even take us beyond Earth to examine the wild winds of other planets.\n\n-----------------------------------------------------------------------------------------------------------------\n\nA short poem about rivers - [link](https:\/\/docs.google.com\/document\/d\/1LVyiP43r8A357T6gyisozJ0Faeis_Yx3QlxcKgIeM1g\/edit?usp=sharing).\n\n alcohol water bodies earth and nature environment chemistry energy public safety","83":" IR wildlife Infrared Small Object Detection for Wildlife Conservation Monitoring of protected areas to curb illegal activities like poaching is a monumental task. Real-time data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which allow surveillance at night when poaching typically occurs. However, it is still a challenge to accurately and quickly process large amounts of the resulting TIR data. The Benchmarking IR Dataset for Surveillance with Aerial Intelligence (BIRDSAI, pronounced \u201cbird\u2019s-eye\u201d) is a long-wave thermal infrared (TIR) dataset containing nighttime images of animals and humans in Southern Africa. The dataset allows for testing of automatic detection and tracking of humans and animals with both real and synthetic videos, in order to protect animals in the real world. europe animals beginner image automl","84":" Cat and Dog Detection   **Description**\n\nThis dataset comprises a collection of images that are categorized into two main classes: cats and dogs. Each class further contains subcategories based on color, namely white and black.\n\nThe dataset includes the following structure:\n\n    Cats\n        White: Images of cats with white fur.\n        Black: Images of cats with black fur.\n    Dogs\n        White: Images of dogs with white fur.\n        Black: Images of dogs with black fur.\n\nThe dataset is well-structured and labeled, making it suitable for tasks like image classification. It is particularly useful for training and evaluating models designed to classify animal types (cats or dogs) and distinguish their colors (white or black).\nContents\n\n    Cats (White): [Number of images: 400]\n    Cats (Black): [Number of images: 400]\n    Dogs (White): [Number of images: 400]\n    Dogs (Black): [Number of images: 400]\n\nThese images are sourced from diverse collections, ensuring a wide variety of cat and dog breeds across various backgrounds and environments. The dataset's labeling and diversity provide a robust foundation for developing and testing image classification models.\n**Potential Use Cases**\n\n    Image classification: Developing models capable of accurately distinguishing between cats and dogs based on their colors.\n    Color-based analysis: Identifying the prevalence or distinguishing features of different fur colors in cats and dogs.\n\nThe dataset's diversity, labeling, and balanced distribution among classes make it a valuable resource for both training and evaluating machine learning models, especially those focused on image classification tasks related to pets' color and type. animals","85":" BrackishSGD Expansion of the brackish dataset with synthetically generated data This dataset was created for a 7.th semester project which uses and expands upon \"The Brackish Dataset\" https:\/\/www.kaggle.com\/datasets\/aalborguniversity\/brackish-dataset?rvi=1 by adding synthetically generated data. Github for instructions and code https:\/\/github.com\/Sebastian-Whitehead\/MED7-TheFishening\n\nSynthetic data generation in the context of data scarcity and diversity could help bolster the effectiveness of object detection. This paper proposes a synthetic framework for generating realistic underwater fish data, using BOIDs and high-definition rendering. The synthetic data is used to train and test an EfficientDet model, a state-of-the-art object detection algorithm, on the Brackish data set, an underwater fish detection data set captured in the turbid waters of the Limfjord, Denmark. The results demonstrate that there is no significant disparity observed when synthetic data is incorporated with the Brackish method, assumed to be attributed to the limited number of epochs trained. Even so, the inclusion of synthetic data exhibits a promising potential for enhancing the model. Within this paper, the challenges and limitations of such an approach are explored, allowing for potential insights in promising fields for future work.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F44fb71a061901d29601aeb9b96cc4a54%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0256_prediction1.jpg?generation=1702645091439627&alt=media)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F3b2cfa75d850ae55ad8dae170a134e2d%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0225_prediction2.jpg?generation=1702645118933655&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18002263%2F35e5beeb9125c1fcc08a312763187a18%2F2019-02-21_06-52-16to2019-02-21_06-52-34_1-0156_prediction3.jpg?generation=1702645127887965&alt=media) earth and nature fish and aquaria","86":" Malflow Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg ## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n``` data visualization python image classification graph cyber security","87":" Different Colors in challenging lightening Different color classification in different lightening Environment **Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation}, arts and entertainment","88":" Classified NIH Dataset NIH x-ray dataset in classified format. I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n health classification deep learning cnn health conditions tensorflow","89":" Cats Dogs Classification dataset contains total of 400 images of cats and dogs for making a classification model. A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user.  computer vision classification deep learning image binary classification","90":" ISIC 2020 JPG 256x256 RESIZED  **ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine cancer image classification","91":" ISIC 2020 JPG 224x224 RESIZED  **ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine classification cancer image classification","92":" ISIC 2019 JPG 256x256 RESIZED  **ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine cancer image classification","93":" ISIC 2019 JPG 224x224 RESIZED ISIC 2019 resized dataset **ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n  arts and entertainment medicine cancer image classification","94":" Dresden Image Database  The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\" computer science computer vision classification image image classification","95":" IMAGE CLASSIFICATION   online communities","96":" ImageNet100-INRs-Dataset Implicit neural representation of ImageNet100 as part of work Implicit Zoo This data is release as part of work of ***[Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes.](https:\/\/github.com\/qimaqi\/Implicit-Zoo)***\nFollowing the standard procedure for ImageNet-1K classification, we resize images to 256x256 and then center crop them to 224x224 for further processing. To better fit the high-res images, we employ a 4-layer 256 width SIREN. The normalization parameters are consistent with those used for CIFAR-10. We increase the training iterations to 3000, each taking 50.24 secs for each image.\n earth and nature","97":" Riped and Unriped Tomato Dataset Annotated Images for Tomato Ripeness Classification This dataset contains annotated images of tomatoes at various stages of ripeness. It is designed to support research and development in agricultural automation, specifically for training machine learning models to distinguish between ripe and unripe tomatoes. The dataset includes annotated images created using an [annotation lab](https:\/\/github.com\/sumn2u\/annotate-lab), ensuring precise and accurate labeling of ripeness status.\n\n agriculture computer vision deep learning image classification","98":" Song Popularity Classification ML Classification  Models Classification:\n\u25aa Split your dataset into 80% training and 20% testing. \n\u25aa Train at least 3 different models to classify each sample into distinct \nclasses.\n\u25aa Choose at least two hyperparameters to vary. Study at least three \ndifferent choices for each hyperparameter. When varying one \nhyperparameter, all the other hyperparameters should be fixed\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6407243%2F0e9835ec9319dc098fafd88676ca7ae3%2FScreenshot%20(25).png?generation=1719419036630363&alt=media) ","99":" Industrial Tools Classification See It, Sort It: An Image Dataset for Industrial Tool Quality Control  This dataset contains images of industrial tools, specifically focusing on screws and ball screws, categorized as either defective or non-defective. The dataset is intended for use in training and evaluating machine learning models for automated defect detection in industrial settings.\n\nThis dataset is primarily designed for educational and research purposes, specifically to aid in the development and evaluation of machine learning models. The core objective is to provide a collection of images featuring industrial tools, like screws and ball screws, categorized as either defective or non-defective. This classification task allows researchers and students to experiment with various machine learning algorithms, aiming to train models that can accurately distinguish between faulty and functional tools based solely on image data.\n\nThe dataset is comprised of two main files: \"train\" and \"test\". Both files contain images of industrial tools, specifically focusing on screws and valves. Each image is categorized into one of two classes: \"defective\" or \"non-defective\". This classification is crucial for training machine learning models that can automatically distinguish faulty tools from functional ones based solely on image data. The \"train\" file serves as the foundation for the model's learning process, while the \"test\" file is used to evaluate the model's performance on unseen data. By analyzing the accuracy on the test data, we can gauge the model's effectiveness in real-world scenarios of identifying defects in industrial tools. earth and nature computer science computer vision keras image classification image augmentation","100":" NIH Chest X-rays Bbox version 880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset # **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov). computer vision multiclass classification health conditions image classification","101":" Rock Classification Dataset Classify Different Types of Rocks (Stones) The dataset includes 4212 images.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 640x640 (Stretch)\n\nThe following augmentation was applied to create 3 versions of each source image:\n* 50% probability of horizontal flip\n* Randomly crop between 0 and 20 percent of the image\n* Random shear of between -10\u00b0 to +10\u00b0 horizontally and -10\u00b0 to +10\u00b0 vertically\n* Random brigthness adjustment of between -15 and +15 percent\n* Random exposure adjustment of between -10 and +10 percent\n* Salt and pepper noise was applied to 0.1 percent of pixels\n\n**Types OF Rocks**\n\n* Basalt\n* chert\n* Clay\n* Conglomerate\n* Diatomite\n* gypsum\n* olivine-basalt\n* Shale-(Mudstone)\n* Siliceous-sinter earth science geology intermediate computer vision image classification","102":" The Russia-Ukraine War Images on Spanish News Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n research classification image","103":" lung x-ray image+clinical text dataset Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co **Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases. health health conditions","104":" Signclusive Mediapipe Blacked Out Background Using Mediapipe Dataset This dataset includes images of all 26 letters of the alphabet and the \"space\" sign, totaling 27 classes. The dataset includes images from five different signers, with each signer providing 100 samples for each letter. This effort resulted in a total of 500 samples per letter, amounting to 13,500 images.\n\n**Dataset Collection Process:**\n\u2022\tSigners: 5 different signers\n\u2022\tSamples per Letter: 500\n\u2022\tTotal Images: 13,500\n\u2022\tImage Size: 640x480 pixels (resized to 224x224 pixels for training)\n\n**Data Preprocessing**\nAfter collecting the images, we resized them to 224x224 pixels to standardize the input size for the model. This resizing ensures that all images are compatible with the input requirements of the neural network. We then drew the hand landmarks on all the images and blacked out the background in order for the model to focus only on the landmarks when it came to training a CNN model using images. artificial intelligence computer vision classification image online communities","105":" Iman(357)-Wawan(339)-neww  Flower Samples Dataset\n\nThis dataset contains images of two different types of flowers: roses and orchid. Each image is labeled with its corresponding flower type. The dataset is suitable for tasks such as image classification or object recognition. The images are captured from various angles and under different lighting conditions to provide a diverse set of samples for training and evaluation purposes. Researchers and developers can use this dataset to train machine learning models to classify images of roses and orchid accurately.\n\nAdenium obesum (Desert Rose)\n\nAdenium obesum, commonly known as the Desert Rose, is a flowering plant native to the Sahel regions of Africa and the Arabian Peninsula. This plant is admired for its striking appearance, featuring a swollen stem base (caudex) that stores water, making it well-suited to arid environments. The Desert Rose produces vibrant, trumpet-shaped flowers in shades of pink, red, and white. Its glossy, dark green leaves contrast beautifully with its flowers, adding to its ornamental appeal. Due to its ability to thrive in dry conditions, Adenium obesum is popular among bonsai enthusiasts and gardeners who favor drought-tolerant plants.\n\nCatharanthus roseus (Madagascar Periwinkle)\n\nCatharanthus roseus, also known as Madagascar Periwinkle or Vinca, is a perennial plant native to Madagascar. This hardy and versatile plant is celebrated for its continuous blooming habit, producing abundant pink, white, or red flowers throughout the year. It is often used in gardens and landscaping due to its ability to thrive in a variety of conditions, from full sun to partial shade. Beyond its ornamental value, Catharanthus roseus is renowned in the pharmaceutical industry for its alkaloid compounds, which have been used in the development of important cancer treatment drugs, such as vincristine and vinblastine. ","106":" Road Accident Data Analysis using Microsoft Excel. Analyzing Road Safety: An Interactive Excel Dashboard for Accident Data Insights This project entails the development of an advanced data analysis dashboard using Microsoft Excel to evaluate and visualize road accident and casualty statistics for the years 2021 and 2022. The primary objective is to provide a comprehensive, user-friendly interface that offers insightful analysis into the patterns, trends, and factors contributing to road accidents and their resultant casualties.\n\n** Column Descriptions:\n\n1. **Accident_Index**: A unique identifier for each accident recorded in the dataset. This ensures that each accident can be referenced individually.\n\n2. **Accident Date**: The date on which the accident occurred. This includes day, month, and year.\n\n3. **Month**: The month in which the accident occurred, extracted from the accident date. This helps in analyzing monthly trends.\n\n4. **Year**: The year in which the accident occurred, extracted from the accident date. Useful for annual trend analysis.\n\n5. **Day_of_Week**: The day of the week on which the accident occurred. This can help in understanding if there are more accidents on specific days.\n\n6. **Junction_Control**: Indicates the type of control present at the junction where the accident occurred (e.g., traffic lights, stop sign).\n\n7. **Junction_Detail**: Provides more detailed information about the junction's layout (e.g., T-junction, roundabout).\n\n8. **Accident_Severity**: The severity of the accident, which can range from slight, serious, to fatal.\n\n9. **Latitude**: The geographical latitude where the accident occurred. This helps in mapping the location of accidents.\n\n10. **Light_Conditions**: Describes the lighting conditions at the time of the accident (e.g., daylight, darkness with street lighting).\n\n11. **Local_Authority_(District)**: The local authority district where the accident took place. This is useful for regional analysis.\n\n12. **Carriageway_Hazards**: Any hazards present on the carriageway that might have contributed to the accident (e.g., roadworks, oil spill).\n\n13. **Longitude**: The geographical longitude where the accident occurred. Used in conjunction with latitude for mapping.\n\n14. **Number_of_Casualties**: The total number of casualties resulting from the accident. This includes all levels of injury severity.\n\n15. **Number_of_Vehicles**: The number of vehicles involved in the accident. This helps in understanding the scale of the accident.\n\n16. **Police_Force**: The police force that responded to and recorded the accident. This can indicate jurisdiction and reporting standards.\n\n17. **Road_Surface_Conditions**: Describes the condition of the road surface at the time of the accident (e.g., dry, wet).\n\n18. **Road_Type**: The type of road where the accident occurred (e.g., single carriageway, dual carriageway).\n\n19. **Speed_limit**: The speed limit in effect on the road where the accident occurred. This can be a factor in accident severity.\n\n20. **Time**: The exact time of day when the accident occurred. This can help in analyzing patterns related to time.\n\n21. **Urban_or_Rural_Area**: Indicates whether the accident occurred in an urban or rural area. This can influence accident characteristics.\n\n22. **Weather_Conditions**: The weather conditions at the time of the accident (e.g., fine, raining). This can impact driving conditions.\n\n23. **Vehicle_Type**: The type of vehicle(s) involved in the accident. This can help in understanding which types of vehicles are most frequently involved in accidents.** software transportation data cleaning data visualization data analytics data storytelling","107":" KaggleX Skill Assessment Challenge Cohort4 Dataset Used Car Price Prediction Dataset This Dataset is **forked from the Kaggle Cohort-4 Skill Assessment Challenge.** \nThe dataset for this competition (both train and test) was generated from a deep learning model fine-tuned on the [Used Car Price Prediction Dataset](https:\/\/www.kaggle.com\/datasets\/taeefnajib\/used-car-price-prediction-dataset) dataset. Feature distributions are close to, but not exactly the same, as the original. **Feel free to use the original dataset** as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance, but that is not required.\n\n**train.csv ** - the training dataset; refer to the original dataset link above for column descriptions\n\n**test.csv **- the test dataset; your objective is to predict the value of the target Price\n\n\n**Brand & Model:** Identify the brand or company name along with the specific model of each vehicle.\n\n**Model Year:** Discover the manufacturing year of the vehicles, crucial for assessing depreciation and technology advancements.\n\n**Mileage:** Obtain the mileage of each vehicle, a key indicator of wear and tear and potential maintenance requirements.\n\n**Fuel Type:** Learn about the type of fuel the vehicles run on, whether it's gasoline, diesel, electric, or hybrid.\n\n**Engine Type:** Understand the engine specifications, shedding light on performance and efficiency.\n\n**Transmission:** Determine the transmission type, whether automatic, manual, or another variant.\n\n**Exterior & Interior Colors:** Explore the aesthetic aspects of the vehicles, including exterior and interior color options.\n\n**Accident History:** Discover whether a vehicle has a prior history of accidents or damage, crucial for informed decision-making.\n\n**Clean Title:** Evaluate the availability of a clean title, which can impact the vehicle's resale value and legal status.\n\n***Price:*** Access the listed prices for each vehicle, aiding in price comparison and budgeting.\n automobiles and vehicles beginner xgboost decision tree tabular","108":" Boston - Somerville Traffic Crash Dataset Exploring Traffic Incidents, Causes, and Trends in Somerville \n# Crash Data in Somerville, USA\n\nThis dataset provides detailed information about traffic crashes in Somerville, USA, including various factors and circumstances related to each incident.\n\n## Columns Description\n\n- **Crash Number**: Unique identifier for each crash event.\n- **Date and Time of Crash**: Date and time when the crash occurred.\n- **Police Shift**: Shift of the police responding to the crash.\n- **Crash Location**: Location where the crash occurred.\n- **Light Conditions**: Conditions of light at the time of the crash (e.g., day, night).\n- **Weather Conditions**: Weather conditions during the crash.\n- **Road Surface**: Type of road surface at the crash location.\n- **Road Contributing Circumstances**: Contributing factors or circumstances related to the road condition.\n- **Traffic Control Device Type**: Type of traffic control device present at the crash location.\n- **Roadway Intersection Type**: Type of intersection where the crash occurred.\n- **Trafficway Description**: Description of the trafficway involved in the crash.\n- **Manner of Collision**: Manner in which the collision occurred (e.g., rear-end, head-on).\n- **First Harmful Event**: Initial event that caused harm in the crash sequence.\n- **First Harmful Event Location**: Location where the first harmful event occurred.\n- **Speed Limit**: Speed limit at the crash location.\n- **Work Zone**: Indicates if the crash occurred within a work zone.\n- **Count Fatal Injury**: Number of fatalities resulting from the crash.\n- **Count Suspected Serious Injury**: Number of suspected serious injuries.\n- **Count Suspected Minor Injury**: Number of suspected minor injuries.\n- **Count Possible Injury**: Number of possible injuries.\n- **Count No Apparent Injury**: Number of individuals with no apparent injuries.\n- **Count Unknown Injury**: Number of injuries with unknown severity.\n- **Count Not Reported Injury**: Number of injuries not reported.\n- **Total Non-Motorists**: Total number of non-motorists involved in the crash.\n- **Pedestrian Involvement (Non-Motorist)**: Indicates pedestrian involvement in non-motorist incidents.\n- **Cyclist Involvement (Non-Motorist)**: Indicates cyclist involvement in non-motorist incidents.\n- **Other Non-Motorist Involvement**: Involvement of other non-motorists in the crash.\n- **Hit and Run Flag**: Indicates if the crash was a hit-and-run incident.\n- **Latitude**: Geographic latitude coordinate of the crash location.\n- **Longitude**: Geographic longitude coordinate of the crash location.\n- **Ward**: Ward where the crash occurred.\n- **Block Code**: Code representing the specific block of the crash location.\n\nThis dataset is valuable for analyzing traffic safety, identifying trends, and understanding factors contributing to crashes in Somerville. It can be used for research, policy-making, and implementing interventions to improve road safety and reduce accidents.\n\n\n\n\nThis data set contains Somerville crashes that occurred from May 2018 to present. Crash reports are completed when a motor vehicle crash occurs on a public way and involves at least one of the following: Any person is killed, any person is injured,  or damage is in excess of $1,000 to any one vehicle or other property. Data does not include crashes that are under active investigation, nor those that occur on state roads, which are under the jurisdiction of the Massachusetts State Police.   State crash data may be accessed on the Massachusetts Department of Transportation.\n\n\nData source : https:\/\/catalog.data.gov\/dataset\/police-data-crashes law","109":" Vehicle Maintenance Data  ### Vehicle Maintenance Dataset\n\n#### Overview\nThis dataset provides synthetic data related to vehicle maintenance to help predict whether a vehicle requires maintenance or not based on various features.\n\n#### Features\n\n1. **Vehicle_Model**: Type of the vehicle (Car, SUV, Van, Truck, Bus, Motorcycle)\n2. **Mileage**: Total mileage of the vehicle\n3. **Maintenance_History**: Maintenance history of the vehicle (Good, Average, Poor)\n4. **Reported_Issues**: Number of reported issues\n5. **Vehicle_Age**: Age of the vehicle in years\n6. **Fuel_Type**: Type of fuel used (Diesel, Petrol, Electric)\n7. **Transmission_Type**: Transmission type (Automatic, Manual)\n8. **Engine_Size**: Size of the engine in cc (Cubic Centimeters)\n9. **Odometer_Reading**: Current odometer reading of the vehicle\n10. **Last_Service_Date**: Date of the last service\n11. **Warranty_Expiry_Date**: Date when the warranty expires\n12. **Owner_Type**: Type of vehicle owner (First, Second, Third)\n13. **Insurance_Premium**: Insurance premium amount\n14. **Service_History**: Number of services done\n15. **Accident_History**: Number of accidents the vehicle has been involved in\n16. **Fuel_Efficiency**: Fuel efficiency of the vehicle in km\/l (Kilometers per liter)\n17. **Tire_Condition**: Condition of the tires (New, Good, Worn Out)\n18. **Brake_Condition**: Condition of the brakes (New, Good, Worn Out)\n19. **Battery_Status**: Status of the battery (New, Good, Weak)\n20. **Need_Maintenance**: Target variable indicating whether the vehicle needs maintenance (1 = Yes, 0 = No)\n\n#### Target Variable\n- **Need_Maintenance**: Indicates whether the vehicle requires maintenance or not based on specified conditions.\n\n#### Data Range\n- Total number of records: 50,000\n\n#### Source\nThis dataset is synthetic and was generated using Python. It is intended for educational and research purposes.\n\n#### Acknowledgements\n- The dataset was generated using Python and the data is synthetic.\n\n\n earth and nature automobiles and vehicles","110":" Road Accident Data by Vehicle Type Road Accident Data by Vehicle Type and Provices in Sri Lanka - 2012 The inspiration behind creating this dataset stems from the need to analyze and understand road safety trends, identify risk factors, and develop targeted interventions to reduce road accidents. By categorizing accidents based on vehicle types, researchers, policymakers, and transportation authorities can gain insights into the unique challenges and characteristics associated with different types of vehicles on the road. This dataset aims to serve as a valuable resource for research, analysis, and policymaking efforts aimed at improving road safety and reducing the impact of accidents on society.\n\n categorical automobiles and vehicles intermediate clustering","111":" Road Accidents Data \"Roadway Woes: A Comprehensive Dataset on Road Accidents\" \"Road Accidents Dataset\":\n\nDescription: This comprehensive dataset provides detailed information on road accidents reported over multiple years. The dataset encompasses various attributes related to accident status, vehicle and casualty references, demographics, and severity of casualties. It includes essential factors such as pedestrian details, casualty types, road maintenance worker involvement, and the Index of Multiple Deprivation (IMD) decile for casualties' home areas.\n\nColumns:\n\n1.Status: The status of the accident (e.g., reported, under investigation).\n\n2.Accident_Index: A unique identifier for each reported accident.\n\n3.Accident_Year: The year in which the accident occurred.\n\n4.Accident_Reference: A reference number associated with the accident.\n\n5.Vehicle_Reference: A reference number for the involved vehicle in the accident.\n\n6.Casualty_Reference: A reference number for the casualty involved in the accident.\n\n7.Casualty_Class: Indicates the class of the casualty (e.g., driver, passenger, pedestrian).\n\n8.Sex_of_Casualty: The gender of the casualty (male or female).\n\n9.Age_of_Casualty: The age of the casualty.\n\n10.Age_Band_of_Casualty: Age group to which the casualty belongs (e.g., 0-5, 6-10, 11-15).\n\n11.Casualty_Severity: The severity of the casualty's injuries (e.g., fatal, serious, slight).\n\n12.Pedestrian_Location: The location of the pedestrian at the time of the accident.\n\n13.Pedestrian_Movement: The movement of the pedestrian during the accident.\n\n14.Car_Passenger: Indicates whether the casualty was a car passenger at the time of the accident (yes or no).\n\n15.Bus_or_Coach_Passenger: Indicates whether the casualty was a bus or coach passenger (yes or no).\n\n16.Pedestrian_Road_Maintenance_Worker: Indicates whether the casualty was a road maintenance worker (yes or no).\n\n17.Casualty_Type: The type of casualty (e.g., driver\/rider, passenger, pedestrian).\n\n18.Casualty_Home_Area_Type: The type of area in which the casualty resides (e.g., urban, rural).\n\n19.Casualty_IMD_Decile: The IMD decile of the area where the casualty resides (a measure of deprivation).\n\n20.LSOA_of_Casualty: The Lower Layer Super Output Area (LSOA) associated with the casualty's location.\n\nThis dataset provides valuable insights for analyzing road accidents, identifying trends, and implementing safety measures to reduce casualties and enhance road safety. Researchers, policymakers, and analysts can leverage this dataset for evidence-based decision-making and improving overall road transportation systems. research categorical beginner data analytics tabular","112":" Urban traffic density in cities contains traffic densities of cities Dataset Overview\n\nThis dataset provides a detailed view of traffic data in a futuristic urban environment, containing over 1.2 million records. Each record represents a unique snapshot of various factors affecting traffic conditions in six fictional cities.\n\nFeatures\n\nCity: Name of the city (e.g., MetropolisX, SolarisVille).\nVehicle Type: Type of vehicle (e.g., Car, Flying Car).\nWeather Conditions: Current weather (e.g., Clear, Rainy).\nEconomic Conditions: Economic state of the city (e.g., Booming, Recession).\nDay of Week: Day of the week.\nHour of Day: Hour of the day when the data was recorded.\nSpeed: Recorded vehicle speed.\nEnergy Consumption: Estimated energy consumption based on vehicle type and speed.\nIs Peak Hour: Indicator if the record was during peak traffic hours.\nRandom Event Occurred: Indicator if a random event (e.g., accidents, road closures) occurred.\nTraffic Density: Density of traffic at the time of recording.\nFile Format\n\nThe dataset is provided in CSV format, suitable for analysis in various data processing tools and programming languages.\n\nPotential Uses\n\nThis dataset can be utilized for a variety of studies and analyses, including:\n\nUnderstanding traffic patterns in futuristic urban environments.\nAnalyzing the impact of factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\nDeveloping and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n exploratory data analysis data visualization feature engineering deep learning","113":" Asian-Related Factors in Fatal Crashes In Texas Unveiling Asian Influence: Understanding Contributing Factors in Fatal Crashes This dataset provides insights into fatal car crashes with a focus on Asian-related factors, including vehicle make and model. It includes information on lighting conditions, weather, state, city, and driver behavior. Explore the correlation between these factors and fatal accidents involving Asian car brands. united states automobiles and vehicles beginner classification data storytelling","114":" Road Accidents CSV and EXCEL CSV NUMERICAL DATA SET  \n****Dataset Description: Road Accident Records****\n\nThis dataset contains detailed records of road accidents occurring within a specific geographic region over a defined period. The data encompasses various attributes related to each accident, providing valuable insights into factors contributing to road safety issues.\n\n**Attributes Included:**\n\n1. **Accident ID:** A unique identifier assigned to each accident for reference and tracking purposes.\n2. **Date and Time:** The date and time when the accident occurred, facilitating temporal analysis and trend identification.\n3. **Location:** The precise geographical coordinates or address where the accident took place, enabling spatial analysis and mapping.\n4. **Severity:** The severity level of the accident, categorized based on the extent of injuries, property damage, or fatalities.\n5. **Weather Conditions:** Information about weather conditions prevailing at the time of the accident, such as clear, rainy, foggy, or snowy weather.\n6. **Road Conditions:** Description of road conditions, including dry, wet, icy, or slippery surfaces.\n7. **Vehicle Involved:** Details about vehicles involved in the accident, including types, models, and counts.\n8. **Contributing Factors:** Factors contributing to the accident, such as speeding, distracted driving, drunk driving, road defects, or mechanical failures.\n9. **Injuries and Fatalities:** Number of individuals injured or killed as a result of the accident.\n10. **Vehicle Maneuvers:** Description of maneuvers or actions taken by vehicles involved, such as turning, braking, or overtaking.\n\n**Potential Uses:**\n\n1. **Safety Analysis:** Identifying high-risk locations, times, and conditions to develop targeted safety measures and interventions.\n2. **Predictive Modeling:** Building predictive models to anticipate and prevent accidents based on historical patterns and contributing factors.\n3. **Policy Formulation:** Informing policymaking and infrastructure planning to improve road safety and reduce accident rates.\n4. **Public Awareness Campaigns:** Designing targeted awareness campaigns to educate drivers about common risk factors and safe driving practices.\n5. **Law Enforcement:** Supporting law enforcement efforts by identifying areas with high accident rates and enforcing traffic regulations accordingly.\n\n**Data Source:**\n\nThe dataset is sourced from official accident reports, police records, or other reliable sources authorized to collect and maintain such data. Care has been taken to ensure accuracy and completeness, although some discrepancies may exist due to reporting errors or data collection limitations.\n\n**Note:** Use of this dataset for research, analysis, or other purposes should adhere to relevant data privacy and ethical guidelines, ensuring responsible use and respect for individual privacy rights. categorical law automobiles and vehicles intermediate data analytics english","115":" Road accidents in the Czech Republic Detailed dataset of road accidents in the Czech Republic (2016-2022) The police of Czech Republic regularly gathers and releases detailed data on traffic incidents throughout the nation, typically on an monthly basis. This dataset covers various aspects such as geographic locations, weather conditions, vehicle types, casualty counts, and vehicle maneuvers. The wealth of information makes it a compelling and extensive dataset for analysis and research purposes.\n\n- Most coded variables in the data have been translated into text strings using lookup tables, making the analysis more efficient and user-friendly.\n- After analyzing the dataset, I've filled in the majority of NaN values.\n\nContent\nThe data come from the [police of Czech Republic](https:\/\/www.policie.cz\/clanek\/statistika-nehodovosti-900835.aspx) \n\nThe dataset comprises of two csv files:\n\n- road_accidents_czechia_2016_2022.csv: every line in the file represents a unique traffic accident, featuring various properties related to the accident as columns\n\n- pedestrian.csv: Each line in the file corresponds to a distinct pedestrian-involved traffic incident, with various incident-related attributes captured as columns.\n law demographics automobiles and vehicles exploratory data analysis public safety","116":" Road Traffic Collision Data in Northern Ireland Statistics on injury road traffic collisions (RTCs) from 2017 to 2022 ## Content  \nThe [**Police Service of Northern Ireland (PSNI)**](https:\/\/www.psni.police.uk\/) compiles data on **road traffic collisions (RTCs)** that result in injuries. This data is used to monitor and identify trends in the number of individuals killed or injured (either seriously or slightly) due to RTCs on Northern Ireland\u2019s roads.\n\nThe PSNI\u2019s injury collision data for Northern Ireland, combined with those for England, Scotland and Wales, provide a comprehensive overview of all such collisions across the UK.\n\nThis dataset is retrieved from data.gov.uk, a website that is built and maintained by the Government Digital Service. It contains four csv files:\n1. **casualty2017-2022.csv**: The casualties involved in collisions, including casualty information (whether the casualty was a driver, passenger, pedestrian or cyclist) and vehicle involved.\n2. **collision2017-2022.csv**: The conditions under which the collisions occurred, including collision severity, number of vehicles and casualties involved, time, location, weather, road conditions and carriageway hazards.\n3. **vehicle2017-2022.csv**: The vehicles involved in each collision, including type of vehicle, operation at the time of the collision, object involved and driver information.\n4. **vehicle-index.csv**: A list of variables and values for vehicle2017-2022.csv.\n  \n## Possible Explorations\n- Assess how environmental conditions affect accident likelihood and severity.\n- Examine the influence of time (day and hour) on accident occurrence and severity.\n- Show differences in casualty demographics.\n- Identify regional variations in road traffic collisions.\n- Analyze trends in traffic accidents and fatality rates.\n\n## Important Note\nThe following variables are documented for collisions resulting in fatal and serious injuries only:  \n&gt;**Collision Records**  \n1. a_jdet - Junction Detail\n2. a_jcont - Junction Control\n3. a_pedphys - Pedestrian Crossing Facilities - Physical\n4. a_pedhum - Pedestrian Crossing Facilities - Human\n5. a_light - Light Conditions\n6. a_weat - Weather Conditions\n7. a_roadsc - Road Surface Conditions\n8. a_speccs - Special Conditions at Site\n9. a_chaz - Carriageway Hazard\n10. a_scene - Did a Police Officer Attend the Scene of the Collision  \n  \n&gt;**Casualty Records**  \n1. c_loc - Pedestrian Location\n2. c_move - Pedestrian Movement\n3. c_pcv - Bus or Coach Passenger\n4. c_pedinj - Pedestrian casualty injured in the course of on the road work  \n  \n&gt;**Vehicle Records**  \n1. v_junc - Junction Location of Vehicle at Time of Impact\n2. v_skid - Skidding \/ Overturning\n3. v_hit - First Object Hit in Carriageway\n4. v_leave - Vehicle Leaving Carriageway\n5. v_hitoff - First Object Hit off Carriageway\n6. v_forreg - Foreign Registered Vehicle\n\n## Licence\nUK Open Government Licence (OGL)\n\n## Acknowledgements  \nThe data were publicly visible and published by the Police Service of Northern Ireland. cities and urban areas people and society transportation geospatial analysis public safety","117":" FutureFlow: Navigating Tomorrow's Urban Traffic Decoding Urban Motion: Analyzing Traffic Density in the Cities of Tomorrow ## Overview\n\nThis dataset provides a comprehensive look at traffic data in a futuristic urban setting. It includes over 1.2 million records, each representing a unique snapshot of various factors influencing traffic conditions in six fictional cities.\n\n## Features\n\n- **City**: The name of the city (e.g., MetropolisX, SolarisVille).\n- **Vehicle Type**: Type of vehicle in use (e.g., Car, Flying Car).\n- **Weather Conditions**: Current weather conditions at the time of data capture (e.g., Clear, Rainy).\n- **Economic Conditions**: Economic state of the city at the time of the record (e.g., Booming, Recession).\n- **Day of Week**: The day of the week.\n- **Hour of Day**: The hour of the day when the data was recorded.\n- **Speed**: Recorded speed of the vehicle.\n- **Energy Consumption**: An estimate of energy consumption based on vehicle type and speed.\n- **Is Peak Hour**: Indicator of whether the record was during peak traffic hours.\n- **Random Event Occurred**: Indicator of whether a random event (like accidents or road closures) occurred.\n- **Traffic Density**: The density of traffic at the time of recording.\n\n## File Format\n\nThe dataset is provided in a CSV format, suitable for analysis in various data processing tools and programming languages.\n\n## Potential Uses\n\nThis dataset can be used for a range of studies and analyses, including but not limited to:\n\n- Understanding traffic patterns in futuristic urban environments.\n- Analyzing the impact of various factors like weather, economic conditions, and vehicle types on traffic flow and energy consumption.\n- Developing and testing traffic management algorithms, especially for autonomous vehicles and smart city solutions.\n\n---\n\n*Note: This is a simulated dataset created for analytical and educational purposes.* cities and urban areas demographics transportation","118":" Car Accident Dataset Road Accident Records in Kensington and Chelsea (January 2021) This dataset provides detailed records of road accidents that occurred during *January 2021*. It includes information such as the accident date, day of the week, junction control, accident severity, geographical coordinates, lighting and weather conditions, vehicle details, and more. The data is valuable for analyzing and understanding the factors contributing to road accidents in this urban area, aiding in the development of strategies for improved road safety.\n\n\n**Accident_Index:** A unique identifier for each accident record.\n\n**Accident Date:** The date on which the accident occurred (format: DD\/MM\/YYYY).\n\n**Day_of_Week:** The day of the week when the accident took place.\n\n**Junction_Control** : Describes the type of junction control at the accident location (e.g., \"Give way or uncontrolled\").\n\n**Junction_Detail:** Provides additional details about the junction where the accident occurred (e.g., \"T or staggered junction\").\n\n**Accident_Severity:** Indicates the severity of the accident (e.g., \"Serious\").\n\n**Latitude:** The geographic latitude of the accident location.\n\n**Light_Conditions:** Describes the lighting conditions at the time of the accident (e.g., \"Daylight\").\n\n**Local_Authority_(District)**: The local authority district where the accident occurred.\n\n**Carriageway_Hazards:** Describes any hazards present on the carriageway at the time of the accident (e.g., \"None\").\n\n**Longitude:** The geographic longitude of the accident location.\n\n**Number_of_Casualties:** The total number of casualties involved in the accident.\n\n**Number_of_Vehicles:** The total number of vehicles involved in the accident.\n\n**Police_Force:** The police force that handled the accident.\n\n**Road_Surface_Conditions:** Describes the surface conditions of the road at the time of the accident (e.g., \"Dry\").\n\n**Road_Type:** Specifies the type of road where the accident occurred (e.g., \"One way street\").\n\n**Speed_limit:** The speed limit applicable to the road where the accident occurred.\n\n**Time:** The time of day when the accident happened (format: HH:MM).\n\n**Urban_or_Rural_Area:** Indicates whether the accident occurred in an urban or rural area.\n\n**Weather_Conditions:** Describes the weather conditions at the time of the accident (e.g., \"Fine no high winds\").\n\n**Vehicle_Type:** Specifies the type of vehicle involved in the accident (e.g., \"Car,\" \"Taxi\/Private hire car\"). beginner intermediate exploratory data analysis data visualization data analytics","119":" 2022 UK Road Safety Data Casualty, Collision and Vehicle information for vehicular accidents in the UK This dataset is sourced from the United Kingdom's Open Data website (https:\/\/www.data.gov.uk\/).\n\nIn this dataset, you will find extensive information on the characteristics of individual vehicular accidents. The data is split into three main datasets and one data guide which can be used to reference dummy variables.\n\nThe three main data sets are:\n\n- Casualty\n- Collision\n- Vehicle\n\nThe data sets can be combined using the 'accident_index' key across the data sets. europe law transportation travel urban planning","120":" Road Accident Casualties Understanding Accident Severity for Effective Road Management ## **Introduction:**\n\nRoad accidents pose significant threats to public safety and necessitate a comprehensive understanding of various factors influencing their occurrence. This article explores key aspects related to accident severity and emphasizes the importance of effective road management strategies.\n\n##**Exploring Geographic and Temporal Aspects of Road Incidents:**\n\nTo enhance road safety, it is crucial to delve into the geographic and temporal dimensions of road incidents. Analyzing the locations and times at which accidents frequently occur enables authorities to implement targeted interventions. This section discusses the significance of spatial and temporal analysis in devising proactive safety measures.\n\n##**A Comprehensive Dataset for Traffic Incident Research:**\n\nA robust dataset forms the foundation for meaningful research in traffic incident analysis. This segment highlights the need for comprehensive data collection, emphasizing variables such as road infrastructure, vehicle types, and driver demographics. The article emphasizes the importance of open and accessible datasets to facilitate research and policy development.\n\n##**Impacts of Weather and Road Conditions on Accident Rates:**\n\nWeather and road conditions play a pivotal role in determining accident rates. This section explores the correlations between adverse weather, poor road conditions, and increased accident severity. Understanding these relationships can aid in developing strategies to mitigate risks during inclement weather.\n\n##**Identifying Hotspots and Risk Factors in Road Safety:**\n\nEffective road management involves identifying accident hotspots and understanding the underlying risk factors. By employing data-driven analysis techniques, authorities can pinpoint areas with high accident rates and implement targeted interventions. This portion of the article discusses methodologies for hotspot identification and risk factor analysis.\n\n##**Data-driven Approaches to Reduce Road Accidents:**\n\nHarnessing the power of data is essential for developing proactive strategies to reduce road accidents. This section focuses on data-driven approaches, including predictive modeling and machine learning, to identify potential accident scenarios and implement preventive measures. The integration of technology and analytics is crucial for achieving substantial improvements in road safety.\n\n##**Traffic Collision Analysis for Urban Planning Strategies:**\n\nUrban planning plays a crucial role in shaping road safety outcomes. This part of the article explores how traffic collision analysis can inform urban planning strategies. By incorporating safety considerations into urban design, cities can create environments that minimize the risk of accidents and enhance overall road safety.\n\n##**Patterns of Driver Behavior and Their Influence on Accidents:**\n\nUnderstanding patterns of driver behavior is paramount for effective road management. This section examines the impact of driver behavior on accident rates and discusses how insights into these patterns can inform targeted educational campaigns and enforcement strategies.\n\n##**Conclusion:**\n\nIn conclusion, this article emphasizes the multifaceted nature of road safety and the importance of a holistic approach to accident prevention. By considering factors such as geographic and temporal aspects, comprehensive datasets, weather and road conditions, hotspots and risk factors, data-driven approaches, urban planning, and driver behavior, authorities can formulate effective road management strategies to enhance public safety. business law deep learning bigquery audio command detection audio event classification data storytelling","121":" US Automatic Traffic Recorder Stations Data Vehicle Traffic Counts and Locations at US ATR Stations _____\n# US Automatic Traffic Recorder Stations Data\n### Vehicle Traffic Counts and Locations at US ATR Stations\nBy Homeland Infrastructure Foundation [[source]](https:\/\/data.world\/dhs)\n_____\n\n### About this dataset\n> This comprehensive dataset records important information about Automatic Traffic Recorder (ATR) Stations located across the United States. ATR stations play a crucial role in traffic management and planning by continuously monitoring and counting the number of vehicles passing through each station.\n> \n> The data contained in this dataset has been meticulously gathered from station description files supplied by the Federal Highway Administration (FHWA) for both Weigh-in-Motion (WIM) devices and Automatic Traffic Recorders. In addition to this, location referencing data was sourced from the National Highway Planning Network version 4.0 as well as individual State offices of Transportation.\n> \n> The database includes essential attributes such as a unique identifier for each ATR station, indicated by 'STTNKEY'. It also indicates if a site is part of the National Highway System, denoted under 'NHS'. Other key aspects recorded include specific locations generally named after streets or highways under 'LOCATION', along with relevant comments providing additional context in 'COMMENT'.\n> \n> Perhaps one of the most critical factors noted in this data set would be traffic volume at each location, measured by Annual Average Daily Traffic ('AADT'). This metric represents total vehicle flow on roads or highways for a year divided over 365 days \u2014 an essential numeric analyst's often call upon when making traffic-related predictions or decisions.\n> \n> Location coordinates incorporating longitude and latitude measurements of every ATR station are documented clearly \u2014 aiding geospatial analysis. Furthermore, X and Y coordinates correspond to these locations facilitating accurate map plotting.\n> \n> Additional information contained also includes postal codes labeled as 'STPOSTAL' where stations are located with respective state FIPS codes indicated under \u2018STFIPS\u2019. County specific FIPS code are documented within \u2018CTFIPS\u2019. Versioning information helps users track versions ensuring they work off latest datasets with temporal geographic attribute updates captured via \u2018YEAR_GEO\u2019.\n> \n> \n> Reference Source: [Click Here](https:\/\/hifld-dhs-gii.opendata.arcgis.com\/datasets\/7d1ba903a056438b9a136bf13e3ee9c6_0)\n\n### How to use the dataset\n> \n> ### Introduction\n> \n> \n> ### Diving into the data\n> The dataset comprises a collection of attributes for each station such as its location details (latitude, longitude), AADT or The Annual Average Daily Traffic amount, classification of road where it's located etc. Additionally, there is information related to when was this geographical information last updated.\n> \n> #### Understanding Columns\n> Here's what primary columns represent:\n> - **Sttnkey:** A unique identifier for each station.\n> - **NHS:** Indicates if the station is part of national highway system.\n> - **Location:** Describes specific location of a station with street or highway name.\n> - **Comment:** Any additional remarks related to that station.\n> - **Longitude,**Latitude: Geographic coordinates.\n> - **STPostal:** The postal code where a given station resides.\n> - menu 4 dots indicates show more items**\n> - ADT: Annual Average Daily Traffic count indicating average volume of vehicles passing through that route annually divided by 365 days \n> - Year_GEO: The year when geographic information was last updated - can provide insight into recency or timeliness of recorded attribute values\n> - Fclass: Road classification i.e interstate,dis,e tc., providing context about type\/stature\/importance or natureof theroad on whichstationlies\n> 11.Stfips,Ctfips- FIPS codes representing state,county respectively \n> \n> ### Using this information\n> \n> Given its structure and contents,thisdatasetisveryusefulforanumberofpurposes:\n> \n> **1.Urban Planning & InfrastructureDevelopment**\n> Understanding traffic flows and volumes can be instrumental in deciding where to build new infrastructure or improve existing ones. Planners can identify high traffic areas needing more robust facilities.\n> \n> **2.Traffic Management & Policies**\n> Analysing chronological changes and patterns of traffic volume, local transportation departments can plan out strategic time-based policies for congestion management.\n> \n> **3.Residential\/CommercialRealEstateDevelopment**\n> Real estate developers can use this data to assess the appeal of a location based on its accessibility i.e whether it sits on high-frequency route or is located in more peaceful, low-traffic areas etc\n> \n> **4.Environmental AnalysisResearch:**\n> Researchers may\n\n### Research Ideas\n> - Traffic Planning: This dataset can be used for developing intelligent traffic management solutions by visualizing and predicting the level of traffic in different locations at various times of the day.\n> - Infrastructure development: The data can reveal which road networks need more investment based on usage and functional classification, aiding in smarter infrastructure planning and development decisions.\n> - Commercial analysis: For businesses considering where to open new retail locations or billboards, understanding where vehicular traffic is highest could inform strategic decisions.\n>   \n> - Accident Prevention: Areas with high volumes of daily vehicle traffic could be prone to accidents particularly if they are not part of the National Highway System; this information can guide local authorities to take preventative measures such as implementing additional safety rules, speed limits or installing more signage.\n> - Environmental Studies: By studying areas with high levels of vehicular traffic, environmental scientists can better understand the impact on air quality in those regions which could inform policies aimed at reducing pollution levels.\n>    \n> - Urban design and Real Estate Development - understanding which intersections are busiest may assist designers urban planners guide pedestrians routes and also property developers to strategically develop properties knowing there will be high visibility due their proximity busy streets\/roads \n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https:\/\/data.world\/dhs)\n> \n>\n\n\n### License\n> \n> \n> **License: [Dataset copyright by authors](https:\/\/creativecommons.org\/licenses\/by\/4.0\/)**\n> - You are free to:\n>      - **Share** - copy and redistribute the material in any medium or format for any purpose, even commercially.\n>      - **Adapt** - remix, transform, and build upon the material for any purpose, even commercially.\n> - You must:\n>      - **Give appropriate credit** - Provide a link to the license, and indicate if changes were made.\n>      - **ShareAlike** - You must distribute your contributions under the same license as the original.\n>      - **Keep intact** - all notices that refer to this license, including copyright notices.\n\n### Columns\n\n**File: Automatic_Traffic_Recorder_ATR_Stations.csv**\n| Column name   | Description                                                                                                                         |\n|:--------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| **X**         | The X coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **Y**         | The Y coordinate of the station's location on a 2D map. (Numeric)                                                                   |\n| **STTNKEY**   | A unique identifier for each station. (Numeric)                                                                                     |\n| **NHS**       | An indicator determining whether a particular station is part of the National Highway System or not. (Boolean)                      |\n| **LOCATION**  | Detailed location identification like street or highway names where the recording stations are installed. (String)                  |\n| **COMMENT**   | Extra notes that describe additional characteristics of specific recording stations if available. (String)                          |\n| **LONGITUDE** | The longitude of the station's location. (Numeric)                                                                                  |\n| **LATITUDE**  | The latitude of the station's location. (Numeric)                                                                                   |\n| **STPOSTAL**  | The postal code where the station is located. (String)                                                                              |\n| **AADT**      | The Average Annual Daily Traffic flow through the station. (Numeric)                                                                |\n| **YEAR_GEO**  | The year when the station's geographic information was last updated or recorded. (Numeric)                                          |\n| **FCLASS**    | The functional classification of the road where the station is located (e.g., highway, interstate, arterial or collector). (String) |\n| **STFIPS**    | The Federal Information Processing Standards (FIPS) code for the state where the station is located. (Numeric)                      |\n| **CTFIPS**    | The Federal Information Processing Standards (FIPS) code for the county where the station is located. (Numeric)                     |\n| **VERSION**   | The version number of the dataset. (Numeric)                                                                                        |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [Homeland Infrastructure Foundation](https:\/\/data.world\/dhs).\n\n united states automobiles and vehicles data visualization","122":" Major Road Accidents   Road Accident Details:\nThis dataset comprises 5000 entries detailing road accidents. Each entry contains 15 columns covering crucial accident-related information. It includes unique accident IDs, accident locations, dates, severity levels, weather conditions during the accidents, types of vehicles involved, injury counts, fatalities, causes (such as human error or weather conditions), road types (like highways or city streets), vehicle speeds, alcohol involvement, road conditions, and time of the accidents. The dataset is diverse, offering insights into the circumstances, factors, and outcomes of these accidents, crucial for analysis and pattern recognition in road safety.\n\nAccident Investigation and Reporting:\nComplementing the first dataset, this collection of 5000 entries provides detailed information regarding accident investigations and subsequent reporting. It contains 15 columns, including accident IDs linking it to the first dataset, accident descriptions, police station details involved in the investigation, investigation statuses (such as pending or completed), reporters (like witnesses or involved parties), insurance claim information, fault attributions, injured persons' names and contact information, details about vehicle damages, repair costs, and legal proceedings. This dataset offers a comprehensive view of the post-accident processes, legal aspects, and financial implications, aiding in understanding the aftermath and resolution of road accidents. ","123":" Airbag Recommendation Recommend Airbags for vehicles The Airbag Recommendation Dataset: An Analysis and Insights\n\nIn recent decades, the automotive industry has witnessed substantial advancements in safety measures, notably the integration of airbag systems to reduce the impact of collisions and safeguard occupants. The efficacy of airbags largely depends on various factors such as their placement, design, and the dynamics of the crash scenario. To better understand and improve airbag systems, researchers and engineers heavily rely on datasets that capture real-world collision data, aiding in the refinement and enhancement of these safety mechanisms.\n\nThe Airbag Recommendation Dataset serves as a pivotal resource within this domain. This dataset aggregates information from a multitude of sources, encompassing diverse vehicular collisions, encompassing crash-test data, real-world accidents, and simulations. It consolidates information on the performance of different airbag systems across various makes and models of vehicles, shedding light on their effectiveness under different crash conditions.\n\nThis dataset encompasses a wide array of variables and features, offering a comprehensive view of airbag deployment scenarios. Some of the crucial data points included in this dataset are:\n\n1. **Vehicle Information:** Details about the vehicle involved in the crash, including make, model, year, and specifications relevant to airbag deployment.\n  \n2. **Crash Parameters:** Information about the crash, including impact speed, collision angle, and severity.\n\n3. **Occupant Details:** Data pertaining to the occupants involved, such as their seating position, age, and whether they were wearing seat belts at the time of the crash.\n\n4. **Airbag Deployment:** Details about the airbag system, including the type of airbags deployed (frontal, side, curtain), their deployment timing, and effectiveness in mitigating injury.\n\n5. **Injury Assessment:** Information about injuries sustained by the occupants, ranging from minor to severe, correlated with the airbag deployment scenarios.\n\nAnalyzing this dataset provides valuable insights into various aspects of airbag deployment:\n\n1. **Effectiveness of Different Airbag Systems:** By comparing the performance of diverse airbag systems across different vehicles and collision scenarios, researchers can ascertain which designs are more effective in specific crash situations. For instance, frontal airbags might excel in head-on collisions, while side airbags might prove more beneficial in T-bone accidents.\n\n2. **Optimal Deployment Strategies:** Understanding the correlation between deployment timing and injury mitigation is critical. This dataset aids in evaluating the timing and sequence of airbag deployment to maximize protection while minimizing potential harm caused by airbag impact.\n\n3. **Occupant Positioning and Protection:** The dataset helps in determining the impact of passenger positioning within the vehicle concerning airbag effectiveness. This insight is pivotal in recommending safe seating positions for occupants of different ages and sizes.\n\n4. **Improving Design and Engineering:** Engineers and vehicle manufacturers can use this dataset to refine airbag designs and systems, optimizing their performance in real-world crash scenarios.\n\n5. **Policy and Regulation Implications:** Insights from this dataset can influence safety standards, regulations, and policies related to airbag systems, ensuring higher safety standards in the automotive industry.\n\nHowever, while the Airbag Recommendation Dataset holds immense potential, it also comes with challenges and limitations. Data quality, consistency across different sources, and the representation of various vehicle types and crash scenarios are some of the challenges that researchers need to address when utilizing this dataset.\n\nIn conclusion, the Airbag Recommendation Dataset stands as a valuable asset for researchers, engineers, policymakers, and automotive manufacturers alike. Its insights play a crucial role in advancing the safety standards of airbag systems, contributing significantly to the ongoing efforts to enhance vehicle safety and reduce the severity of injuries sustained in automotive accidents. Continued research and analysis of this dataset are pivotal in driving innovation and improving safety measures within the automotive industry. automobiles and vehicles","124":" Hourly Vehicle Crossing data for Time Series Hourly Vehicle data on an Highway, good dataset for beginers The data contains hourly vehicle counts of particular highway in the year of 1995 beginning from the month of August. The data is of around 70 days span.\n\nThe dataset is good for beginners to practice on Time series automobiles and vehicles beginner data visualization time series analysis linear regression","125":" Motor Vehicle Collisions - Crashes-New York  Dataset of Motor Collisions or Crashes in New York The Motor Vehicle Collisions crash table contains details on the crash event. Each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC. The police report (MV104-AN) is required to be filled out for collisions where someone is injured or killed, or where there is at least $1000 worth of damage (https:\/\/www.nhtsa.gov\/sites\/nhtsa.dot.gov\/files\/documents\/ny_overlay_mv-104an_rev05_2004.pdf). \n\nIt should be noted that the data is preliminary and subject to change when the MV-104AN forms are amended based on revised crash details. For the most accurate, up to date statistics on traffic fatalities, please refer to the NYPD Motor Vehicle Collisions page (updated weekly) or Vision Zero View (updated monthly).\n\nDue to success of the CompStat program, NYPD began to ask how to apply the CompStat principles to other problems. Other than homicides, the fatal incidents with which police have the most contact with the public are fatal traffic collisions. Therefore in April 1998, the Department implemented TrafficStat, which uses the CompStat model to work towards improving traffic safety. Police officers complete form MV-104AN for all vehicle collisions. The MV-104AN is a New York State form that has all of the details of a traffic collision. Before implementing Trafficstat, there was no uniform traffic safety data collection procedure for all of the NYPD precincts. Therefore, the Police Department implemented the Traffic Accident Management System (TAMS) in July 1999 in order to collect traffic data in a uniform method across the City. TAMS required the precincts manually enter a few selected MV-104AN fields to collect very basic intersection traffic crash statistics which included the number of accidents, injuries and fatalities. As the years progressed, there grew a need for additional traffic data so that more detailed analyses could be conducted. The Citywide traffic safety initiative, Vision Zero started in the year 2014. Vision Zero further emphasized the need for the collection of more traffic data in order to work towards the Vision Zero goal, which is to eliminate traffic fatalities. Therefore, the Department in March 2016 replaced the TAMS with the new Finest Online Records Management System (FORMS). FORMS enables the police officers to electronically, using a Department cellphone or computer, enter all of the MV-104AN data fields and stores all of the MV-104AN data fields in the Department\u2019s crime data warehouse. Since all of the MV-104AN data fields are now stored for each traffic collision, detailed traffic safety analyses can be conducted as applicable.\n\n united states automobiles and vehicles beginner intermediate english","126":" Spotify dataset  A Comprehensive Collection of Spotify Tracks Across Various Genres # Description for Spotify Songs Dataset on Kaggle\n__________________________________________________________________________________________________________________\n##Dataset Title: Spotify Songs Dataset\n**Description**:\nThis dataset contains a collection of songs fetched from the Spotify API, covering various genres including \"acoustic\", \"afrobeat\", \"alt-rock\", \"alternative\", \"ambient\", \"anime\", \"black-metal\", \"bluegrass\", \"blues\", \"bossanova\", \"brazil\", \"breakbeat\", \"british\", \"cantopop\", \"chicago-house\", \"children\", \"chill\", \"classical\", \"club\", \"comedy\", \"country\", \"dance\", \"dancehall\", \"death-metal\", \"deep-house\", \"detroit-techno\", \"disco\", \"disney\", \"drum-and-bass\", \"dub\", \"dubstep\", \"edm\", \"electro\", \"electronic\", \"emo\", \"folk\", \"forro\", \"french\", \"funk\", \"garage\", \"german\", \"gospel\", \"goth\", \"grindcore\", \"groove\", \"grunge\", \"guitar\", \"happy\", \"hard-rock\", \"hardcore\", \"hardstyle\", \"heavy-metal\", \"hip-hop\", \"holidays\", \"honky-tonk\", \"house\", \"idm\", \"indian\", \"indie\", \"indie-pop\", \"industrial\", \"iranian\", \"j-dance\", \"j-idol\", \"j-pop\", \"j-rock\", \"jazz\", \"k-pop\", \"kids\", \"latin\", \"latino\", \"malay\", \"mandopop\", \"metal\", \"metal-misc\", \"metalcore\", \"minimal-techno\", \"movies\", \"mpb\", \"new-age\", \"new-release\", \"opera\", \"pagode\", \"party\", \"philippines-opm\", \"piano\", \"pop\", \"pop-film\", \"post-dubstep\", \"power-pop\", \"progressive-house\", \"psych-rock\", \"punk\", \"punk-rock\", \"r-n-b\", \"rainy-day\", \"reggae\", \"reggaeton\", \"road-trip\", \"rock\", \"rock-n-roll\", \"rockabilly\", \"romance\", \"sad\", \"salsa\", \"samba\", \"sertanejo\", \"show-tunes\", \"singer-songwriter\", \"ska\", \"sleep\", \"songwriter\", \"soul\", \"soundtracks\", \"spanish\", \"study\", \"summer\", \"swedish\", \"synth-pop\", \"tango\", \"techno\", \"trance\", \"trip-hop\", \"turkish\", \"work-out\", \"world-music\". Each entry in the dataset provides detailed information about a song, including its name, artists, album, popularity, duration, and whether it is explicit.\n__________________________________________________________________________________________________________________\n**Data Collection Method**:\nThe data was collected using the Spotify Web API through a Python script. The script performed searches for different genres and retrieved the top tracks for each genre. The fetched data was then compiled and saved into a CSV file.\n__________________________________________________________________________________________________________________\n**Columns Description**:\nid: Unique identifier for the track on Spotify.\nname: Name of the track.\ngenre: genre of the song.\nartists: Names of the artists who performed the track, separated by commas if there are multiple artists.\nalbum: Name of the album the track belongs to.\npopularity: Popularity score of the track (0-100, where higher is more popular).\nduration_ms: Duration of the track in milliseconds.\nexplicit: Boolean indicating whether the track contains explicit content.\n__________________________________________________________________________________________________________________\n**Potential Uses**:\nThis dataset can be used for a variety of purposes, including but not limited to:\n\n- Music Analysis: Analyze the popularity and characteristics of songs across different genres.\n- Recommendation Systems: Develop and test music recommendation algorithms.\n- Trend Analysis: Study trends in music preferences and popularity over time.\n- Machine Learning: Train machine learning models for tasks like genre classification or popularity prediction.\n__________________________________________________________________________________________________________________\n**Acknowledgements**:\nThis dataset was created using the Spotify Web API. Special thanks to Spotify for providing access to their extensive music library through their API.\n__________________________________________________________________________________________________________________\n**License**:\nThis dataset is made available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You are free to use, modify, and distribute this dataset, provided you give appropriate credit to the original creator.\n__________________________________________________________________________________________________________________ music exploratory data analysis data analytics recommender systems sql","127":" Anime Dataset 2024 Latest dataset for Animes This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry. anime and manga","128":" Complete Pokemon Dataset 9th Gen (img + tabular) Pokemon image dataset + Base stats will 9th Generation ## Complete Pok\u00e9mon Dataset 9th Gen (img + tabular)\nThe Complete Pok\u00e9mon Dataset 9th Gen (img + tabular) is a comprehensive collection of information covering the entirety of the 9th generation Pok\u00e9mon. It combines both image data of Pok\u00e9mon sprites and tabular data containing their base statistics and index ranks. This dataset serves as a valuable resource for researchers, developers, and enthusiasts interested in exploring and analyzing the characteristics of Pok\u00e9mon in the latest generation.\n\n## **Dataset Overview**:\n### Image Data:\n\n**Pok\u00e9mon Sprites**: High-quality images representing the visual appearance of each Pok\u00e9mon in the 9th generation.\nFormat: Images are provided in a standardized format for easy integration into various applications and analysis tools.\nIndexing: Each image is indexed to correspond with the respective entry in the tabular data for seamless cross-referencing.\n\n### Tabular Data:\n\n**Base Stats**: Comprehensive information on the base statistics of each Pok\u00e9mon, including HP, Attack, Defense, Special Attack, Special Defense, and Speed.\n**Index Rank**: An index rank assigned to each Pok\u00e9mon based on various attributes, such as overall power, rarity, or popularity.\n**Additional Attributes**: Any additional relevant attributes, such as Pok\u00e9mon type, abilities, evolutionary line, and regional variants. games anime and manga","129":" Most Popular Anime of all Time IMDb's Most Popular Anime of All Time **Dataset Description: Most Popular Anime of All Time**\n\nThis dataset compiles information on the most popular anime titles of all time, drawing its data from IMDb.com. IMDb, known for its extensive database of films and television series, serves as a reputable source for aggregating ratings, reviews, and other relevant details.\n\n****Key Features:**\n\n**Name:** The name of the anime series, representing the title under consideration.\n\n**Type: **Categorization of the anime, distinguishing between series, movies, or other formats.\n\n**Aired: **The release year or range of years during which the anime was originally aired.\n\n**Rating:** IMDb ratings, reflecting the overall reception and viewer satisfaction with each anime.\n\n**Votes:** The number of votes received by each anime on IMDb, providing insights into its popularity and audience engagement.\n\n**Description:** A brief summary or description of the anime's storyline, offering a glimpse into its thematic elements.\n\n**Studio:** Information about the production studio responsible for creating the anime, shedding light on the creative forces behind its development.\n\n**Data Collection Methodology:**\n\nThe dataset is populated through web scraping IMDb.com, utilizing Python libraries such as BeautifulSoup for parsing HTML and extracting relevant information. IMDb's comprehensive database ensures the inclusion of a diverse array of anime titles, from classic series to contemporary releases.\n\n**Potential Applications:**\n\nThis dataset can be a valuable resource for anime enthusiasts, researchers, and data analysts interested in exploring trends, patterns, and the factors contributing to the popularity of anime titles. The information contained herein facilitates comparative analyses of different anime series, aiding in the identification of trends in viewer preferences over time.\n\n**Usage Considerations:**\n\nUsers are encouraged to exercise caution and verify the dataset periodically, as IMDb ratings and details may change with time. Additionally, proper attribution to IMDb as the source is recommended when utilizing the dataset for research or any other purpose. beginner intermediate tabular anime and manga pandas","130":" Anime Data from 1970 to 2024 Explore Anime Evolution: Comprehensive Dataset and Scraping Script (1970-2024) This dataset contains a comprehensive collection of anime titles spanning the years 1970 to 2024. The data was collected from MyAnimeList using web scraping techniques. It includes essential information about each anime, such as its unique ID, title, genre, description, studio, release year, and user ratings. The dataset offers a valuable resource for exploring the evolution of anime over the decades and understanding trends in the industry. Researchers, anime enthusiasts, and data analysts can use this dataset to analyze various aspects of anime production and consumption, including popular genres, top-rated studios, and changes in audience preferences over time. The dataset is presented in a CSV format and is suitable for a wide range of data analysis and machine learning applications.\n\n## Columns in the data.json :\nmal_id: Unique identifier for the anime entry.\ntitles: List of titles associated with the anime. In this case, \"Attack No.1\".\ntype: Type of the anime, e.g., TV series.\nsource: Source material of the anime, here it's based on a manga.\nepisodes: Number of episodes in the anime (104 in this case).\nrating: Audience rating category, PG-13 in this example.\nscore: Average score given to the anime by users.\nscored_by: Number of users who have scored the anime.\nrank: Ranking of the anime based on score or popularity.\npopularity: Popularity ranking of the anime.\nmembers: Number of members who have added this anime to their list.\nfavorites: Number of users who have favorited this anime.\nsynopsis: Plot summary or synopsis of the anime.\nstudios: Production studio responsible for creating the anime.\ngenres: List of genres the anime belongs to (e.g., Drama, Sports).\nthemes: List of themes present in the anime (e.g., Team Sports).\n\n## Columns in the  user_recommendation.csv :\nmal_id: This column represents the ID of an anime that users have watched or interacted with.\nmal_id_recomm: This column lists the IDs of anime recommended by users for a specific mal_id.\nvotes: The votes column indicates the number of votes or recommendations given by users for the recommendation of mal_id_recomm for mal_id.\n\n\nThe dataset is ready for exploration, analysis, and visualization to uncover insights into the world of anime and its dynamic landscape. anime and manga","131":" Anime Dataset \"Anime Series Statistics and Metadata\" The \"Anime TV Shows Database\" is a comprehensive collection of data related to various anime TV shows. This dataset provides a wealth of information about anime series, including details about their production, ratings, related shows, genres, and more.\n\n\nKey Features and Attributes:\n\nBasic Information: Includes the title of the show, its type (e.g., TV, OVA, Movie), the year of release, and the season it premiered.\n\nProduction and Studios: Contains information about the studios responsible for producing the anime series, along with studio locations.\n\nRanking and Ratings: Provides details on the ranking of each show, user ratings, and any available reviews.\n\nRelated Shows and Franchises: Lists related shows and provides information about any franchises to which the show belongs. anime and manga","132":" Anime Dataset 2024 Latest dataset for Animes This dataset is a comprehensive collection of anime data, fetched from the MyAnimeList website using the Jikan API. It includes information on the latest animes, making it a valuable resource for up-to-date recommendations.\n\nThe dataset is primed for building an anime recommendation system. But you can also perform Exploratory data analysis , Data Cleaning .\n\nOverall, this dataset provides a rich source of information for anime enthusiasts and data scientists alike, offering a solid foundation for developing sophisticated recommendation systems and conducting insightful data analysis. It stands as a testament to the power of data in enhancing user experiences and driving innovation in the entertainment industry. anime and manga","133":" anime_recommendation_system_weight   ","134":" Anime Content Based Recommendation System Datasets Dive into Dynamic Datasets for Tailored Recommendations! # Description\n\nOur dataset comprises comprehensive user preference data gathered from 73,516 avid anime enthusiasts, spanning across 12,294 diverse anime titles. Each individual user has the autonomy to curate their own completed anime list, supplemented with personal ratings reflecting their viewing experience. This rich compilation of user-generated ratings forms the backbone of our dataset, offering invaluable insights into the nuanced preferences and tastes of anime enthusiasts worldwide.\n\n# Key Features\n\n##### User Profiles:-\nExplore the preferences and behaviors of over 73,000 users, each with their unique anime consumption habits and rating patterns.\n\n##### Anime Titles:-\nDive into a vast collection of 12,000+ anime titles, ranging from timeless classics to contemporary releases across various genres and themes.\n\n##### Completed Lists:-\nGain access to users' completed anime lists, providing a glimpse into the breadth and depth of their viewing history.\n\n##### Ratings:-\nUncover users' subjective evaluations of anime titles, quantified through personalized ratings, offering a granular understanding of viewer satisfaction and engagement.\n\n# Content\n\n##### Anime.csv\n\nanime_id - unique id identifying an anime.\nname - full name of anime.\ngenre - comma separated list of genres for this anime.\ntype - movie, TV, OVA, etc.\nepisodes - how many episodes in this show. (1 if movie).\nrating - average rating out of 10 for this anime.\nmembers - number of community members that are in this anime's\n\"group\".\n\n##### Rating.csv\n\nuser_id - non identifiable randomly generated user id.\nanime_id - the anime that this user has rated.\nrating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).\n exploratory data analysis data visualization feature engineering recommender systems anime and manga","135":" MyAnimesList Datasets - 2023 Listed Animes, Users and Ratings on MyAnimeList (MAL) <h1>\ud83d\udda5\ufe0f My Animes List Datasets - 2023 \ud83d\udda5\ufe0f<\/h1>\n\n    Listed Animes, Users and Ratings on MyAnimeList (MAL)\n\n----\n\n<h2>\ud83d\udcc5 Extraction Period \ud83d\udcc5<\/h2>\n\nThe data was extracted between August 1th in 2023 and October 6th in 2023 via Python Programming Language and using [anime-data-scrapper](https:\/\/github.com\/CSFelix\/anime-data-scrapper) available as a Repository on GitHub.\n\n---\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    anime-dataset-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all listed Animes in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Anime ID** - anime id on MyAnimeList;\n- **Name** - anime original name;\n- **English Name** - English version name;\n- **Other Name** - Japanese version name;\n- **Score** - weighted\/Bayesian average score. You can check out for more details at [\"How are MyAnimeList scores calculated?\"](https:\/\/myanimelist.net\/info.php?go=topanime);\n- **Genres** - related genres;\n- **Synopsis** - briefly description;\n- **Type** - type of animation (movie, anime, OVA...);\n- **Episodes** - number of episodes. Movies are considered having 1 episode;\n- **Aired** - period when anime was aired;\n- **Premiered** - season when the anime was released;\n- **Status** - current status (airing, hiatus, finished...);\n- **Producers** - related production companies;\n- **Licensors** - related streaming platforms and licensors;\n- **Studios** - related animation studios;\n- **Source** - source material of the anime (originated from manga, light novel, movie or tv);\n- **Duration** - duration of the movie or each episode;\n- **Rating** - age restriction;\n- **Rank** - rank position on MyAnimeList website (based on Score criteria);\n- **Popularity** - popularity position on MyAnimeList;\n- **Favorites** - number of users that marked the anime as favorite;\n- **Scored By** - number of users that rated the anime;\n- **Members** - number of users that added the anime to the watch list;\n- **Image Url** - banner url.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-details-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all users in MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Mal ID** - user id;\n- **Username** - nickname;\n- **Gender** - user gender;\n- **Birthday** - birthday;\n- **Location** - user's location or country;\n- **Joined** - the joined date on MyAnimeList Platform (ISO format);\n- **Days Watched** - total number of days the user spent on MyAnimeList;\n- **Mean Score** - the average score the user gives to the watched animes;\n- **Watching** - number of animes currently being watched by the user;\n- **Completed** - number of animes finished by the user;\n- **On Hold** - number of animes that the user stopped watching but kept it into its list;\n- **Dropped** - number of animes that the user stopped watching and removed from its list;\n- **Plan to Watch** - number of animes that the user has added into the list but did not started watching;\n- **Total Entries** - total number of animes into the user's list;\n- **Rewatched** - number of animes rewatched;\n- **Episodes Watched** - number of episodes watched from all animes.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    users-score-2023.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nFull dataset containing all ratings from all users on MyAnimeList.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **User ID** - user id on MyAnimeList Platform;\n- **Username** - nickname;\n- **Anime ID** - anime id on MyAnimeList Platform;\n- **Anime Title** - anime original name;\n- **Rating** - score that the user rated the anime.\n\n----\n\n<h2>\ud83d\udd10 Dataset File \ud83d\udd10<\/h2>\n\n    .\/benchmark\/full-benchmark.csv\n\n<br>\n\n<h2>\u2753 Description \u2753<\/h2>\n\nData about all Recommendation System Algorithms' performances.\n\n<br>\n\n<h2>\ud83d\udcdd Variables \ud83d\udcdd<\/h2>\n\n- **Iteration** - iteration running number;\n- **Algorithm** - filtering recommendation and data basis approach;\n- **Execution Time** - execution time in seconds;\n- **Average CPU Usage** - average CPU usage in percent over the iteration;\n- **Minimum CPU Usage** - minimum CPU usage in percent over the iteration;\n- **Maximum CPU Usage** - maximum CPU usage in percent over the iteration;\n- **Average RAM Usage** - average RAM usage in percent over the iteration;\n- **Minimum RAM Usage** - minimum RAM usage in percent over the iteration;\n- **Maximum RAM Usage** - maximum RAM usage in percent over the iteration.\n\n----\n\n<h2>\ud83d\udd10 Others Datasets \ud83d\udd10<\/h2>\n\nAll others datasets has been created from one of the three previous ones.\n\n----\n\n<h2>\ud83c\udf89 Acknowledgements \ud83c\udf89<\/h2>\n\nThanks to:\n\n1. [Sajid](https:\/\/www.kaggle.com\/dbdmobile) for providing the [Anime Dataset 2023](https:\/\/www.kaggle.com\/datasets\/dbdmobile\/myanimelist-dataset) that inspired this whole dataset.\n\n----\n\n<h2>\u2696\ufe0f Dataset License \u2696\ufe0f<\/h2>\n\n    MIT License\n\nMore details are available on the [Open Source Initiative - The MIT License](https:\/\/opensource.org\/license\/mit\/). anime and manga","136":" USA Bank Financial Data Data set for Tableau practice **Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis. business banking","137":" Nifty Energy Sector Monthly % Returns Dataset Monthly Percentage Returns of Nifty Energy Index Components, Ready for Analysis - **\"This dataset contains monthly percentage returns of constituent stocks comprising the Nifty Energy Index.\"\n\n\"Each observation includes the percentage change in stock prices for leading energy companies listed on the National Stock Exchange of India (NSE).\"\n\n\"The dataset spans multiple years, offering insights into the volatility and performance trends within the energy sector.\"\n\n\"Researchers and analysts can utilize this dataset to analyze market dynamics, assess risk, and develop predictive models for energy stock movements.\"\n\n\"Ideal for studying correlations between individual stock performances and broader market trends affecting the energy sector.\"** business data cleaning data visualization time series analysis tabular","138":" Leading Indian Bank & CIBIL Real-World Dataset Banking and CIBIL Data for Predictive Credit Risk Analysis #### **Description:**\n\nExplore a comprehensive dataset combining internal banking data and CIBIL credit information from a leading Indian bank. This dataset is ideal for developing predictive credit risk models and gaining valuable financial insights. It includes detailed information on customer transactions, credit scores, and more, providing a robust foundation for advanced analytics and risk assessment.\n\n#### Features for Internal Banking Data \n\n| Variable Name               |                         Description                                   |\n|---------------------------|-----------------------------------------------|\n| Total_TL                  | Total trade lines\/accounts in Bureau          |\n| Tot_Closed_TL             | Total closed trade lines\/accounts             |\n| Tot_Active_TL             | Total active accounts                         |\n| Total_TL_opened_L6M       | Total accounts opened in last 6 Months        |\n| Tot_TL_closed_L6M         | Total accounts closed in last 6 months        |\n| pct_tl_open_L6M           | Percent accounts opened in last 6 months      |\n| pct_tl_closed_L6M         | Percent accounts closed in last 6 months      |\n| pct_active_tl             | Percent active accounts                       |\n| pct_closed_tl             | Percent closed accounts                       |\n| Total_TL_opened_L12M      | Total accounts opened in last 12 Months       |\n| Tot_TL_closed_L12M        | Total accounts closed in last 12 months       |\n| pct_tl_open_L12M          | Percent accounts opened in last 12 months     |\n| pct_tl_closed_L12M        | Percent accounts closed in last 12 months     |\n| Tot_Missed_Pmnt           | Total missed Payments                         |\n| Auto_TL                   | Count of Automobile accounts                  |\n| CC_TL                     | Count of Credit card accounts                 |\n| Consumer_TL               | Count of Consumer goods accounts              |\n| Gold_TL                   | Count of Gold loan accounts                   |\n| Home_TL                   | Count of Housing loan accounts                |\n| PL_TL                     | Count of Personal loan accounts               |\n| Secured_TL                | Count of secured accounts                     |\n| Unsecured_TL              | Count of unsecured accounts                   |\n| Other_TL                  | Count of other accounts                       |\n| Age_Oldest_TL             | Age of oldest opened account                  |\n| Age_Newest_TL             | Age of newest opened account                  |\n\n\n\n#### Features for External Cibil Data \n\n**Variable Description**\n\n| Variable                         | Description                                      |\n|----------------------------------|--------------------------------------------------|\n| time_since_recent_payment        | Time Since recent Payment made                   |\n| time_since_first_deliquency      | Time since first Delinquency (missed payment)    |\n| time_since_recent_deliquency     | Time Since recent Delinquency                    |\n| num_times_delinquent             | Number of times delinquent                       |\n| max_delinquency_level            | Maximum delinquency level                        |\n| max_recent_level_of_deliq        | Maximum recent level of delinquency              |\n| num_deliq_6mts                   | Number of times delinquent in last 6 months      |\n| num_deliq_12mts                  | Number of times delinquent in last 12 months     |\n| num_deliq_6_12mts                | Number of times delinquent between last 6 months and last 12 months |\n| max_deliq_6mts                   | Maximum delinquency level in last 6 months       |\n| max_deliq_12mts                  | Maximum delinquency level in last 12 months      |\n| num_times_30p_dpd                | Number of times 30+ dpd                          |\n| num_times_60p_dpd                | Number of times 60+ dpd                          |\n| num_std                          | Number of standard Payments                      |\n| num_std_6mts                     | Number of standard Payments in last 6 months     |\n| num_std_12mts                    | Number of standard Payments in last 12 months    |\n| num_sub                          | Number of substandard payments - not making full payments |\n| num_sub_6mts                     | Number of substandard payments in last 6 months  |\n| num_sub_12mts                    | Number of substandard payments in last 12 months |\n| num_dbt                          | Number of doubtful payments                      |\n| num_dbt_6mts                     | Number of doubtful payments in last 6 months     |\n| num_dbt_12mts                    | Number of doubtful payments in last 12 months    |\n| num_lss                          | Number of loss accounts                          |\n| num_lss_6mts                     | Number of loss accounts in last 6 months         |\n| num_lss_12mts                    | Number of loss accounts in last 12 months        |\n| recent_level_of_deliq            | Recent level of delinquency                      |\n| tot_enq                          | Total enquiries                                  |\n| CC_enq                           | Credit card enquiries                            |\n| CC_enq_L6m                       | Credit card enquiries in last 6 months           |\n| CC_enq_L12m                      | Credit card enquiries in last 12 months          |\n| PL_enq                           | Personal Loan enquiries                          |\n| PL_enq_L6m                       | Personal Loan enquiries in last 6 months         |\n| PL_enq_L12m                      | Personal Loan enquiries in last 12 months        |\n| time_since_recent_enq            | Time since recent enquiry                        |\n| enq_L12m                         | Enquiries in last 12 months                      |\n| enq_L6m                          | Enquiries in last 6 months                       |\n| enq_L3m                          | Enquiries in last 3 months                       |\n| MARITALSTATUS                    | Marital Status                                   |\n| EDUCATION                        | Education level                                  |\n| AGE                              | Age                                              |\n| GENDER                           | Gender                                           |\n| NETMONTHLYINCOME                 | Net Monthly Income                               |\n| Time_With_Curr_Empr              | Time with current Employer                       |\n| pct_of_active_TLs_ever           | Percent active accounts ever                     |\n| pct_opened_TLs_L6m_of_L12m       | Percent accounts opened in last 6 months to last 12 months |\n| pct_currentBal_all_TL            | Percent current balance of all accounts          |\n| CC_utilization                   | Credit card utilization                          |\n| CC_Flag                          | Credit card Flag                                 |\n| PL_utilization                   | Personal Loan utilization                        |\n| PL_Flag                          | Personal Loan Flag                               |\n| pct_PL_enq_L6m_of_L12m           | Percent enquiries PL in last 6 months to last 12 months |\n| pct_CC_enq_L6m_of_L12m           | Percent enquiries CC in last 6 months to last 12 months |\n| pct_PL_enq_L6m_of_ever           | Percent enquiries PL in last 6 months to last 6 months |\n| pct_CC_enq_L6m_of_ever           | Percent enquiries CC in last 6 months to last 6 months |\n| max_unsec_exposure_inPct         | Maximum unsecured exposure in percent            |\n| HL_Flag                          | Housing Loan Flag                                |\n| GL_Flag                          | Gold Loan Flag                                   |\n| last_prod_enq2                   | Latest product enquired for                      |\n| first_prod_enq2                  | First product enquired for                       |\n| Credit_Score                     | Applicant's credit score                         |\n| Approved_Flag                    | Priority levels                                  |\n\n lending banking","139":" Loan Approval Dataset Exploring Loan Applicant Characteristics and Risk Assessment This dataset provides insights into loan applicants' characteristics and their risk assessment. It comprises information on various attributes of loan applicants, including demographic details, financial status, employment history, and ownership status. The dataset includes both numerical and categorical features, making it suitable for diverse analytical approaches.\n\nKey Features:\n\n1. Id: Unique identifier for each loan applicant.\n2. Income: The income level of the applicant.\n3. Age: Age of the applicant.\n4. Experience: Years of professional experience.\n5. Married\/Single: Marital status of the applicant.\n6. House_Ownership: Indicates whether the applicant owns or rents a house.\n7. Car_Ownership: Indicates whether the applicant owns a car.\n8. Profession: Occupation or profession of the applicant.\n9. CITY: City of residence of the applicant.\n10. STATE: State of residence of the applicant.\n11. CURRENT_JOB_YRS: Duration of employment in the current job.\n12. CURRENT_HOUSE_YRS: Duration of residence in the current house.\n13. Risk_Flag: Binary indicator of loan risk, where 1 represents a flagged risky applicant and 0 represents a non-risky applicant.\n\nThis dataset contains 252,000 entries and provides a comprehensive overview of loan applicants' profiles, enabling analysis and modeling for risk assessment and decision-making in lending processes. research finance banking classification insurance","140":" Gesture Recognition System Dataset Computer Vision Task The following steps were taken to prepare the dataset:\n\n1.1. Video Recording: Videos of the gestures (thumbs up, fist, open palm) were recorded for both the left and right hands.\n1.2. Frame Extraction: Frames were extracted from the videos, resulting in a total of 250 images for each gesture from both the left and right hands. Each image is of dimension: 64X64\n\nThere are 3 gestures : fist, open_palm and thumbs_up\n\nEach image\/frame has a name starting with \u2018l\u2019 or \u2018r\u2019 where \u2018l\u2019 refers to left and \u2018r\u2019 refers to right. \nAnd second character as \u2018f\u2019 ,\u2018t\u2019 , or \u2018p\u2019 which stands for fist, thumbs up and open palm, respectively.\nFor example, image named as lf_0011 means it is an image of left fist taken from video\u2019s 11th frame. \nIt was carried out with the help of OpenCV (cv2) and os libraries as shown in code in figure 1 and the frames were downloaded as a zip file with the help of code in figure 2.\n\n1.3. Dataset Splitting: The total dataset consisted of 1500 images (250 images per gesture per hand).\nThe dataset was split into training and testing sets:\nTraining Set: 300 images (150 images for left and right hands combined) for each gesture.\nTesting Set: 200 images (100 images for left and right hands combined) for each gesture.\n computer science computer vision deep learning neural networks cnn image classification","141":" Gesture Recognition Data  # Gesture Recognition Data in Online Conference Scenes\n\n\n## Description\n2,341 People Gesture Recognition Data in Meeting Scenes includes Asians, Caucasians, blacks, and browns, and the age is mainly young and middle-aged. It collects a variety of indoor office scenes, covering meeting rooms, coffee shops, libraries, bedrooms, etc. Each person collected 18 pictures and 2 videos. The pictures included 18 gestures such as clenching a fist with one hand and heart-to-heart with one hand, and the video included gestures such as clapping.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1292?source=Kaggle\n\n## Data size\n2,341 people, each person collects 2 videos and 18 images\n## Race distribution\n786 Asians, 1,002 Caucasians, 401 black, 152 brown people\n## Gender distribution\n1,209 males, 1,132 females\n## Age distribution\nfrom teenagers to the elderly, mainly young and middle-aged\n## Collection environment\nindoor office scenes, such as meeting rooms, coffee shops, libraries, bedrooms, etc.\n## Collection diversity\ndifferent gestures data, different races, different age groups, different meeting scenes\n## Collection equipment\ncellphone, using the cellphone to simulate the perspective of laptop camera in online conference scenes\n## Collection content\ncollecting the  gestures data in online conference scenes\n## Data format\n.mp4, .mov, .jpg\n## Accuracy rate\nthe accuracy exceeds 97% based on the accuracy of the actions; the accuracy of action naming is more than 97%\n# Licensing Information\nCommercial License\n arts and entertainment online communities","142":" Hand Gesture Recognition (3 Classes, 64X64)  There are 300 images in train folder and 200 images in test folder. Dimension of each image is 64px X 64px. Each image contains one of the following three hand gestures :\n- Open Palm\n- Thumbs Up\n- Fist categorical image","143":" Enhanced Sign Language MNIST Dataset Enhanced Sign Language MNIST Dataset for Hand Gesture Recognition The Enhanced Sign Language MNIST dataset is a comprehensive collection of grayscale images representing American Sign Language (ASL) gestures. This dataset serves as an enhancement to the original Sign Language MNIST dataset, providing a more diverse and extensive set of hand gesture samples for machine learning tasks.\n\nInspired by the need for more challenging benchmarks in image-based machine learning, this dataset is consistent with the original [Sign Language MNIST dataset](https:\/\/www.kaggle.com\/datasets\/datamunge\/sign-language-mnist\/data) to acquire a self-generated dataset, resulting in a more robust and varied collection of hand gesture images. The original Sign Language MNIST dataset, available on Kaggle, provided a solid foundation with 27,455 training cases and 7,172 test cases, each representing a label (0-25) mapped to an alphabetic letter A-Z (excluding J and Z). \n\nThe Enhanced Sign Language MNIST dataset builds upon this foundation by incorporating additional images generated through a process involving various image manipulation techniques. These techniques include hand tracking using MediaPipe, cropping, grayscale conversion, and resizing, to create approximately 1400 samples of each alphabetic letter. The enhanced dataset contains 69,252 samples in total, with 55,402 samples for training and validation, and 13,850 samples for testing.\n\nThis dataset is invaluable for researchers and developers working on sign language recognition, hand gesture detection, and related computer vision tasks. It offers a challenging benchmark for evaluating the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in recognizing ASL gestures.\n\nThe dataset is divided into training and testing sets following the methodology outlined in [Oladayo's research (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2), ensuring the consistency and reproducibility of experimental setups. The experimentation framework incorporated four distinct Convolutional Neural Network (CNN) models: CNN1, CNN2, CNN3, and CNN4. Additionally, four diverse data augmentation techniques were employed, denoted as DAM1, DAM2, DAM3, and DAM4. Notably, DAM1 represents the scenario where no data augmentation is applied.\n\n\nCNN2 achieved a remarkable 99.89% validation accuracy on the enhanced test samples and 99.78% on the generated test samples. Training the model on a GPU\/TPU took approximately 209 seconds (3.5 minutes), which is close to the results reported in the research report. This success underscores the effectiveness of sample generation in enhancing the model's performance, showcasing its superiority over traditional data augmentation methods.\n\nWith the Enhanced Sign Language MNIST dataset, researchers can explore new approaches to sign language recognition, develop more robust machine learning models, and ultimately contribute to the advancement of assistive technologies for the deaf and hard-of-hearing community.\n\nIf you use this code or the datasets in your research, please cite the following dissertation:\n[Oladayo Luke. (2024)](https:\/\/www.proquest.com\/dissertations-theses\/enhancing-sign-language-recognition-hand-gesture\/docview\/3054372234\/se-2). Enhancing Sign Language Recognition and Hand Gesture Detection using Convolutional Neural Networks and Data Augmentation Techniques. (Doctoral dissertation, Nova Southeastern University). computer science","144":" Bangla Sign Language Video Dataset  The Bangla Sign Language Video Dataset is a comprehensive collection of videos captured from the Badhir School in Dhaka, Bangladesh. This dataset is meticulously curated, containing over 8000 videos showcasing sign language gestures corresponding to 40 common Bangla words. Developed with the aim of facilitating research and development in the field of sign language recognition and interpretation, this dataset serves as a valuable resource for both academia and industry professionals.\n\nAs part of the preprocessing pipeline for the Bangla Sign Language Video Dataset, advanced techniques were employed to detect the hand region and remove the background from the captured videos. This crucial preprocessing step enhances the quality and usability of the dataset for subsequent analysis and research endeavors in sign language recognition and interpretation.\n\nHand Region Detection:\nUtilizing state-of-the-art computer vision algorithms, the hand region in each video frame was accurately identified. MediaPipe, a robust framework for building multimodal perceptual pipelines, was employed for its efficiency and reliability in hand detection tasks. By isolating the hand region, the dataset ensures a focused and standardized input for subsequent processing stages, laying the foundation for precise gesture analysis.\n\nBackground Removal:\nFollowing hand region detection, sophisticated background removal techniques were applied to eliminate extraneous visual elements from the video frames. This process enhances the clarity and saliency of the hand gestures, minimizing distractions and facilitating more accurate feature extraction during subsequent analysis stages. Through meticulous background removal, the dataset offers researchers and developers clean and unambiguous data for training and evaluation purposes. ","145":" Hand Navigation Landmarks Deciphering Hand Gestures: A Comprehensive Image Dataset for Gesture Recognition \nThe \"Hand Gesture Landmark\" dataset is an image dataset designed for hand gesture recognition tasks. It encompasses a variety of hand navigation movements, each categorized into one of nine distinct classes: \n\n anticlockwise,\n clockwise,\n up,\n down,\n left,\n right,\n forward,\n backward. \n\nThe dataset is generated utilizing the capabilities of both Mediapipe and OpenCV technologies.\n\nStructured into separate directories for training, evaluation, and testing, the dataset ensures effective model training and evaluation processes. Within the training directory, there are 2000 images for each class, providing ample data for robust model learning. The test directory contains 300 images for each class, enabling comprehensive model testing across various scenarios. Lastly, the evaluation directory includes 900 images, likely intended for further validation or fine-tuning of trained models. With its comprehensive coverage of hand gestures and sufficient data distribution, the \"Hand Gesture Landmark\" dataset serves as a valuable resource for researchers and developers working on gesture recognition applications. computer science computer vision feature engineering image","146":" Hand Gesture Recognition Sample Outputs   art","147":" Hand Gesture Recognition Dataset for YOLOv8 Diverse Hand Gestures: A collection of 20,000 images featuring a variety of hand This dataset comprises 20,000 images of hand gestures, divided into 10 distinct classes. Each class represents a specific hand gesture commonly used in gesture recognition tasks. The dataset is organized into \"train\" and \"val\" folders, adhering to the structure recommended for YOLOv8 data preparation.\n\nThe hand gestures included in the dataset are meticulously labeled to facilitate training and evaluation of YOLOv8-based models for hand gesture recognition applications. Researchers, developers, and enthusiasts interested in exploring deep learning-based approaches for gesture recognition tasks can utilize this dataset to train and validate their models effectively.\n\nPlease note that the dataset is provided solely for research and educational purposes. Any commercial use or redistribution of this dataset should be done with appropriate permissions and acknowledgments. We hope this dataset contributes to advancements in gesture recognition technology and encourages further exploration in this field. intermediate computer vision classification cv2 image classification","148":" ISL-CSLTR: Indian Sign Language Dataset Dataset for Continuous Sign Language Translation and Recognition Sign language is a cardinal element for communication between deaf and dumb community. Sign language has its own grammatical structure and gesticulation nature. Research on SLRT focuses a lot of attention in gesture identification. Sign language comprises of manual gestures performed by hand poses and non-manual features expressed through eye, mouth and gaze movements. \n\nThe sentence-level completely labelled Indian Sign Language dataset for Sign Language Translation and Recognition (SLTR) research is developed. The ISL-CSLTR dataset assists the research community to explore intuitive insights and to build the SLTR framework for establishing communication with the deaf and dumb community using advanced deep learning and computer vision methods for SLTR purposes. This ISL-CSLTR dataset aims in contributing to the sentence level dataset created with two native signers from Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India and four student volunteers from SASTRA Deemed University, Thanjavur, Tamilnadu. The ISL-CSLTR corpus consists of a large vocabulary of 700 fully annotated videos, 18863 Sentence level frames, and 1036 word level images for 100 Spoken language Sentences performed by 7 different Signers. This corpus is arranged based on signer variants and time boundaries with fully annotated details and it is made available publicly.\n\nThe main objective of creating this sentence level ISL-CSLRT corpus is to explore more research outcomes in the area of SLTR. This completely labelled video corpus assists the researchers to build framework for converting spoken language sentences into sign language and vice versa. This corpus has been created to address the various challenges faced by the researchers in SLRT and significantly improves translation and recognition performance. The videos are annotated with relevant spoken language sentences provide clear and easy understanding of the corpus data.\n\nAcknowledgements:\nThe research was funded by the Science and Engineering Research Board (SERB), India under Start-up Research Grant (SRG)\/2019\u20132021 (Grant no. SRG\/2019\/001338). And also, we thank all the signers for their contribution in collecting the sign videos and the successful completion of the ISL-CSLTR corpus. We would like to thank Navajeevan,  Residential School for the Deaf, College of Spl. D.Ed & B.Ed, Vocational Centre, and Child Care & Learning Centre, Ayyalurimetta, Andhra Pradesh, India for their support and contribution.  computer vision lstm keras tensorflow pytorch","149":" HGR-Dataset  # Hand Gesture Recognition Dataset\n\n## Overview\n\nThis dataset contains images for hand gesture recognition, supporting research and development in the field of human-computer interaction, particularly in augmented reality (AR) and virtual reality (VR) applications.\n\n## Dataset Description\n\nThe dataset consists of hand images captured using a standard camera, with corresponding labels for hand gestures. The text input method is based on hand-gesture recognition using a trained neural network. The process involves hand segmentation, gesture recognition, and hand movement tracking using a convex hull algorithm.\n\n## License\n\nThis dataset is provided under the license of Nizamuddin and Nooruddin. Please refer to the LICENSE.md file for detailed licensing information.\n\n## Usage\n\nResearchers and developers are encouraged to use this dataset for various applications, including but not limited to:\n\n- Human-Computer Interaction\n- Augmented Reality (AR) Systems\n- Virtual Reality (VR) Systems\n\n## Dataset Structure\n\nThe dataset is organized into folders containing hand images and corresponding labels. The file structure is as follows:\n\n- `images\/`: Directory containing hand images.\n- `labels.csv`: CSV file containing labels for each hand image.\n\n## Citation\n\nIf you use this dataset in your work, please cite the following paper:\n\nN. Nooruddin, R. Dembani and N. Maitlo, \"HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices,\" 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Toronto, ON, Canada, 2020, pp. 744-751, doi: 10.1109\/SMC42975.2020.9283348.\n\n**Keywords:** Human computer interaction; Solid modeling; Image segmentation; Computational modeling; Neural networks; Gesture recognition; Computer architecture; Augmented Reality; Virtual Reality; Hand Segmentation; Hand Gesture Recognition; Text-Based Input; CNN.\n\n@InProceedings{Nooruddin_2020_SMC,\n    author    = {Nooruddin, N. and Dembani, R. and Maitlo, N.},\n    title     = {HGR: Hand-Gesture-Recognition Based Text Input Method for AR\/VR Wearable Devices},\n    booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},\n    month     = {October},\n    year      = {2020},\n    pages  &nbsp;&nbsp;&nbsp;=&nbsp;{744-751}\n}\n\nPlease visit our linkedin for further details:\n- [Mr. Nizamuddin Maitlo](https:\/\/www.linkedin.com\/in\/nizamuddin-maitlo-b27417a0\/)\n- [Dr. Nooruddin Noonari](https:\/\/www.linkedin.com\/in\/noonari\/)\n arts and entertainment science and technology computer vision classification deep learning image cv2","150":" Sensor based (American) Sign Language Recognition  This dataset features detailed sensor data from smart gloves designed to translate sign language into spoken language. The gloves were equipped with **5 flex sensors** and a \"**[Grove - 6 - Axis Accelerometer & Gyroscope](https:\/\/wiki.seeedstudio.com\/Grove-6-Axis_AccelerometerAndGyroscope\/)**\" for each hand. The dataset consists of different files, each recorded by a different individual, showcasing a variety of hand movements and signs. Key features include:\n**- `Flex-Left\/Right-p-Frame-n` :** Measures the degree of flexion for each finger over 20 frames, with 'n' indicating the frame and 'p' the finger number.\n**- `Acceleration-X\/Y\/Z-Left\/Right-Frame-n` :** Captures the hand's acceleration in 3D space across 20 frames.\n**- `Orientation-X\/Y\/Z-Left\/Right-Frame-n` :** Details the hand's orientation in 3D space for each frame.\n So for each hand, there are 3 types of measurements (Flex, Acceleration, Orientation), each with 3 axes (X, Y, Z) for Acceleration and Orientation, and 5 measurements for Flex (one per finger), resulting in 11 measurements per hand per frame. Multiplied by 20 frames, and then doubled for both hands, this amounts to **440 columns**. Additionally, there's a **\"SIGN\" column**, which is the target for each data entry, identifying the specific sign language gesture.\n\nEach recording in the dataset represents a sequence of 20 frames, capturing detailed motion for both left and right hands.\n\n**This dataset is not yet complete and is still being collected.** languages artificial intelligence electronics intermediate classification english","151":" Malaysian Sign Language (MSL) Image Dataset  This dataset is a collection of images designed for training machine learning models in the recognition and classification of Malaysian Sign Language (MSL) signs. It encompasses three distinct categories, each critical for a comprehensive understanding of MSL:\n\nAlphabets (Alphabet_MSL): This segment consists of images depicting individual letters of the MSL alphabet. Each image is a clear, focused representation of a hand gesture corresponding to an MSL letter, formatted in .jpg for optimal compatibility with machine learning algorithms.\n\nSingle Words (SingleWord_MSL): Here, key basic words used in MSL communication are visually represented. This category is vital for models intended to recognize common vocabulary in sign language.\n\nNumbers (Number_MSL): This part contains images illustrating numeric signs in MSL, covering a range from simple digits. languages earth and nature classification image","152":" Masked Gesture Dataset A dataset comprising 14,000 images of hand gestures. A dataset comprising 14,000 images of hand gestures has been created. These images were manually captured. The dataset, as yet unnamed, offers a substantial resource for various applications in computer vision and machine learning, particularly in the realm of gesture recognition and sign language interpretation. The dataset's size and diversity make it a valuable asset for researchers and developers.\n\nPotential uses for this dataset include training and evaluating machine learning models for gesture recognition, enabling research in computer vision, supporting accessibility and assistive technology development, and contributing to educational and research initiatives. Depending on the dataset's licensing and labeling, it can be made publicly available for broader use within the research community, fostering advancements in the field.\n\nDocumentation, labeling, and clear licensing terms are essential considerations for sharing the dataset with the wider community.\n\n\n\n\n online communities","153":" Myoelectric Control for Hand Motions Myoelectric Control for Hand Motions with Myo Armband This dataset comprises surface electromyography (sEMG) recordings acquired for myoelectric control applications. Collected using the Myo Armband device, the dataset encompasses ten distinct hand motion classes. These recordings serve as a valuable resource for researchers and practitioners in the fields of biomedical signal processing, machine learning, and human-computer interaction. The dataset's accessibility and diversity make it suitable for training, testing, and optimizing sEMG-based hand gesture recognition algorithms. By sharing this dataset, we aim to foster advancements in myoelectric control technologies and contribute to the development of more intuitive and efficient human-machine interfaces. earth and nature business","154":" ASL Alphabet Wireframes A dataset of wireframe images pulled from ASL alphabet images.  The Hand Gesture Recognition Dataset is a comprehensive collection designed for training and evaluating machine learning models for hand gesture recognition tasks. This dataset comprises 24 distinct classes, representing each letter of the alphabet excluding 'J' and 'Z'. Each gesture is depicted against a consistent black background, enhancing the clarity of the hand signs. The visual representation of the gestures is presented as wireframes, providing a clear outline of the hand's position.\n\n\nKey Features:\n\n24 Distinct Classes: The dataset includes signs corresponding to all letters of the alphabet, except 'J' and 'Z'. These two classes require video input so they were removed from the still image dataset. \n\nConsistent Background: All gestures are set against a black background, ensuring uniformity across the dataset. This consistent background simplifies preprocessing tasks and ensures that models are trained on signs alone, without any background distractions.\n\nPrecise Hand Landmarks: The dataset's accuracy is enhanced by utilizing Google MediaPipe to extract detailed hand landmarks. These landmarks are essential for understanding the intricate movements of fingers, enabling the development of highly accurate gesture recognition models.\n image health conditions","155":" Serbian Sign Language Dataset From Hands to Words: A Dataset for Sign Language Translation and Recognition The dataset is a collection of hand gesture and expression videos captured using a webcam and processed using the OpenCV and Mediapipe libraries. It consists of 33 classes, each representing a different hand gesture or expression. For each class, there are 30 videos, and each video contains 75 frames. The dataset is organized into directories, with each class having its own directory. Within each class directory, there are 30 subdirectories numbered from 0 to 29, each containing 75 frames as numpy arrays.\n\nThe keypoints of the hand in each frame were detected using the Mediapipe library, resulting in numpy arrays that represent the coordinates of these keypoints. These keypoints are crucial for understanding the hand's pose and movement in each frame. The dataset is intended to support various applications, such as gesture recognition and machine learning model development, offering a substantial resource for research and development in these domains. computer science computer vision classification random forest numpy serbian","156":" Sign Language Gestures Recognition Data  **Description**\nThe data diversity includes multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, and multiple light conditions. In terms of data annotation, 21 landmarks, gesture types, and gesture attributes were annotated. This dataset can be used for tasks such as gesture recognition and sign language translation.\nFor more details, please visit: https:\/\/www.nexdata.ai\/datasets\/980?source=Kaggle\n\n**Specifications**\nData size\n180,717 images, including 83,012 images of static gestures, 97,705 images of dynamic gestures\nPopulation distribution\nthe race distribution is Asian, the gender distribution is male and female, the age distribution is mainly young people and middle-aged people\nCollection environment\nincluding indoor scenes and outdoor scenes\nCollection diversity\nincluding multiple scenes, 41 static gestures, 95 dynamic gestures, multiple photographic angles, multiple light conditions\nDevice\ncellphone\nData forma\nthe image data format is .jpg, the annotation file format is .json\nCollecting content\nsign language gestures were collected in different scenes\nAnnotation content\n21 landmarks annotation (each landmark includes the attribute of visible or invisible), gesture type annotation, gesture attributes annotation (left hand or right hand)\nAccuracy\naccuracy requirement: the point location errors in x and y directions are less than 3 pixels, which is considered as a qualified annotation; accuracy of landmark annotation: the annotation part (each landmark) is regarded as the unit, the accuracy rate shall be more than 95%\n\n**Get the Dataset**\nThis is just an example of the data. To access more sample data or request the price, contact us  at info@nexdata.ai computer vision image","157":" Sign Language Recognition Dataset  The Hand Sign Recognition Image Dataset is a rich collection of high-quality images featuring diverse **single-hand signs**. With comprehensive coverage, annotations, and multi-modal data, it's an ideal resource for developing sign language recognition models. Whether for sign language translation or gesture-based interaction, this dataset supports a range of applications and is ready for machine learning integration. ","158":" Gesture Classification HaGRID Gesture images including thumbs_up(like) and thumbs_down(dislike). ## Data Sources:\nThis Dataset is collected from HAnd Gesture Recognition Image Dataset(HaGRID) and its 512p version created by [Innominate817](https:\/\/www.kaggle.com\/innominate817). I modify it  for gesture classification.\nHaGRID: https:\/\/github.com\/hukenovs\/hagrid\nHaGRID-512p: https:\/\/www.kaggle.com\/datasets\/innominate817\/hagrid-classification-512p\n## Data Analysis:\nit includes 2 gestures: thumbs up (like) and thumbs down(dislike).\nThe statistics for each sample are as follows:\n| shape | train | valid | test | total|\n| --- | --- | --- | --- | --- |\n| like | 16554 | 5518 | 5519 | 27591|\n| dislike | 17007 | 5669 | 5669 | 28345|\n\n## Usage:\nThis dataset can be used in gesture classification. My demo can be seen on GitHub: https:\/\/github.com\/RuiNov1st\/InternWork\/tree\/main\/L6_GestureClassification. I use pretrained ResNet18 as base model and obtain its capability to recognize 'thumb up' and 'thumb down' gestures through retraining. The accuracy in the test set can reach 99%. I also take some my own gestures photos for testing, but now the performance is not good.\nAny suggestions will be greatly appreciated! \n literature","159":" Hand Gestures Dataset Empower Your ML Models with Hand Gesture Dataset !! ### TRAIN \/ TEST SPLIT:\n\n**Training Set:** 70% of the dataset with 642 images\n**Validation Set:** 19% of the dataset with 178 images\n**Testing Set:** 10% of the dataset with 94 images\n### PREPROCESSING:\n\n**Auto-Orient:** Applied to ensure consistent orientation of images\n**Resize:** All images have been stretched to a standard size of 640x640 for uniformity\n\n### AUGMENTATIONS:\n\nNo augmentations were applied to the dataset, preserving the original image integrity.\n\n**_annotations.csv contains following columns:**\n\n**filename:** The name of the image file associated with the annotation.\n**width:** The width of the image in pixels.\n**height:** The height of the image in pixels.\n**class:** The class label of the hand gesture present in the image.\n**xmin:** The x-coordinate of the top-left corner of the bounding box around the hand gesture.\n**ymin:** The y-coordinate of the top-left corner of the bounding box around the hand gesture.\n**xmax:** The x-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n**ymax:** The y-coordinate of the bottom-right corner of the bounding box around the hand gesture.\n\n### Here are some potential use cases for this dataset:\n\n**Gesture-Controlled User Interfaces:** The dataset can be used to build gesture-controlled user interfaces for various devices and applications, such as smartphones, tablets, computers, and IoT devices.\n**Sign Language Translation:** Hand gestures are an essential part of sign language used by the hearing-impaired community. This dataset can be used to develop sign language recognition systems that can translate sign language into text or speech, enabling better communication with non-sign language users.\n\n computer vision classification image yolov5 english","160":" British Airways Reviews  The dataset containing reviews of British Airways has been meticulously gathered from the Skytrax website. This dataset is intended for comprehensive data analysis and visualization. By analyzing this data, we aim to uncover insights and trends regarding customer experiences and satisfaction levels with British Airways. The visualization of this data will further enhance the understanding by presenting the findings in a clear and engaging manner, making it easier to identify patterns and areas for improvement. ratings and reviews","161":" online review.csv Online shopping sentiment analysis flikcart  The [\/kaggle\/input\/online-review-csv\/online_review.csv ](url)file contains customer reviews from Flipkart. It includes the following columns:\n\nreview_id: Unique identifier for each review.\nproduct_id: Unique identifier for each product.\nuser_id: Unique identifier for each user.\nrating: Star rating (1 to 5) given by the user.\ntitle: Summary of the review.\nreview_text: Detailed feedback from the user.\nreview_date: Date the review was submitted.\nverified_purchase: Indicates if the purchase was verified (true\/false).\nhelpful_votes: Number of users who found the review helpful.\nreviewer_name: Name or alias of the reviewer.\nUses\nSentiment Analysis: Understand customer sentiments.\nProduct Improvement: Identify areas for product enhancement.\nMarket Research: Analyze customer preferences.\nRecommendation Systems: Improve recommendation algorithms.\nThis dataset is ideal for practicing data analysis and machine learning techniques. tabular image","162":" British Airways Reviews Dataset Exploring British Airways Customer Reviews This dataset captures customer reviews of **British Airways** obtained from the website[ https:\/\/www.airlinequality.com\/airline-reviews\/scoot](url). It consists of raw, uncleaned data directly scraped from the source, providing a rich resource for honing skills in **data cleaning, data visualization, analysis, and potentially predicting customer sentiment**.\n\nThe dataset includes various attributes such as review text, ratings, date of travel, and possibly more contextual information relevant to customer experiences with British Airways. Given its raw state, the dataset presents an opportunity to practice preprocessing techniques to ensure data quality and prepare it for deeper analytical insights. intermediate exploratory data analysis data cleaning data visualization tabular","163":" Capterra Ticketsystem - Jira, Zendesk - Reviews Cappterra Ticketsystem Reviews - Jira, Zoho, ServiceNow, Zendesk - Text, Ratings ## Capterra Reviews for Ticket Systems\n\n### Dataset Description\n\nThis dataset contains reviews of various ticket systems from Capterra, providing insights into the features, pros, and cons as experienced by users. The dataset is designed to facilitate analysis and comparison of different ticket systems based on user feedback. Each review includes ratings for various aspects such as ease of use, customer service, features, and overall value for money, as well as the likelihood to recommend the system.\n\n### Content\n\nThe dataset consists of the following columns:\n\n- **ticket_system**: The name of the ticket system being reviewed.\n- **title**: The title of the review.\n- **overall_text**: The overall text of the review.\n- **pros_text**: The positive aspects of the ticket system as mentioned by the reviewer.\n- **cons_text**: The negative aspects of the ticket system as mentioned by the reviewer.\n- **overall_rating**: The overall rating given by the reviewer (out of 5).\n- **ease_of_use**: The rating for ease of use (out of 5).\n- **customer_service**: The rating for customer service (out of 5).\n- **features**: The rating for features (out of 5).\n- **value_for_money**: The rating for value for money (out of 5).\n- **likelihood_to_recommend**: The likelihood to recommend the system (out of 10).\n\nIn addition, the dataset includes columns for ten key features, indicating whether each feature was mentioned positively, negatively, or not at all in the review. The feature columns are:\n\n- **Ticket Creation and Assignment**\n- **Status Tracking and Updates**\n- **Priority and SLA Management**\n- **Reporting and Analytics**\n- **Automated Ticket Routing**\n- **Knowledge Base Integration**\n- **Customer and Agent Portals**\n- **Multi-Channel Support (Email, Chat, Phone)**\n- **Email Notifications and Alerts**\n- **Customizable Workflows**\n\nEach feature column contains:\n- `1` if the feature was mentioned positively,\n- `-1` if the feature was mentioned negatively,\n- `0` if the feature was not mentioned.\n\n### Usage\n\nThis dataset can be used for:\n- **Sentiment Analysis**: Analyzing user sentiments towards different features of ticket systems.\n- **Feature Importance**: Identifying which features are most positively or negatively received.\n- **Comparative Analysis**: Comparing different ticket systems based on user reviews and ratings.\n- **Market Research**: Understanding customer needs and preferences in the ticketing system market.\n\n### Acknowledgements\n\nThe reviews in this dataset are sourced from Capterra. The dataset is intended for educational and research purposes.\n\n### Keywords\n\nTicket systems, Capterra reviews, customer service, sentiment analysis, feature analysis, user feedback, software reviews, helpdesk solutions, comparative analysis, market research\n\n### License\n\nThe dataset is available for public use. If you use this dataset, please credit the original source and this dataset on Kaggle.\n\n### Contact\n\nFor any questions or contributions regarding this dataset, please reach out via Kaggle or through the provided contact information in the dataset repository.\ndata@softoft.de business","164":" Airline Customer Reviews Dataset Customers Reviews in Airline Industry  ","165":" commerce Furniture This dataset comprises 2,000 entries scraped from AliExpress. this graph was created in PowerBi,Loocker studio and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fea63e695d326e3f57b044725376024c2%2Fgraph1.jpg?generation=1718574239511915&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Fc285144b368e9b25b2d576d5bf6d4b81%2Fgraph2.png?generation=1718574245118394&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F80adbea61286526a5910b803a8d4822e%2Fgraph3.png?generation=1718574250267201&alt=media)\n\n\nAliExpress Data is vast collection of information and statistics gathered from the AliExpress online marketplace. It includes details about products, sellers, prices, customer reviews, ratings, and other relevant data points. This data can be analyzed to gain insights into market trends, consumer behavior, and business performance, helping businesses make informed decisions and optimize their operations on the platform.\nExamples of AliExpress data include product listings, customer reviews, seller ratings, and transaction details. AliExpress data is used for various purposes such as market research, competitor analysis, trend identification, and customer behavior analysis. In this page, you\u2019ll find the best data sources for AliExpress data, AliExpress dataset, AliExpress data analysis. asia finance data cleaning data analytics","166":" Online Store Selling  This is a synthetic dataset of an online store selling natural cosmetics. It is closely approximated to real data, with logic maintained in the number of sales during certain periods, the influence of user ratings on products, and even the seasonality of some product sales. make-up and cosmetics","167":" Exchange Rates:USD to PKR,INR, and CNY (2004-2024)  ## Introduction:\n\nThis dataset contains historical daily exchange rates for the US Dollar (USD) against three major currencies: Pakistani Rupee (PKR), Indian Rupee (INR), and Chinese Yuan (CNY). The data covers the period from January 2004 to June 2024, providing a comprehensive view of currency fluctuations over time.\n\n## About the Dataset\n\nThis dataset contain daily exchange rate from (2004 - 2024) of Dollar to PKR, Dollar to INR, Dollar to Chinese Yuan and contains 9 columns.\n\n## Columns:\n\n- `Date`: The date of the exchange rate.\n- `Open_pkr, High_pkr, Low_pkr, Close_pkr, Adj Close_pkr`: Exchange rate details from USD to PKR.\n- `Open_inr, High_inr, Low_inr, Close_inr, Adj Close_inr`: Exchange rate details from USD to INR.\n- `Open, High, Low, Close, Adj Close`: Exchange rate details from USD to CNY.\n- `PKR_pct_change`: Daily percentage change in the exchange rate from USD to PKR.\n- `INR_pct_change`: Daily percentage change in the exchange rate from USD to INR.\n- `CNY_pct_change`: Daily percentage change in the exchange rate from USD to CNY.\n\n## Source:\nThe data has been compiled from various reliable financial sources to ensure accuracy and consistency.\n\n## Usage:\nThis dataset is valuable for financial analysts, economists, and researchers interested in studying currency trends, economic events, and their impacts on exchange rates. It can also be used for machine learning projects involving time series analysis, forecasting, and financial modeling.\n\n## Acknowledgements:\nWe acknowledge the data sources for providing the necessary exchange rate information. finance currencies and foreign exchange","168":" Longest Running Indian Television Series A Journey Through Timeless Classics This dataset provides detailed information on the longest-running Indian television series in the Hindi language, specifically focusing on non-fiction genres. The data encompasses various attributes, including the length of time each show has been on air, the title, the network it was broadcasted on, the genre, start and end dates, the number of episodes, and additional notes. Below is a summary of the dataset structure:\n\n## Dataset Columns:\n- **Length (years)**: The duration for which the show has been running, expressed in years.\n- **Title**: The name of the television series.\n- **Network**: The TV network(s) that aired the show.\n- **Genre**: The category or type of the show (e.g., Agriculture, Music, Reality, Game show, Talk show, Cooking show, Stunt based Reality show, Cultural).\n- **Start date**: The date when the show first premiered.\n- **End date**: The date when the show ended, if applicable.\n- **No. of episodes**: The total number of episodes aired.\n- **Notes**: Additional information or significant achievements related to the show. exploratory data analysis data visualization data storytelling","169":" Top5000 Albums on Spotify Data Analysis Data Analysis of Spotify's top 5000 albums of all time. This script, a potent analytical tool, is a potential game-changer for music industry stakeholders such as data analysts, marketing strategists, and business decision-makers. It enables users to predict music release dates and trends with precision, leveraging historical data. The script's modularity ensures it can seamlessly adapt to new datasets and analytical methods. In real-life scenarios, the script's placeholder variables and paths can be effortlessly replaced with data fields and file locations, enhancing its practical value. The script's visualisations, predictive modelling, and time series analysis are pivotal for music streaming data-driven decision-making.  ","170":" USA Bank Financial Data Data set for Tableau practice **Dataset Description:**\n\nThe `myusabank.csv` dataset contains daily financial data for a fictional bank (MyUSA Bank) over a two-year period. It includes various key financial metrics such as interest income, interest expense, average earning assets, net income, total assets, shareholder equity, operating expenses, operating income, market share, and stock price. The data is structured to simulate realistic scenarios in the banking sector, including outliers, duplicates, and missing values for educational purposes.\n\n**Potential Student Tasks:**\n\n1. **Data Cleaning and Preprocessing:**\n   - Handle missing values, duplicates, and outliers to ensure data integrity.\n   - Normalize or scale data as needed for analysis.\n\n2. **Exploratory Data Analysis (EDA):**\n   - Visualize trends and distributions of financial metrics over time.\n   - Identify correlations between different financial indicators.\n\n3. **Calculating Key Performance Indicators (KPIs):**\n   - Compute metrics such as Net Interest Margin (NIM), Return on Assets (ROA), Return on Equity (ROE), and Cost-to-Income Ratio using calculated fields.\n   - Analyze the financial health and performance of MyUSA Bank based on these KPIs.\n\n4. **Building Tableau Dashboards:**\n   - Design interactive dashboards to present insights and trends.\n   - Include summary cards, bar charts, line charts, and pie charts to visualize financial performance metrics.\n\n5. **Forecasting and Predictive Modeling:**\n   - Use historical data to forecast future financial performance.\n   - Apply regression or time series analysis to predict market share or stock price movements.\n\n6. **Business Insights and Reporting:**\n   - Interpret findings to derive actionable insights for bank management.\n   - Prepare reports or presentations summarizing key findings and recommendations.\n\n**Educational Goals:**\n\nThe dataset aims to provide hands-on experience in data preprocessing, analysis, and visualization within the context of banking and finance. It encourages students to apply data science techniques to real-world financial data, enhancing their skills in data-driven decision-making and strategic analysis. business banking","171":" fake-news-and-true-news-dataset   news","172":" US Collegiate Sports Dataset US Collegiate Sports Dataset from 2015 to 2019  ## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media) sports","173":" sentinel   news","174":" LinkedIn Posts about Layoffs (2023) Huge corpus of textual data about LinkedIn posts related to layoffs  This dataset contains around 136k posts scrapped form linkedIn from Dec 2022 to May 2023, specifically about the mass layoffs which have occurred during that time.\n\nThe data was scraped on a regular basis through the time period and many of the entries were deduplicated for better usability.  business news","175":" Fake News Prediction datasets   news","176":" celeb-a-2ammu20-a3   news","177":" Facebook's Supreme Court Meta's oversight-board & outcomes impacting decisions around content-moderation In 2020, Meta (then Facebook) created a quasi-independent, 20 member oversight board with the power to override the company's decisions around content-moderation. These are their major cases and outcomes.  people and society government law news social networks","178":" israel-hamas-full-articles-nyt  This dataset comprises articles from the New York Times, covering the israel-hamas war. It is ideal for Natural Language Processing (NLP) tasks such as text classification, sentiment analysis, and named entity recognition (NER). The dataset includes the following features:\n\ncomprises all articles related to israel hamas war published by the nytimes during the year 2024 until 26\/june.\n\nheadline: The headline of the article.\ntext: The full text of the article. demographics international relations nlp text news middle east","179":" News headlines dataset with company names News headlines, company names, top headlines date wise  Dataset Overview\nThis dataset comprises a collection of 324 news headlines from various leading companies, structured over several months in 2024. Each entry in the dataset is associated with a specific company and date, along with the primary news headline of that day and the headline from the previous day.\n\nContent\nCompany Name: Indicates the company to which the news is related.\nDate: The publication date of the news.\nNews Headlines: The headline of the news corresponding to the company and date.\nPrevious Day Headlines: News headline from the day before, for the same company.\nPotential Uses\nThis dataset is ideal for temporal text analysis, news trend monitoring, and sentiment analysis. Researchers and data scientists can explore changes in news coverage over time, analyze the impact of news on stock prices, or use the dataset for training machine learning models for text classification or summarization.\n\nAcknowledgements\nThis data was compiled and curated with the intent to provide a rich source for news headline analysis related to major companies. It is presented here for educational and informational purposes.\n\nInspiration\nSome questions and tasks this dataset could inspire include:\n\nHow do headlines about a company change over time?\nCan the impact of news on a company's public perception be quantified?\nDeveloping models that predict the sentiment conveyed in the headlines.\nComparing news coverage between companies within the same time frame. news","180":" NIH Chest X-rays Bbox version 880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset # **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov). computer vision multiclass classification health conditions image classification","181":"  Youtube Channel tmrospacenews  Youtube Channel tmrospacenews Description for the tmrospacenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024. advanced video python english","182":"  Youtube Channel truecrimenews  Youtube Channel truecrimenews Description for the truecrimenews , youtube channel videos. include title, description, view_count,  publish_date, length, author. This CSV dataset contains detailed information about videos YouTube channel, last update on June 24, 2024. advanced video python english","183":" celeb-df11parts   news","184":" The Russia-Ukraine War Images on Spanish News Annotated News Images from Spanish Broadcasts Covering the Russia-Ukraine War This dataset contains over 10,000 manually annotated images extracted from nightly news broadcasts of three Spanish television channels: Antena 3 (a3), La Sexta (la6), and Telecinco (t5). The images span a period from December 2022 to April 2024.\n\nEach image has been labelled into one of three categories: Military, Physical Damage from the War, and Not War. These two positive classes serve as proxies for \u201cwar images\u201d, covering the vast majority of war images shown in news broadcasts. Although this categorisation does not include all aspects of war (e.g., injured civilians), we believe it strikes a balance between a feasible classification task and an appropriate representation of war-related images. Moreover, images were labelled based on their content alone, without additional context. For an image to be classified as one of the positive classes, it had to clearly depict war-related content without relying on the context that all of these images may be related to war in some capacity.\n\nThis dataset serves as a valuable resource for exploring media coverage of the Russia-Ukraine war, providing a structured and annotated collection that can be used for training and evaluating machine learning models in image classification and analysis.\n\nKey features:\n\n- Total Images: 10,000+\n- Categories: Military, Physical Damage from the War, Not War\n- Channels: Antena 3 (a3), La Sexta (la6), Telecinco (t5)\n- Period: December 2022 - April 2024\n\nClass distribution:\n- Not War - 8,179 images\n- Military - 1,298 images\n- Physical Damage from the War - 862 images\n research classification image","185":" Unified Dataset for Fake, Spam and Legit data A Diverse Text Dataset for Classifying Fake, Spam, and Legit Information # Information Classification Dataset\n\n## Description\n\nThis dataset is designed for classifying information into three categories: fake, spam, and legit. It combines data from multiple sources to provide a diverse and comprehensive collection.\n\n## Source Attribution\n\n1. **YouTube Spam and Ham Data**:\n   - Source: [YouTube Spam Collection](https:\/\/doi.org\/10.24432\/C58885)\n   - Description: Contains spam and legitimate comments from YouTube videos.\n\n2. **Email Spam Dataset**:\n   - Source: [Email Spam Collection](https:\/\/www.kaggle.com\/datasets\/venky73\/spam-mails-dataset)\n   - Description: Collection of spam and legitimate emails.\n\n3. **SMS Spam Dataset**:\n   - Source: [SMS Spam Collection](https:\/\/www.kaggle.com\/datasets\/uciml\/sms-spam-collection-dataset)\n   - Description: Dataset of spam and legitimate SMS messages.\n\n4. **WELFake Dataset**:\n   - Source: [WELFake Fake News Dataset](https:\/\/www.kaggle.com\/datasets\/vcclab\/welfake-dataset)\n   - Description: Contains fake and legitimate news articles.\n\n5. **GossipCop Dataset**:\n   - Source: [GossipCop Fake News Dataset](Retrieved from https:\/\/github.com\/KaiDMML\/FakeNewsNet )\n   - Description: Dataset of fake and legitimate news articles from GossipCop.\n\nThe dataset was carefully cleaned and processed to ensure quality and consistency. Duplicate entries were removed.\n\n## Files\n\n- `data.csv`: The main dataset file.\n- `README.md`: Detailed description of the dataset creation process and source attribution.\n categorical tabular text","186":" NewsQA   ","187":" Classified Arabic News   news","188":" ag_news_dataset   ","189":" CNN-Transformer   news","190":" armenian_news   ","191":" Derana News Sinhala Ada Derana News Articles Dataset (2008-2024) over 200,000 articles # Sinhala Ada Derana News Articles Dataset (2008-2024)\n\nThis dataset is a comprehensive collection of news articles from [Sinhala Ada Derana](https:\/\/sinhala.adaderana.lk\/), spanning from 2008 to 2024. It includes **over 200,000** articles, offering a rich resource for a variety of applications in both machine learning and non-machine learning domains.\n\n## Dataset Features:\n- **ID:** Unique identifier for each news article from the website.\n- **Title:** The title of the news article.\n- **Description:** The full text of the news article.\n- **Date:** The publication date of the news article.\n- **URL:** The URL of the image associated with the news article.\n\n## Potential Uses:\n1. **Natural Language Processing (NLP):**\n   - Sentiment analysis\n   - Topic modeling\n   - Named entity recognition\n   - Language translation\n   - Text summarization\n\n2. **Machine Learning:**\n   - Embedding generation\n   - News classification\n   - Fake news detection\n\n3. **Non-Machine Learning Applications:**\n   - Historical analysis\n   - Media studies\n   - Sociopolitical research\n\n## Additional Information:\n- **Historical Context:** This dataset provides a detailed view of the historical news landscape in Sri Lanka, capturing the socio-political and cultural narratives over a significant period.\n- **Data Format:** The dataset is structured in a user-friendly format, making it easy to parse and analyze.\n\nThis dataset serves as a valuable resource for researchers, data scientists, and anyone interested in the evolution of news media in Sri Lanka. It provides insights into the past events, public sentiment, and media trends in the country, offering a broad range of possibilities for analysis and application.\n\nFeel free to explore, analyze, and build upon this dataset to uncover interesting patterns and insights.\n nlp text news sinhalese","192":" Fake_News_Detection   ","193":" SocialNet-Weibo-Version-2 A publicly available dataset based on the microblogging (Weibo)  social platform # Description\n\n**Weibo V2**: Includes **11,329** number of news from the Chinese microblogging social media platform. There are **5,661 fake news** items and **5,668 real news** items. Comparable to version 1 (V1), version 2 (V2) expands the data magnitude on the basis of V1. Meanwhile, V2 provides news **multi-modal data**, including **news posts**, **comment collections**, **images**, **videos** and **voice information**. As a result, V2 provides a better simulation of the real environment of social networks, thus supporting downstream tasks. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n``` music","194":" SocialNet-Weibo-Version-1 A public dataset of fake news identification for the microblogging social platfo # Description\n\n**Weibo V1**: Includes data on 2,067 news items from the microblogging platform in the half of 2023. There are **1,000 fake news** and **1,067 real news**. The dataset consists of comment data on news spreads and contains **user** and **comment information**. \n\n**Future work**: for the next version of the dataset check out https:\/\/github.com\/yzhouli\/SocialNet.\n\n# Please Cite \n\n**Source Paper**: Yang Z, Pang Y, Li Q, et al. A model for early rumor detection base on topic-derived domain compensation and multi-user association[J]. Expert Systems with Applications, 2024: 123951. [Online]. Available: https:\/\/doi.org\/10.1016\/j.eswa.2024.123951.\n\n```\n@article{yang2024model,\n  title={A model for early rumor detection base on topic-derived domain compensation and multi-user association},\n  author={Yang, Zhou and Pang, Yucai and Li, Qian and Wei, Shihong and Wang, Rong and Xiao, Yunpeng},\n  journal={Expert Systems with Applications},\n  pages={123951},\n  year={2024},\n  publisher={Elsevier}\n}\n``` music artificial intelligence computer science data analytics classification chinese","195":" fake_news _Detection   ","196":" Fake News Detection Data Tabular summary about each news article The internet and social media have led to a major problem\u2014fake news. Fake news is false information presented as real news, often with the goal of tricking or influencing people. It's difficult to identify fake news because it can look very similar to real news.\nThe Fake News detection dataset deals with the problem indirectly by using tabular summary statistics about each news article to attempt to predict whether the article is real or fake. This dataset is in a tabular format and contains features such as word count, sentence length, unique words, average word length, and a label indicating whether the article is fake or real. categorical tabular news","197":" Fake news detection dataset   ","198":" Scam Detection | Fake News Labelled  Dataset   ","199":" fake-news-detection11   news","200":" fake-and-real-news-dataset nlp fake news detection dataset # **Fake News detection dataset**\n## \nDataset separated in two files:\n1. Fake.csv (23502 fake news article)\n2. True.csv (21417 true news article)\n\nDataset columns:\n1. Title: title of news article\n2. Text: body text of news article\n3. Subject: subject of news article\n4. Date: publish date of news article nlp text news text classification english","201":" News Detection (Fake or Real) Dataset The Battle Against Misinformation: A Text Classification Dataset The Fake News Detection Dataset is created to assist researchers, data scientists, and machine learning enthusiasts in tackling the challenge of distinguishing between genuine and false information in today's digital landscape inundated with social media and online channels. With thousands of news items labeled as either \"Fake\" or \"Real,\" this dataset provides a robust foundation for training and testing machine learning models aimed at automatically detecting deceptive content.\n\nEach entry in the dataset contains the full text of a news article alongside its corresponding label, facilitating the development of supervised learning projects. The inclusion of various types of content within the news articles, ranging from factual reporting to potentially misleading information or falsehoods, offers a comprehensive resource for algorithmic training.\n\nThe dataset's structure, with a clear binary classification of news articles as either \"Fake\" or \"Real,\" enables the exploration of diverse machine learning approaches, from traditional methods to cutting-edge deep learning techniques.\n\nBy offering an accessible and practical dataset, the Fake News Detection Dataset aims to stimulate innovation in the ongoing battle against online misinformation. It serves as a catalyst for research and development within the realms of text analysis, natural language processing, and machine learning communities. Whether it's refining feature engineering, experimenting with state-of-the-art transformer models, or creating educational tools to enhance understanding of fake news, this dataset serves as an invaluable starting point for a wide range of impactful projects. earth and nature beginner intermediate classification text news","202":" Fake News .csv fake news as tweets in tweeter The research on fake news and machine learning focuses on using advanced techniques to detect and classify fake news from real news. Various studies have shown the effectiveness of machine learning models, such as Naive Bayes Classifier, in distinguishing between fake and true news based on language patterns and content analysis\n\nEfforts have been made to automate the detection of fake news due to the limitations of human ability in consistently identifying fake news. Machine learning and natural language processing techniques have been employed to create tools that can analyze language patterns and classify news articles as real or fake, demonstrating the potential of machine learning in this task\n1\n5\n. While some deep learning models have been explored, they may require more data to predict accurately for fake news detection earth and nature","203":" fake_news_detection_with_nlp_and_lstm   ","204":" Fake news detection   news","205":" Fake News Detection The Battle Against Misinformation: A Text Classification Dataset Overview\nIn an era where information spreads rapidly through social media and other digital channels, distinguishing between real and fake news has become increasingly challenging. The Fake News Detection Dataset aims to provide researchers, data scientists, and machine learning enthusiasts with a robust dataset for training models to automatically detect fake news with high accuracy.\n\nDataset Description\nThis dataset comprises thousands of news items labeled as \"Fake\" or \"Real\", providing a rich foundation for developing and testing machine learning models capable of identifying deceptive information. Each entry in the dataset consists of a news text and a corresponding label, offering a straightforward yet powerful resource for supervised learning projects.\n\nData Fields\nText: The full text of the news article. This field includes the body of the article, representing a mix of factual reporting, opinions, and potentially misleading information or falsehoods.\nLabel: A categorical label indicating whether the news article is \"Fake\" or \"Real\". This binary classification makes it suitable for a wide range of machine learning approaches, from traditional models to advanced deep learning techniques.\n\nInspiration\nThe Fake News Detection Dataset is designed to inspire the development of innovative algorithms that can contribute to the fight against misinformation online. By providing a readily accessible and useable dataset, we aim to foster research and development in text analysis, natural language processing, and machine learning communities. Whether you're exploring feature engineering, experimenting with the latest transformer models, or developing educational tools to help understand fake news, this dataset offers a starting point for a myriad of impactful projects. news","206":" Fake News Detection   news","207":" Fake News detection   news","208":" Welfake dataset for fake news  A larger and more generic Word Embedding over Linguistic Features for Fake News Detection (WELFake) dataset of 72,134 news articles with 35,028 real and 37,106 fake news. Merged four popular news datasets (i.e. Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent over-fitting of classifiers and to provide more text data for better ML training.\n\nThe dataset contains four columns: Serial number (starting from 0); Title (about the text news heading); Text (about the news content); and Label (0 = fake and 1 = real).\n\nThere are 78098 data entries in csv file out of which only 72134 entries are accessed as per the data frame. \n\n[Here is the demo project on this dataset!] (https:\/\/youtu.be\/IGOCtD4HFFQ) ","209":" Fake news master   news","210":" Fake News detection   news","211":" Retail_Stores_Sales_Dataset   ","212":" Sales dataset   ","213":" Dataset Dashboard   ","214":" Pizza Sales - Excel Mastersheet   ","215":" Pharma Data Analysis  The dataset is a comprehensive sales record from Gottlieb-Cruickshank, detailing various transactions that took place in Poland in January 2018. The data includes information on customers, products, and sales teams, with a focus on the pharmaceutical industry. Below is a detailed description of the dataset:\n\n### Dataset Description\n\n**Columns:**\n\n1. **Distributor:** The name of the distributing company, which is consistent across all records as \"Gottlieb-Cruickshank.\"\n2. **Customer Name:** The name of the customer or the purchasing entity.\n3. **City:** The city in Poland where the customer is located.\n4. **Country:** The country of the transaction, consistently listed as \"Poland.\"\n5. **Latitude:** The latitude coordinate of the customer's city.\n6. **Longitude:** The longitude coordinate of the customer's city.\n7. **Channel:** The distribution channel, either \"Hospital\" or \"Pharmacy.\"\n8. **Sub-channel:** Specifies whether the sub-channel is \"Private,\" \"Retail,\" or \"Institution.\"\n9. **Product Name:** The name of the pharmaceutical product sold.\n10. **Product Class:** The classification of the product, such as \"Mood Stabilizers,\" \"Antibiotics,\" or \"Analgesics.\"\n11. **Quantity:** The number of units sold.\n12. **Price:** The price per unit of the product.\n13. **Sales:** The total sales amount, calculated as Quantity * Price.\n14. **Month:** The month of the transaction, which is \"January\" for all records.\n15. **Year:** The year of the transaction, which is \"2018\" for all records.\n16. **Name of Sales Rep:** The name of the sales representative handling the transaction.\n17. **Manager:** The manager overseeing the sales representative.\n18. **Sales Team:** The sales team to which the sales representative belongs, such as \"Delta,\" \"Bravo,\" or \"Alfa.\"\n\n### Example Rows:\n\n1. **Row 1:**\n   - **Customer Name:** Zieme, Doyle and Kunze\n   - **City:** Lublin\n   - **Product Name:** Topipizole\n   - **Product Class:** Mood Stabilizers\n   - **Quantity:** 4\n   - **Price:** 368\n   - **Sales:** 1472\n   - **Sales Rep:** Mary Gerrard\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\n2. **Row 2:**\n   - **Customer Name:** Feest PLC\n   - **City:** \u015awiecie\n   - **Product Name:** Choriotrisin\n   - **Product Class:** Antibiotics\n   - **Quantity:** 7\n   - **Price:** 591\n   - **Sales:** 4137\n   - **Sales Rep:** Jessica Smith\n   - **Manager:** Britanny Bold\n   - **Sales Team:** Delta\n\nThis dataset can be utilized for various analyses, including sales performance by city, product, and sales teams, as well as geographical distribution of sales within Poland. It provides valuable insights into the pharmaceutical sales strategies and their execution within a specific time frame. business","216":" Indonesian Spices Dataset A dataset consisting of classifications of Indonesian spice images This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes. deep learning image food image classification","217":" Padang Cuisine (Indonesian Food Image Dataset) Masakan Padang (Dataset Gambar Makanan Indonesia) # Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613 categorical classification cnn image cooking and recipes food","218":" Indonesian Food Recipes 14000 recipes of chicken, lamb, beef, egg, tofu, tempe, and fish ### Context  \n\nIndonesian foods are well-known for their rich taste. There are many spices used even for daily foods. This dataset may give insight on how to prepare Indonesian food, in many ways. \n\n### Content  \n\nThis dataset contains 14000 recipes divided in 7 categories:  \n- `dataset-ayam.csv` (chicken recipes)  \n- `dataset-kambing.csv` (lamb recipes)   \n- `dataset-sapi.csv` (beef recipes)   \n- `dataset-telur.csv` (egg recipes)  \n- `dataset-tahu.csv` (tofu recipes)  \n- `dataset-ikan.csv` (fish recipes)  \n- `dataset-tempe.csv` (tempe recipes)  \n\nFor each category, there are 5 columns:  \n- Title  \n- Ingredients  \n- Steps  \n- Love  \n- URL\n\n### Acknowledgements\n\nAll the data were taken from Cookpad on 23 Feb 2018.\n\n\n### Inspiration\n\nDare to find out what is the unique recipe? The most strange? Or the common way to cook particular ingredients.   \nCan you create your own recipe based on this dataset? nutrition text cooking and recipes food","219":" Indonesian Spices Dataset A dataset consisting of classifications of Indonesian spice images This dataset consists of 31 folders, each containing image files of different spices commonly used in Indonesian cuisine. Each folder is dedicated to a specific type of spice, ensuring a well-organized structure for easy access and analysis. The spices included range from common ones like garlic and shallots to more unique ingredients such as kaffir lime leaves and basil leaves. This dataset is ideal for projects related to image classification, culinary applications, and educational purposes. deep learning image food image classification","220":" Indian Food Dataset  Spice Tales: Exploring the Flavors of Indian Cuisine - An Indian Food Dataset Known for its aromatic spices, bold flavors, and regional specialties, Indian food is a delightful journey for the taste buds. From the tandoori delights of the North to the coconut-infused dishes of the South, each region offers a unique culinary experience. Indian cuisine not only caters to various palates but also holds a significant place in Indian traditions and celebrations.\nThis Dataset is inspired by the Dataset \"Indian Food 101\" By NEHA PRABHAVALKAR.\nIn addition to that, it also contains the image URLs for all the dishes. cooking and recipes food","221":" Arabic Food 101 Data about 16 traditional and famous dishes in Jordan # **Context**\nThere's a story behind every dataset and here's your opportunity to share yours.\n\n# **Content**\nJordan cuisine consists of a variety of regional and traditional to the Arabic subcontinent. Given the diversity in soil, climate, culture, ethnic groups, and occupations, these cuisines vary substantially and use locally available spices, herbs, vegetables, and fruits.\n\nThis dataset consists of information about various Jordan dishes, their ingredients, their place of origin, etc.\n\n# **Challenge**\nThe first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions\/image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.\n\n#**About**\nFor more information: http:\/\/arartawil.com\/\n earth and nature business image cooking and recipes food","222":" Padang Cuisine (Indonesian Food Image Dataset) Masakan Padang (Dataset Gambar Makanan Indonesia) # Introduction to Padang Cuisine Image Dataset\nPadang cuisine is food that comes from the Minangkabau area, West Sumatra, Indonesian. So far, Padang cuisine still has loyal fans in West Sumatra and throughout Indonesia. Padang cuisine is cooked using a lot of herbs and spices, which makes this dish much loved by everyone. Not only that but internationally, Padang cuisine is famous for its rendang which has been named the most delicious food in the world. Rendang was ranked first as the most delicious food in the world version of CNN International's 50 Most Delicious Foods version. For eight times in a row, rendang continued to stay in first place from 2011 to 2019.\n\nThis dataset contains images of the most popular Padang cuisine, to be more specific:\n1. Beef rendang\n2. Chicken Pop\n3. Fried Chicken\n4. Dendeng Batokok\n5. Fish Curry\n6. Tambusu Curry\n7. Tunjang Curry\n8. Balado Egg\n9. Padang Omelette\n\n\n\n# Image Scraping Method\nThis image dataset is collected using a library from python called bing image downloader.\nReference: https:\/\/www.youtube.com\/watch?v=eMdW4pI7gnk&t=6s\n\n# Citation\nFaldo Fajri Afrinanto. (2022). <i>Padang Cuisine (Indonesian Food Image Dataset)<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DSV\/4053613 categorical classification cnn image cooking and recipes food","223":" Peruvian Food Reviews Analyze the sentiment of one of the best cuisine Pd: The banner image was obtained [here](https:\/\/radioambulante.org). Credits to them.\n\n### Context \nPeru is one of best culinary destination in the world. This country has diverse climates and ecological floors, where various crops have been developed. In this way, it has a lot of natural and unique inputs. So, peruvian food is a cuisine of opposites: hot and cold on the same plate. Acidic tastes melding with the starchy. Robust and delicate at the same time. This balance occurs because traditional Peruvian food relies on spices and bold flavors, ranging from the crisp and clean to the heavy and deep. Each flavor counters or tames the other. While many people see Peru as a land of cloud-topped mountains and ruins of ancient civilizations, Peru\u2019s true treasure is its rich culinary heritage. Ingredients and cooking techniques from Africa, Europe, and East Asia come together in a delightful melange that is utterly unique the world over. But what kind of food do Peruvians eat? And, **what restaurants should you visit?** \ud83d\ude0a \n\n![gg](https:\/\/th.bing.com\/th\/id\/R.2ba072e671a2fdb017da8c135fc2bc08?rik=g8HU8568d8RHog&riu=http%3a%2f%2f2.bp.blogspot.com%2f-KhLPAIsagTQ%2fVie_1VlvTUI%2fAAAAAAAAACc%2fzBjaekoIkUM%2fs1600-r%2f2.jpg&ehk=stHuhD5xQj5P5tx3MNqlLctSB72pLLTMm2A7Wi%2b6Msk%3d&risl=&pid=ImgRaw)\n\n### Content\nThis kaggle dataset contains information scraped from GooglePlaces and Tripadvisor using Selenium, Requests, BeautifulSoup and Rvest. More info about the used web-scraping in this [github repository](https:\/\/github.com\/Lazaro-97\/satisfaction-in-restaurants)\nThe content here have a lot (at the moment not all) of restaurants reviews in Lima, Peru between 2010 and 2021. In total exist more of 8791 restaurants and more of 1160666 reviews. With a total of 20 features with a high diversity: geospatial, text, date ,categoric and numeric feafures!\n\nThis has two general sections. The first is the *Restaurants*. This contains general and geospatial information.  The second is the *Reviews*. This contains the interaction between user and restaurant, with this way is possible to see the satisfaction of the client with a service.\nExist a possible third section: the *Users*. This information maybe will be added in two months.\n\nAbout the collection methodology, this is explained below:\n\n-**The sample:** The scraped reviews are the most recent reviews in all possible restaurants in the province of Lima. \n\n-**Set of items:** In one way, the users. In other way: the restaurants.\n\n-**Set of variables:** Exist two general tables. See the information below\n\nThe following diagram and table summarise all.\n\n**Table 1:** Restaurants\n|Variable|Description|\n| --- | ---|\n| Id | Id of the restaurant |\n|Name| Name of the restaurant |\n|Tag| The category of the restaurant |\n|x, y| Geospatial information and exact location of restaurant |\n|District|District where the restaurant is located |\n|Direction|District where the restaurant is located|\n|Stars|Mean Stars of restaurant in all time|\n|N_reviews|Number of reviews of restaurant in all time |\n|Min_Price| Minimum price in the menu of restaurant |\n|Max_Price| Maximum price in the menu of restaurant |\n|Platform| Platform where the information was downloaded |\n\n**Table 2:** Reviews\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review |\n|Id_nick| Id of the user. With this is possible to get the profile link |\n|Date| Date when the review was written |\n|Service| Id of the restaurant. Conection with Table 1 |\n|Review|Content of the review. This describe the satisfaction of the user |\n|Title|Title of the review. Only available in Tripadvisor|\n|Score|Punctuation in the review|\n|Likes|Number of votes in the publication|\n|Platform|Platform where the information was downloaded |\n\nAlso, exist auxiliar information related with the sentiment and emotion. This probabilities was obtained with a Spanish NrcLexicon, however, that results is not ok. Anyway, that is a reference and you can propose a fine tuning here. In adittion, also exist the probability to get a specific star, however, this was obtained with a simple logistic regression. Also i showed the information about Spanish NrcLexicon and Geospatial Borders. The author ands more information you can find [there](https:\/\/github.com\/jboscomendoza\/lexicos-nrc-afinn) and [there](https:\/\/www.geogpsperu.com\/2018\/02\/limite-distrital-politico-shapefile-ign.html).\n\n**Table 3:** Models\n\n|Variable|Description|\n| --- | ---|\n| Id_review | Id of the review. Conection with Table 1 |\n|Positive| Probability of review that it will  show positive sentiment|\n|Negative| Probability of review that it will show negative sentiment|\n|Anger| Probability of review that it will show anger emotion|\n|Anticipation|Probability of review that it will show anticipation emotion |\n|Disgust|Probability of review that it will show disgust emotion|\n|Fear|Probability of review that it will show fear emotion|\n|Joy|Probability of review that it will show joy emotion|\n|Sadness|Probability of review that it will show sadness emotion|\n|Surprise|Probability of review that it will show surprise emotion|\n|Stars_1|Probability of review that it will get 1 star|\n|Stars_2|Probability of review that it will get 2 stars|\n|Stars_3|Probability of review that it will get 3 stars|\n|Stars_4|Probability of review that it will get 4 stars|\n|Stars_5|Probability of review that it will get 5 stars|\n\nThe entity relationship diagram! \n\n![erd](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1251020\/2455759\/erd.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210803%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210803T000105Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=53e7abea86abbc9efd45a0a7bdc40e950ba972565fd08ecd354f04e7ccb7ac6f7cc216f6e82d147b55ea3ab32badfb95501549291dfa783968246633b73dad6fb25206696d134b8a2352f7741c3002fa427d12b0320bfaedaeed7a1f8de0658804931e99708899ef95ad44a1bbbafc612f3a2e110ef4442f623003d79d12d8ae572988e8271c038d31d436ee2406e7da07a503548f443d24f65769324034c4ab63efc081a653e86f93f98e36c95198feb925515e487319f07009446d462ee71befeab29e3ef33c7bb48c9bc950974ee34c9fea648f9c3b0d125168035b75d6d6b2de5d09749b1cfce30208c65fe90653713461aa7acd5a1002dbbb4f50ecf44e)\n\n## Usage\n**Text classification:** The main topic in this types of datasets. Vectorize the reviews and define a predictive model. Identify strong and weak points of each restaurant.\n\n**Find patterns:** Compare districts (or restaurants) along the time. What is the common words in an excellent restaurant? Why these restaurants are better? \n\n**Reduction of dimention:** Detect similarities and then, clustering the reviews.\n\n## Acknowledgements\nThanks to Kaggle and its community. In general, thanks to the learners and teachers in machine learning, deep learning, natural language processing and computer vision.\n\n## Inspiration\nNatural language processing is a great tool. One application that I'm interested is detect bullies messages in any social network. I know that exist many notebooks and papers, but I'd like to build a bot that detect **all** possible cases and surely, there exist!\n arts and entertainment nlp geospatial analysis classification restaurants","224":" Bahama Breeze Menu Nutrition Data Bahama Breeze Restaurant Menu Nutritional Dataset The Bahama Breeze Menu Nutrition Data dataset provides detailed nutritional information for various items available at Bahama Breeze, a restaurant known for its Caribbean-inspired cuisine. The dataset is organized into the following columns:\n\n- Item: The name of the food or beverage item offered on the Bahama Breeze menu.\n- Category: The type or classification of the item, such as appetizers, entrees, desserts, or beverages.\n- Calories: The total number of calories contained in one serving of the item.\n- Total_Carbs_g: The total amount of carbohydrates in grams present in one serving of the item.\n- Sodium_mg: The amount of sodium in milligrams found in one serving of the item.\n- Weight_Watchers: The Weight Watchers points value assigned to one serving of the item, useful for individuals following the Weight Watchers diet program.\n\nThis dataset can be used to analyze the nutritional content of Bahama Breeze's offerings, helping customers make informed dietary choices based on their nutritional needs and preferences. nutrition","225":" Combined Wine Dataset: Red & White Wines Comprehensive data on the chemical and quality aspects of wines This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes. alcohol exploratory data analysis data analytics classification recommender systems","226":" The LAMBADA Dataset for Word Prediction Evaluating text understanding through word prediction ## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n languages literature computer science nlp text mining deep learning neural networks","227":" SNIPS Natural Language Understanding Benchmark:  # Natural Language Understanding benchmark\n\nThis repository contains the results of three benchmarks that compare natural language understanding services offering:\n1. **built-in intents** (Apple\u2019s SiriKit, Amazon\u2019s Alexa, Microsoft\u2019s Luis,\nGoogle\u2019s API.ai, and [Snips.ai](https:\/\/snips.ai\/)) on a selection of\nvarious intents. This benchmark was performed in December 2016. Its results\nare described in length in the [following post](https:\/\/medium.com\/snips-ai\/benchmarking-natural-language-understanding-systems-d35be6ce568d).\n2. **custom intent engines** (Google's API.ai, Facebook's Wit, Microsoft's Luis, Amazon's Alexa, and Snips' NLU) for seven chosen intents. This benchmark was performed in June 2017. Its results are described in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/@alicecoucke\/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19).\n3. **extension of Braun et al., 2017** (Google's API.AI, Microsoft's Luis, IBM's Watson, Rasa)\nThis experiment replicates the analysis made by Braun et al., 2017, published in Evaluating Natural Language Understanding Services for Conversational Question Answering Systems as part of SIGDIAL 2017 proceedings. Snips and Rasa are added. Details are available in a [paper](https:\/\/arxiv.org\/abs\/1805.10190) and a [blog post](https:\/\/medium.com\/snips-ai\/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a).\n\nThe data is provided for each benchmark and more details about the methods are available in the README file in each folder.\n\n**Any publication based on these datasets must include a full citation to the following paper in which the results were published by the Snips Team:** \n\n[Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system \nfor private-by-design voice interfaces.\" 2018,](https:\/\/arxiv.org\/abs\/1805.10190)\n\naccepted for a spotlight presentation at the [Privacy in Machine Learning and Artificial Intelligence workshop](https:\/\/pimlai.github.io\/pimlai18\/#papers) colocated with ICML 2018.\n\n\n\n *The Snips team has joined Sonos in November 2019. These open datasets remain available and their access is now managed by the Sonos Voice Experience Team. Please email sve-research@sonos.com with any question.* education","228":" college_level_IntroCS_class_dataset|chinese Json format with (instruction input output) The dataset roughly include 750 question answering pairs in json format. With three name component (instruction , input and output).Data set was collected via college level intro cs class in China. computer science beginner intermediate text chinese","229":" Sigma Dolphin Filtered and Cleaned Cleaned and Filtered Version Of Sigma Dolphin ### Dataset Description for Filtered Sigma Dolphin Dataset\n\n#### Overview\nThis dataset is a cleaned and filtered version of the Sigma Dolphin dataset (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin), designed to aid in solving maths word problems using AI techniques. This was used as an effort towards taking part in the AI Mathematical Olympiad - Progress Prize 1 (https:\/\/www.kaggle.com\/competitions\/ai-mathematical-olympiad-prize\/overview). The dataset was processed using TF-IDF vectorisation and K-means clustering, specifically targeting questions relevant to the AIME (American Invitational Mathematics Examination) and AMC 12 (American Mathematics Competitions).\n\n#### Context\nThe Sigma Dolphin dataset is a project initiated by Microsoft Research Asia, aimed at building an intelligent system with natural language understanding and reasoning capacities to automatically solve maths word problems written in natural language. This project began in early 2013, and the dataset includes maths word problems from various sources, including community question-answering sites like Yahoo! Answers.\n\n#### Source and Original Dataset Details\n- **Original Dataset:** Sigma Dolphin (https:\/\/www.kaggle.com\/datasets\/saurabhshahane\/sigmadolphin)\n- **Original Source:** https:\/\/msropendata.com\/datasets\/f0e63bb3-717a-4a53-aa79-da339b0d7992\n- **Project Page:** http:\/\/research.microsoft.com\/en-us\/projects\/dolphin\/\n- **References:**\n  - Shuming Shi, et al. \"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning.\" EMNLP 2015.\n  - Danqing Huang, et al. \"How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation.\" ACL 2016.\n  - JSON: http:\/\/json.org\/\n\n#### Content\nThe filtered dataset includes problems that are relevant for preparing for maths competitions such as AIME and AMC. The data is structured to facilitate the training and evaluation of AI models aimed at solving these types of problems. \n\n#### Datasets:\nThere are several filtered versions of the dataset based on different similarity thresholds (0.3 and 0.5). These thresholds were used to determine the relevance of problems from the original Sigma Dolphin dataset to the AIME and AMC problems.\n\n1. **Number Word Problems Filtered at 0.3 Threshold:**\n   - File: `number_word_test_filtered_0.3_Threshold.csv`\n   - Description: Contains problems filtered with a similarity threshold of 0.3, ensuring moderate relevance to AIME and AMC 12 problems.\n\n2. **Number Word Problems Filtered at 0.5 Threshold:**\n   - File: `number_word_std.test_filtered_0.5_Threshold.csv`\n   - Description: Contains problems filtered with a higher similarity threshold of 0.5, ensuring higher relevance to AIME and AMC 12 problems.\n\n3. **Filtered Number Word Problems 2 at 0.3 Threshold:**\n   - File: `filtered_number_word_problems2_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.3 similarity threshold.\n\n4. **Filtered Number Word Problems 2 at 0.5 Threshold:**\n   - File: `filtered_number_word_problems_Threshold.csv`\n   - Description: Another set of problems filtered at a 0.5 similarity threshold.\n\n#### Why Different Similarity Thresholds?\nDifferent similarity thresholds (0.3 and 0.5) are used to provide flexibility in selecting problems based on their relevance to AIME and AMC problems. A lower threshold (0.3) includes a broader range of problems, ensuring a diverse set of questions, while a higher threshold (0.5) focuses on problems with stronger relevance, offering a more targeted and precise dataset. This allows users to choose the level of specificity that best fits their needs.\n\nFor a detailed explanation of the preprocessing and filtering process, please refer to the [Sigma Dolphin Filtered & Cleaned Notebook](https:\/\/www.kaggle.com\/code\/ryanmutiga\/sigma-dolphin-filtered-cleaned-notebook).\n\n#### Acknowledgements\nWe extend our gratitude to all the original authors of the Sigma Dolphin dataset and the creators of the AIME and AMC problems. This project leverages the work of numerous researchers and datasets to build a comprehensive resource for AI-based problem solving in mathematics.\n\n#### Usage\nThis dataset is intended for research and educational purposes. It can be used to train AI models for natural language processing and problem-solving tasks, specifically targeting maths word problems in competitive environments like AIME and AMC.\n\n#### Licensing\nThis dataset is shared under the Computational Use of Data Agreement v1.0.\n\n---\n\nThis description provides an extensive overview of the dataset, its sources, contents, and usage. If any specific details or additional sections are needed, please let me know! beginner tabular question answering english mathematics","230":" Financial Q&A - 10k A Comprehensive Financial Question-Answer Dataset from Company Filings This dataset, titled \"Financial-QA-10k\", contains 10,000 question-answer pairs derived from company financial reports, specifically the 10-K filings. The questions are designed to cover a wide range of topics relevant to financial analysis, company operations, and strategic insights, making it a valuable resource for researchers, data scientists, and finance professionals. Each entry includes the question, the corresponding answer, the context from which the answer is derived, the company's stock ticker, and the specific filing year. The dataset aims to facilitate the development and evaluation of natural language processing models in the financial domain.\n\nAbout the Dataset\nDataset Structure:\n\n- **Rows**: 7000\n- **Columns**: 5\n- **question**: The financial or operational question asked.\n- **answer**: The specific answer to the question.\n- **context**: The textual context extracted from the 10-K filing, providing additional information.\n- **ticker**: The stock ticker symbol of the company.\n- **filing**: The year of the 10-K filing from which the question and answer are derived.\n\n\n**Sample Data:**\n\nQuestion: What area did NVIDIA initially focus on before expanding into other markets?\nAnswer: NVIDIA initially focused on PC graphics.\nContext: Since our original focus on PC graphics, we have expanded into various markets.\nTicker: NVDA\nFiling: 2023_10K\n\nPotential Uses:\n\n**Natural Language Processing (NLP):** Develop and test NLP models for question answering, context understanding, and information retrieval.\n**Financial Analysis:** Extract and analyze specific financial and operational insights from large volumes of textual data.\n**Educational Purposes:** Serve as a training and testing resource for students and researchers in finance and data science. finance nlp question answering","231":" NLP_Neymar_ChatBot_Dataset  Embark on a captivating exploration of Neymar, the revered Brazilian football sensation, with this meticulously curated dataset. Derived from a diverse array of dialogues between users and an interactive chatbot, this dataset provides an immersive journey into Neymar's multifaceted world, sourced directly from the wealth of information available on Wikipedia (https:\/\/en.wikipedia.org\/wiki\/Neymar)  . \n\nDelve into the intricacies of Neymar's illustrious career, from his groundbreaking exploits on the football field to his profound impact beyond it. Each conversation within this dataset offers a nuanced glimpse into Neymar's professional milestones, statistical feats, and personal anecdotes, carefully selected and distilled from the extensive discourse surrounding his life and legacy.\n\nThis dataset is well-suited for Natural Language Processing (NLP) tasks and deep learning applications. With its rich collection of conversational data, it provides an ideal resource for training NLP models, exploring sentiment analysis, question answering systems, and other advanced deep learning techniques.\n\nWhether you're a passionate football aficionado, a data enthusiast, or simply curious about Neymar's remarkable journey, this dataset promises to ignite your imagination and deepen your appreciation for one of football's most iconic figures.\n\nLicense: Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)\n\n football nlp deep learning text generation text conversation","232":" CoQA QndA Dataset with ground truth Processed version of https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversation ## **Source Dataset:**\n\nThis data is a processed version of [https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa](https:\/\/www.kaggle.com\/datasets\/jeromeblanchet\/conversational-question-answering-dataset-coqa) dataset. \n\n## **Fields are mapped as below:**\n\n1. data -&gt; questions -&gt; input_text to **question**\n2. data -&gt; answers -&gt; input_text to **answer**\n3. data -&gt; answers -&gt; span_text to **ground_truth**\n4. data -&gt; story to **context**\n5. data -&gt; id to **data_id**\n6. **question_id** is generated sequentially. \n\n\n## **Processed using the notebook:**\n\n[https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth](https:\/\/www.kaggle.com\/code\/vigneshboss\/coqa-dataset-to-qnda-dataset-with-ground-truth)\n\n\n## **Dataset Logo Credit:**\n\nPhoto by <a href=\"https:\/\/unsplash.com\/@wesleyphotography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Wesley Tingey<\/a> on <a href=\"https:\/\/unsplash.com\/photos\/yellow-and-white-10-card-FIq7K_wD4jM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash<\/a>\n  \n\n data cleaning text retrieval question answering english","233":" Astronautics Multiple Choice Questions and Answers AstroMCQA is designed for comparative assessment of LLMs in Astronautics domain # AstroMCQA Dataset\n\n## Purpose and scope\n\nThe primary purpose of AstroMCQA is for application developers in the domain of space engineering to be able to comparatively assess LLM performances on the specific task of multiple-choice question-answering\n\n## Intended Usage\n\nComparative assessement of differents LLMs, Model evaluation, audit, and model selection. Assessment of different quantization levels, different prompting strategies, and assessing effectiveness of domain adaptation or domain-specific fine-tuning.\n\n## Quickstart\n\n- Explore the dataset here:  https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/viewer\/default\/train\n- Evaluate an LLM (Mistral-7b) on AstroMCQA on collab here:<a target=\"_blank\" href=\"https:\/\/colab.research.google.com\/github\/patrickfleith\/astro-llms-notebooks\/blob\/main\/Evaluate_an_HuggingFace_LLM_on_a_Domain_Specific_Benchmark_Dataset.ipynb\">\n  <img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\">\n<\/a>\n\n## What is AstroMCQA GOOD for?\n\nWhat is AstroMCQA good for?\nThe primary purpose of AstroMCQA is for application developers in the domain of space mission design and operations to be able to address some questions such as: which LLM to use and how does it perform in the different subdomains? It enables to benchmark different models, different size, quantization methods, prompt engineering strategies, effectiveness of fine-tuning on the specific task of multiple-choice question-answering in space engineering.\n\n## What is AstroMCQA NOT GOOD for?\n\nIt is not suitable for training \/ fine-tuning LLM due to the very limited size of the dataset even if it could be combined with other tasks and science dataset for meta-learning.\n\n# DATASET DESCRIPTION\n### Access\n- Manual download from Hugging face hub: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\n- Or with python:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"patrickfleith\/Astro-mcqa\")\n```\n\n### Structure\n200 expert-created Multiple Choice Questions and Answers, one question per row in a comma separated file. Each instance is made of the following field (column):\n- **question**: a string.\n- **propositions**: a list of string. Each item in the list is one choice. At least one of the propositions correctly answer the question, but there can be multiple correct propositions. Even all propositions can be correct.\n- **labels**: list of integer (0\/1). Each element in the labels list correspond to proposition at the same position within the proposition list. A label of 0 means that the proposition is incorrect. A label of 1 means that the proposition is a correct choice to answer the question.\n- **justification**: Optional string. An optional field which may provide a justification of the answer.\n- **answerable**: A boolean, whether the question is answerable or not. At the moment, AstroMCQA only includes answerable questions.\n- **uid**: A unique identifier for the MCQA instance. May be useful for traceability in further processing tasks.\n\n### Metadata\nDataset is version controlled and commits history is available here: https:\/\/huggingface.co\/datasets\/patrickfleith\/Astro-mcqa\/commits\/main\n \n### Languages\nAll instances in the dataset are in english\n \n### Size\n200 expert-created Multiple Choice Questions and Answers\n \n### Types of Questions\n- Some questions request expected generic knowledge in the field of space science and engineering.\n- Some questions require reasoning capabilities\n- Some questions require mathematical operations since a numerical result is expected (exam-style questions)\n\n### Topics Covered\nDifferent subdomains of space engineering are covered, including propulsion, operations, human spaceflight, space environment and effects, space project lifecycle, communication and link analysis, and more.\n\n# USAGE AND GUIDELINES\n#### License\nAstroMCQA \u00a9 2024 by Patrick Fleith is licensed under Creative Commons Attribution 4.0 International\n\n#### Restrictions\nNo restriction. Please provide the correct attribution following the license terms.\n \n#### Citation\nP. Fleith, AstroMCQA \u2013 Astronautics multiple choice questions and answers benchmark dataset for domain of Space Mission Engineering for LLM Evaluation, (2024). \n \n#### Update Frequency\nMay be updated based on feedbacks. If you want to become a contributor, let me know.\n \n#### Have a feedback or spot an error?\nUse the community discussion tab directly on the huggingface Astro-mcqa dataset page.\n \n#### Contact Information\nReach me here on the community tab or on LinkedIn (Patrick Fleith) with a Note.\n\n#### Current Limitations and future work\n- Only 200 multiple choice questions and answers. This makes it useless for fine-tuning purpose, although it could be integrated as part of a larger pool of datasets compiled for a larger fine-tuning.\n- While being a descent size enabling LLM evaluation, the space engineering expert time is scarce and expensive. On average it takes 8 minutes to create one MCQA example. Having more examples would be much better for robustness.\n- The dataset might be biased toward the very low number of annotators.\n- The dataset might be biased toward European Space Programs.\n- The dataset might not cover all subsystems or subdomain of astronautics although we tried to do our best covering the annotator\u2019s domains of expertise.\n- No peer-reviewing. Ideally we would like to have a Quality Control process to ensure high quality, and correctness of each example in the dataset. Given the limited resources, this is not yet possible. Feel free to come and contribute if you feel that is an issue business education science and technology","234":" EVJVQA  EVJVQA, the first multilingual Visual Question Answering dataset with three languages: English, Vietnamese, and Japanese, is released in this task. UIT-EVJVQA includes question-answer pairs created by humans on a set of images taken in Vietnam, with the answer created from the input question and the corresponding image. EVJVQA consists of 33,000+ question-answer pairs for evaluating the mQA models. ","235":" SE-PQA: Personalized Community Question Answering  Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to fill this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new resource to design and evaluate personalized models related to the two tasks of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with both questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization improves remarkably the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.\n\nMore details can be found here: https:\/\/github.com\/pkasela\/SE-PQA\n\n```\n@inproceedings{10.1145\/3589335.3651445,\nauthor = {Kasela, Pranav and Braga, Marco and Pasi, Gabriella and Perego, Raffaele},\ntitle = {SE-PQA: Personalized Community Question Answering},\nyear = {2024},\nisbn = {9798400701726},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https:\/\/doi.org\/10.1145\/3589335.3651445},\ndoi = {10.1145\/3589335.3651445},\nbooktitle = {Companion Proceedings of the ACM on Web Conference 2024},\npages = {1095\u20131098},\nnumpages = {4},\nkeywords = {personalization, question answering, user model},\nlocation = {},\nseries = {WWW '24}\n}\n``` internet","236":" GSM8K - Grade School Math 8K dataset for LLM GSM8K use for test and evaluate LLM  Math solve performance Dataset Summary\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n\nThese problems take between 2 and 8 steps to solve.\nSolutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ \u2212 \u00d7\u00f7) to reach the final answer.\nA bright middle school student should be able to solve every problem: from the paper, \"Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable.\"\nSolutions are provided in natural language, as opposed to pure math expressions. From the paper: \"We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models\u2019 internal monologues\" education","237":" Arabic Instruct chatbot dataset Fine tuning LLMs on Arabic language for better performance ## Discerption :\nThis dataset is a .parquet file prepared on instruction Question Answer dataset in Arabic language on multiple different categories.\n\n## Usage :\nbest used for instruct fine-tuning large language models to perform better on Arabic language.\n\n## License and Usage :  \nThis dataset is a Kaggle version of generated Arabic dataset is created by Yasbok from hugging face. due to issues extracting it directly from \"HF\" using the `Dataset` library, i downloaded it and uploaded it here on kaggle so it can be used easily.\n[Hugging face dataset link](https:\/\/huggingface.co\/datasets\/Yasbok\/Alpaca_arabic_instruct)\n computer science nlp text-to-text generation question answering arabic","238":" A Case Study: Bike Share Divvy tripdata on bike 2023   business data analytics tabular tidyverse question answering","239":" A New Benchmark for Financial Question Answering Financial Question Answering ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8442057%2Ff7c62f9d14b5620b8452f39690a24568%2Ffig1.png?generation=1715577026848120&alt=media)\n\nAbstract: FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). This repository contains an open source sample of 150 annotated examples used in the evaluation and analysis of models assessed in the FinanceBench paper. FinanceBench comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.\n\n**Contact:** To evaluate your models on the full FinanceBench dataset, or if you have questions about this work, you can email us at contact@patronus.ai\n\n**Dataset Overview:** The provided open-source dataset (n=150) consists of the following attributes:\n\n```python\n- financebench_id:  unique question identifier  \n    - question:         question of interest\n    - answer:           gold answer\n    - question_type:    type of the question {domain-relevant, metrics-generated, novel-generated}\n    - doc_name:         name of the relevant financial document to answer the question\n    - doc_link:         url to retrieve the relevant financial document\n    - doc_period:       period of the relevant financial document\n    - evidence_text:    extracted evidence text\n    - page_number       page number(s) of evidence text\n```\n\n**Citation:** If you use our open-source dataset or refer to our result, please use the following citation:\n\n```python\n@misc{islam2023financebench,\n      title={FinanceBench: A New Benchmark for Financial Question Answering}, \n      author={Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},\n      year={2023},\n      eprint={2311.11944},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n finance computer science","240":" Visual_Question_Answering-CV-NLP  VQA, or Visual Question Answering, is a multifaceted challenge that tasks models with providing coherent responses to natural language questions posed about images. This intricate task demands a deep understanding of both the visual content and the semantic nuances embedded within the questions. By bridging the realms of Computer Vision and Natural Language Processing, VQA addresses a plethora of sub-problems, including object detection, scene classification, and more, rendering it a quintessential benchmark in the realm of AI jobs and career","241":" GenAssocBias Bias Detection Dataset **This work has been accepted to ACL 2024.** You can find the pre-print related to this work https:\/\/arxiv.org\/abs\/2309.08902\n\n**Dataset Summary:**\n\n**GenAssocBias** is a dataset that measures **stereotype bias in LLMs**. GenAssocBias consists of 11,940 sentences that measure model preferences across **ageism, beauty, beauty_profession, nationality, and institutional bias.**\n\nSupported Tasks and Leaderboards\n\nmultiple-choice question answering\n\nLanguages\n\nEnglish (en)\n\nDataset Descriptions:\n\nThere are 8 columns in our dataset. The description of each column is given below:\n\nbias_type: This column indicates different types of biases including ageism, beauty, beauty_profession, nationality, and institutional.\n\ntarget_gender: This column indicates the particular gender type. There are three unique gender types namely 'male', 'female', and 'not_specified'.\n\ncontext: This column indicates different sentences. These are the context sentences.\n\nitem_category: This column is either 'positive' or 'negative'. When the attribute or stimulus in the context sentence is positive, we named it as 'positive' and when the attribute or stimulus is negative, then we named it as 'negative'.\n\ntype_category: This column tells us, which direction the data is. There are two different types of direction, namely SAI and ASA.\n\nanti_stereotype: When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is positive among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is negative among the options.\n\nstereotype: This column is the opposite of the 'anti_stereotype' column. When the 'item_category' column is 'negative', then this column contains attribute\/stimulus that is negative among the options according to our definition. On the other hand, when the 'item_category' column is 'positive', then this column contains attribute\/stimulus that is positive.\n\nunrelated: This column contains the neutral attributes or stimuli.\n\nCitation Information:\n\n@article{kamruzzaman2023investigating, title={Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models}, author={Kamruzzaman, Mahammed and Shovon, Md Minul Islam and Kim, Gene Louis}, journal={arXiv preprint arXiv:2309.08902}, year={2023} } artificial intelligence nlp text english","242":" Boardgames_Rulebooks Pdfs for different rulebooks of boardgames PDFs of different boardgames rulebooks\n- Root\n- Lords of Waterdeep\n- Troyes board games image text retrieval question answering retrieval\/ranking","243":" question-answering-dataset   ","244":" blooms-taxonomy-dataset   education tabular question answering","245":" VizWiz_QA VizWiz Question Answering dataset which answer type is \"other\".  ","246":" Red Wine Quality   alcohol","247":" wine quality   alcohol","248":" Combined Wine Dataset: Red & White Wines Comprehensive data on the chemical and quality aspects of wines This dataset contains comprehensive data on red and white wines, sourced from various vineyards. It includes chemical properties and quality ratings to provide a detailed insight into the characteristics that determine wine quality.\n<b>Original-source:<\/b> <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167923609001377?via%3Dihub\"> Modeling wine preferences by data mining from physicochemical properties<\/a>\n<b> PDF link of the paper:<\/b> <a href=\"https:\/\/d1wqtxts1xzle7.cloudfront.net\/31035672\/wine5-libre.pdf?1392220671=&response-content-disposition=inline%3B+filename%3DModeling_wine_preferences_by_data_mining.pdf&Expires=1719069704&Signature=O-dGluHcV7Ot~~P9gqa1tgs0Jc0EWE7zTBFwcyQFwFnJIv0sDUU6USBbrzKfdfjsXx9sDFeRnq4n2rUUVmcpYTkP7JP~oOWULfokaaEzCHwbHILxHhVg7Pmz3aGidhuXzfie9mabHP9OH6PdKpiBs6Li49MA~B4BCJPAcJ02miDEZO8zzurZ79HMiM98WeWXpEMNaa~bUNGAIQ9SS5DO3GP5YW-UMlnpVtGyrwguO37jzAz-X5cG7Vj2wLKioMdAB1ZVa~oHFfjOgtQl1QJsVSEru2f4XW1YawhuHWwRfTmYQW-meUMzM~EfO84YHpBwIiw4om9Hgt0yRg~2MW7HaQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">wine5-libre.pdf<\/a>\n<b>Created by:<\/b> Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n<b>Context<\/b>\nThe dataset was collected from multiple wine producers to create a broad and representative sample of red and white wines. The chemical properties include acidity, sugar levels, pH, sulfates, and alcohol content, while expert wine tasters gave the quality ratings.\n<b>Inspiration<\/b>\nThis dataset was inspired by the need to better understand the factors influencing wine quality. By analyzing these properties, one can predict wine quality and type, providing valuable insights for wine producers, retailers, and enthusiasts.\n<b>Usage<\/b>\nThis dataset can be used to build predictive models to classify wine type (red or white) and predict the quality rating of a wine based on its chemical properties. It is ideal for machine learning projects, data analysis, and educational purposes. alcohol exploratory data analysis data analytics classification recommender systems","249":" Modeling wine We propose a data mining approach to predict human wine. this graph was created in Loocker studio,PowerBi and Tableau:\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2Ff35dd0bdb2f87ea2badf1ea1d57086bc%2Fgraph1.jpg?generation=1718398785729815&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F52689afbe7380af61e578767366ac925%2Fgraph2.jpg?generation=1718398791128270&alt=media)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F16731800%2F992bd2cc851499b2f8f0bcb34732fab7%2Fgraph3.png?generation=1718398796130065&alt=media)\n\nAbstract\nWe propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.\n\nIntroduction\nOnce viewed as a luxury good, nowadays wine is increasingly enjoyed by a wider range of consumers. Portugal is a top ten wine exporting country, with 3.17% of the market share in 2005 [11]. Exports of its vinho verde wine (from the northwest region) have increased by 36% from 1997 to 2007 [8]. To support its growth, the wine industry is investing in new technologies for both wine making and selling processes. Wine certification and quality assessment are key elements within this context. Certification prevents the illegal adulteration of wines (to safeguard human health) and assures quality for the wine market. Quality evaluation is often part of the certification process and can be used to improve wine making (by identifying the most influential factors) and to stratify wines such as premium brands (useful for setting prices). healthcare business data cleaning data analytics health conditions","250":" Red Wine Quality Evaluating factors influencing the quality and characteristics of red wine  alcohol","251":" Red wine quality classifier using a Neural network   biology","252":" Wine Quality Dataset This dataset is public available for research.   Available at: [@Elsevier] http:\/\/dx.doi.org\/10.1016\/j.dss.2009.05.016\n                [Pre-press (pdf)] http:\/\/www3.dsi.uminho.pt\/pcortez\/winequality09.pdf\n                [bib] http:\/\/www3.dsi.uminho.pt\/pcortez\/dss09.bib\n\n1. Title: Wine Quality \n\n2. Sources\n   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n   \n3. Past Usage:\n\n  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n  Modeling wine preferences by data mining from physicochemical properties.\n  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n\n  In the above reference, two datasets were created, using red and white wine samples.\n  The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n  between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n  these datasets under a regression approach. The support vector machine model achieved the\n  best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n  etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n  analysis procedure).\n \n4. Relevant Information:\n\n   The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n   For more details, consult: http:\/\/www.vinhoverde.pt\/en\/ or the reference [Cortez et al., 2009].\n   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\n   These datasets can be viewed as classification or regression tasks.\n   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n   or poor wines. Also, we are not sure if all input variables are relevant. So\n   it could be interesting to test feature selection methods.  alcohol","253":" Red-wine quality   alcohol","254":" Dataset   ","255":" Dataset: LQR House Inc. (LQR) Stock Performance Stock Performance Dataset This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting. business","256":" Dataset: Full House Resorts, Inc. (FLL) Stock P... Stock Performance Dataset This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting. business hotels and accommodations","257":" House Prices India Detailed Analysis of House Prices in Urban India: Trends and Insights. This dataset contains detailed information on residential property prices in India. It includes features such as the number of bedrooms and bathrooms, living and lot area, number of floors, waterfront presence, house condition and grade, construction and renovation years, and geographic coordinates. Additional features include the number of nearby schools, distance from the airport, and property price. This dataset is ideal for real estate market analysis and predictive modeling, providing insights into factors influencing property prices across different regions in India. india housing intermediate data visualization english","258":" Bengaluru House Dataset Bengaluru House Prediction The Bengaluru House Prediction project is a machine learning-based solution designed to help potential homebuyers in Bangalore predict home prices accurately. It addresses a common problem faced by homebuyers who struggle to make informed decisions due to the complexity of factors that influence property prices.\n\nOverall, the Bengaluru House Prediction project is a valuable tool that empowers users with the right information to make informed decisions and demonstrates the potential of machine learning in the real estate industry. real estate","259":" Transformed Housing Data 2 Data regarding Sale Prices of several houses and their respective features This house costing data is transformed for data modelling and is the updated version of my previous dataset 'Transformed Housing Data'. The updation is the creation of dummy variables of certain categorical features for modelling purpose. There are a total of 31 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment. housing data visualization statistical analysis data analytics tabular","260":" Transformed Housing Data Data regarding Sale Prices of several houses and their respective features This house costing data is the transformed version of my previous dataset titled 'Raw Housing Cost Data (imputed)'; it's imputed and the variables in it are transformed (for modelling). Same as the earlier data, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment. housing data analytics tabular data storytelling","261":" Raw Housing Cost Data (imputed) Data regarding Sale Prices of several houses and their respective features This house costing data has 21609 rows of information about different Houses. It's the imputed version of my previous dataset titled Raw Housing Cost Data. Same as earlier, there are a total of 21 columns, out of which Sale Price can be supposedly taken as a dependent variable. The other variables are different features, locations and date, etc. regarding the houses. Several EDA or regression modelling can be performed in this data after a little polishing and treatment. housing exploratory data analysis data cleaning data analytics tabular","262":" Dataset: Tree House Education & Accessories Lim... Dr. Jagadish Tawade & Nitiraj Kulkarni This dataset provides historical stock market performance data for specific companies. It enables users to analyze and understand the past trends and fluctuations in stock prices over time. This information can be utilized for various purposes such as investment analysis, financial research, and market trend forecasting. movies and tv shows education","263":" House_Prices_Dataset House Prices Dataset Dataset includes house sale prices for King County in USA. \nHomes that are sold in the time period: May, 2014 and May, 2015.\n\nColumns:\n    - ida: notation for a house\n    - date: Date house was sold\n    - price: Price is prediction target\n    - bedrooms: Number of Bedrooms\/House\n    - bathrooms: Number of bathrooms\/House\n    - sqft_living: square footage of the home\n    - sqft_lot: square footage of the lot\n    - floors: Total floors (levels) in house\n    - waterfront: House which has a view to a waterfront\n    - view: Has been viewed\n    - condition: How good the condition is ( Overall )\n    - grade: overall grade given to the housing unit, based on King County grading system\n    - sqft_abovesquare: footage of house apart from basement\n    - sqft_basement: square footage of the basement\n    - yr_built: Built Year\n    - yr_renovated: Year when house was renovated\n    - zipcode: zip\n    - lat: Latitude coordinate\n    - long: Longitude coordinate\n    - sqft_living15: Living room area in 2015(implies-- some renovations) \n    - sqft_lot15: lotSize area in 2015(implies-- some renovations) united states real estate neural networks text python","264":" Housing Sales: Factors Influencing Sale Prices Exploring Property Characteristics, Location, and Amenities in Real Estate Marke This dataset contains information related to housing sales, in the form of individual properties. Here's a breakdown of the columns:\n\n| Column Name    | Description                                         |\n|----------------|-----------------------------------------------------|\n| Lot_Frontage   | Linear feet of street connected to the property    |\n| Lot_Area       | Lot size in square feet                             |\n| Bldg_Type      | Type of building|\n| House_Style    | Style of the house      |\n| Overall_Cond   | Overall condition rating of the house               |\n| Year_Built     | Year the house was built                            |\n| Exter_Cond     | Exterior condition rating of the house               |\n| Total_Bsmt_SF  | Total square feet of basement area                  |\n| First_Flr_SF   | First-floor square feet                             |\n| Second_Flr_SF  | Second-floor square feet                            |\n| Full_Bath      | Number of full bathrooms                            |\n| Half_Bath      | Number of half bathrooms                            |\n| Bedroom_AbvGr  | Number of bedrooms above ground                     |\n| Kitchen_AbvGr  | Number of kitchens above ground                     |\n| Fireplaces     | Number of fireplaces                                |\n| Longitude      | Longitude coordinates of the property location      |\n| Latitude       | Latitude coordinates of the property location       |\n| Sale_Price     | Sale price of the property                          |\n\nThe dataset contains 2413 entries and has a mixture of numerical and categorical data. It's likely used for analyzing various factors influencing housing sale prices, such as location, size, condition, and amenities. real estate","265":" House Price Predication Predicting the house price House price prediction\nPredicting house prices is a common task in data science and machine learning. Here's a high-level overview of how you might approach it:\n\nData Collection:\nGather a dataset containing features of houses (e.g., size, number of bedrooms, location, amenities) and their corresponding prices. Websites like Zillow, Kaggle, or government housing datasets are good sources.\n\nData Preprocessing:\nClean the data by handling missing values, encoding categorical variables, and scaling numerical features if necessary. This step ensures that the data is in a suitable format for training a model. Feature Selection\/Engineering: Choose relevant features that are likely to influence house prices. You may also create new features based on domain knowledge or data analysis.\n\nModel Selection:\nSelect a regression model suitable for predicting continuous target variables like house prices. Common choices include Linear Regression, Decision Trees, Random Forests, Gradient Boosting, and Neural Networks.\n\nModel Training:\nSplit your dataset into training and testing sets to train and evaluate the performance of your model. You can further split the training set for validation purposes or use cross-validation techniques.\n\nModel Evaluation:\nAssess the performance of your model using appropriate evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or Root Mean Squared Error (RMSE).\n\nHyperparameter Tuning:\nFine-tune your model's hyperparameters to improve its performance. Techniques like grid search or random search can be employed for this purpose.\n\nDeployment:\nOnce satisfied with your model's performance, deploy it to make predictions on new data. This could be as simple as saving the trained model and creating an interface for users to input house features. ","266":" Housing Price Prediction  Real Estate Prices Dataset\nThis dataset comprises information on 4,600 real estate transactions, providing a detailed snapshot of the housing market in various locations. Each record captures the characteristics of a house, its surroundings, and transaction details from transactions that occurred around May 2, 2014. The dataset includes the following fields:\n\ndate: The date of the transaction.\nprice: The sale price of the property (in USD).\nbedrooms: The number of bedrooms.\nbathrooms: The number of bathrooms, represented in half-baths (e.g., 1.5 indicates one full bath and one half bath).\nsqft_living: The square footage of the home's living area.\nsqft_lot: The square footage of the lot.\nfloors: The number of floors.\nwaterfront: A binary indicator for whether the property is on the waterfront (1) or not (0).\nview: An index from 0 to 4 indicating the quality of the view.\ncondition: An index from 1 to 5 on the condition of the property.\nsqft_above: The square footage of the house apart from the basement.\nsqft_basement: The square footage of the basement.\nyr_built: The year the property was built.\nyr_renovated: The year of the last renovation.\nstreet: The street address of the property.\ncity: The city in which the property is located.\nstatezip: The state and ZIP code.\ncountry: The country of the property.\n\n\nThis dataset can be particularly useful for projects involving real estate market analysis, price prediction models, and economic research related to housing trends. Researchers and enthusiasts can explore aspects such as the impact of property characteristics on price, trends over time, and geographical price variations. real estate","267":" Singapore Home Resale Prices (HDB) Transacted resale flat prices w\/ distance to MRT & CBD. Official SG Gov data. # General Information and License\n- HDB Resale Price data made available by SG government through the Open Data License.\n    1. https:\/\/beta.data.gov.sg\/open-data-license\n    2. https:\/\/beta.data.gov.sg\/collections\/189\/datasets\/d_ebc5ab87086db484f88045b47411ebc5\/view\n- Geodata made available by SG government through the OneMap\n    1. https:\/\/www.onemap.gov.sg\/apidocs\/apidocs\n- I claim NO rights whatsoever to this data. This data has been uploaded here for educational purposes.\n- There is no personally identifiable information in this data.\n\n# Data Availability\n- 1990 - 02-2012 (based on registration date)\n- 03-2012 - 2023-09 (based on approval date)\n\nContains resale prices only because most of Singapore's population lives in (very well-made and maintained!) public housing. \n\nDoes not contain condo\/landed house prices.\n\n# Special Notes\n- Public housing in Singapore is generally sold on a leasehold (typically 99-year) basis.\n- Prices tend to decrease as the remaining lease period decreases (see Bala's Curve).\n\n# Data Representation\nEach row represents 1 transaction.\n\n# Data Dictionary\n\n| Column | Description  |\n| --- | --- |\n| latitude | Latitude coordinate.  |\n| longitude | Longitude coordinate. |\n| postal_code | 6-digit postal code. |\n| address | Street address. |\n| closest_mrt | Name of closest Mass Rapid Transit (MRT) station. |\n| closest_mrt_dist| Distance (metres) to closest MRT station. |\n| cbd_dist | Distance (metres) to Central Business District (1.2830, 103.8513 -- Raffles Place Station) | \n| month| YYYY-MM representation of the transaction date. |\n| town | Name of the neighbourhood in which the flat is located. |\n| flat_type | Generic type of flat, e.g. \"3 ROOM\", \"4 ROOM\". |\n| block | Block number. |\n| street name | Street name. |\n| storey_range | Height of the flat. Specific floors are never given (as of 2024). |\n| floor_area | Floor area in Square Metres. |\n| flat_model | Model\/subtype of flat. Each flat type has certain specifications and configurations. |\n| lease_commence_date | The year that the leasehold contract of the flat began. Most (not all) flats have 99-year leases.|\n| remaining_lease| Remaining lease of the contract in years and months. Not all files have this column. It only starts appearing in the file with data from Jan 2015 to Dec 2016.|\n| resale_price| Target column. This is the ***transacted*** resale price as agreed between buyer and seller. |\n|years remaining| Years remaining on the lease. |\n\n# Update Frequency\n- This dataset may be updated every year or so as new transaction data becomes available. asia housing lending tabular english","268":" Amazon Books Dataset: Genre, Sub-genre, and Books Exploring Amazon's Book Diversity # **About Dataset**\nThis project is all about taking a closer look at the books available on Amazon. We've collected information on different types of genres, sub-genres, and individual books like their titles, authors, prices, and ratings. By digging into this data, we hope to learn interesting things about what kinds of books are popular, how they're priced, and what people like to read. This can help us understand more about the world of books and what makes them tick on Amazon.\n\n### **Dataset 1: Genre**\n- **Title**: This column contains the main genres of books available on Amazon.\n- **Number of Sub-genres:** Indicates the count of sub-genres associated with each main genre.\n- **URL:** Provides the link to the page on Amazon where books of this genre are listed.\n### **Dataset 2: SubGenre**\n- **Title:** Lists the specific sub-genres within each main genre.\n- **Main Genre:** Indicates the overarching genre to which each sub-genre belongs.\n- **No. of Books:** Shows the count of books categorized under each sub-genre.\n- **URL:** Provides the link to the page on Amazon where books of this sub-genre are listed.\n### **Dataset 3: Books_df**\n- **Title:** The title of the book.\n- **Author:** Name of the author or publication house.\n- **Main Genre:** The main genre the book belongs to.\n- **Sub Genre:** The specific sub-genre of the book.\n- **Type:** Indicates the format of the book, such as paperback, Kindle, audiobook, or hardcover.\n- **Price:** The price of the book.\n- **Rating:** The average rating of the book given by users.\n- **No. of People Rated:** Indicates the count of users who have rated the book.\n- **URLs:** Provides the link to the book's page on Amazon for further details and purchase options. literature exploratory data analysis data cleaning data visualization data analytics tabular","269":" Portuguese Apartment Listings Dataset Comprehensive Dataset of Apartment Listings in Portugal from Imovirtual The dataset provides information about apartments available for sale in Portugal, gathered from Imovirtual. Each entry in the dataset represents an apartment and includes the following attributes:\n\nIndex: A unique identifier for each entry in the dataset.\nName: The apartment type, indicating the number of bedrooms.\nPrice: The selling price of the apartment.\nArea: The area of the apartment in square meters.\nLocation: The district where the apartment is situated.\n\nThe provided data sample encompasses a variety of apartment types, prices, areas, and locations in different districts of Portugal, covering both new and used apartments. These data are valuable for real estate market analysis and price predictions. housing real estate","270":" House Prices  This dataset provides detailed information about residential properties, including their prices, area sizes, number of bedrooms, and several other key features. The dataset aims to assist in the analysis and prediction of housing prices based on various property attributes.\n\n ","271":" Indian Rental House Price Regression analysis, mutiple regression,linear regression, prediction This dataset provides comprehensive information about rental house prices across various locations in India. It includes details such as house type, size, location, city, latitude, longitude, price, currency, number of bathrooms, number of balconies, negotiability of price, price per square foot, verification date, description of the property, security deposit, and status of furnishing (furnished, unfurnished, semi-furnished).\n\n`Note: This is Recently scraped data of April 2024.`\n\n### **Dataset Glossary (Column-Wise)**\n- **House Type**: Type of house (e.g., apartment, villa, duplex).\n- **House Size**: Size of the house in square feet or square meters.\n- **Location**: Specific area or neighborhood where the property is located.\n- **City**: City in India where the property is situated.\n- **Latitude**: Geographic latitude coordinates of the property location.\n- **Longitude**: Geographic longitude coordinates of the property location.\n- **Price**: Rental price of the house.\n- **Currency**: Currency in which the price is denoted (e.g., INR - Indian Rupees).\n- **Number of Bathrooms**: Total number of bathrooms in the house.\n- **Number of Balconies**: Total number of balconies in the house.\n- **Negotiability**: Indicates whether the price is negotiable (Yes\/No).\n- **Price per Square Foot**: Price of the house per square foot.\n- **Verification Date**: Date when the rental information was verified.\n- **Description**: Additional description or details about the property.\n- **Security Deposit**: Amount of security deposit required for renting the property.\n- **Status**: Indicates the furnishing status of the property (furnished, unfurnished, semi-furnished).\n\n### **Usage**\nThis dataset aims to provide valuable insights into the rental housing market in India, enabling analysis of rental trends, comparison of prices across different locations and property types, and understanding the impact of various factors on rental prices. Researchers, analysts, and policymakers can utilize this dataset for a wide range of applications, including real estate market analysis, urban planning, and economic research.\n\n### **Acknowledgement**\nThis Dataset is created from [https:\/\/www.makaan.com\/](https:\/\/www.makaan.com\/). If you want to learn more, you can visit the Website.\n\nCover Photo by: [Playground.ai](https:\/\/playground.com\/post\/a-dynamic-and-lively-shot-of-a-daman--diu-haveli-of-two-flo-clrrmmk6a039gs6019ez0xpjm) india housing real estate regression","272":" California Real state Dataset  The California Housing dataset contains information about various factors affecting housing prices in the California area. It includes features such as the per capita crime rate, average number of rooms per dwelling, proportion of residential land zoned for lots over 25,000 square feet, and more.\n# Datasource: The data were derived from information collected by the U.S. Census Service concerning housing in the area of California.\n\nColumns:\nLocation: Region\nAddress: Address of the house\nPrice: Price in millions\nCRIMS per capita crime rate by town\nZN proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS proportion of non-retail business acres per town\nCHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nNOX nitric oxides concentration (parts per 10 million)\nRM average number of rooms per dwelling\nAGE proportion of owner-occupied units built prior to 1940\nDIS weighted distances to five Boston employment centres\nRAD index of accessibility to radial highways\nTAX full-value property-tax rate per 10,000usd\nPTRATIO pupil-teacher ratio by town\nB 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT % lower status of the population\n housing real estate","273":" Gurgoan properties (Cleaned) dataset  Welcome to the Gurgaon Real Estate Dataset tailored specifically for training predictive models to forecast house prices in one of India's most dynamic urban centers. This dataset is meticulously curated, merged, and cleaned to facilitate seamless model training and accurate predictions.\n\nEncompassing a diverse array of attributes crucial for predictive modeling, this dataset provides an unparalleled opportunity for data scientists, machine learning enthusiasts, and real estate professionals to develop robust pricing models. Each entry in the dataset represents a unique property, meticulously curated and standardized to ensure consistency and reliability.\n\nKey attributes included in the dataset comprise property type, location, area, number of bedrooms and bathrooms, amenities, parking facilities, and additional features. Additionally, historical transaction data, such as previous sale prices and dates, are provided to enrich the predictive modeling process.\n\nBy leveraging this dataset, users can embark on a journey to:\n\n1. Develop sophisticated machine learning models to accurately predict house prices in Gurgaon.\n2. Identify significant predictors and drivers of house prices, aiding in market analysis and decision-making.\n3. Explore feature engineering techniques to enhance model performance and interpretability.\n4. Evaluate the impact of location, amenities, and other property attributes on pricing dynamics.\n5. Assess model performance through rigorous evaluation metrics and validation techniques.\n\nThis dataset serves as a catalyst for innovation and collaboration within the data science community, empowering practitioners to unlock insights and drive value in the real estate domain. Whether you're a seasoned data scientist or an aspiring enthusiast, this dataset offers an exciting opportunity to delve into the complexities of real estate pricing dynamics and make meaningful contributions to the field.\n\nWe encourage users to explore, analyze, and experiment with this dataset, pushing the boundaries of predictive modeling and advancing our understanding of house price dynamics in Gurgaon. Together, let's harness the power of data to unravel the mysteries of real estate pricing and pave the way for informed decision-making in one of India's most vibrant urban landscapes. real estate","274":" Classified NIH Dataset NIH x-ray dataset in classified format. I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n health classification deep learning cnn health conditions tensorflow","275":" NIH Chest X-rays Bbox version 880 Bounding Box Images for Fast Image Detection Model Training from NIH Dataset # **NIH Chest X-ray Dataset**\n\n## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, [Openi](https:\/\/openi.nlm.nih.gov\/) was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n[Link to paper](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n\n## Data limitations\n\n- The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be &gt;90%.\n- Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n\n## File contents\n\n- Image format: 880 total images with size 1024 x 1024\n- bbox_img: Contains 880 bbox images\n- README_ChestXray.pdf: Original README file\n- BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n   - Image Index: File name\n   - Finding Label: Disease type (Class label)\n   - Bbox x\n   - Bbox y\n   - Bbox w\n   - Bbox h\n- Data_entry_2017.csv: Class labels and patient data for the entire dataset\n   - Image Index: File name\n   - Finding Labels: Disease type (Class label)\n   - Follow-up #\n   - Patient ID\n   - Patient Age\n   - Patient Gender\n   - View Position: X-ray orientation\n   - OriginalImageWidth\n   - OriginalImageHeight\n   - OriginalImagePixelSpacing_x\n   - OriginalImagePixelSpacing_y\n- label.csv: Class labels\n- tesnorlfow.csv: tensorflow version of the dataset\n\n## Class descriptions\n\nThere are 8 classes . Images can be classified as one or more disease classes:\n- Infiltrate\n- Atelectasis\n- Pneumonia\n- Cardiomegaly\n- Effusion\n- Pneumothorax\n- Mass\n- Nodule\n\n## Citations\n\n- Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, [ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n- NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community](https:\/\/www.nih.gov\/news-events\/news-releases\/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n-Original source files and documents: https:\/\/nihcc.app.box.com\/v\/ChestXray-NIHCC\/folder\/36938765345\n\n## Acknowledgements\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov). computer vision multiclass classification health conditions image classification","276":" lung x-ray image+clinical text dataset Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co **Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases. health health conditions","277":" Improved_X-Ray_data NIH Chest X-rays validated images  biology health medicine computer science software","278":" Covid-19 X-Ray Classification Dataset Covid-19 Patient Chest X-Ray Dataset (Positive and Negative) This dataset contains 284 images of human chest X-ray belonging to 2 classes (Covid-19 Positive and Negative). The dataset has been divided into train and validation splits with 112 and 30 images respectively. The dataset shall be used to train deep learning models such as CNN.\n\n**Dataset Hierarchy**\n\nDataset.zip\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 normal\n\u2502   \u2514\u2500\u2500 infected\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 normal\n    \u2514\u2500\u2500 infected\n\n**Citations** : \n- Covid-19 Positive Patient Chest X-ray images (Source : [ https:\/\/github.com\/ieee8023\/covid-chestxray-dataset\/tree\/master](url))\n- Kaggle Human Lung X-ray Image Dataset (Extracted only \"Normal\") (Source : [https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia](url)) health cnn covid19 image classification image augmentation image generator","279":" ChestXray 8 Object Detection Yolo and Pascal VOC YOLO and Pascal VOC Scheme Dataset for Chest X-ray 8 Object Detection # Overview\nThe Chest X-ray 8 Subset dataset is a curated collection of chest radiographs specifically designed for the development and evaluation of object detection models focused on thoracic diseases. This subset consists of 790 images and 984 annotated bounding boxes, capturing various abnormalities within the lungs and surrounding structures. The annotations are provided in both YOLO and Pascal VOC formats, facilitating their use in a wide range of machine learning frameworks.\n\n# Data Composition\n- Total Images: 790\n- Total Bounding Boxes: 984\n- Image Format: PNG\n- Annotation Formats: YOLO and Pascal VOC\n- All Images are resized to 512x512 pixels.\n\nTest set: 631 images\nVal set: 159 images\n\n# Classes and Labels\nThe dataset includes annotations for 14 different classes of thoracic diseases:\n\n1. Atelectasis,\n2. Cardiomegaly,\n3. Effusion,\n4. Infiltrate,\n5. Nodule,\n6. Mass,\n7. Pneumonia,\n8. Pneumothorax\n\n health medicine computer vision image yolo object detection","280":" VinBigData Chest X-ray - YOLO   ","281":" Normal Tuberculosis Covid-19 Chest Xrays images  ### Content\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Normal\/Covid-19\/Tuberculosis). A total of 7135 x-ray images are present.\n\n### Acknowledgements\nSources:\nhttps:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia\nhttps:\/\/www.kaggle.com\/tawsifurrahman\/tuberculosis-tb-chest-xray-dataset\nhttps:\/\/www.kaggle.com\/prashant268\/chest-xray-covid19-pneumonia\nhttps:\/\/github.com\/ieee8023\/covid-chestxray-dataset\n\n### Inspiration\nApplication of Deep Learning techniques to detect and classify lung diseases from x-ray images deep learning multiclass classification keras tensorflow image classification","282":" Pulmonary-Chest-X-Ray-Abnormalities  <div><div><div style=\"min-height: 80px\"><div><h1>Content and context<\/h1>\n<p>Tuberculosis is a disease that affects many people in developing countries. While treatment is possible, it requires an accurate diagnosis first. In these countries projects there are in many cases available X-ray machines (through low-cost projects and donations), but often the radiological expertise is missing for accurately assessing the images. An algorithm that could perform this task quickly and cheaply could drastically improve the ability to diagnose and ultimately treat the disease.<\/p>\n<p>In more developed countries, X-ray radiography is often used for screening new arrivals and determining eligibility for a work-permit. The task of manually examining images is time consuming and an algorithm could increase efficiency, improve performance and ultimately reduce cost of this screening. <\/p>\n<p>This dataset contains over 500 x-rays scans with clinical labels collected by radiologists.<\/p>\n<h1>Acknowledgements<\/h1>\n<p>The two datasets were published together in an analysis here: <a target=\"_blank\" href=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/\">https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4256233\/<\/a>.<br>\nThe datasets come from Shenzhen and Montgomery respectively.<\/p>\n<h3>China Set - The Shenzhen set - Chest X-ray Database<\/h3>\n<p>The standard digital image database for Tuberculosis is created by the National Library of Medicine, Maryland, USA in collaboration with Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China. The Chest X-rays are from out-patient clinics, and were captured as part of the daily routine using Philips DR Digital Diagnose systems. <br>\nNumber of X-rays: <\/p>\n<ul>\n<li>336 cases with manifestation of tuberculosis, and <\/li>\n<li>326 normal cases.<\/li>\n<\/ul>\n<p>It is requested that publications resulting from the use of this data attribute the source (National Library of Medicine, National Institutes of Health, Bethesda, MD, USA and Shenzhen No.3 People\u2019s Hospital, Guangdong Medical College, Shenzhen, China) and cite the following publications:  <\/p>\n<ul>\n<li>Jaeger S, Karargyris A, Candemir S, Folio L, Siegelman J, Callaghan F, Xue Z, Palaniappan K, Singh RK, Antani S, Thoma G, Wang YX, Lu PX, McDonald CJ.  Automatic tuberculosis screening using chest radiographs. IEEE Trans Med Imaging. 2014 Feb;33(2):233-45. doi: 10.1109\/TMI.2013.2284099. PMID: 24108713<\/li>\n<li>Candemir S, Jaeger S, Palaniappan K, Musco JP, Singh RK, Xue Z, Karargyris A, Antani S, Thoma G, McDonald CJ. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Trans Med Imaging. 2014 Feb;33(2):577-90. doi: 10.1109\/TMI.2013.2290491. PMID: 24239990<\/li>\n<\/ul>\n<h3>Montgomery County X-ray Set<\/h3>\n<p>X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays are abnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities, including effusions and miliary patterns. The data set includes radiology readings available as a text file.<\/p>\n<h1>Ideas<\/h1>\n<ul>\n<li>Experiment with lung segmentation<\/li>\n<li>Build disease classifiers for various conditions<\/li>\n<li>Test models on data across different manufacturers <\/li>\n<li>Build GANs that are able to make the datasets indistinguishable (Adversarial Discriminative Domain Adaptation: <a target=\"_blank\" href=\"https:\/\/arxiv.org\/abs\/1702.05464\">https:\/\/arxiv.org\/abs\/1702.05464<\/a>)<\/li><\/ul><\/div><\/div><\/div><\/div> health conditions","283":" Chest X-Ray Dataset  Please cite: **Enhancing COVID-19 prediction using transfer learning from Chest X-ray images**\n\nP. -H. Huynh, T. -N. Tran and V. H. Nguyen, \"Enhancing COVID-19 prediction using transfer learning from Chest X-ray images,\" 2021 8th NAFOSTED Conference on Information and Computer Science (NICS), Hanoi, Vietnam, 2021, pp. 398-403, doi: 10.1109\/NICS54270.2021.9701516.\n\n&gt;@INPROCEEDINGS{9701516,\n  author={Huynh, Phuoc-Hai and Tran, Trung-Nguyen and Nguyen, Van Hoa},\n  booktitle={2021 8th NAFOSTED Conference on Information and Computer Science (NICS)}, \n  title={Enhancing COVID-19 prediction using transfer learning from Chest X-ray images}, \n  year={2021},\n  volume={},\n  number={},\n  pages={398-403},\n  keywords={COVID-19;Training;Pandemics;Pulmonary diseases;Computational modeling;Transfer learning;Predictive models;COVID-19;transfer learning;imbalanced dataset;X-Ray images},\n  doi={10.1109\/NICS54270.2021.9701516}}\n\n\n health","284":" DS4 Work - Operation Datasets Data Science for Business - Operation Department In this case study, you are hired by a hospital in Toronto as a deep learning consultant and tasked with automating the detection and classification process of pulmonary diseases.\n\nThe team collected extensive X-Ray chest data and asked you to develop a model that could detect and classify diseases in less than a minute.\n\nThey provided a dataset consisting of 133 images and divided into 4 classes:\n\n0 - Covid-19\n1 - Healty X-ray\n2 - Viral Pneumonia X-ray\n3 - Bacterial Pneumonia X-ray\n\nHowever, it's crucial to note that the use of AI in healthcare raises some ethical and social concerns. For instance, there are worries that AI systems may be biased and result in misdiagnoses. Additionally, there are concerns that AI may replace doctors and other healthcare professionals. business health medicine image canada","285":" Chest X-Ray Images   arts and entertainment","286":" Pneumonia  Tuberculosis  Normal X Ray Images Dataset -- 3 Classes --  In this dataset we have 3 classes.\n\n1. Pneumonia\n2. TB\n3. Normal\n\n\nA brief idea about the diseases ,\nPneumonia:\nPneumonia is an infection that inflames the air sacs  in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. Pneumonia can range in seriousness from mild to  life-threatening. It is most serious for infants and young children, people older than age 65, and people with weakened immune systems. Common causes  of pneumonia include bacteria, viruses, and fungi. Treatment depends on the type and severity of pneumonia, but typically involves antibiotics for bacterial pneumonia and antiviral medications for viral pneumonia. Vaccines  are available to prevent some types of pneumonia, such as pneumococcal pneumonia and influenza pneumonia.\n\nTuberculosis (TB):\nTuberculosis (TB) is a bacterial infection primarily affecting the lungs but can  also impact other parts of the body. It spreads through the air when an infected person coughs or sneezes, releasing infectious particles. Symptoms include persistent cough, chest pain,  coughing up blood or sputum, weakness, weight loss, lack of appetite,  chills, fever, and night sweats. TB can be latent, without symptoms and not contagious, but it can become active if untreated. Active TB is  contagious and requires several  months of antibiotic treatment. Drug-resistant strains may need stronger medications and longer treatment. Prevention involves screening, early  detection, treatment, and vaccination with the BCG vaccine, though  its effectiveness varies. healthcare intermediate deep learning image classification english","287":" Chest Radiograph Images (Pneumonia & Normal) 11,236 Images, 2 Categories This dataset is what I used to train my model. I made this dataset by combining two different datasets which are available for use in Kaggle. So I would like to cite them here. I am also thanking them for this. \n\nThis dataset combines chest X-ray images from two sources for pneumonia detection. The primary dataset, sourced from Kaggle Chest X-Ray Images (Pneumonia) [1], includes 5,863 images (1,583 normal, 4,273 pneumonia). Supplementary data from the COVID-19 Radiography Database [2], contributes 1,345 pneumonia images and 4,053 normal images. Thus making the number of pneumonia and the normal images the same that is 5618 images and in total, the dataset comprises 11,236 images. \n\nThe two sources from which I took the images are trust worthy as you can see in the source dataset pages.\n\n[1]. Chest X-Ray Images (Pneumonia) :\n https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n[2]. COVID-19 Radiography Database\nhttps:\/\/www.kaggle.com\/datasets\/tawsifurrahman\/covid19-radiography-database?rvi=1 health","288":" Chest X-ray Images   arts and entertainment","289":" Covid-19 Chest X-ray  Context\n\n  The whole world is suffering from COVID-19 disease and its proper and timely diagnosis is the need of the hour. So for building an efficient AI-based diagnostic system we have collected Chest ray images from different sources so that we can train our CNN model for automating the whole diagnosis method. Therefore we have collected Chest ray images from different sources and research papers and combined them to create one comprehensive dataset that can be used by research community.\n\n  This dataset is also used in COVID Lite paper which has shown significant results by building novel CNN based solution.\n\nContent\n\n  This dataset consists of a posteroanterior (PA) view of chest X-ray images comprising Normal, Viral, and CVOID-19 affected patients. There are total 1709 CXR images. health deep learning cnn image classification","290":" covid-19 a chest X-ray images of person diagnosed negative and positive with COVID-19 This dataset is shared by Lecturer **Mr. Wahyono, Ph. D.** as a base for his Computer Vision course at Universitas Gadjah Mada. This is the provided dataset for individual assignment COVID-19 Classification. \n\nThe source as described in the `Sources.txt` file is this GitHub link: \n\n[https:\/\/github.com\/ieee8023\/covid-chestxray-dataset](url) education","291":" chest x-ray images pneumonia dataset   health conditions","292":" pneumonia CXR dataset 6,126 images and corresponding lung masks, 4 categories ### pneumonia CXR dataset\nThe dataset is organized into 3 folders (train, validation, test) and contains subfolders for each image category (Normal\/COVID-19\/Bacteria\/Viral). There are 6,126 chest X-ray (CXR) images and 4 categories (Normal\/COVID-19\/Bacteria\/Viral).\n- 1,535 Normal\n- 1,708 COVID-19\n- 1,390 Bacteria pneumonia \n- 1,493 Viral pneumonia \nGround-truth lung segmentation masks are provided for the entire dataset. \n\n### References\nIn pneumonia CXR dataset, the X-ray images are collected from the following repositories:\n1. Chest X-Ray Images (Pneumonia): https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/chest-xray-pneumonia\n2. COVID-QU-Ex Dataset: https:\/\/www.kaggle.com\/datasets\/cf77495622971312010dd5934ee91f07ccbcfdea8e2f7778977ea8485c1914df\n diseases health computer vision image image classification","293":" Lung Disease Lung Disease classification **Description**\n\nLung disease encompasses a wide range of conditions that affect the lungs and their ability to function effectively. These conditions can be caused by various factors, including infections, environmental factors, genetic predispositions, and lifestyle choices. Lung diseases can result in symptoms such as coughing, shortness of breath, chest pain, and reduced lung function. Detecting and diagnosing lung diseases is crucial for patient care, as they can have a significant impact on an individual's health and quality of life.\n\n**Global Impact:**\n\nLung diseases have a substantial global impact. According to the World Health Organization (WHO), respiratory diseases, including lung diseases, are responsible for a significant portion of global mortality. In 2016, respiratory diseases were the fourth leading cause of death worldwide, with an estimated 3.0 million deaths attributed to them. Conditions like pneumonia, chronic obstructive pulmonary disease (COPD), and lung cancer contribute to this high mortality rate. Early detection and accurate diagnosis are essential for reducing the burden of lung diseases on public health.\n\n**The Need to Detect Lung Diseases:**\n\n**Detecting lung diseases is vital for several reasons:**\n\nEarly Intervention: Early detection allows for timely medical intervention and treatment, increasing the chances of successful management and recovery.\n\n**Disease Classification:** Differentiating between various lung diseases, such as pneumonia, tuberculosis, and lung cancer, is crucial for appropriate treatment planning.\n\n**Public Health:** Effective disease detection and management can have a positive impact on public health by reducing the overall disease burden.\n\n**Lung X-Ray Image Dataset:**\n\nThe \"Lung X-Ray Image Dataset\" is a comprehensive collection of X-ray images that plays a pivotal role in the detection and diagnosis of lung diseases. This dataset contains a large number of high-quality X-ray images, meticulously collected from diverse sources, including hospitals, clinics, and healthcare institutions.\n\n**Dataset Contents:**\n\nTotal Number of Images: The dataset comprises a total of 3,475 X-ray images.\nClasses within the Dataset:\n\nNormal (1250 Images): These images represent healthy lung conditions, serving as a reference for comparison in diagnostic procedures.\n\nLung Opacity (1125 Images): This class includes X-ray images depicting various degrees of lung abnormalities, providing a diverse set of cases for analysis.\n\nViral Pneumonia (1100 Images): Images in this category are associated with viral pneumonia cases, contributing to the understanding and identification of this specific lung infection.\n\nIn conclusion, the \"Lung X-Ray Image Dataset\" plays a crucial role in the healthcare sector by providing a diverse and well-documented collection of X-ray images that support the detection, classification, and understanding of lung diseases. This resource is instrumental in advancing the field of respiratory medicine and improving patient outcomes. diseases health medicine classification deep learning image","294":" A Song of Ice and Fire - Dialogues Dialogue dataset from George R. R. Martin's \"A Song of Ice and Fire\" books # Context\n\"A Song of Ice and Fire\" is an epic series of fantasy novels by George R.R. Martin. The series is set in the fictional continents of Westeros and Essos and chronicles the power struggles among noble families as they vie for control of the Iron Throne of the Seven Kingdoms. The series is renowned for its complex characters, intricate political plots, and the interweaving of multiple storylines.\n\nThe first book, \"A Game of Thrones,\" was published in 1996, and the series has since expanded to include five published novels with two more planned. The books have inspired the highly successful HBO television series \"Game of Thrones,\" which has brought the world of Westeros to a global audience.\n\n# Description of the Dataset\nThis dataset contains dialogues from the \"A Song of Ice and Fire\" book series. Each entry corresponds to a line of dialogue, along with metadata about the context in which the dialogue occurs. This dataset is valuable for those studying narrative structure, character interactions, and dialogue patterns in literature.\n\n##Column Descriptions\n- **Release Date:** The date when the book was released.\n- **Book:** The title of the book from which the dialogue is taken.\n- **Chapter Number:** The sequential number of the chapter within the book.\n- **Chapter Name:** The name of the chapter.\n- **Speaker(s):** The character(s) who is\/are speaking the line of dialogue.\n- **Dialogue:** The actual line of dialogue spoken by the character(s).\n- **Addressee(s):** The character(s) to whom the dialogue is directed.\n- **Listener(s):** The character(s) who hear the dialogue, regardless of whether it is directed to them.\n- **Mentioned Character(s):** Characters mentioned in the dialogue but not necessarily present.\n- **Present:** A boolean indicating whether the dialogue is in the present or in the past.\n\n# Acknowledgment\nThis dataset was created using a combination of artificial intelligence and human corrections to ensure accuracy and comprehensiveness. We acknowledge the effort and time invested by both AI and human editors in compiling this dataset.\n\n# Usage\nThe original work belongs to the author, George R.R. Martin. This dataset is provided for educational and research purposes only. No commercial use is allowed or will be conducted with this dataset. Users must adhere to these terms and respect the intellectual property rights of the original author. literature","295":" The LAMBADA Dataset for Word Prediction Evaluating text understanding through word prediction ## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https:\/\/aclanthology.org\/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development\/test\/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http:\/\/www.aclweb.org\/anthology\/P16-1144}\n}\n\nFirst released on 2016, September 26.\n languages literature computer science nlp text mining deep learning neural networks","296":" BookSales   ","297":" Percy Jackson Book Series NLP Dataset A Comprehensive Text Corpus of the First Five Books for Natural Language Process ### Description:\n\n**Percy Jackson Book Series NLP Dataset**\n\nThis dataset contains the raw text from the first five books of the Percy Jackson series by Rick Riordan. It provides a rich corpus for various Natural Language Processing (NLP) tasks and is ideal for researchers, educators, and developers interested in text analysis, sentiment analysis, named entity recognition, and more.\n\n**Contents:**\n- The Lightning Thief\n- The Sea of Monsters\n- The Titan's Curse\n- The Battle of the Labyrinth\n- The Last Olympian\n\n**Key Features:**\n- **Raw Text:** The dataset includes unprocessed text from the books, providing an authentic and comprehensive source for a variety of NLP experiments.\n- **Rich in Entities:** With a plethora of characters, places, and mythical references, this dataset is perfect for named entity recognition and entity linking tasks.\n- **Diverse Sentiments:** The narrative includes a wide range of emotional expressions, making it suitable for sentiment analysis projects.\n- **Complex Sentences:** The books contain varying sentence structures and vocabularies, beneficial for language modeling and text generation.\n\n**Potential Use Cases:**\n- Sentiment Analysis\n- Named Entity Recognition\n- Text Classification\n- Language Modeling\n- Text Generation\n- Tokenization and Lemmatization Practice\n\n**Notes:**\n- **Raw Data:** The text is presented as-is from the books, with no preprocessing performed. Users may need to clean and preprocess the text according to their specific project requirements.\n- **Ethical Use:** Please ensure the dataset is used in compliance with copyright laws and is intended for educational and research purposes only.\n\nUnlock the potential of Rick Riordan's engaging narratives to enhance your NLP models and algorithms!\n literature nlp pca nltk token classification word2vec skip-gram","298":" Books Dataset   ","299":" lung x-ray image+clinical text dataset Comprehensive Lung Disease X-ray and Clinical Text Dataset with Annotated and Co **Description:**\nThis dataset is a rich resource for the study and classification of lung diseases using X-ray images and clinical text data. It is divided into two main folders:\n\n**Annotated Images and Clinical Texts (Main Dataset Folder)**: This folder contains 80,000 annotated X-ray images, organized into eight subfolders corresponding to different lung diseases. Each subfolder has 10,000 images. Each image is accompanied by a CSV file that includes clinical texts with labels, providing a detailed context for each X-ray image.\n\n**Composite Image Data (Merged Data (image+text) Folder)**: This folder mirrors the structure of the first but without the CSV file. Instead, it features composite images that merge the X-ray image with encoded clinical text data. These composite images offer a unique approach by embedding the textual information directly into the visual data.\n\nThe dataset covers the following lung diseases:\n- Obstructive Pulmonary Diseases\n- Higher Density\n- Lower Density\n- Chest Changes\n- Encapsulated Lesions\n- Degenerative Infectious Diseases\n- Normal\n- Mediastinal Changes\n\nThis dual-format dataset supports a variety of machine learning tasks, including image classification, multi-modal learning, and the integration of text and image data for enhanced diagnostic models.\n\nOriginal images were collected from this dataset and then scaled: [X-ray Lung Diseases Images (9 classes)](https:\/\/www.kaggle.com\/datasets\/fernando2rad\/x-ray-lung-diseases-images-9-classes) by Fernando Feltrin.\nThe Clinical_text data was created with the help of GPT-3.5 and medical books containing symptoms for each of the lung diseases. health health conditions","300":" 1000 books data This dataset contains prices along with star rantings of one thousand books. **Context:**\nThis dataset was created through web scraping to compile a comprehensive collection of book information, including titles, prices, and star ratings. The purpose of creating this dataset was to analyze and utilize the book data available on the Books to Scrape website, which offers a wide range of books with various attributes that can be beneficial for data analysis and project development.\n**Source:**\nThis dataset was created by web scraping this website (https:\/\/books.toscrape.com). The source code used for web scraping can be found in this GitHub repository: https:\/\/github.com\/NoorFatimaAfzal\/web-scraping\n**Inspiration:**\nThis dataset can be used for a variety of projects, including but not limited to:\n\nData Analysis and Visualization: Analyzing trends in book prices, and ratings. Creating visual representations of the data to identify patterns and insights.\nRecommendation Systems: Building a book recommendation system based on star ratings and other book attributes.\nInventory Management: Developing a system to manage and track book inventory, helping bookstores optimize their stock levels.\nPrice Comparison Tools: Creating tools to compare book prices across different platforms or over time.\nMachine Learning Projects: Using the dataset to train models for predicting book ratings or prices based on other attributes. literature asia education beginner data analytics","301":" Book Recommendation System Dataset of different books The main dataset  on which all the operations are performed are **books.tsv** containing around 406000+ rows. The file contains two columns, **bookurl** and **data** column. Rest of the csv files are extracted files after running the **jupyter** **code**(book0-20.csv, nbook0-20.csv and book_details.csv with pkl files like tags, simi, tfidf.\nWe have built an **UI** on **Streamlit** with a file name **App1.py** and all the subsidary files required for UI are given in dataset section.\n  literature nlp deep learning text feature extraction text pre-processing","302":" Open Library Books API - 12k Open Library Books API with more than 12k book registers! This is a database taken from the open library api. It can be used to perform different types of analysis such as: visual analysis; recommendation analysis; machine learning; among others. \n\nThe initial objective of taking this data is to use it in the application I am developing called LitRecs (literature recommendations). Now that I have obtained this data, and given that it is something initial, I have decided to share because it is very difficult to find databases related to books that are complete in information. literature data visualization deep learning recommender systems tabular","303":" arabicBooks100   ","304":" ArabicBooks500   ","305":" Retail Analysis on Large Dataset In this dataset i founded so many insights also in last i developed Recom.. Sys. ##Dataset Description:\n* The dataset represents retail transactional data. It contains information about customers, their purchases, products, and transaction details. The data includes various attributes such as customer ID, name, email, phone, address, city, state, zipcode, country, age, gender, income, customer segment, last purchase date, total purchases, amount spent, product category, product brand, product type, feedback, shipping method, payment method, and order status.\n\n##Key Points:\n##Customer Information:\n* Includes customer details like ID, name, email, phone, address, city, state, zipcode, country, age, and gender.\nCustomer segments are categorized into Premium, Regular, and New.\n##Transaction Details:\n* Transaction-specific data such as transaction ID, last purchase date, total purchases, amount spent, total purchase amount, feedback, shipping method, payment method, and order status.\n##Product Information:\n* Contains product-related details such as product category, brand, and type.\nProducts are categorized into electronics, clothing, grocery, books, and home decor.\n##Geographic Information:\n* Contains location details including city, state, and country.\nAvailable for various countries including USA, UK, Canada, Australia, and Germany.\n##Temporal Information:\n* Last purchase date is provided along with separate columns for year, month, date, and time.\nAllows analysis based on temporal patterns and trends.\n##Data Quality:\n* Some rows contain null values, and others are duplicates, which may need to be handled during data preprocessing.\nNull values are randomly distributed across rows.\nDuplicate rows are available at different parts of the dataset.\n##Potential Analysis:\n* Customer segmentation analysis based on demographics, purchase behavior, and feedback.\nSales trend analysis over time to identify peak seasons or trends.\nProduct performance analysis to determine popular categories, brands, or types.\nGeographic analysis to understand regional preferences and trends.\nPayment and shipping method analysis to optimize services.\nCustomer satisfaction analysis based on feedback and order status.\n##Data Preprocessing:\n* Handling null values and duplicates.\nParsing and formatting temporal data.\nEncoding categorical variables.\nScaling numerical variables if required.\nSplitting data into training and testing sets for modeling. geography business computer science exploratory data analysis data cleaning data analytics retail and shopping","306":" Ancient Inscription Database (BDC_Db) Ancient Inscription Image Database from Maharashtra This describes creating an inscription image database for the ancient Marathi inscription image enhancement system. The database was created from inscription images of carved ancient Marathi text. Thus, the database provides the basis for developing practical Marathi inscription enhancement systems. It provides a standard benchmark for comparing different algorithms for Ancient Marathi Inscription Image text and helps in the research and development of ancient inscription image enhancement and recognition systems. The database contains more than 1000 sample inscription images with different scales. It is consisting of more than 10,000 characters from ancient times. Ancient Marathi Inscription Database servers like MNIST are a dataset for handwritten inscription image enhancement and recognition for Marathi carved characters. Augmentation techniques play a pivotal role in enhancing image processing models' performance and generalization capabilities. \nData Collection\nData collection is a significant step in constructing the database. Various methods are used to collect data from different sources:\n1. Utilizing research papers, archaeological books, and relevant materials.\n2. Consulting with archaeological experts.\n3. Conducting field visits and making observations.\nResearch papers, Archaeology related Books and Material:\n Both quantitative and qualitative data are collected from various archaeological books, including \"Kumbhar Anand Sanshodhan Tarang, Navbharat Prakashan, Mumbai\", \"Madhugin Itihaasaachi Sadhne Bharat Itihas Sanshodhak Mandal Khand 2\", \"Prachin Marathi Koriv Lekh, Pune Vidyapeeth Prakashan\", \"Punashya Pandharpur Shilalekh Mahathi Sahitya Patrika Ank\", and \"Maharashtra va Goa Shilalekh: Tamrapatachi Varnanatmak Suchi Mumbai.\"\nFrom these books, the following activities are conducted:\n1. Collection of Marathi inscription images.\n2. Compilation of details about Marathi inscriptions.\n3. Retrieval of information providing various ancient Marathi inscription sites.\n4. Gathering information on the location and specifics of ancient Marathi inscriptions, among other relevant details.\n Archaeology Experts:\nEngaging in communication with experts in the field of archaeology is a crucial approach to gathering data. These experts possess deep knowledge of ancient inscriptions and various ancient languages. Discussions with these experts help collect images and analyze image data. They also consulted with various archaeology Professors to find inscription reading methods, ancient Marathi characters, writing styles, inscription site locations, and relevant literature. During these discussions with experts, variations in each character were identified, which were documented for future research work. \nField Visits and Observations:\nObtaining reliable evidence through field visits and observation is very useful. The authors conducted field visits to Tuljapur, Pandharpur, Nashik, and other locations in the Solapur district, capturing original photos of relevant inscriptions. Additionally, they engaged with various tourists to gather data.  A DSLR camera was utilized to capture photos of inscriptions. The authors noted that identifying suitable inscriptions was challenging throughout these visits.\nInscription image selection:\n            In the preliminary investigation, an experimental study involved collecting and analyzing three distinct types of inscription images. These types include stone, document, and metal plate inscriptions. The inscription font, style, and size are not standard. However, a document uses standard font, style, and size. In stone inscriptions, characters are carved by different individuals, leading to font, style, and character variations. Binarization of documents proves more suitable for enhancing document images. Initially, enhancement based on binarization is applied to all three types of inscriptions, and various metrics are used to check accuracy. The findings reveal that the accuracy of stone inscription images is lower than the other types. For this research, stone inscription images are specifically chosen.\nImages are captured by using a digital camera NIKON D7200 (Focal Length -140 mm, Horizontal and vertical resolution -300*300 dpi, Bit depth-24, Max Aperture- 5, Metering mode- Pattern, Flash Mode \u2013 No flash)\n culture and humanities","307":" Ayurveda Texts (English) Texts on Ayurveda in English containing 20+ Books and 2000+ Articles Texts related to Ayurveda in English scraped from 21 Books and 2000+ Articles from the internet.\n\nCan be used for finetuning or RAG applications.\n\nThe dataset contains two directories:\nayurveda_books contains book texts and pdfs both\nayurveda_texts contains only texts mental health healthcare health nlp text generation","308":" Project Geutenberg Books An arbitrary collection of public domain works from Project geutenberg. # Project Geutenberg Books\n\n1. The Life And Adventures of Robinson Crusoe\n1. Frankenstein, Or The Modern Prometheus\n\nI wanted to begin work towards literature as data. Thus, I shall begin adding books as rapidly as possible to this dataset.\n\n\n**Original Credit and License:**\n```\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n``` literature","309":" booksi   ","310":" Mawqif Dataset Stance Detection in Arabic Language Shared Task The vast growth of social media platforms, online news outlets, and digital communication has increased user-generated content exponentially in recent years. This unprecedented surge in online discourse has sparked an urgent need to develop automated tools and techniques to effectively analyze the opinions and attitudes expressed within these expansive streams of text. Stance detection, a critical task within the field of Natural Language Processing (NLP), aims to identify the position or perspective of a writer towards a specific topic or entity by analyzing their written text and\/or social media activity, such as preferences and connections. The applications of stance detection are diverse and encompass domains such as politics, marketing, and social media analysis.\n\n## **Classes**\nThe possible stance labels are:\n\n**FAVOR** means that we can infer from the post that the author supports the target (e.g., explicitly supporting the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is in favor of the target).\n\n**AGAINST** means that we can infer from the tweet that the author is against the target (e.g., explicitly opposing the target or something aligned with the target, or if the post contains information such as news, a quote, a story, which reveals that the author is against the target).\n\n**NONE** means that the tweet provides no hint as to the author's stance toward the target (e.g., there is no evidence in the tweet to judge the author's stance, such as inquiries, or news that does not express any positive or negative position).\n\n# **Dataset**\nMawqif comprises 4,121 entries distributed across \"COVID-19 vaccine\" (1,373 entries), \"digital transformation\" (1,348 entries), and \"women empowerment\" (1,400 entries). \n\nIt is structured as a multi-label dataset with labels including  stance (Favor, Against, None), sentiment (Positive, Negative, Neutral), and sarcasm (Sarcastic and Non-sarcastic). earth and nature","311":" Sarcasm detection   ","312":" sarcasm-detection-dataset   ","313":" Sarcasm detection   ","314":" Arabic YouTube Comments by Khalaya Arabic YouTube Comments: A Multi-tagged Dataset by Khalaya # YouTube Comments Sentiment Analysis Dataset\n\n###Description\nThis dataset comprises a collection of YouTube comments extracted from videos of a specific channels, \"BidonWaraq\", \"mmr_sa1\", \"POWR-Esports\" and \"thmanyahPodcasts\". It provides a rich source of data for text analysis, sentiment analysis, sarcasm detection, and speech act recognition tasks. Each comment is annotated with several features, making it suitable for a variety of natural language processing (NLP) applications.\n\n### Dataset Content\n1. **title:** The title of the YouTube video.\n2. **comment:** The text of the comment posted on the YouTube video.\n3. **video_id:** The unique identifier for the YouTube video.\n4. **channel_id:** The unique identifier for the YouTube channel.\n5. **sentiment:** The sentiment of the comment (e.g., Positive, Neutral, Negative).\n6. **sarcasm:** A boolean indicating whether the comment is sarcastic.\n7. **speech_act:** The type of speech act (e.g., Assertion, Expression).\n8. **dangerous:** A boolean indicating whether the comment contains dangerous content.\n9. **sentiment_reasoning:** Explanation for the sentiment annotation.\n10. **sarcasm_reasoning:** Explanation for the sarcasm annotation.\n11. **speech_act_reasoning:** Explanation for the speech act annotation.\n12. **channel_name:** The name of the YouTube channel.\n\n### Potential Uses\nThis dataset can be used for:\n\nSentiment analysis to understand the general sentiment of comments towards the video content.\nSarcasm detection to identify sarcastic remarks within the comments.\nSpeech act recognition to categorize comments according to their communicative intent.\nDangerous content detection to filter out or study comments that may contain harmful content.\n\n### Challenges\nThe dataset poses several challenges including but not limited to:\n\nDealing with multilingual text, as comments might be in different languages.\nUnderstanding context for accurate sentiment and sarcasm detection.\nManaging imbalanced classes, especially for less common annotations like sarcasm or dangerous content.\n\n### Acknowledgments\nThis dataset has been collected and annotated by **Khalaya**, aiming to facilitate research and development in natural language processing and machine learning fields. nlp text multitask classification arabic","315":" arabic_Sarcasm_detection  Titre du Projet&nbsp;: La d\u00e9tection du sarcasme en arabe\nObjectifs du Projet :\nLa d\u00e9tection du sarcasme est le processus visant \u00e0 d\u00e9terminer si un morceau de texte est sarcastique ou non. Le sarcasme repr\u00e9sente l'un des principaux d\u00e9fis pour les syst\u00e8mes d'analyse de sentiment. La raison en est qu'une phrase sarcastique porte g\u00e9n\u00e9ralement un sentiment implicite n\u00e9gatif, bien qu'il soit exprim\u00e9 \u00e0 l'aide d'expressions positives. Cette contradiction entre le sentiment apparent et celui intentionnel cr\u00e9e un d\u00e9fi complexe pour les syst\u00e8mes d'analyse de sentiment.\nLa d\u00e9tection du sarcasme a attir\u00e9 l'attention dans d'autres langues, mais l'arabe est encore en retard \u00e0 cet \u00e9gard. Il y a eu peu d'efforts pour la d\u00e9tection du sarcasme en arabe, tels que les travaux de (Karoui et al., 2017 ; Ghanem et al., 2020) et la t\u00e2che partag\u00e9e organis\u00e9e par (Ghanem et al., 2019). Des efforts r\u00e9cents ont \u00e9t\u00e9 d\u00e9ploy\u00e9s pour construire des ensembles de donn\u00e9es standard pour cette t\u00e2che, tels que (Abbes et al., 2020 ; Abu Farha et Magdy, 2020). La t\u00e2che partag\u00e9e sur la d\u00e9tection du sarcasme et du sentiment en arabe se tiendra avec WANLP@EACL2021. Cette t\u00e2che partag\u00e9e se concentrera sur l'analyse des tweets et l'identification de leur sentiment ainsi que la d\u00e9termination de leur caract\u00e8re sarcastique ou non.\nM\u00e9thodologie&nbsp;:\n1. Collecte de Donn\u00e9es : Collecter des donn\u00e9es \u00e0 partir de diff\u00e9rentes plateformes de m\u00e9dias sociaux, en se concentrant sur des sujets sp\u00e9cifiques pertinents.\n2. Pr\u00e9traitement des Donn\u00e9es : Nettoyer et pr\u00e9traiter les donn\u00e9es textuelles pour les rendre compatibles avec les mod\u00e8les de Machine Learning.\n3. Construction d'un Mod\u00e8le de Machine Learning : Mettre en \u0153uvre un mod\u00e8le de Machine Learning adapt\u00e9 \u00e0 l'analyse de sentiments, tel qu'un mod\u00e8le de classification.\n4. Entra\u00eenement du Mod\u00e8le : Entra\u00eener le mod\u00e8le sur un ensemble de donn\u00e9es annot\u00e9 pour la classification des sentiments.\n5. \u00c9valuation du Mod\u00e8le : \u00c9valuer les performances du mod\u00e8le sur un ensemble de test et ajuster les hyper param\u00e8tres si n\u00e9cessaire.\nLivrables Attendus :\n1. Rapport Technique : Document d\u00e9taill\u00e9 d\u00e9crivant chaque \u00e9tape du projet, y compris les choix de conception et les r\u00e9sultats obtenus.\n2. Code Source : Ensemble de scripts, modules, et fichiers n\u00e9cessaires \u00e0 la reproduction du projet.\nCalendrier Pr\u00e9visionnel :\nSemaines 0 : T\u00e9l\u00e9chargement des donn\u00e9es.\n Semaines 1-2 : Uploader les donn\u00e9es T\u00e9l\u00e9charg\u00e9es puis effectuer les pr\u00e9traitements n\u00e9cessaires suivant l\u2019application\n2. Semaines 3-4 : Construction du mod\u00e8le et entra\u00eenement initial.\n3. Semaines 3-4 : \u00c9valuation du mod\u00e8le et ajustements.\n4. Semaines 5-6 : D\u00e9veloppement de l'interface utilisateur.\n5. Semaines 9-10 : Finalisation, r\u00e9daction du rapport, et le d\u00e9p\u00f4t de rapport.\nCrit\u00e8res d'\u00c9valuation :\nLe projet sera \u00e9valu\u00e9 en fonction de la qualit\u00e9 du mod\u00e8le de Machine Learning, de la pr\u00e9cision de l'analyse de sentiments, de la convivialit\u00e9 de l'interface utilisateur, et de la clart\u00e9 du rapport final.\nBaseline_score=33.71\n ","316":" SemEval22 Sarcasm Detection   ","317":" Sarcasm Detection Publication   ","318":" Sarcasm Detection   ","319":" ArSarcasm-V2  ArSarcasm-v2 is provided in a CSV format, we provide the same split that was used for the shared task. The training set contains 12,548 tweets, while the test set contains 3,000 tweets.\n\nThe dataset contains the following fields:\n\n- tweet: the original tweet text.\n- sarcasm: boolean that indicates whether a tweet is sarcastic or not.\n- sentiment: the sentiment of the tweet (positive, negative, neutral).\n- dialect: the dialect used in the tweet, we used the 5 main regions in the Arab world, follows the labels and their - meanings:\n- msa: modern standard Arabic.\n- egypt: the dialect of Egypt and Sudan.\n- levant: the Levantine dialect including Palestine, Jordan, Syria and Lebanon.\n- gulf: the Gulf countries including Saudi Arabia, UAE, Qatar, Bahrain, Yemen, Oman, Iraq and Kuwait.\n- magreb: the North African Arab countries including Algeria, Libya, Tunisia and Morocco.\n\nSource: https:\/\/github.com\/iabufarha\/ArSarcasm-v2\n\n@inproceedings{abufarha-etal-2021-arsarcasm-v2,\ntitle = \"Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic\",\n    author = \"Abu Farha, Ibrahim  and\n    Zaghouani, Wajdi  and\n    Magdy, Walid\",\n    booktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\n    month = april,\n    year = \"2021\",\n    }\n ","320":" Final-Saved-Model   ","321":" Urdu Sarcastic Tweets Dataset Towards Computational Sarcastic Tweets Identification : An Open-Source Dataset  This dataset is part of the research \"Towards Computational Sarcastic Tweets identification: An open-Source Dataset and Developmental Framework\"\n\nIt contains 19955 rows and columns (features), that are, 'urdu_tweets' & 'is_sarcastic' (labels with 0 and 1, where 1 denotes sarcastic tweet ). This dataset can be used for sentiment analysis, specifically for sarcasm detection in urdu tweets.   data visualization data analytics deep learning pandas urdu","322":" 2.5+ Million Rows Egyptian Datasets Collection  # Table of Contents\n\n[Objective](#1)\n\n[Title](#2)\n\n[Description](#3)\n\n[Team Members](#4)\n\n[Websites](#5)\n\n[Pandas Read](#6)\n\n[Notes](#7)\n\n[Egyptian Datasets Description](#8)\n\n- [Format](#9)\n\n- [1.Arabic Online Commentary (AOC)](#10)\n\n- [2.Arabic Egyptian Tweets](#11)\n\n- [3.TaghreedT](#12)\n\n- [4.Topic Extraction Data](#13)\n\n- [5.Habibi Lyrics Corpus](#14)\n\n- [6.Arabic Political Tweets](#15)\n\n- [7.ArabicReddit](#16)\n\n- [8.ar_arz_wiki_corpus](#17)\n\n- [9.QCRI](#18)\n\n- [10.SADID Benchmark Dataset](#19)\n\n- [11.DART](#20)\n\n- [12.Callhome Corpus](#21)\n\n[Other 40+ Arabic Datasets Links](#22)\n\n- [Contact](#23)\n\n- [Papers](#24)\n\n- [License](#25)\n\n# <a name=\"1\">Objective<\/a>\n\nThe datasets were part of our AI & Data Science Master's graduation project sponsored by Microsoft.\n\nCheck all the projects here: [Link](https:\/\/github.com\/Mostafanofal453)\n\n# <a name=\"2\">Title<\/a>\n\nAutomating novel terms and usage detection in Egyptian Arabic dialect\n\n# <a name=\"3\">Description<\/a>\n\nThe project aims at identifying unfamiliar terms relatively new in the Egyptian Arabic dialect. In addition to words that their meaning was changed over time with another context or misspelled to enhance the translation corpus.\n\n# <a name=\"4\">Team Members<\/a>\n\n[Khaled Elsaka](https:\/\/www.linkedin.com\/in\/khaled-el-saka-962700161\/)\n\n[Nada Montasser](https:\/\/www.linkedin.com\/in\/nada-montasser-014b1616b\/)\n\n[Shaimaa Mamdouh](https:\/\/www.linkedin.com\/in\/shaimaa-mamdouh-2616a9159\/)\n\n[Aya Reda](https:\/\/www.linkedin.com\/in\/aya-reda-5202b2191\/)\n\n[Mostafa Nofal](https:\/\/www.linkedin.com\/in\/mostafa-nofal-772625194\/)\n\n---\n\n# <a name=\"5\">Websites<\/a>\n\n We searched for the datasets in the related work papers and the most well-known websites like:\n\n\n* Kaggle\n\n* Github\n\n* Huggingface\n\n* Google Datasets\n\n* ScienceDirect\n\n* ResearchGate\n\n* IEEE Xplore\n\nThe datasets had different formats like XML, JSON, CSV and text so we unified them all to excel files format.\n\n\n\nWe also worked on labeling the datasets with the year label and filtered them to combine our final output.\n\n---\n\n# <a name=\"6\">Pandas Read<\/a>\n\n`import pandas as pd`\n\n`data = pd.read_excel(\"Name.xlsx\")`\n\n# <a name=\"7\">Notes<\/a>\n\n\u2022 All Datasets have 3 columns (Text, Year, Source)\n\n\u2022 Maximum Rows for excel file is 1,048,576 rows\n\n\u2022 If Arabic is encoded in excel:\n           - Create new excel file\n           \n           - In the Data tab, click on (From Text\/CSV) button\n           \n           - Browse and select the excel file\n           \n           - Change the File_origin to \"Unicode (UTF-8)\"\n           \n           - Save\n\n---\n\n# <a name=\"8\">Egyptian Datasets Description<\/a>\n\n## <a name=\"9\">Format<\/a>\n\nEvery dataset has the following format\n\n\u2022 Source Name\n\n\u2022 Year\n\n\u2022 Information\n\n\u2022 Rows Number\n\n\u2022 File Name\n\n---\n\n## <a name=\"10\">1.Arabic Online Commentary (AOC)<\/a>\n\n2010\n\nOnline version of Arabic newspaper: Al-Youm Al-Sabe\u2019.\n\n688,550 rows AOC_youm7_articles (ar)\n\n1,048,576 rows AOC_youm7_comments\n\n333,225 rows RestOf_AOC_youm7_comments\n\n564,854 rows AOC_subtitles_authors\n\n---\n\n## <a name=\"11\">2.Arabic Egyptian Tweets<\/a>\n\n2019\n\nEgyptian tweets cover a blend of different general topics (sentiment analysis)\n\n40,000 rows\n\nEgyptian Tweets\n\n---\n\n## <a name=\"12\">3.TaghreedT<\/a>\n\n2021\n\nEgyptian Dialect Corpus (EDC) from Facebook\n\n13,740 rows\n\nTaghreedT\n\n---\n\n## <a name=\"13\">4.Topic Extraction Data<\/a>\n\n2019\n\nEgyptian dialect tweets used for topic extraction and topic modelling research\n\n2,256 rows TE_News\n\n2,052 rows TE_Sports\n\n2,358 rows TE_Telecom\n\n2,065 rows TE_Tweets\n\n---\n\n## <a name=\"14\">5.Habibi Lyrics Corpus<\/a>\n\n2019\n\nArabic Song Lyrics corpus\n\n139,162 rows\n\nHabibi\n\n---\n\n## <a name=\"15\">6.Arabic Political Tweets<\/a>\n\n2019\n\nTwitter hashtag\n\n431,452 rows\n\nPolitical Tweets\n\n---\n\n## <a name=\"16\">7.ArabicReddit<\/a>\n\n2021\n\nArabic dataset of Reddit titles and comments from Arab and Egypt subreddits\n\n10,129 rows\n\nReddit\n\n---\n\n## <a name=\"17\">8.ar_arz_wiki_corpus<\/a>\n\n2017\n\nArabic (Modern Standard) and Egyptian Arabic dialect comparable documents from Wikipedia\n\n9126 rows\n\nArabic_Egyptian_Wikipedia (ar)\n\n---\n\n## <a name=\"18\">9.QCRI<\/a>\n\n2018\n\nManually segmented and POS tagged tweets\n\n350 rows\n\nQCRI\n\n---\n\n## <a name=\"19\">10.SADID Benchmark Dataset<\/a>\n\n2020\n\nSADID Evaluation Datasets for Low-Resource Spoken Language Machine Translation of Arabic Dialects\n\n8,989 rows\n\nSADID\n\n---\n\n## <a name=\"20\">11.DART<\/a>\n\n2018\n\nA Large Dataset of Dialectal Arabic Tweets (c) 2018 Qatar University\n\n5,889 rows\n\nDART\n\n---\n\n## <a name=\"21\">12.Callhome Corpus<\/a>\n\n2014\n\nEgyptian Arabic Speech Translation Corpus\n\n9,637 rows\n\nCallhome\n\n---\n\n# <a name=\"22\">Other 40+ Arabic Datasets Links<\/a>\n\n## <a name=\"23\">Contact<\/a>\n\nTEAD:\n\nTwo million  Egyptian tweets for sentiment analysis\n\nhttps:\/\/github.com\/HSMAabdellaoui\/TEAD\n\n\nQADI\n\n67,783 egyptian tweets\n\nhttps:\/\/alt.qcri.org\/resources\/qadi\n\n\n2016\n\n5963 words\n\nEgyptian Arabic and Modern Standard Arabic sentiment words and their polarity\n\nhttps:\/\/github.com\/NileTMRG\/NileULex\n\n\nEgyptian-Dialect-Gender-Annotated-Dataset\n\nhttps:\/\/github.com\/shery91\/Egyptian-Dialect-Gender-Annotated-Dataset\n\n\n2020\n\nar_cov19\n\n1M tweets of COVID-19 pandemic include both retweets and conversational threads\n\nhttps:\/\/huggingface.co\/datasets\/ar_cov19\n\n\nArabic Poetry\n\n 55K Arabic poem from different Categories with poets from different countries and era\n\nhttps:\/\/www.kaggle.com\/datasets\/ahmedabelal\/arabic-poetry\n\n\n2016\n\n93,700 hotel reviews in Modern Standard Arabic as well as dialectal Arabic from Booking.com\n\nhttps:\/\/huggingface.co\/datasets\/hard\n\n\n2015\n\n8364 restaurant reviews from qaym.com in Arabic for sentiment analysis\n\nhttps:\/\/huggingface.co\/datasets\/ar_res_reviews\n\n\n2019\n\nArabic Flood Twitter Dataset\n\nhttps:\/\/github.com\/alaa-a-a\/Arabic-Twitter-Corpus-for-Flood-Detection\n\n\n2021\n\nArabizi transliteration\n\nhttps:\/\/github.com\/bashartalafha\/Arabizi-Transliteration\n\n\n2018\n\n15,050 labelled YouTube comments in Arabic\n\nAnti-Social Behaviour in Online Communication\n\nhttps:\/\/onedrive.live.com\/?authkey=!ACDXj_ZNcZPqzy0&id=6EF6951FBF8217F9!105&cid=6EF6951FBF8217F9\n\n\narabic-hatespeech-data\n\nhttps:\/\/github.com\/motazsaad\/arabic-hatespeech-data\/blob\/master\/OSACT4\/README.md\n\n\nArabic Offensive Comments dataset from Multiple Social Media Platforms\n\nhttps:\/\/github.com\/shammur\/Arabic-Offensive-Multi-Platform-SocialMedia-Comment-Dataset\n\n\n2017\n\n75 million of fully vocalized words mainly 97 books from classical and modern Arabic language\n\nhttps:\/\/huggingface.co\/datasets\/tashkeela\n\n\nNADiA:\n\nNews Articles Dataset in Arabic for Multi-Label Text Categorization\nSkyNewsArabia will be referred to as NADiA1, while the latter would be NADiA2. NADiA1 is a big dataset containing 37,445 files, while NADiA2 is a huge dataset that contains 678,563 files. However, after filtering and cleaning we reduced the numbers to 35,416 and 451,230 for NADiA 1 and 2, respectively.\nNews, North Africa, Levant, Middle East, The Americas, Research, Finance & Economy, War & Terrorism, Gulf, Europe, Political Figures, Iran, Technology, Russia, Sports, Tennis, Football, English League, Arabian Sports, Spanish League, Health, East Asia, Environment, Other Countries\n\nhttps:\/\/data.mendeley.com\/datasets\/hhrb7phdyx\/1\n\n\nArabic dialect dictionary  (Django)\nArabic to English or English to Arabic definitions and see results in Levantine, Gulf, or Egyptian\nhttps:\/\/github.com\/moraesc\/arabic-dialect-dict\n\n\n2018\n\nArabic POS Dialect\n\n350 manually segmented and POS tagged tweets for each of four dialects: Egyptian, Levantine, Gulf, and Maghrebi\n\nhttps:\/\/huggingface.co\/datasets\/arabic_pos_dialect\n\n\n2017\n\n10,547 tweets, 1,682 (16%) of which are sarcastic\nArabic sentiment analysis datasets (SemEval 2017 and ASTD) and adds sarcasm and dialect labels to them\n\n{ \"Name\": \"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"2,383\", \"Unit\": \"sentences\" }  and others\n\nhttps:\/\/huggingface.co\/datasets\/ar_sarcasm\n\n\n2021\n\n50K posts\n\nSentiment Analysis for social media posts in Arabic dialect\n\n{ \"Name\":\"Egyptian\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"7,519\", \"Unit\": \"sentences\" }\n\nhttps:\/\/msda.um6p.ma\/msda_datasets\n\n\n2018\n\n { \"Name\": \"EGY\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"4,061\", \"Unit\": \"sentences\" }\n\nhttps:\/\/www.lancaster.ac.uk\/staff\/elhaj\/corpora.html\n\n\n2020 (Register)\n\n{ \"Name\": \"Egypt\", \"Dialect\": \"ar-EG: (Arabic (Egypt))\", \"Volume\": \"6,635\", \"Unit\": \"sentences\" }\n\nhttps:\/\/sites.google.com\/view\/nadi-shared-task\n\n\n2015\n\nASTD: Arabic Sentiment Tweets Dataset\n\n10k Arabic sentiment tweets classified into four classes subjective positive, subjective negative, subjective mixed, and objective \n\nhttps:\/\/github.com\/mahmoudnabil\/ASTD\n\n\n2013\n\nA Large Scale Arabic Book Reviews Dataset\n\n63,000 book reviews in Arabic\n\nhttps:\/\/huggingface.co\/datasets\/labr\n\n\n2017\n\nEmotional-Tone\n\n10065 tweets in Arabic for Emotion detection in Arabic text\n\nhttps:\/\/huggingface.co\/datasets\/emotone_ar\n\n\n2016\n\nBRAD: Books Reviews in Arabic Dataset\n\n510,600 book reviews in Arabic language\n\nhttps:\/\/github.com\/elnagara\/BRAD-Arabic-Dataset\n\n\nDialectal system \n\nhttps:\/\/mt.qcri.org\/api\n\nhttps:\/\/alt.qcri.org\/resources1\/mt\/arabench\n\n2020\n\nCOVID-19-Arabic-Tweets-Dataset\n\n6 milllion\n\nhttps:\/\/github.com\/SarahAlqurashi\/COVID-19-Arabic-Tweets-Dataset\n\n\n2014\n\neducational video subtitles\n\nMachine learning translation\n\nEnglish and arabic\n\nhttps:\/\/huggingface.co\/datasets\/qed_amara\n\n\n2020\n\nQCRI Parallel Tweets\n\nhttps:\/\/huggingface.co\/datasets\/tweets_ar_en_parallel\n\n\n2015\n\nArabic News Article Classification\n\n14 million words with 15,891,729 tokens contained in 57,827 different articles\n\nhttps:\/\/github.com\/saidziani\/Arabic-News-Article-Classification\n\n\n2015\n\nLarge Multi-Domain Resources for Arabic Sentiment Analysis\n\nhttps:\/\/github.com\/hadyelsahar\/large-arabic-sentiment-analysis-resouces\n\n## <a name=\"24\">Papers <\/a>\n\nFreely Available Arabic Corpora: A Scoping Review  (various)\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666990022000015#bib0039\n\n\ncombination of subsets of five corpora: DART, SHAMI, TSAC, PADIC and AOC\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340921010519\n\nNileULex: A Phrase and Word Level Sentiment Lexicon for Egyptian and Modern Standard Arabic\n\nhttps:\/\/aclanthology.org\/L16-1463.pdf\n\nDART: A Large Dataset of Dialectal Arabic Tweets\nL18-1579.pdf\n\nhttps:\/\/aclanthology.org\/L18-1579.pdf\n\n\nColloquial Arabic Tweets: Collection, Automatic Annotation, and Classification \n\nhttps:\/\/ieeexplore.ieee.org\/abstract\/document\/9310507\n\n\nA Multidialectal Parallel Corpus of Arabic\n\nhttps:\/\/aclanthology.org\/L14-1435\/#:~:text=The%20daily%20spoken%20variety%20of,within%20other%20Arabic%20speaking%20communities\n\n\nAutomatic Building of Arabic Multi Dialect Text Corpora by Bootstrapping Dialect Words\n\nhttps:\/\/www.researchgate.net\/publication\/261489194_Automatic_building_of_Arabic_multi_dialect_text_corpora_by_bootstrapping_dialect_words\n\n## <a name=\"25\">License<\/a>\n\n2021\n\n10,828 Arabic tweets annotated with 10 different labels\nmulti-label Arabic COVID-19 fake news and hate speech detection dataset\n\nhttps:\/\/github.com\/MohamedHadjAmeur\/AraCOVID19-MFH\n\n\n2016\n\nArabic Web16\n\n150,211,934 Arabic Web pages with high coverage of dialectal Arabic\n\nhttps:\/\/sites.google.com\/view\/arabicweb16\/\n\n\nCamel Resources\n\nhttps:\/\/docs.google.com\/forms\/d\/e1FAIpQLSfQqhxslVSkBN5ScQ2bvvM0xUVCUnjXxtvkAjupvxm3SSeZGw\/viewform\n\nhttps:\/\/camel.abudhabi.nyu.edu\/madar-parallel-corpus\/\n\n\nBOLT Egyptian Arabic PropBank and Sense -- Discussion Forum, SMS\/Chat, and Conversational Telephone Speech\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/YS81IR\n\n\nBOLT Egyptian-English Word Alignment -- Discussion Forum Training\n400,448 words of Egyptian Arabic and English parallel text enhanced with linguistic tags to indicate word relations\n\nhttps:\/\/abacus.library.ubc.ca\/dataset.xhtml?persistentId=hdl:11272.1\/AB2\/AR1QCS\n\n\nBOLT Egyptian Arabic SMS\/Chat Parallel Training Data\n723,000 tokens of Egyptian Arabic SMS\/Chat data collected for the DARPA BOLT program along with their corresponding English translations.\n\nhttps:\/\/datasetsearch.research.google.com\/search?src=0&query=egyptian%20dialect&docid=L2cvMTFwNjZfMXRieg%3D%3D\n africa social science artificial intelligence data analytics text arabic","323":" MUStARD (Multimodal Sarcasm Detection Dataset) Sarcasm Detection Dataset This is MUStARD (Multimodal Sarcasm Detection Dataset).\nThese are sarcasm data taken from various famous TV Series.\n\nYou can find paper attached with dataset here:\nhttps:\/\/github.com\/soujanyaporia\/MUStARD intermediate text binary classification","324":" Twitter sracastic classification dataset  Tweets data for sarcastic detection & categorical classificaiton of sarcasm This dataset contains a number of tweets classified into sarcastic and further into sub-categories of sarcastic.\n\nIt is very helpful for learning purposes if you are new to NLP you can use this dataset to train and test your models. social networks","325":" Politifact Fact Check Dataset High-quality dataset with 21k fact check statements between 2008 to 2022 ### Context\n\nWe present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https:\/\/www.politifact.com\/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n\nCover photo credits: https:\/\/blog.condati.com\/5-claims-retail-marketing-analytics-fact-check\n\n### Content\nEach record consists of 8 attributes:\n\n* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n* ```statement_originator```: the person who made the statement being fact checked\n* ```statement```:  statement being fact checked\n* ```statement_date```:  the date when statement being fact checked was made\n* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n* ```factchecker```: name of the person who fact checked the claim\n* ```factcheck_date```:  date when the fact checked article was published\n* ```factcheck_analysis_link```:  link to the fact checked analysis article\n\nThe cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https:\/\/gist.github.com\/rishabhmisra\/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n\n### Citation\n\nIf you're using this dataset for your work, please cite the following articles:\n\nCitation in text format:\n```\n1. Misra, Rishabh and Jigyasa Grover. \"Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140\/RG.2.2.29923.22566 (2022).\n```\n\nCitation in BibTex format:\n```\n@incollection{misra2022not,\n  title={Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n  author={Misra, Rishabh and Grover, Jigyasa},\n  booktitle={Deep Learning for Social Media Data Analytics},\n  pages={213--235},\n  year={2022},\n  publisher={Springer}\n}\n@dataset{misra2022politifact,\n  author = {Misra, Rishabh},\n  year = {2022},\n  month = {09},\n  pages = {},\n  title = {Politifact Fact Check Dataset},\n  doi = {10.13140\/RG.2.2.29923.22566}\n}\n```\n\nPlease link to [rishabhmisra.github.io\/publications](https:\/\/rishabhmisra.github.io\/publications\/) as the source of this dataset. Thanks!\n\n### Acknowledgements\n\nThis dataset was collected from [PolitiFact](https:\/\/www.politifact.com\/). \n\n### Inspiration\n\n* Can you categorize facts as true or false?  \n\n* Does the sources of false facts have a temporal pattern?\n\n* Is there a linguistic pattern in false facts?\n\n### Want to contribute your own datasets?\n\nIf you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https:\/\/www.amazon.com\/dp\/B08RN47C5T).\n\n### Other datasets\nPlease also checkout the following datasets collected by me:\n\n* [News Headlines Dataset For Sarcasm Detection](https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection)\n\n* [News Category Dataset](https:\/\/www.kaggle.com\/rmisra\/news-category-dataset)\n\n* [Clothing Fit Dataset for Size Recommendation](https:\/\/www.kaggle.com\/rmisra\/clothing-fit-dataset-for-size-recommendation)\n\n* [IMDB Spoiler Dataset](https:\/\/www.kaggle.com\/rmisra\/imdb-spoiler-dataset)\n earth and nature linguistics software nlp classification deep learning news","326":" Bangla SARC BanglaSarc: A Dataset for Sarcasm Detection  ## Data Article: [BanglaSarc: A Dataset for Sarcasm Detection](https:\/\/arxiv.org\/abs\/2209.13461)\n## Implementation Article: [Interpretable Bangla Sarcasm Detection using BERT and Explainable AI](https:\/\/arxiv.org\/abs\/2303.12772)\n\n## Implementation: [Sarcasm Detection](https:\/\/github.com\/Sakibapon\/SarcasmDetection)\n\n#### **Citation** \n@article{apon2022banglasarc,\n  title={BanglaSarc: A Dataset for Sarcasm Detection},\n  author={Apon, Tasnim Sakib and Anan, Ramisa and Modhu, Elizabeth Antora and Suter, Arjun and Sneha, Ifrit Jamal and Alam, MD},\n  journal={arXiv preprint arXiv:2209.13461},\n  year={2022}\n} text","327":" Labeled Dataset for Sarcasm Detection   earth and nature","328":" Sarcasm Detection Data Set   ","329":" Italian Sarcasm_Detection   ","330":" ISIC melanoma detection  Three classes of lesions:\n\n- Malignant Melanoma: Melanoma, also known as malignant melanoma, is the most common type of skin cancer and arises from pigment-producing cells known as melanocytes. Melanomas typically appear on the skin and rarely in other locations such as the mouth, intestines, or eye.\n\n- Seborrheic Keratosis: Seborrheic keratosis is a non-cancerous (benign) skin tumor that originates in the cells of the outer layer of the skin (keratinocytes), making it a non-melanocytic lesion.\n\n- Benign Nevus (Mole): A benign skin tumor originating from melanocytes (melanocytic).\n\n# Dataset Description\nThe dataset has been obtained from the 'International Skin Imaging Collaboration' (ISIC) archive. It contains 2750 images divided into 3 sets:\n\nTraining Set: 2000 images\n\nValidation Set: 150 images\n\nTest Set: 600 images\n\nFor each clinical case, we have two images available:\n\nDermoscopic image of the lesion (in the 'images' folder).\n\nBinary mask with segmentation between lesion (mole) and skin (in the 'masks' folder).\n\nAdditionally, there is a CSV file for each dataset (training, validation, and test), where each row corresponds to a clinical case, defined with two fields separated by commas:\n\nThe numeric id of the lesion: which allows defining the paths to the files containing the image and the mask.\n\nThe label of the lesion: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1). cancer","331":" Multi-Races Human Body Semantic Segmentation Data  # Multi-Races Human Body Semantic Segmentation Data\n\n\n## Description\n602 People \u20133,010 Images Multi-Races Human Body Semantic Segmentation Data,The data diversity includes headphones, body, background,and glasses.In terms of annotation, we adpoted segmentation annotations on headphones, body, background and glasses.The data can be used for tasks such as human body segmentation and the behavior detection of Video conference.\nFor more details, please refer to the link: https:\/\/www.nexdata.ai\/datasets\/1181?source=Kaggle\n\n## Data size\n602 people, 5 images for each person\n## Collection environment\nOffice, coffee shop, supermarket, apartment\n## Race distribution\n151 Asian people, 151 black people, 150 Caucasians people, 150  brown people ,ranging from teenager to middle-aged people, (Aged between 16 and 60)\n## Gender distribution\n301 males, 301 females\n## Data diversity\ndifferent poses, different ages, different races, different collection backgrounds\n## Device\ncomputer, cellphone\n## Collecting angles\neye-level angle\n## Data format\nthe image data format is .jpg, the annotation file (mask) format is .png\n## Annotation content\nsegmentation annotation of headphones, body, background, glasses\n## Accuracy\nbased on the accuracy of the actions, the accuracy  is more than 97%; Accuracy of semantic segmentation annotation: for each object, the mask edge location errors in x and y directions are less than 5 pixels, and the category label was correctly labeled, which were considered as a qualified annotation; Annotation accuracy: each object is regarded as the unit, annotation accuracy is more than 97%\n# Licensing Information\nCommercial License\n arts and entertainment business eyes and vision","332":" Object Detection Using Adaptive Mask RCNN   ","333":" CVC-ClinicDB Datasets Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/ CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 . education","334":" Parking_Lot_Detection_Counter Computer Vision Project.  Overview\nThis dataset is designed for the task of detecting and counting empty and occupied parking spots in a parking area. It includes images, a mask image, a video, and a utility file (util.py) that provides functions to process these images and video frames.\n\nContents\nImages: The dataset includes images of a parking area under different conditions (e.g., various times of the day). Each image captures the entire parking area and shows the status of each parking spot (empty or occupied).\n\nMask Image: A mask image is provided to identify and locate the parking spots within the parking area. This mask is a binary image where the white regions correspond to the parking spots.\n\nVideo: A video file is included to facilitate real-time detection of parking spot occupancy. The video captures the parking area over a period, showing changes in parking spot status.\n\nUtility File: The util.py file contains helper functions:\n\nempty_or_not: Determines if a given parking spot is empty or occupied.\nget_parking_spots_bboxes: Extracts bounding boxes for each parking spot based on the mask image.\nFiles\nImages Directory:\n\nContains images of the parking area.\nExamples:\nimage_001.jpg\nimage_002.jpg\n...\nMask Image:\n\nmask_1920_1080.png: A binary mask image of size 1920x1080 pixels. The white regions indicate the parking spots.\nVideo File:\n\nparking_1920_1080_loop.mp4: A video file capturing the parking area over a period. This video is used for detecting changes in the occupancy status of parking spots in real-time.\nModel File:\n\nmodel.p: A pre-trained machine learning model used by the empty_or_not function to classify parking spots as empty or occupied.\nUtility File:\n\nutil.py: Contains the utility functions for processing the images and detecting parking spots.\nUsage\nThis dataset can be used for training and testing machine learning models aimed at automating the detection of empty and occupied parking spots. The provided utility functions and pre-trained model facilitate quick experimentation and validation of results.\n\nExample Workflow\nLoad Images: Read the images and the mask image.\nExtract Parking Spots: Use the mask image and get_parking_spots_bboxes function to locate and extract parking spots from each image.\nClassify Spots: Use the empty_or_not function to classify each parking spot as empty or occupied.\nCount Available Spots: Count the number of empty parking spots and display the result.\n\nPotential Applications\nAutomated parking management systems.\nSmart city infrastructure to monitor parking lot occupancy.\nReal-time parking guidance systems for drivers.\n\nInstructions for Kaggle\nTo run the provided code on Kaggle, ensure that all necessary files (images, mask, model, and utility file) are correctly uploaded and paths are set appropriately. Use the provided functions and example code snippets to implement the parking spot detection and counting logic.\n\nBy using this dataset, researchers and developers can develop and test sophisticated parking lot detection algorithms, contributing to more efficient and automated parking management solutions. artificial intelligence automobiles and vehicles intermediate computer vision cv2","335":" Silicone Mask Biometric Attack Dataset Anti spoofing dataset with Silicone 3D mask attacks (7000 videos) # Silicone Mask Biometric Attack Dataset\n\n## This is a demo version, full dataset is coming soon. Share with us your feedback and recieve additional samples for free!\ud83d\ude0a\n\n##Introduction\nThe Silicone Mask Attack Dataset is designed to address security challenges in liveness detection systems through 3D silicone mask attacks. These presentation attacks are key for testing Passive Liveness Detection systems crucial for iBeta Level 2 certification. This dataset significantly enhances the capabilities of liveness detection models \n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2Fe70f55bdfc175f2d96b3dfcfabe4eb11%2FIllustration.jpg?generation=1715795985085319&alt=media)\n\n##Why Silicone Mask Data?\nThis dataset is crucial for companies preparing to comply with iBeta Level 2 certification which requires anti-spoofing technologies. In today's digital security landscape, the Silicone Mask Dataset serves as a critical resource for training Machine Learning (ML) models and advanced biometric techniques to detect spoofing attempts. \n\n##Dataset Features\n\u2022\t**Variety of Masks:** Encompasses 8 unique silicone masks (male and female, Caucasian ans Asian ethnicity)\n\u2022\t**Video Collection:** There are roughly 7,000 videos that showcase detailed spoofing detection scenarios.\n\u2022\t**Helpful for Active liveness:** Head movements and blinking are included in the dataset\n\u2022\t**5 Different shooting angles:** Front Selfie and far\/ close back camera, two side shots available\u200b\n\u2022\t**Capture Devices:** Three different recording devices in selfie mode to mirror real-life conditions (Modern iPhone, Xiaomi and Samsung)\n\u2022\t **Environmental Conditions:** Captures videos across diverse lighting and background settings to ensure robustness.\n\n\u2022 **Additional Flexibility:** We can recreate this dataset using both RGB and USB camera inputs to accommodate various research needs.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n##Technical Specifications\n\u2022\t**File Format:** Videos are formatted to be compatible with mainstream ML frameworks.\n\u2022\t**Resolution and Frame Rate:** Tailored for high-resolution and optimal frame rates to capture quick mask placements.\n\n##Best Uses\nThis dataset is ideal for entities striving to meet or exceed iBeta Level 2 certification. By integrating this dataset, organizations can greatly enhance the training effectiveness of anti-spoofing algorithms, ensuring a robust and accurate performance in practical scenarios.\n\n##Conclusion\nWith its comprehensive features and simulation of real attack scenarios, the Silicone Mask Biometric Attack Dataset is indispensable for anyone involved in developing and certifying facial recognition and liveness detection technologies. Utilize this dataset to strengthen your systems against the most deceptive digital security threats.\n\n people business public safety","336":" Mask Detection (VOC\/YOLO)   ","337":" 3D Mannequin Dataset for Liveness Detection Explore 3D mannequins for anti-spoofing models (1000+ images) ## 3D Mannequin Face Dataset for Liveness Detection (1K+ pictures)\n\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nOur 3D Mannequin Anti-Spoofing Dataset provides a comprehensive collection of mannequin images, optimized for enhancing liveness detection models in face anti-spoofing. Utilizing retail mannequins, this dataset simulates 3D faces, presenting a realistic challenge for spoofing scenarios. By incorporating 3D textures, it significantly improves the capability of anti-spoofing algorithms\n\n## Some Liveness detection SDK do not recognize this attack - here is an example\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F20109613%2F484daafbdd3b29fa87822fb7d54405c1%2FTest%20example.png?generation=1715709935878279&alt=media)\n\n**Why Using Mannequin Data?**\n\nIntegrating 3D mannequin images into training scenarios boosts the effectiveness of anti-spoofing models. By training with varied 3D representations of masks, algorithms enhance their ability to distinguish between genuine and spoofed faces. This training is crucial for increasing the security and reliability of biometric systems.\n\n**Dataset Description:**\n- Scope: Features over 100 mannequins from retail environments.\n- Diversity: Includes female, male, and children mannequins, some sporting natural hair.\n- Image Capture: Utilizes both selfie and frontal camera perspectives.\n- Variations: Encompasses accessories such as glasses, sunglasses, scarves, and hats.\n- Lighting Conditions: Offers a range of lighting scenarios for well-rounded algorithm training.\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n**Best Used For Anti-Spoofing Training:** The dataset\u2019s 3D characteristics elevate the training efficiency of anti-spoofing algorithms, ensuring a more robust learning experience for detection models.\n\n\nKeywords: 3D Mannequin Face Dataset, Liveness Detection Models, Anti-Spoofing, Comprehensive Dataset, Realistic Features, Detection of Genuine Faces, Sophisticated Spoofing Scenarios, Diverse Lighting, Retail Mannequins, Variability in Facial Accessories, Frontal Camera Usage, Enhanced Anti-Spoofing Algorithms, Security in Biometric Systems, Comprehensive Exposure, Facial Recognition Training, Mannequin-Based Anti-Spoofing, 3D Mask Simulation.\n arts and entertainment earth and nature people computer science neural networks","338":" Construction Site Safety Image Dataset Roboflow v2 Modification on the original dataset, where some classes were removed\/renamed. ## Source:\n\n- Derived off the original \"[Construction Site Safety Image Dataset Roboflow](https:\/\/www.kaggle.com\/datasets\/snehilsanyal\/construction-site-safety-image-dataset-roboflow\/data)\" dataset.\n\n## Modifications:\n\n- The original dataset had 10 different classes namely `{0: 'Hardhat', 1: 'Mask', 2: 'NO-Hardhat', 3: 'NO-Mask', 4: 'NO-Safety Vest', 5: 'Person', 6: 'Safety Cone', 7: 'Safety Vest', 8: 'machinery', 9: 'vehicle'}` for object classification after detection.\n- This modified dataset has a total of 6 classes namely `{0: 'Hardhat', 1: 'NO-Hardhat', 2: 'NO-Safety Vest', 3: 'Person', 4: 'Safety Vest', 5: 'NOT-Person'}`.\n- It combines the classes `{6: 'Safety Cone', 8: 'machinery', 9: 'vehicle'}` into a single class named 'NOT-Person' and completely removes the classes `{1: 'Mask', 3: 'NO-Mask'}`. earth and nature","339":" object-detection-case-study-2   ","340":" Lung nodule infused images Infusing Tumor Masks for Enhanced Classification It provides a brief overview of the recent developments in tumor classification approaches, tumor shape and size detection techniques. The aim is to generate imposed images for future classification by fusing the tumor mask with image pixels.\n\n**Refer:** M. Rana and M. Bhushan, \"Effective Tumor Diagnosis based on Shape and Size of Tumor,\" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-6, doi: [10.1109\/ICCCNT56998.2023.10308022](url). health cancer","341":" Liveness detection dataset: Cutout 2D attacks Face Spoofing dataset Cutout Mask: 1,5K+ participants  ## Cutout Photo Print attack dataset (1,5K individuals+) for Presentation Attack Detection level 1 (PAD)\n## Contact us and share your feedback - recieve additional samples for free!\ud83d\ude0a\n\n\nThis dataset focuses on cutout photo print attacks which might be used by iBeta and NIST FATE to assess liveness detection algorithms. This dataset is tailored for training AI models to identify a variation of cutout 2D print attack. \n\n\n**Dataset Description**\n- 1,500+ Participants: Engaged in the project\n- Diverse Representation: Balanced mix of genders and ethnicities\n- 1,500+ Cutout Mask Attacks on the participants\n\n**Photo Print attack description**\n- Each attack comprises of 10-15 sec. video \n- High-quality cutout photos with realistic colors\n- Paper attacks conducted on flat photos with a straight view on the camera (not bent or skewed)\n\n## Full version of dataset is availible for commercial usage - leave a request on our website [Axonlabs ](https:\/\/axonlabs.pro\/?utm_source=Kaggle&utm_medium=Account&utm_campaign=Account&utm_id=1)to purchase the dataset \ud83d\udcb0\n\n\n## Potential Use Cases:\n\nLiveness detection: This dataset is ideal for training and evaluating liveness detection models, enabling researchers to distinguish between selfies and photo cutout print mask with high accuracy\n\nKeywords: \nCutout Print photo attack dataset, Antispoofing for AI, Liveness Detection dataset for AI, Spoof Detection dataset, Facial Recognition dataset, Biometric Authentication dataset, AI Dataset, PAD Attack Dataset, Anti-Spoofing Technology, Facial Biometrics, Machine Learning Dataset, Deep Learning\n\n image","342":" CVC-ClinicDB Datasets Datasets from https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/ CVC-ClinicDB database consists of two different types of images:\n1) Original images: original\/frame_number.tiff\n2) Polyp mask: ground truth\/frame_number.tiff\n\nCVC-ClinicDB is the official database to be used in the training stages of MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos .\n\n**Download link**\nYou can download CVC-ClinicDB database from the following link: CVC-ClinicDB.rar\n\n**Copyright**\nImages from folder \u2018Original\u2019 are property of Hospital Clinic, Barcelona, Spain\nImages from folder \u2018Ground Truth\u2019 are propery of Computer Vision Center, Barcelona, Spain\n\n**Referencing**\nThe use of this database is completely restricted for research and educational purposes. The use of this database is forbidden for commercial purposes.\n\nIf you use this database for your experiments please include the following reference:\n\nBernal, J., S\u00e1nchez, F. J., Fern\u00e1ndez-Esparrach, G., Gil, D., Rodr\u00edguez, C., & Vilari\u00f1o, F. (2015). WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized Medical Imaging and Graphics, 43, 99-111 . education","343":" COVID-19 & the virus that causes it: SARS-CoV-2. Are Human Coronaviruses Tranransmissible by Air & Is the COVID-19 Vaccine Safe ? **Introduction:**\n\nCoronaviruses, a family of enveloped RNA viruses, have long captivated the interest of scientists and public health experts due to their ability to cause a wide range of diseases in animals and humans. The ongoing COVID-19 pandemic has underscored the significant impact coronaviruses can have on global health and economy. In this article, we delve into the rich history of coronaviruses, tracing their origins, evolution, and the impact they have had on human populations.\n\nInfluenza viruses belong to a different family called Orthomyxoviridae. While both influenza viruses and coronaviruses can cause respiratory illnesses, they are distinct types of viruses with different genetic structures, modes of transmission, and clinical presentations.\n\nInfluenza viruses are characterised by segmented RNA genomes, and they are further classified into types A, B, and C. Influenza A viruses are known to infect a wide range of animals, including birds and mammals, whereas influenza B and C viruses primarily infect humans.\n\nCoronaviruses, on the other hand, belong to the family Coronaviridae and are characterised by their single-stranded RNA genomes. Coronaviruses are named for the crown-like spikes that protrude from their surface under electron microscopy. They can cause illnesses ranging from the common cold to more severe diseases such as severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), and COVID-19.\n\nWhile both influenza viruses and coronaviruses are respiratory viruses that can lead to similar symptoms, such as fever, cough, and respiratory distress, they are genetically distinct and require different approaches for prevention, diagnosis, and treatment.\n\nAnother project of mine is entitled *FluNet, Global Influenza Programme - WHO.* Which looks at the importance of monitoring the global uptrend of influenza. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/flunet-global-influenza-programme-who)\n\n**Origins and Discovery:**\n\nCoronaviruses were first identified in the mid-20th century. The term \"coronavirus\" originates from the Latin word \"corona,\" meaning crown or halo, referring to the characteristic appearance of the virus under electron microscopy, with spike proteins protruding from its surface. The first coronavirus, infectious bronchitis virus (IBV), was discovered in chickens in the 1930s. However, it wasn't until the 1960s that human coronaviruses were identified.\n\n**Human Coronaviruses:**\n\nThe first human coronaviruses, HCoV-229E and HCoV-OC43, were identified in the 1960s and were primarily associated with mild respiratory illnesses such as the common cold. These early discoveries led to further investigations into the diversity and pathogenicity of coronaviruses. Subsequently, other human coronaviruses, including HCoV-NL63 and HCoV-HKU1, were identified.\n\nA table summarising the key information about known human coronaviruses, in chronological order of discovery. [link](https:\/\/docs.google.com\/document\/d\/1CqXfzEVTmBWFpkAfww51zSmZ6_8ZMdktnnt5kQzekzE\/edit?usp=sharing)\n\n**Emergence of Severe Acute Respiratory Syndrome (SARS):**\n\nThe emergence of Severe Acute Respiratory Syndrome Coronavirus (SARS-CoV) in 2002 marked a significant turning point in the understanding of coronaviruses. SARS-CoV originated in bats and likely crossed into humans through intermediate hosts such as palm civets in wet markets in China. The virus caused severe respiratory illness with high mortality rates, leading to a global outbreak that affected over 8,000 people in 29 countries. The SARS outbreak highlighted the potential of coronaviruses to cause severe and sometimes fatal diseases in humans.\n\n**Middle East Respiratory Syndrome (MERS):**\n\nIn 2012, another novel coronavirus, Middle East Respiratory Syndrome Coronavirus (MERS-CoV), emerged in the Arabian Peninsula. MERS-CoV is believed to have originated in bats and transmitted to humans through dromedary camels. MERS-CoV causes severe respiratory illness with a high fatality rate, particularly in individuals with underlying health conditions. While the spread of MERS-CoV has been limited compared to SARS-CoV, sporadic outbreaks continue to occur in the Middle East.\n\n**COVID-19 Pandemic:**\n\nThe most recent and devastating coronavirus pandemic began in late 2019 when a novel coronavirus, SARS-CoV-2, emerged in Wuhan, China. The virus quickly spread globally, leading to the declaration of a pandemic by the World Health Organisation in March 2020. COVID-19, the disease caused by SARS-CoV-2, has resulted in millions of infections and deaths worldwide, overwhelmed healthcare systems, and caused profound social and economic disruptions. The rapid spread of SARS-CoV-2 has been facilitated by its being highly transmissible, including transmission from asymptomatic individuals, as well as factors such as global travel and urbanisation. While most individuals experience mild to moderate symptoms, COVID-19 can lead to severe respiratory illness, acute respiratory distress syndrome (ARDS), multi-organ failure, and death, particularly in older adults and those with underlying health conditions.\n\n**COVID-19 Origin:**\n\nThe origins of COVID-19, caused by the SARS-CoV-2 virus, remain a subject of ongoing investigation and debate among scientists, public health experts, and policymakers. Several hypotheses have been proposed regarding the emergence of the virus, including zoonotic spillover from animals to humans, laboratory-related incidents, and other scenarios. Here are some key points regarding the latest developments on the origin of COVID-19:\n- Zoonotic Spillover:\n  - The leading hypothesis is that SARS-CoV-2 originated in bats and may have been transmitted to humans through an intermediate host, possibly in a wet market in Wuhan, China, where live animals were sold.\n  - Research indicates that bats harbour a diverse array of coronaviruses, some of which are closely related to SARS-CoV-2. However, the exact intermediate host species and the circumstances of transmission to humans remain uncertain. \n  - The hypothesis of recombination between viruses harboured by bats and pangolins, leading to the emergence of SARS-CoV-2, is compelling. The observed genomic concordance in specific regions, particularly in the ACE (Angiotensin Converting Enzyme 2) receptor binding domain crucial for viral entry into human cells, suggests a potential role for pangolins as intermediate hosts in the transmission of the virus to humans. However, further research is needed to confirm this hypothesis and elucidate the exact mechanisms and conditions under which such recombination events occurred. [link](https:\/\/www.weforum.org\/agenda\/2020\/03\/coronavirus-origins-genome-analysis-covid19-data-science-bats-pangolins\/)\n- Laboratory-related Incident:\n  - Another hypothesis suggests the possibility of a laboratory-related incident, such as accidental release from a research laboratory studying coronaviruses. This theory has gained attention due to concerns about bio-safety practices and the proximity of the Wuhan Institute of Virology (WIV) to the initial COVID-19 outbreak.\n  - However, investigations into this hypothesis have been hampered by limited access to relevant data, samples, and facilities in China, as well as geopolitical tensions and lack of international cooperation.\n- Ongoing Investigations:\n  - The World Health Organisation (WHO) conducted a joint study with China in early 2021 to investigate the origins of COVID-19. The study concluded that zoonotic spillover was the most likely scenario and that a laboratory-related incident was \"extremely unlikely.\" However, the study was criticised for its limited access to data and the need for further investigation. [link](https:\/\/www.thelancet.com\/journals\/lanmic\/article\/PIIS2666-5247(23)00074-5\/fulltext)\n  - The WHO has called for additional studies, transparency, and collaboration to better understand the origins of the virus. In May 2021, the WHO proposed a second phase of studies, including audits of relevant laboratories and markets in Wuhan.\n  - BEIJING, July 22 (Reuters) - China rejected on Thursday a World Health Organisation (WHO) plan for a second phase of an investigation into the origin of the coronavirus, which includes the hypothesis it could have escaped from a Chinese laboratory, a top health official said. [link](https:\/\/www.reuters.com\/world\/china\/china-will-not-follow-whos-suggested-plan-2nd-phase-covid-19-origins-study-2021-07-22\/)\n- International Calls for Investigation:\n  - The origins of COVID-19 have sparked international debate and calls for independent, transparent, and comprehensive investigations. Some countries, including the United States, have called for further inquiry into the possibility of a laboratory-related incident and have urged China to cooperate fully with international investigations.\n  - Efforts to investigate the origins of COVID-19 have been complicated by geopolitical tensions, lack of cooperation, and challenges in accessing relevant data and facilities in China.\n\nIn summary, the origins of COVID-19 remain a complex and contentious issue, and investigations into the virus's origins are ongoing. While the zoonotic spillover hypothesis remains the most widely accepted explanation, questions and uncertainties persist, highlighting the need for continued research, collaboration, and transparency to better understand the origins of the pandemic and prevent future outbreaks. Although zoonotic diseases have become increasingly prominent in modern health agendas, historical zoonoses have received scant attention, despite the potential importance of earlier transmission events in shaping past and present health landscapes. [link](https:\/\/www.cell.com\/current-biology\/fulltext\/S0960-9822(24)00446-9)\n\n**Long COVID:**\n\n\"Long COVID,\" also known as post-acute sequelae of SARS-CoV-2 infection (PASC), refers to a range of persistent symptoms that continue for weeks or months after the acute phase of COVID-19 has resolved. While many individuals recover from COVID-19 within a few weeks, a significant proportion experience lingering symptoms that can significantly impact their quality of life and ability to function normally.\n\nSymptoms of long COVID can vary widely among individuals and may affect multiple organ systems. Common symptoms include:\n- Fatigue: Persistent feelings of exhaustion or lack of energy, even after minimal physical or mental exertion.\n- Shortness of breath: Difficulty breathing or shortness of breath, particularly with exertion.\n- Cognitive impairment: Difficulty concentrating, memory problems, \"brain fog,\" or other cognitive deficits.\n- Muscle and joint pain: Persistent muscle aches, joint pain, or weakness.\n- Headaches: Recurring headaches or migraines.\n- Chest pain: Persistent chest discomfort or tightness.\n- Loss of smell or taste: Changes in the sense of smell or taste that persist beyond the acute phase of illness.\n- Heart palpitations: Awareness of irregular or rapid heartbeat.\n- Gastrointestinal symptoms: Digestive issues such as diarrhoea, nausea, or abdominal pain.\n- Sleep disturbances: Insomnia, disrupted sleep patterns, or excessive daytime sleepiness.\n\nThe exact mechanisms underlying long COVID are not fully understood, but several factors may contribute to its development. These include persistent inflammation, immune dysregulation, organ damage, neurological effects, and psychological factors such as stress and anxiety.\n\nLong COVID can affect individuals of all ages, including those who had mild or asymptomatic COVID-19 infections initially. Risk factors for developing long COVID may include the severity of the initial illness, pre-existing health conditions, age, and genetic predisposition.\n\nManaging long COVID requires a multidisciplinary approach tailored to individual symptoms and needs. Treatment strategies may include medications to alleviate specific symptoms, such as pain relievers, anti-inflammatory drugs, or medications to manage cardiovascular or respiratory symptoms. Rehabilitation therapies, including physical therapy, occupational therapy, and cognitive-behavioural therapy, may also be beneficial in improving functional abilities and quality of life.\n\nSupportive care, such as adequate rest, hydration, nutrition, and stress management, is essential for promoting recovery. Additionally, ongoing monitoring and follow-up with healthcare providers are crucial to identify and address any new or worsening symptoms and to adjust treatment plans as needed. Understanding the burden of long COVID. [link](https:\/\/impact.economist.com\/perspectives\/health\/incomplete-picture-understanding-burden-long-covid?utm_source=facebook&utm_medium=paid-ei-social&utm_campaign=Pfizer-Long-Covid-2024&utm_term=Image-A&utm_content=ei&fbclid=IwAR0AwyiIMiuAN0p0Z-K344o0uReI1pPhfbCW4oyLKEdLuaLSwdhgJkg0Tmk_aem_AW5RNhV355tdLnvhrLUmUAm6hpJelWa4EVaEwe7xo_5u3vIk00bFaGswm8zj92sUU3QeSd2-MYhCdHMxb3Lm1u-0)\n\nResearch into long COVID is ongoing, with efforts focused on better understanding its underlying mechanisms, identifying risk factors, and developing effective interventions to support recovery and improve outcomes for affected individuals. As our understanding of long COVID continues to evolve, healthcare professionals and public health authorities are working to raise awareness, improve access to care, and provide support for individuals living with this complex and challenging condition. [link](https:\/\/www.news-medical.net\/news\/20240218\/New-study-pinpoints-key-markers-for-Long-COVID-diagnosis.aspx)\n\n**Excess Deaths:**\n\nExcess deaths, in the context of public health, refer to the difference between the number of deaths actually observed in a specific period and the number of deaths expected during that same period under normal circumstances.\nHere's a breakdown:\n- What are \"normal circumstances\"?\n  - This usually refers to historical data, like the average number of deaths observed in the same period during previous years (e.g., 2019 for pre-pandemic data).\n- How is it measured?\n  - You compare the actual number of deaths to the estimated \"expected\" number of deaths based on past trends.\n  - The difference between these two figures represents the excess deaths.\n  - This can be expressed as a number or a percentage.\n- Why is it important?\n  - Excess deaths provide a broader picture of mortality beyond deaths directly attributed to a specific cause, like a pandemic or natural disaster.\n  - It can capture indirect deaths caused by disruptions to healthcare systems, changes in behaviour (e.g., less access to healthcare), or worsening chronic conditions due to stress or lack of resources.\n  - This information helps public health officials understand the overall impact of an event and guide resource allocation and interventions.\n- Examples:\n  - During the COVID-19 pandemic, excess mortality figures were significantly higher than reported COVID-19 deaths, highlighting the indirect effects of the pandemic on healthcare systems and individuals' health.\n  - Heatwaves or natural disasters can lead to excess deaths due to heatstroke, injuries, and disruptions to essential services.\n- Additional points:\n  - Calculating excess deaths can be complex due to data limitations and the need to account for seasonal variations and other factors.\n  - Different organisations may use slightly different methods to estimate expected deaths, leading to variations in reported excess death figures.\n  - It is crucial to interpret excess death data with caution and consider other relevant information for a comprehensive understanding.\n\n**Data Visualisations.**\n\n**Total COVID -19 Cases & Deaths Over Time, Distribution of Excess Deaths & Excess Deaths in Individuals Aged 65 and Older Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F34b130a7bf221017f22928be73666efe%2FScreenshot%202024-02-13%2020.35.58.png.jpg?generation=1708960518243773&alt=media)\nA Markdown document with the R code for the 4 above plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148518)\n\nCharts:\n- Total COVID Cases Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of cases in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 703 million confirmed cases reported worldwide.\n- Total COVID Deaths Over Time:\n  - This plot shows a cumulative increase in the number of confirmed COVID-19 deaths reported globally since the beginning of the pandemic in early 2020.\n  - The y-axis shows the number of deaths in millions, while the x-axis shows the year.\n  - As of February 25, 2024, there have been over 6.9 million confirmed deaths reported worldwide.\n- Distribution of Excess Deaths:\n  - This plot shows the distribution of excess deaths, which are deaths that are above the expected number for a specific time period, across different age groups.\n  - The y-axis shows the frequency of deaths, while the x-axis shows the age group.\n  - It is important to note that this plot does not necessarily show COVID-19 deaths specifically, but rather all excess deaths, which could be caused by a variety of factors.\n- Excess Deaths in Individuals Aged 65 and Older Over Time:\n  - This plot shows the number of excess deaths in individuals aged 65 and older over time.\n  - The y-axis shows the number of deaths, while the x-axis shows the year.\n  - The plot shows that the number of excess deaths in this age group has been increasing since the beginning of the pandemic.\n\nIt is important to note that these graphs are just a snapshot of the COVID-19 pandemic. The pandemic is a complex and constantly evolving situation, and the data is constantly changing. However, these graphs can help us to understand some of the general trends of the pandemic in the world.\n\n**Excess Deaths by Country & Age: 2020-2023, from the data - ED.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1b516bc43f75831fad46763a85986d1a%2FScreenshot%202024-02-23%2015.58.26.png?generation=1708704678274541&alt=media)\n\nA Markdown document with the R code for the above plot from the data: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152620)\n\nA Markdown document with the R code for data examination for the data set: ED.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1154622)\n\nA document that explains the R code for data examination: ED.csv - [link](https:\/\/docs.google.com\/document\/d\/1VLKPWwDHsteCZNfq4Md7bU6zB-_OoooEEX_iFMLW1nA\/edit?usp=sharing)\n\nThe x-axis of the chart shows the year, from 2020 to 2023. The y-axis shows the number of excess deaths. The lines on the chart show the number of excess deaths in each country for each age group. The different colours represent different age groups:\n- 0 to 44 years old (red).\n- 45 to 64 years old (green).\n- 65 and over (blue).\n\nThe chart shows that there have been excess deaths in all of the countries shown, for all age groups. However, the number of excess deaths varies by country and age group. For example, there have been more excess deaths in the United States than in any other country shown. There have also been more excess deaths among people aged 65 and over than in any other age group.\n\nIt is important to note that this chart does not show the total number of deaths in each country. It only shows the number of deaths that are above what would be expected based on historical data. This means that the chart may not give a complete picture of the mortality situation in each country. [link](https:\/\/ourworldindata.org\/excess-mortality-covid)\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F1aa1750245dba7bec23d8c674f15668d%2FScreenshot%202024-02-14%2013.49.16.png?generation=1708522039619398&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148820)\n\n- The chart is a line chart that shows the trends of cardiovascular death rate and diabetes prevalence in the data from 2020 to 2024:\n  - The x-axis represents the date, and the y-axis represents the rate or prevalence. \n  - The two lines in the chart represent the two variables: the blue line represents cardiovascular death rate per 100,000 population, and the orange line represents diabetes prevalence in percentage of the population.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ff7ba44839fc7ffa73d9111db5437ab33%2FScreenshot%202024-02-21%2013.31.03.png?generation=1708522442795050&alt=media)\n\n- The tibble output summarises the data for each variable: \n  - The first column, \"variable\", shows the name of the variable. \n  - The second column, \"mean_value\", shows the mean value of the variable. \n  - The third column, \"median_value\", shows the median value of the variable. \n  - The fourth column, \"sd_value\", shows the standard deviation of the variable. \n  - The fifth column, \"min_value\", shows the minimum value of the variable. \n  - The sixth column, \"max_value\", shows the maximum value of the variable.\n\nAs you can see from the chart, both cardiovascular death rate and diabetes prevalence have been increasing over time. However, the rate of increase has been slowing down in recent years (this trend is easier to see in the below chart). The tibble output confirms this trend, as the mean and median values for both variables are higher in 2024 than in 2020. However, the standard deviation is also higher in 2024, which suggests that there is more variability in the data.\n\n**Cardiovascular Death Rate and Diabetes Prevalence Over 65 yrs Old: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Ffd99edf0e358b4ae436ba6448f38cb0c%2FScreenshot%202024-02-14%2012.49.54.png?generation=1708524151064064&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1148809)\n\n- For the \"Cardiovascular Death Rate\" plot:\n  - The y-axis scale ranges from 0 to 750, as specified in the `coord_cartesian()` function.\n  - This scale represents the cardiovascular death rate per 100,000 population. For example, if the y-axis value is 250, it means there were 250 cardiovascular deaths per 100,000 population at that specific time point.\n- For the \"Diabetes Prevalence\" plot:\n  - Since I haven't explicitly set the y-axis limits for this plot, it will use the same scale as the \"Cardiovascular Death Rate\" plot.\n  - However, based on the dataset (covid.csv), the maximum diabetes prevalence is around 30.53, so the y-axis scale will adjust accordingly.\n  - This scale represents the percentage of individuals aged 65 and older who have diabetes. For example, if the y-axis value is 10, it means that 10% of the population aged 65 and older have diabetes at that specific time point.\n\n**Cardiovascular Death Rate and Diabetes Prevalence All Ages: 2020-2024, from the data - covid.csv**\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F59fd6daaca76fc6bedaf3938d17c3cf8%2FScreenshot%202024-02-22%2014.03.05.png?generation=1708611458450648&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1152194)\n\nChart:\n- The cardiovascular death rate plot shows a downward trend from 2020 to 2023. Then, there is a slight upward trend in 2024. \n- The diabetes prevalence plot shows an upward trend from 2020 to 2024.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F317a37af3a70f4ac76f9ca31ab50bbe8%2FScreenshot%202024-02-22%2014.25.38.png?generation=1708612095161435&alt=media)\n\nTibble Output:\n- Cardiovascular Death Rate: Has a higher average (mean) and median value compared to diabetes prevalence, indicating a generally higher death rate.\n- Cardiovascular Death Rate: Shows a much larger standard deviation compared to diabetes prevalence. This suggests greater variability in cardiovascular death rates across the observations.\n- Cardiovascular Death Rate: Exhibits a notably wider range (difference between minimum and maximum) compared to diabetes prevalence. The presence of extreme values in the cardiovascular data is influencing this result.\n\n**COVID-19 Transmission:**\n\nCoronaviruses, like many respiratory viruses, can be transmitted through various routes, including respiratory droplets, aerosols, and contact with contaminated surfaces. However, not all coronaviruses are primarily airborne. The mode of transmission can vary depending on the specific virus strain, its characteristics, and the environmental conditions: [link](https:\/\/www.nature.com\/articles\/s41467-020-19393-6)\n- Respiratory Droplets: \n  - Many coronaviruses, including the common human coronaviruses (e.g., HCoV-229E, HCoV-OC43, HCoV-NL63, and HCoV-HKU1) and the viruses responsible for severe acute respiratory syndrome (SARS-CoV), Middle East respiratory syndrome (MERS-CoV), and COVID-19 (SARS-CoV-2), are primarily spread through respiratory droplets produced when an infected person coughs, sneezes, talks, or breathes. These droplets can travel through the air over short distances and can be inhaled by individuals nearby, leading to infection.\n- Aerosol Transmission: \n  - Some coronaviruses, particularly COVID-19 (SARS-CoV-2), have been shown to spread through aerosols, which are smaller respiratory particles that can remain suspended in the air for longer periods and travel farther distances. Aerosol transmission may occur in enclosed or poorly ventilated spaces, especially in settings where there is prolonged exposure to respiratory emissions from infected individuals. Aerosol transmission has been implicated in certain outbreaks, particularly in indoor environments such as healthcare facilities, workplaces, and crowded gatherings.\n- Contact Transmission: \n  - Coronaviruses can also be transmitted through direct contact with respiratory secretions from infected individuals or indirect contact with contaminated surfaces or objects. When a person touches a surface or object contaminated with the virus and then touches their mouth, nose, or eyes, they can become infected. Proper hand hygiene and surface disinfection are essential for reducing the risk of contact transmission.\n\nWhile respiratory droplets and aerosols are important modes of transmission for many coronaviruses, including COVID-19 (SARS-CoV-2), the relative contribution of each route to overall transmission may vary depending on factors such as viral load, infectiousness, environmental conditions, and human behaviour. Public health measures, such as wearing masks, maintaining physical distance, improving ventilation, and practising hand hygiene, are critical for reducing the spread of coronaviruses and mitigating the risk of infection. [link](https:\/\/www.ecdc.europa.eu\/en\/infectious-disease-topics\/z-disease-list\/covid-19\/facts\/transmission-covid-19)\n\n**COVID-19: Airborne Transmission.**\n\nThe understanding that COVID-19 can be transmitted through airborne particles evolved over time as scientists conducted research, gathered evidence, and analysed epidemiological data. Here are key milestones in the recognition of airborne transmission of COVID-19:\n- Early Recognition of Respiratory Transmission: \n  - In the early stages of the COVID-19 pandemic, it became evident that the virus primarily spread through respiratory droplets produced when an infected person coughed, sneezed, talked, or breathed. Public health guidance emphasised measures such as wearing masks, physical distancing, and hand hygiene to reduce the risk of droplet transmission.\n- Emerging Evidence of Airborne Transmission: \n  - Throughout 2020, researchers began to accumulate evidence suggesting that SARS-CoV-2 could also be transmitted through aerosols, smaller respiratory particles that can remain suspended in the air for longer periods. Studies documented instances of COVID-19 transmission in indoor settings, including poorly ventilated spaces, where aerosols could play a role.\n- Scientific Studies and Investigations: \n  - Researchers conducted laboratory experiments and field studies to investigate the behaviour of SARS-CoV-2 in aerosols and to assess the risk of airborne transmission in different environments. These studies provided evidence of virus-containing aerosols in indoor air and highlighted the potential for airborne transmission, particularly in enclosed spaces with poor ventilation.\n- Expert Consensus and Updated Guidance: \n  - As scientific understanding of COVID-19 transmission evolved, leading public health organisations, including the World Health Organisation (WHO), the Centres for Disease Control and Prevention (CDC), and other health authorities, revised their guidance to acknowledge the role of airborne transmission. Recommendations for ventilation, air filtration, and other measures to reduce indoor airborne transmission were emphasised.\n- Recognition by Health Authorities: \n  - In July 2021, Chinese scientists published a study in the journal Environment International. Providing evidence supporting the airborne transmission of SARS-CoV-2. The study, titled \"Airborne transmission of COVID-19 in indoor environments,\" highlighted the potential for airborne transmission of the virus, particularly in enclosed and poorly ventilated spaces. This acknowledgement of airborne transmission by Chinese scientists was an important development in our understanding of how COVID-19 spreads and informed public health measures to mitigate transmission risks.\n  - In July 2021, the WHO issued a scientific brief acknowledging the possibility of airborne transmission of SARS-CoV-2 in certain circumstances, particularly in crowded and inadequately ventilated indoor settings. The CDC also updated its guidance to highlight the importance of indoor air quality and ventilation in mitigating the risk of COVID-19 transmission.\n\nIt's worth noting that while there may have been earlier indications and scientific discussions regarding the potential for airborne transmission of COVID-19, the formal acknowledgement and recognition of airborne transmission by health authorities, including those in China, occurred as scientific evidence accumulated and understanding of the virus evolved.\n\nOverall, the recognition of airborne transmission of COVID-19 was a gradual process, informed by scientific research, epidemiological investigations, and expert consensus. As our understanding of the virus continues to evolve, ongoing research and surveillance efforts are essential for refining public health strategies and minimising the spread of COVID-19.\n\n**The Effects of Temperature and Humidity on Transmission:**\n\nThe role of temperature and humidity in the transmission of COVID-19 has been a subject of scientific investigation since the early stages of the pandemic. While these environmental factors can influence the stability of the virus and the dynamics of transmission, their precise impact is complex and multifaceted:\n- Temperature:\n  - Laboratory studies have shown that SARS-CoV-2, the virus that causes COVID-19, can survive for varying lengths of time on different surfaces and in different environmental conditions. Generally, higher temperatures have been associated with shorter survival times of the virus on surfaces.\n  - In outdoor environments, higher temperatures may reduce the viability of respiratory droplets and aerosols containing the virus, potentially decreasing the risk of transmission. However, it's important to note that outdoor transmission can still occur, particularly in crowded or close-contact settings.\n  - Some studies have suggested a possible seasonal variation in COVID-19 transmission, with higher transmission rates observed during colder months in certain regions. However, the extent to which temperature directly influences transmission dynamics remains uncertain, as other factors such as human behaviour, indoor crowding, and public health interventions also play significant roles.\n- Humidity:\n  - Humidity levels can also affect the stability and transmission of respiratory viruses like SARS-CoV-2. Low humidity levels may lead to drier conditions, which can help respiratory droplets and aerosols remain suspended in the air for longer periods, potentially increasing the risk of airborne transmission.\n  - On the other hand, higher humidity levels can cause respiratory droplets to settle more quickly, reducing the risk of airborne transmission. Additionally, some studies suggest that higher humidity levels may also affect the viability of the virus on surfaces, potentially reducing its persistence.\n   - Like temperature, the relationship between humidity and COVID-19 transmission is complex and may vary depending on other factors such as ventilation, population density, and public health measures.\n\nOverall, while temperature and humidity can influence the transmission of COVID-19 to some extent, they are just two of many factors that contribute to the dynamics of the pandemic. Public health interventions such as mask-wearing, physical distancing, vaccination, testing, contact tracing, and ventilation are crucial for controlling the spread of the virus regardless of environmental conditions. Additionally, ongoing research is needed to better understand the interplay between environmental factors and COVID-19 transmission and to inform effective strategies for mitigating the impact of the pandemic. \n\n**Environmental Factors Influencing COVID-19 Incidence and Severity:**\n\nEmerging evidence supports a link between environmental factors\u2014including air pollution and chemical exposures, climate, and the built environment\u2014and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission and coronavirus disease 2019 (COVID-19) susceptibility and severity. Climate, air pollution, and the built environment have long been recognised to influence viral respiratory infections, and studies have established similar associations with COVID-19 outcomes. More limited evidence links chemical exposures to COVID-19. Environmental factors were found to influence COVID-19 through four major interlinking mechanisms: increased risk of preexisting conditions associated with disease severity; immune system impairment; viral survival and transport; and behaviours that increase viral exposure. Both data and methodologic issues complicate the investigation of these relationships, including reliance on coarse COVID-19 surveillance data; gaps in mechanistic studies; and the predominance of ecological designs. We evaluate the strength of evidence for environment\u2013COVID-19 relationships and discuss environmental actions that might simultaneously address the COVID-19 pandemic, environmental determinants of health, and health disparities. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10044492\/#:~:text=We%20propose%20that%20environmental%20factors,response%20modification%2C%20(c)%20regulating)\n\nCarbon dioxide has a vital role in determining the lifespan of airborne viruses like SARS-CoV-2, the virus that caused COVID-19, a new study has revealed. The Nature Communications study also highlights the importance of keeping tabs on  CO\u2082 levels to reduce virus survival and minimise the risk of infection. [link](https:\/\/www.nature.com\/articles\/s41467-024-47777-5)\n\nA previous project of mine entitled *Global CO\u2082 Emissions*. [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/global-co-emissions)\n\n**Viral Load:** [link](https:\/\/www.webmd.com\/covid\/covid-viral-load)\n- The Quantification of Infection: Viral load provides a more tangible metric than simply testing positive for COVID-19. It helps track how much virus is actively replicating within a person's system.\n- Disease Severity \u2013 It's Not Always Clear-Cut: While higher viral load generally correlates with worse symptoms, it's not a perfect predictor. Underlying health conditions and individual immune responses significantly impact how someone experiences COVID-19.\n- Transmission \u2013 Timing Matters: People with COVID-19 tend to be most infectious in the earliest stages of the disease, often before severe symptoms appear. This is linked to high viral loads during the initial period of infection.\n- Asymptomatic Spread: People can have significant viral loads and be highly contagious even if they never show symptoms. This complicates efforts to contain the virus.\n- Treatment Guidance: Monitoring viral load with repeated testing can inform how well antiviral medications are working. If the viral load isn't decreasing, doctors may need to adjust treatment or consider alternative options.\n- Vaccine Breakthroughs: While vaccines dramatically reduce the chance of severe COVID-19, breakthrough infections do happen. Studies are examining if viral load plays a part in how likely someone is to transmit the virus even if they are vaccinated.\n- Viral load is dynamic: It changes throughout the course of the infection, usually peaking early and then declining.\nIndividual variability exists: Age, health factors, and vaccination status can influence viral load trajectories.\n- Public health implications: Understanding viral load patterns has helped shape recommendations on isolation duration, masking guidelines, and targeted testing strategies to reduce the spread of COVID-19.\n\n**Waste Water Testing: Potential for Early Detection of Viruses.**\n\nThe testing of influent waste, which is wastewater entering treatment facilities, has shown potential as an early warning system for viruses like COVID-19. This is because infected individuals shed the virus in their faeces, and this viral RNA can be detected in wastewater. By monitoring wastewater, public health officials can potentially identify the presence of the virus in a community even before clinical cases are reported. [link](https:\/\/www.who.int\/news-room\/commentaries\/detail\/status-of-environmental-surveillance-for-sars-cov-2-virus)\n\nSeveral studies have demonstrated the feasibility of using wastewater surveillance as an early warning system for COVID-19 outbreaks. By analysing wastewater samples, researchers can track trends in virus prevalence and potentially detect increases in viral RNA concentrations before clinical cases are reported. This information can then be used to implement targeted public health interventions, such as increased testing and contact tracing, to control the spread of the virus. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7384408\/)\n\nReports of small amounts of the virus being detected in historical wastewater samples before the first clinical cases are intriguing but should be interpreted with caution. While such findings suggest that wastewater surveillance has the potential to detect early signals of viral outbreaks, further research is needed to validate these findings and understand the implications for public health surveillance. [link](https:\/\/theconversation.com\/was-coronavirus-really-in-europe-in-march-2019-141582)\n\nOverall, wastewater testing holds promise as a complementary tool for early detection of viral outbreaks, including COVID-19, and can provide valuable information for public health decision-making and response efforts. [link](https:\/\/www.scientificamerican.com\/article\/covid-variants-found-in-sewage-weeks-before-showing-up-in-tests\/)\n\n**Vaccination:** \n\nVaccines play a crucial role in preventive medicine by providing protection against infectious diseases, reducing the burden of illness, and saving lives. Here are several key reasons why vaccines are essential in public health:\n- Disease Prevention: \n  - Vaccines are designed to stimulate the body's immune system to recognise and fight specific pathogens, such as viruses or bacteria, without causing the disease itself. By receiving vaccines, individuals develop immunity against infectious diseases, reducing their risk of infection and the subsequent spread of the disease within communities.\n- Eradication and Control of Diseases: \n  - Vaccines have played a significant role in the eradication or control of many infectious diseases worldwide. For example, vaccines have led to the elimination of diseases such as smallpox and nearly eradicated polio. Through comprehensive vaccination programs, diseases like measles, mumps, rubella, pertussis, and hepatitis have been substantially controlled in many regions.\n- Protection of Vulnerable Populations: \n  - Vaccines provide protection not only to vaccinated individuals but also to vulnerable populations, such as infants, elderly individuals, pregnant women, and those with weakened immune systems. By achieving high vaccination coverage rates within communities, a concept known as herd immunity or community immunity, the spread of infectious diseases can be effectively limited, protecting those who may not be able to receive vaccines themselves.\n- Reduction of Healthcare Costs: \n  - Vaccines help to reduce the economic burden associated with infectious diseases by preventing illness, hospitalisations, and complications. By averting medical expenses and productivity losses associated with disease outbreaks, vaccination programs contribute to overall cost savings for individuals, healthcare systems, and society as a whole.\n- Prevention of Outbreaks and Pandemics: \n  -Vaccines are crucial tools for preventing outbreaks and controlling the spread of infectious diseases, including pandemics like COVID-19. By immunising populations against emerging infectious agents, vaccines can help to contain outbreaks and prevent them from escalating into larger-scale public health emergencies.\n- Safe and Effective: \n  - Vaccines undergo rigorous testing and evaluation for safety, efficacy, and quality before being approved for use. Continuous monitoring and surveillance systems are in place to assess vaccine safety and effectiveness post-license. Vaccines are one of the safest and most effective public health interventions available, with a long history of success in preventing disease.\n\nIn conclusion, vaccines are indispensable tools in preventive medicine, offering protection against a wide range of infectious diseases and contributing to improved public health outcomes, reduced healthcare costs, and the prevention of outbreaks and pandemics. Vaccination programs are critical components of comprehensive public health strategies aimed at promoting health and well-being for individuals and communities worldwide.\n\n**COVID-19 Vaccination:**\n\nThe development and deployment of COVID-19 vaccines represent a remarkable feat of scientific collaboration, innovation, and global cooperation. Scientists and researchers from around the world worked tirelessly to accelerate the research and development process, leading to the rapid production and distribution of multiple vaccines to combat the pandemic. However, despite these achievements, significant challenges remain, including disparities in vaccine availability and access due to various factors such as resource limitations, geopolitical considerations, and vaccine distribution mechanisms:\n- Global Scientific Collaboration:\n  - The COVID-19 pandemic prompted an unprecedented level of collaboration among scientists, researchers, pharmaceutical companies, governments, and international organisations. Information sharing, data sharing, and cooperation across borders facilitated the rapid progress in vaccine development.\n  - Scientists and research institutions around the world mobilised their expertise and resources to develop COVID-19 vaccines using various platforms, including mRNA, viral vector, protein sub-unit, and inactivated virus technologies.\n  - International partnerships, such as the Coalition for Epidemic Preparedness Innovations (CEPI), the World Health Organisation (WHO), and initiatives like the Access to COVID-19 Tools (ACT) Accelerator, facilitated coordination and funding for vaccine research, development, and distribution efforts.\n- Speed of Research and Deployment:\n  - The development timeline for COVID-19 vaccines was unprecedentedly fast, with multiple vaccines authorised for emergency use within a year of the identification of the SARS-CoV-2 virus. This rapid progress was made possible by advances in vaccine technology, increased funding, streamlined regulatory processes, and global collaboration.\n  - Vaccine manufacturers leveraged existing platforms and infrastructure, such as mRNA technology and viral vector platforms, to accelerate vaccine development. Clinical trials were conducted with unprecedented speed, enrolling tens of thousands of participants to evaluate vaccine safety and efficacy.\n  - Emergency use authorisations and expedited regulatory approvals allowed for the rapid deployment of vaccines to populations at high risk of COVID-19, including front line healthcare workers, elderly individuals, and individuals with underlying health conditions.\n  - Chinese virologist Zhang Yongzhen, shared the genomic sequence of SARS-CoV-2 with the world, speeding up the development of vaccines. Sleeps on the street after his lab shuts. [link](https:\/\/www.nature.com\/articles\/d41586-024-01293-0)\n  - Zhang's decision to sleep outside his lab, as depicted in social media posts, highlights the extent of his dedication to his research despite the adverse circumstances. His commitment to his work is evident in his willingness to endure discomfort, even sleeping outside in the rain. The dispute between Zhang and the SPHCC raises questions about the treatment of scientists and the importance of providing adequate support and resources for research endeavours, especially during a global health crisis. The lack of clear communication regarding alternative arrangements for Zhang's research team adds further complexity to the situation, emphasising the need for transparency and cooperation between researchers and institutions.\n- Disparities in Vaccine Availability:\n  - Despite the rapid development and production of COVID-19 vaccines, there have been significant disparities in vaccine availability and distribution globally. High-income countries secured large quantities of vaccine doses through advance purchase agreements with manufacturers, leading to limited supplies for low- and middle-income countries.\n  - Limited vaccine manufacturing capacity, supply chain constraints, intellectual property barriers, and vaccine nationalism have hindered equitable access to vaccines, exacerbating global health inequalities.\n  - Initiatives such as COVAX, a global vaccine-sharing mechanism led by WHO, Gavi, the Vaccine Alliance, and CEPI, aim to facilitate equitable access to COVID-19 vaccines for all countries, regardless of income level. However, challenges remain in scaling up vaccine production, overcoming logistical barriers, and ensuring fair allocation and distribution of doses.\n\nCoronavirus (COVID-19) Vaccinations: [link](https:\/\/ourworldindata.org\/covid-vaccinations)\n- 70.6% of the world population has received at least one dose of a COVID-19 vaccine.\n- 13.57 billion doses have been administered globally, and 8,645 are now administered each day.\n- 32.7% of people in low-income countries have received at least one dose.\n\nIn conclusion, the rapid development and deployment of COVID-19 vaccines demonstrate the power of global scientific collaboration and innovation in addressing public health emergencies. However, efforts to achieve equitable access to vaccines for all populations must be intensified, with a focus on overcoming barriers to vaccine distribution, addressing supply shortages, and promoting cooperation among countries and stakeholders to ensure that vaccines reach those most in need. \n\n**Data Visualisations:** \n\n**Excess Deaths & Smoothed COVID-19 Vaccination Rate (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F0ddd89778ef7434412c9b1dcdddd4c54%2FScreenshot%202024-02-12%2009.43.34.png?generation=1708431891582510&alt=media)\n\n**COVID-19 Vaccination Rates vs Excess Deaths (over 65 yrs old) Over Time: 2020-2024, from the data - covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe289248874c9022a5bf58adcb6397d6f%2FScreenshot%202024-02-12%2009.45.37.png?generation=1708433525115760&alt=media)\n\nA Markdown document with the R code for the above 2 plots from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1147668)\n\nPrint out from the above R code: [1] \"Correlation between vaccination rate and excess deaths:-0.24\"\n\nA correlation value of -0.24 indicates a negative correlation between the vaccination rate and excess deaths:\n- Correlation: \n  - Correlation is a statistical measure that describes the extent to which two variables change together. It ranges from -1 to 1.\n- Negative Correlation:\n  - A negative correlation indicates that as one variable increases, the other tends to decrease, and vice versa. In this case, a correlation of -0.24 suggests that there is a weak negative relationship between the vaccination rate and excess deaths. \n- Interpretation: \n  - While the correlation is weak, it suggests that there may be some association between higher vaccination rates and lower excess deaths. However, it is essential to remember that correlation does not imply causation. \n- Strength of Correlation: \n  - The strength of the correlation can be interpreted based on the absolute value of the correlation coefficient. \n\nHere's a comparison of the two plots and some additional insights:\n- Similarities:\n  - Both plots suggest a possible association between higher vaccination rates and lower excess deaths.\n  - The line plot shows a general downward trend in excess deaths as the smoothed vaccination rate increases. \n  - Similarly, the scatter plot shows a weak negative correlation between the two variables.\n- Both plots highlight the limitations of drawing causal conclusions: \n  - Neither plot can definitively establish that vaccination causes lower death rates. \n  - Other factors may be influencing the observed relationships.\n- Differences:\n  - The line plot provides a clearer visual representation of the trend over time. \n  - By smoothing the vaccination rate data, the line plot makes it easier to see how excess deaths have changed in relation to vaccination rates over time.\n  - The scatter plot shows more individual country variation. \n  - While the line plot shows a general trend, the scatter plot allows you to see how individual countries deviate from that trend. \n  - This highlights the importance of considering other factors that might be affecting excess deaths in each country.\n- Additional insights:\n  - It's important to consider the time frame of the data. \n  - Both plots only show data up to a certain point in time. \n- Other factors besides vaccination rates could be influencing excess deaths:\n  - Demographics (e.g., age structure, population density).\n  - Socioeconomic factors (e.g., poverty, inequality).\n  - Healthcare access and quality.\n  - Public health measures (e.g., masking, lock downs).\n  - Non-COVID-19 related deaths.\n- Overall:\n  - These two plots provide valuable insights into the possible relationship between vaccination rates and excess deaths. \n  - However, it's crucial to remember that correlation does not equal causation, and other factors likely play a role. \n\n**Excess Mortality and COVID-19 Vaccination Rate Over Time: 2020-2024, from the data covid.csv**\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2Fe6ee1ee28c626cfcd6b67874d84e0c59%2FScreenshot%202024-02-20%2015.51.58.png?generation=1708446523271176&alt=media)\n\nA Markdown document with the R code for the above plot from the data: covid.csv - [link](http:\/\/rpubs.com\/Paddy_5142\/1151351)\n\nCode explanation:\n- Load Libraries: \n  - We load dplyr for data manipulation, ggplot2 for visualisation, and zoo for creating rolling averages.\n- Pre-processing:\n  - Filter: Remove rows with missing values in either 'excess_mortality_cumulative_per_million' or 'total_vaccinations_per_hundred'.\n  - Convert 'date' to Date: For accurate time-based analysis.\n- Rolling Averages:\n  - Compute 7-day rolling averages for excess_mortality_cumulative_per_million and total_vaccinations_per_hundred to smooth out noise and highlight broader trends.\n- Correlation:\n  - Calculate the correlation coefficient between the rolling averages. We use use = \"complete.obs\" to handle any remaining missing values.\n- Visualisation:\n  - Create a line plot with time ('date') on the x-axis.\n  - Plot both smoothed excess mortality and smoothed vaccination rate with distinct colours.\n  - Add informative labels and a clean theme.\n  - Include an annotation on the plot displaying the calculated correlation value.\n- Chart Correlation:\n  - The correlation coefficient of -0.03 indicates a very weak negative association between the two variables. \n  - This means that there might be a very slight tendency for excess deaths to decrease as vaccination rates increase, but the relationship is very weak and there are many exceptions.\n- Simple Linear Regression Model:\n  - Output from the code.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F13231939%2F96890f1ecb5c3f3a77a075e34b3b6254%2FScreenshot%202024-02-20%2017.14.58.png?generation=1708449762573110&alt=media)\n\nExplanation of the Linear Regression Call and Output:\n- Call:\n  - Code snippet: lm(formula = excess_mortality_rolling ~ vaccination_rate_rolling, data = data_filtered)\n  - lm(): The standard function in R to fit linear regression models.\n  - formula = ...: This defines the model structure. excess_mortality_rolling ~ vaccination_rate_rolling means it is regressing smoothed excess mortality (dependent variable) on the smoothed vaccination rate (independent variable).\n  - data = data_filtered: Specifies the dataset used for the analysis.\n- Output (summary(model)):\n  - Residuals: This summarises the distribution of residuals (the differences between observed and model-predicted excess mortality values). An ideal model would have small, randomly distributed residuals.\n  - Coefficients: The heart of the regression output.\n  - (Intercept): The estimated baseline excess mortality when the vaccination rate is zero.\n  - vaccination_rate_rolling: The estimated slope of the line. In this case, a coefficient of -0.6710 suggests that for every one-unit increase in the smoothed vaccination rate, excess mortality is estimated to decrease by about 0.67 units, all else held equal.\n  - Std. Error: Variability around coefficient estimates.\n  - t-value: Indicates how many standard errors away the coefficient is from zero. It helps assess the effect's strength.\n  - Pr(&gt;|t|): The p-value. A small p-value (typically &lt; 0.05) suggests a statistically significant relationship between the predictor and the outcome.\n- Goodness-of-Fit:\n  - Residual standard error: Variability around the regression line (smaller is better).\n  - Multiple R-squared: This explains the proportion of variation in excess mortality explained by the vaccination rate in the model. A very low value (0.0008317) indicates the model explains virtually none of the variation.\n  - F-statistic and its p-value: Tests the overall model fit, comparing it to a model with no predictors.\n- Interpretation of Results:\n  - Tentative Negative Relationship: The negative coefficient for 'vaccination_rate_rolling' suggests a potential decreasing trend in excess mortality as vaccination rates increase. However, the relationship appears very weak in terms of proportion of variance explained.\n  - Statistical Significance: With a p-value slightly above 0.05, the linear relationship's strength is on the cusp of statistical significance.\n- Essential Caveats:\n  - Causation vs. Correlation: Regression doesn't imply causation. Many other factors influence excess mortality, including per capita GDP.\n\nComparison of Vaccination and Booster Rates and Their Impact on Excess Mortality During the COVID-19 Pandemic in European Countries: PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC10120793\/)\n\nNo Evidence Excess Deaths Linked to Vaccines, Contrary to Claims Online: FactCheck.org - [link](https:\/\/www.factcheck.org\/2023\/04\/scicheck-no-evidence-excess-deaths-linked-to-vaccines-contrary-to-claims-online\/)\n\n**Slow vs. Fast Mutating Viruses:**\n\nThe rate at which viruses mutate significantly impacts how we approach vaccination strategies. Let's dive deeper, using COVID-19 as an example, to understand the key differences and their effect on vaccine development:\n- Mutation Rate:\n  - Slow Mutating Viruses: These viruses, like influenza B, accumulate mutations slowly, leading to gradual changes in their surface proteins (vaccine targets).\n  - Fast Mutating Viruses: Viruses like influenza A and HIV evolve rapidly, quickly accumulating mutations that can potentially render existing vaccines ineffective within months.\n- Impact on Vaccines:\n  - Slow Mutating Viruses: Due to gradual change, a single vaccine can sometimes offer long-term protection, like the current influenza B vaccine. However, occasional updates might be needed for emerging strains.\n  - Fast Mutating Viruses: Rapid mutation necessitates more frequent vaccine updates to keep pace with the evolving virus. For example, the influenza A vaccine requires annual reformulation. In extreme cases, like HIV, developing an effective vaccine becomes extremely challenging.\n- COVID-19 as a Case Study:\n  - Mutation Rate: SARS-CoV-2, the virus causing COVID-19, falls somewhere between slow and fast mutating viruses. It exhibits more mutation than influenza B but less than influenza A.\n  - Vaccine Development: This intermediate mutation rate led to an urgent need for multiple vaccines during the pandemic. Different variants, like Delta and Omicron, necessitated reformulations with updated targets to maintain efficacy.\n  - Current Approach: While initial COVID-19 vaccines offered significant protection, booster shots and variant-specific adaptations (like bivalent vaccines) have become crucial to address evolving strains.\n   - Type of Mutations: Not all mutations in COVID-19 significantly impact vaccine efficacy. Understanding how mutations affect binding to antibodies and immune response is key.\n  - Immune Response Complexity: Vaccines can stimulate broader immune responses beyond surface proteins. This can offer protection against slightly mutated variants.\n  - Combination Vaccines: Multi-strain vaccines targeting dominant variants are being explored to offer wider protection against circulating strains.\n- Single vs. Multiple Vaccines:\n  - Single Vaccine: This would be ideal, but the evolving nature of COVID-19 necessitates multiple vaccines targeting dominant variants like Delta and Omicron.\n  - Multiple Vaccines: Currently, different vaccines and booster shots address the evolving virus.\n- Universal Vaccine: Researchers are actively developing universal vaccines targeting conserved regions in coronaviruses, aiming for broad protection against various strains, including future ones.\n\nThe optimal vaccine strategy for COVID-19, and other viruses, depends on their specific mutation rate and ongoing evolution. Understanding this dynamic relationship is crucial for developing effective vaccination strategies. While COVID-19 presented challenges due to its mutation rate, ongoing research on variant-specific vaccines and universal approaches promise better control in the future.\n\n**Immune Exhaustion:**\n\nThere is a theoretical concern about potential immune exhaustion or weakening of the immune response with repeated vaccinations over a short period. However, current evidence suggests that the benefits of vaccination, including protection against severe illness and death, generally outweigh the potential risks of immune exhaustion: [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9167431\/)\n- Immune exhaustion:\n  - A theoretical concern: This concept suggests that repeatedly triggering the immune system with vaccinations could lead to its \"overwork\" and weakening, making it less effective in fighting off future infections.\n- Lack of convincing evidence: \n  - While theoretically possible, current research hasn't found conclusive evidence linking COVID-19 vaccination to immune exhaustion.\n- Specific concerns for older adults and immunocompromised individuals: \n  - These groups often have weaker immune systems due to age or underlying conditions, making them more susceptible to potential complications. However, studies specifically investigating them haven't found increased risk of immune exhaustion due to vaccination.\n- Weighing benefits and risks:\n  - Benefits of vaccination outweigh potential risks: Numerous studies have established the strong benefits of COVID-19 vaccines in preventing severe illness, hospitalisation, and death. These benefits far outweigh the theoretical risk of immune exhaustion, which lacks concrete evidence.\n- Importance of considering individual circumstances: \n  - While the overall population benefits heavily from vaccination, individual cases require careful consideration. Healthcare professionals can help weigh the risks and benefits for individuals with specific concerns, especially immunocompromised individuals or those with unique medical conditions.\n- Ongoing research:\n  - Scientists continue to study the long-term effects of COVID-19 vaccination, including any potential impact on the immune system.\n  - As more data becomes available, our understanding of the risks and benefits may evolve.\n- Key takeaways:\n  - COVID-19 vaccination remains highly effective and safe for most individuals.\n  - While immune exhaustion remains a theoretical concern, current evidence doesn't support it as a significant risk.\n  - Weighing the proven benefits of vaccination against the theoretical risk is crucial for informed decision-making.\nConsulting healthcare professionals can help individuals with specific concerns make informed choices regarding vaccination.\n- Current evidence does not support a link between excess deaths and over-vaccination:\n  - Extensive research and data analysis by credible health organisations (e.g., WHO, CDC) have found no evidence that COVID-19 vaccines overload the immune system or contribute to excess deaths. In fact, the overwhelming evidence shows these vaccines are safe and effective at preventing severe illness, hospitalisation, and death from COVID-19.\n  - Studies specifically examining the relationship between COVID-19 vaccination and mortality in older adults have found no increased risk of death associated with vaccination.\n- Multiple factors contribute to excess deaths in elderly populations: \n  - Various factors beyond COVID-19 vaccination could be contributing to excess deaths in older adults, including the lingering effects of the pandemic (delayed medical care, Long COVID), economic disruptions, and age-related vulnerabilities to other illnesses.\n- Age-related decline in immune system function: \n  - It's true that the immune system weakens with age, making older adults more susceptible to infections and complications from various illnesses. This natural decline, not vaccination, is a more likely explanation for the observed pattern. [link](https:\/\/immunityageing.biomedcentral.com\/articles\/10.1186\/s12979-019-0164-9)\n\nExhaustion and over-activation of immune cells in COVID-19: Challenges and therapeutic opportunities. PMC - [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC9640209\/#:~:text=Early%20studies%20found%20that%20upregulation,producing%20T%20cells%20%5B8%5D.)\n\n**Excess Deaths are Increasing:**\n\n*UK\u2019s pattern of excess deaths deserves \u2018close scrutiny\u2019*: Research Professional News -  [link](https:\/\/www.researchprofessionalnews.com\/rr-news-uk-politics-2023-1-uk-s-pattern-of-excess-deaths-deserves-close-scrutiny\/?fbclid=IwAR1FhQ_bidRDjjmq3-FbuwdeUd1v3HcLpC9TEvUeUUG9B_c4a2JRU9pVqgE)\n\n*Our mission is to reinforce the EU's preparedness to respond to potential risk of all hazards by a continued operation of the EuroMOMO network which ensures quality checked, standardised weekly mortality monitoring*: EuroMOMO - [link](https:\/\/www.euromomo.eu\/graphs-and-maps\/)\n\nEUROPEAN STATISTICAL Recovery Dashboard: eurostat - [link](https:\/\/ec.europa.eu\/eurostat\/cache\/recovery-dashboard\/)\n\nSome potential theories based on recent news and ongoing research. \n\n- Delayed and missed medical care: \n  - The pandemic disrupted healthcare systems, leading to postponed surgeries, screenings, and treatments for various conditions. This backlog could be contributing to excess deaths from these untreated illnesses.\n- Long COVID: \n  - Studies show a significant portion of COVID-19 survivors experience long-term complications, which could contribute to excess mortality.\n- Mental health impacts: \n  - The pandemic's stressors and social isolation have exacerbated mental health issues, potentially leading to increased suicides and substance abuse-related deaths.\n- Economic disruptions: \n  - The global economic slowdown and inflation could impact access to food, healthcare, and essential services, particularly for vulnerable populations.\n- Toxification:\n  - The increase in air and water pollution across the globe from human endeavours, including radiation exposure. Weakening the immune system and immune response. [link](https:\/\/docs.google.com\/document\/d\/1RWeynujSYgGxKRfEMPcZosjGF-zGep6yrHVUpkv71c8\/edit?usp=sharing)\n- Climate change: \n  - Extreme weather events like heat waves and floods can directly cause deaths and indirectly contribute to excess mortality through disruptions to infrastructure and healthcare systems.\n- War and conflict: \n  - Ongoing conflicts in various regions displace populations, disrupt essential services, and lead to violence-related deaths, contributing to excess mortality.\n- Ageing populations:\n  - In some countries, an ageing population with higher baseline mortality rates could be a contributing factor, although this wouldn't necessarily explain a surge.\n- Data limitations and complexities:\n  - Attributing excess deaths to specific causes can be challenging due to data limitations and complex interactions between various factors.\n  - Differences in data collection and reporting methods between countries can make comparisons difficult.\n\nIt's crucial to remember that these are just potential theories, and the specific causes of excess deaths likely vary depending on the region and context. Public health officials are actively investigating these issues to understand the contributing factors and develop targeted interventions to address them.\n\n**Investigating Potential Links Between COVID-19 Vaccination and Cardiac Events.**\n\nDr. Clare Craig of HART [link]( https:\/\/totalityofevidence.com\/dr-clare-craig\/); noted a potential increase in cardiac arrests following the May 2021 vaccine roll out. HART raises valid concerns about this trend, which warrants closer investigation. There are points within the Pfizer trial that require further scrutiny: [link](https:\/\/www.conservativewoman.co.uk\/ignored-danger-signals\/)\n- Trial Results: Four vaccine group participants died from cardiac arrest versus one in the placebo group. Furthermore, total deaths in the vaccine group (21) exceeded the placebo group (17) through March 2021.\n- Reporting Anomalies: Deaths in the vaccine group took significantly longer to report than the placebo group, suggesting a potential bias within a supposedly blinded trial.\n\nFurther Supporting Evidence:\n- Israeli Study: A correlation exists between increased cardiac hospital visits among 18-39-years-old and vaccination, not COVID-19 infection. [link](https:\/\/www.nature.com\/articles\/s41598-022-10928-z)\n- Post-Mortem Studies: Recent studies suggest a causal link between vaccination and coronary artery disease leading to death within four months of the last dose. It's important to note that the vaccine safety trial was limited to two months, so long-term safety data is lacking. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC8709364\/)\n- Australian Data: Australia offers a compelling case study due to low COVID-19 prevalence during the initial vaccine roll out. In South Australia, historically low COVID-19 cases coincided with a significant rise in cardiac emergencies following vaccination. This trend, specifically among younger age groups, necessitates investigation. [link](https:\/\/www.tga.gov.au\/news\/covid-19-vaccine-safety-reports\/covid-19-vaccine-safety-report-15-12-2022#myocarditis-and-pericarditis-with-mrna-vaccines)\n\nImportant Considerations:\n- Unblinding of Trials: The decision to unblind the trials after two months and vaccinate the placebo group raises ethical concerns and limits longer-term safety data.\n- Excess Deaths: Rising cardiac-related deaths cannot be solely attributed to an aging population, COVID-19 prevalence, or changes in medication use. This leaves the COVID-19 vaccines as the leading potential cause.\n\nCall to Action:\n\nThe potential correlation between COVID-19 vaccination and cardiac events demands urgent and rigorous investigation. Thorough analysis, particularly in populations with low historical COVID-19 prevalence, is crucial to determine the nature of this potential association.\n\nTrends in Excess Deaths Volume 738: debated on Friday 20 October 2023, UK Parliament - [link](https:\/\/hansard.parliament.uk\/commons\/2023-10-20\/debates\/69C5A514-9A04-4ED7-B56B-61A3D40E3226\/TrendsInExcessDeaths)\n\nExcess Death Trends, Andrew Bridgen Excerpts: Tuesday 16th January 2024, Parallel Parliament - [link](https:\/\/www.parallelparliament.co.uk\/mp\/andrew-bridgen\/debate\/2024-01-16\/commons\/westminster-hall\/excess-death-trends#:~:text=In%202020%2C%20there%20were%20607%2C000,should%20be%20a%20significant%20deficit.)\n\n**COVID-19 Lock Downs:**\n- No Universal Definition of \"Lock down\": \n  - Countries had varying approaches with no single definition. Some were full stay-at-home orders, others had curfews, others restricted businesses. \n- The specificity of Measures: \n  - Lock downs weren't monolithic. Some regions within a country were stricter than others. \n- Changing Restrictions: \n  - Lock downs evolved over time, with easing and re-tightening of restrictions. \n\nThe Oxford Coronavirus Government Response Tracker (OxCGRT) project calculates a Stringency Index, a composite measure of nine of the response metrics: [link](https:\/\/ourworldindata.org\/covid-stringency-index)\n- The nine metrics used to calculate the Stringency Index are: \n  - School closures. \n  - Workplace closures. \n  - Cancellation of public events. \n  - Restrictions on public gatherings. \n  - Closures of public transport.\n  - Stay-at-home requirements. \n  - Public information campaigns. \n  - Restrictions on internal movements.  \n  - International travel controls.\n\nCOVID-19 lock downs by country: Wikipedia - [link](https:\/\/en.wikipedia.org\/wiki\/COVID-19_lockdowns_by_country)\n\n**Understanding COVID-19 Isolation:**\n- Purpose: Isolation separates people diagnosed with COVID-19 from others to limit the spread of the virus. It's especially crucial in the first few days of infection when people are most likely to be contagious.\n- CDC Guidelines: The CDC continuously updates its guidance based on evolving scientific understanding. Traditionally, they recommended at least 5 days of isolation. More recent guidance allows people to end isolation after 5 days if they are fever-free for 24 hours (without medication) and their symptoms are improving. [link](https:\/\/www.webmd.com\/covid\/news\/20240223\/cdc-could-cut-covid-isolation-time?src=RSS_PUBLIC)\n\nThe Toll of Isolation:\n- Mental Health - Prolonged isolation can harm mental health, leading to:\n  - Anxiety and stress\n  - Depression\n  - Loneliness\n  - Difficulty sleeping\n- Strained Relationships:\n  - Family: Isolation can cause tensions within families, especially with limited space and childcare responsibilities.\n  - Community: Fear and stigma surrounding COVID-19 can lead to social isolation and strained relationships in communities. Isolation might also limit access to essential resources and support.\n  - Workplace: Isolation can make it difficult to work, contributing to job insecurity and financial strain; this may increase stress and impact relationships with colleagues.\n\nPotential Impact of CDC Changes:\n- Reduced Isolation Time - Moving from 5 days to 24 hours could mean:\n  - Faster return to normal activities: People may feel relief at resuming routine sooner.\n  - Increased Transmission Risk: If people are still infectious at 24 hours, there's potential for increased spread, especially in high-risk settings.\n  - Focus on Fever and Symptoms: Focusing on being fever-free and mild symptoms downplays how contagious someone could still be and the risk for long COVID complications.\n  - Higher-Risk Individuals: The change could put immunocompromised or high-risk people at greater danger, making it crucial for them to take extra precautions.\n  - April 2024 Implementation: Delaying this change may provide time for greater public awareness and allow individuals at high risk to prepare.\n\nWhy the Change?:\n- The CDC likely grapples with these factors:\n  - Transmission Understanding: Some studies suggest the most infectious period is during early illness.\n  - Practical Considerations: Extended isolation is difficult for many people, raising concerns about adherence. A shorter period might be more feasible, even with the potential for some increased transmission.\n  - Behavioural changes: Public fatigue with COVID-19 precautions could make enforcing longer isolation periods challenging.\n\nThis change highlights a crucial point - COVID-19 policies must balance disease control with practical realities:\n- It underscores the need for:\n  - Individualised Precautions: Each person's situation and risk level should be considered, especially for immunocompromised people.\n  - Improved Testing: Accessible and reliable testing can help individuals make informed decisions about when to end isolation.\n  - Clear Communication: The CDC must provide clear reasons for the change and guidance on how individuals can protect themselves.\n\n**Behavioural Manipulation of Viruses:**\n\nHow some viruses can manipulate their host's behaviour to enhance their own transmission. Here's a look at why this happens and some examples:\n- Why Manipulate Behaviour?\n  - From the virus's perspective, it's all about survival and spread. It can only replicate and thrive by infecting new hosts. If a host is confined and isolated, the virus is trapped. So, certain viruses have evolved ways to modify their host's behaviour to maximise the chances of infecting others.\n- Examples of Viral Manipulation:\n  - Influenza (The Flu): There's some evidence that those infected with the flu, while feeling ill, also experience increased sociability. This might drive them to go out, despite feeling sick, potentially spreading the virus to new people.\n  - Rabies: The classic example. Rabies dramatically alters the behaviour of the host, causing aggression and excessive salivation. This compels infected animals to bite others, spreading the virus through saliva.\n  - Toxoplasma gondii: A parasite often carried by cats that can infect rodents. It makes rodents less fearful of cats, increasing the chance of being eaten, which the parasite needs to complete its life cycle.\n  - Zombie Fungus (Ophiocordyceps unilateralis): Infects ants, making them climb high and bite into vegetation before dying. This positions the fungus perfectly to release spores and infect new ants.\n\nHow Manipulation Works.\n - The mechanisms are diverse and complex, but some common themes include:\n  - Altering Brain Chemistry: Viruses and parasites can interfere with neurotransmitters and brain regions controlling behaviour, reducing fear, increasing aggression, or altering motivation.\n  - Inflammation: The host's immune response itself might induce behavioural changes. Inflammation in the brain can sometimes lead to lethargy, irritability, or social withdrawal \u2013 which could be manipulated by the pathogen.\n  - Direct Genetic Manipulation: Some viruses insert their own genes into the host's genome potentially affecting behaviour on a longer-term basis.\n\nImportant Notes:\n- Evolving Science: \n  - We're still unravelling these intricate relationships. It's a blend of virus biology, the host's immune system, and complex brain processes.\n- Not Every Virus: \n  - Many viruses are \"passive\" in terms of host behaviour. Those that cause the common cold, for example, simply rely on us being near others for transmission.\n- Ethical Considerations: \n  - This raises questions about free will and whether we're responsible for actions under the influence of a pathogen.\n\nThere's currently no strong evidence to suggest that COVID-19 directly manipulates human behaviour in the same way that certain other viruses, like the ones above, do. Here's why:\n- Transmission Mechanisms: \n  - COVID-19 primarily spreads through respiratory droplets expelled when people cough, sneeze, talk, or breathe. \n  - It doesn't require specific behavioural changes in the host for successful transmission. \n  - Simply being in close proximity to others when infected puts them at risk.\n- Symptoms and Behaviour: \n  - Some COVID-19 symptoms, such as fatigue and loss of taste and smell, might make infected individuals less likely to socialise, potentially reducing spread. \n  - However, this is an indirect consequence of feeling sick, not a deliberate manipulation by the virus.\n- Focus of Research: \n  - Most research into COVID-19 focuses on its direct physiological effects, how it spreads, and the effectiveness of vaccines and treatments. \n  - Less attention has been paid to potential subtle changes in behaviour as a viral strategy.\n- Long-Term Effects: \n  - There's ongoing research into \"long COVID,\" where some people experience lingering symptoms such as fatigue, brain fog, and potentially mood changes. \n  - It's possible, though not yet proven, that there's an element of prolonged viral impact on the brain and nervous system that is influencing behaviour.\n- Social and Psychological Impacts: \n  - The pandemic itself, the isolation measures, and the general fear and uncertainty have a massive impact on people's behaviour. This is not directly caused by the virus, but a consequence of the situation it has created.\n\nWhile there's no evidence for direct, deliberate behavioural manipulation of COVID-19 like rabies or zombie fungus, there are still open questions about potential long-term neurological effects and the indirect consequences of the pandemic on our behaviour. [link](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7175891\/#:~:text=Although%20not%20widely%20studied%2C%20behavioral,most%20notable%20example%20is%20rabies.)\n\n\n**Why Intensive Farming and Eating Meat are Helping Zoonoses (diseases that can jump from animals to humans) Develop:**\n- Close Confinement: \n  - Intensive farming often involves keeping large numbers of animals in cramped, unsanitary conditions. \n  - This close proximity provides a perfect breeding ground for pathogens to spread rapidly among animals.\n- Reduced Genetic Diversity: \n  - Industrial livestock operations frequently rely on a limited number of animal breeds selected for fast growth and high output. \n  - This reduced genetic diversity makes animal populations more vulnerable to disease outbreaks, as they lack a wider range of natural immunity.\n- Weakened Immune Systems: \n  - The stress of overcrowded living conditions, alongside selective breeding that prioritises production over health, can weaken animals' immune systems. \n  - This makes the animals more susceptible to contracting and spreading diseases.\n- Antibiotic Overuse: \n  - To combat diseases in such conditions and promote even faster growth, intensive farms often overuse antibiotics.\n  - This contributes to the rise of antibiotic-resistant bacteria, making future infections in both animals and humans harder to treat.\n- Increased Human-Animal Contact: \n  - The scale of intensive farming operations requires more workers on-site, increasing the chance of humans being exposed to infected animals or their waste.\n- Globalised Meat Trade: \n  - The global distribution of meat products from large-scale farms can quickly spread a zoonotic disease across borders.\n\nThe Spanish Flu - A Historical Example: [link](https:\/\/en.wikipedia.org\/wiki\/Spanish_flu)\n\nWhile the exact origin of the Spanish flu (caused by the H1N1 influenza virus) is debated, one theory is that it originated in birds and was then transmitted to pigs. The conditions of livestock management at the time potentially played a role in the virus's adaptation and eventual spread to humans.\n\nThe Consequence:\n\nThese factors create an environment where new strains of pathogens can emerge, evolve, and potentially jump to humans \u2013 sometimes leading to pandemics like the avian flu (H5N1), swine flu (H1N1), or historically, the Spanish flu. The more we increase the scale and intensity of meat production, the greater the risk of future zoonotic outbreaks.:\n- Cows Are Potential Spreaders of Bird Flu to Humans. [link](https:\/\/www.webmd.com\/food-recipes\/food-poisoning\/news\/20240510\/cows-are-potential-spreaders-bird-flu-humans)\n- Could bird flu in cows lead to a human outbreak? Slow response worries scientists. [Link](https:\/\/www.nature.com\/articles\/d41586-024-01416-7)\n\n**Conclusion:**\n\nCoronaviruses have a long, complex history, and their ability to cause significant human disease is terrifyingly clear. From the common cold to the devastating COVID-19 pandemic, these viruses expose the constant danger posed by zoonotic diseases. While the exact origins of COVID-19 remain contentious, the lack of a unified global lock down strategy and the initial failure to assume airborne transmission underscore the need for preemptive action against potentially pandemic viruses. We must invest heavily in regular influent wastewater testing to detect emerging strains or novel viruses before they cause widespread clinical harm. The recent regular incidence of new human coronaviruses highlights the urgent need for such surveillance.  Furthermore, the potential correlation between COVID-19 vaccination and cardiac events, however rare, demands urgent and rigorous investigation. Transparent, thorough analysis across diverse populations is crucial to understanding the risks and benefits of COVID-19 vaccination. Finally, the possibility that intensive farming practices contribute to the emergence of zoonotic diseases demands serious investigation and potential reform. As we look ahead, proactive vigilance, investment in surveillance and early warning systems, along with global pandemic preparedness are essential to protect humanity against the inevitable future coronavirus outbreaks \u2013 and the global pandemics they could ignite.\n\n\nPatrick Ford \ud83e\udd87\n\n\n--------------------------------------------------------------------------------------------------------------------------\n\n*Immunity and the Connections of Mental Well Being.\nThe Power of Food to Strengthen the Immune System, to Protect Us.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/immunity-and-the-connections-of-mental-well-being) - A strong immune system is not only vital for physical health but also plays a role in mental health. There is growing evidence to suggest that the immune system and mental health are interconnected. \n\n*We did not weave the web of life !\nWe are merely a strand in the web.* [link](https:\/\/www.kaggle.com\/datasets\/patricklford\/life-but-not-as-we-know-it) - In this project I concentrate on some important factors that will affect humanity's potential to survive on planet Earth:\n- Global Demographic Shifts.\n- Inequality.\n- Climate change.\n- Resource depletion.\n\n--------------------------------------------------------------------------------------------------------------------------\n\nTheoretical discussion regarding the impact of Cesium-137, glyphosate, PFAS, and immune system regulation of thyroid hormone activity on COVID-19 and vaccines. [link](https:\/\/docs.google.com\/document\/d\/1dedZU-akAB0Mwi1zHVn5aU1D8Bd8kNlvT5HlLHqcmkI\/edit?usp=sharing) public health data visualization health conditions public safety r data storytelling","344":" Face Mask  This is a collection of 1698 images featuring medical face masks. The images in this dataset could depict a variety of scenarios, such as individuals wearing masks, masks on different surfaces, or masks in various environments. This dataset could be useful for projects related to public health, medical studies, or machine learning models aimed at mask detection. Please note that the actual content of the images may vary, and a thorough review of the dataset is recommended to understand its specifics and potential applications. ","345":" Retina Blood Vessel Retina Blood Vessel for segmentation **Dataset Overview:**\nWelcome to the Retina Blood Vessel Segmentation dataset, a valuable resource for advancing the field of medical image analysis and enhancing the diagnosis of retinal vascular diseases. This dataset contains a comprehensive collection of retinal fundus images, meticulously annotated for blood vessel segmentation. Accurate segmentation of blood vessels is a critical task in ophthalmology as it aids in the early detection and management of various retinal pathologies, such as diabetic retinopathy and macular degeneration.\n\n**Content:**\nThe dataset comprises a total of X high-resolution retinal fundus images captured using state-of-the-art imaging equipment. Each image comes with corresponding pixel-level ground truth annotations indicating the exact location of blood vessels. These annotations facilitate the development and evaluation of advanced segmentation algorithms.\n\n**Key Features:**\n\nImage Size: The images in the dataset are of varying dimensions, ranging from XXX pixels to XXX pixels, mimicking the real-world diversity of retinal images.\nAnnotations: For each image, corresponding pixel-wise annotations in a binary mask format are provided. Blood vessel pixels are marked as 1, while background pixels are labeled as 0.\nPathological Variation: The dataset encompasses a spectrum of retinal conditions, including varying vessel widths, branching patterns, and presence of anomalies, making it suitable for evaluating the robustness of segmentation models.\nUse Cases:\nResearchers and practitioners in the fields of medical image analysis, computer vision, and artificial intelligence will find this dataset invaluable for several applications:\n\nAlgorithm Development: Use the dataset to train and test innovative segmentation algorithms, leveraging the precise annotations to achieve accurate and reliable results.\nDisease Detection: Create models that can assist in the early detection of retinal pathologies, contributing to timely medical interventions.\nEducation: The dataset can be used for educational purposes to help students and professionals understand the complexities of retinal blood vessel structure.\nEvaluation Metrics:\nPerformance evaluation will primarily involve measuring the segmentation accuracy against the ground truth annotations. Common metrics such as Intersection over Union (IoU), Dice Coefficient, and pixel-wise accuracy can be employed to quantify the model's performance. computer vision deep learning eyes and vision image segmentation","346":" githut-RSNA-MICCAI-Brain-Tumor-Classification-AI Competition RSNA-MICCAI Brain Tumor Radiogenomic Classification # Brain segmentation with mateuszbuda brain segmentation pytorch unet\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n# Introduction\n\nThe RSNA-MICCAI Brain Tumor Radiogenomic Classification Contest is a multi-class classification problem, giving MRIs based on radiomic features, where the goal is to predict the presence of MGMT promoter methylation.\n\nProject: [RSNA-MICCAI Brain Tumor Classification Project](https:\/\/www.kaggle.com\/code\/yannicksteph\/rsna-miccai-brain-tumor-classification)\n\nThere are three classes: \n- LGG (low-grade glioma)\n- HGG (high-grade glioma) \n- WT (hemangioblastoma)\n\nThe dataset we will be working with consists of MRI datasets provided by the Radiological Society of North America (RSNA\u00ae) and the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society). The images are provided in DICOM format and are accompanied by a CSV file containing radiomic features extracted from the images.\n\nHere's the competition [RSNA-MICCAI Brain Tumor Radiogenomic Classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification\/data?select=train_labels.csv)\n\n#### Why\nThe **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was utilized to obtain data for our brain dataset. This library is based on the Unet neural network architecture and specifically designed for brain segmentation from medical images.\n\nBy leveraging the capabilities of this library, accurate brain segmentation was achieved on the images within our dataset. Segmentation is a critical task in medical imaging as it enables the extraction of precise information about different regions or classes, in this case, brain structures.\n\nThe selection of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library was based on its exceptional performance and user-friendly nature. It provides an efficient implementation of the Unet architecture, renowned for its success in biomedical image segmentation. Consequently, our project was able to deliver reliable and accurate results for brain segmentation.\n\nIn conclusion, the utilization of the **\"mateuszbuda_brain-segmentation-pytorch_unet\"** library played a pivotal role in acquiring accurate brain segmentation data for our dataset. By leveraging this library, we efficiently segmented medical images and extracted valuable information to further our project's objectives.\n\nSource: [mateuszbuda_brain-segmentation-pytorch_unet on PyTorch Hub](https:\/\/pytorch.org\/hub\/mateuszbuda_brain-segmentation-pytorch_unet\/)\n\n#### Library\n\nTo achieve tumor segmentation, the U-Net for Brain MRI model will be employed.\n\nU-Net for Brain MRI is a convolutional neural network model specifically designed for segmenting brain MRI images. It features a U-shaped architecture with branch connections, comprising four levels of blocks. Each block consists of two convolution layers with batch normalization, ReLU activation function, and an encoding part with a max pooling layer, while the decoding part utilizes up-convolution. The number of convolution filters varies across the model's levels, ranging from 32 to 256.\n\nTo utilize the model, an input brain MRI image with three channels corresponding to pre-contrast, FLAIR, and post-contrast sequences should be provided. The image should be scaled to a size of 256x256 pixels and normalized using the z-score method per volume.\n\nThe pre-trained U-Net model produces a single-channel probability map indicating anomalous regions in the input image. By applying an appropriate threshold, this probability map can be converted into a binary segmentation mask.\n\nIn summary, U-Net for Brain MRI is a pre-trained model capable of automatically segmenting abnormalities in brain MRI images. Its application extends to various medical imaging tasks, including brain tumor detection and analysis.\n\n\nTo perform shape analysis and extract relevant features, the **2D and 3D** class will be utilized.\n\nData from Source: [rsna-miccai-brain-tumor-radiogenomic-classification](https:\/\/www.kaggle.com\/competitions\/rsna-miccai-brain-tumor-radiogenomic-classification)\n biology cancer","347":" Processed CT ICH Dataset Images The final images and masks obtained after processing Physionet Dataset This dataset is the processed version of the original dataset available at [Computed Tomography Images for Intracranial Hemorrhage Detection and Segmentation](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/). The `split_raw_data.py` provided by the original dataset author has been modified for updated libraries and Python 3.11. `ct_ich.yml` is also updated to reflect the same to get the environment where the data was processed.\n\nThe data present in `image` and `label` folders are in 512 x 512 grayscale PNG files. There are 2814 pairs of image and segmentation mask respectively.\n\nOriginal DOI for this data: https:\/\/doi.org\/10.13026\/4nae-zg36\n\nLicense: The PhysioNet Restricted Health Data License\nVersion 1.5.0\nMake sure to sign in to your Physionet account and agree to the the terms and conditions of the [original dataset](https:\/\/physionet.org\/content\/ct-ich\/1.3.1\/) to use this medical dataset. healthcare computer science programming health conditions image segmentation","348":"  CBIS-DDSM: Breast Cancer Dataset of JPG Images Curated Breast Imaging Subset of DDSM dataset with JPG images This Dataset contains the ***JPG*** images of ***Breast Cancer*** taken from the ***CBIS-DDSM***.\n\n![Breast Cancer Images](https:\/\/i.imgur.com\/rz4rtQI.png)\n\n## Descripton\nThis dataset contains ***JPG*** format images (2.49 GB) of the original ***CBIS-DDSM dataset*** (163 GB) which are in ***DICOM*** format and by maintaining the same resolution of the images as it was in the original dataset.\n\nThe original dataset was split into train and test by having two cases one is ***Mass*** and another is ***Calcification(Calc)*** i.e. ***calc_case_description_test_set.csv, calc_case_description_train_set.csv,  mass_case_description_test_set.csv, mass_case_description_train_set.csv, and metadata.csv*** but in here this dataset is made by *converting the images from DICOM to JPG format, removing the unnecessary columns by Data Cleaning and concatenating both the Mass and Calcification(Calc) cases train test into one* i.e. ***calc_case(with_jpg_img).csv, mass_case(with_jpg_img).csv, and metadata(with_jpg_img).csv***.\n\n<br>\n| Collection | |\n| --- | --- |\n| Number of Studies | 6775 |\n| Number of Series | 6775 |\n| Number of Participants | 1,566(NB) |\n| Number of Images | 10239 |\n| Modalities | MG |\n| Image Size (GB) | 6(.jpg) |\n\n\n<br>\n\n**NB**: *The image data for this collection is structured such that each participant has multiple patient IDs. For example, pat_id 00038 has 10 separate patient IDs which provide information about the scans within the IDs (e.g. Calc-Test_P_00038_LEFT_CC, Calc-Test_P_00038_RIGHT_CC_1) This makes it appear as though there are 6,671 participants according to the DICOM metadata, but there are only 1,566 actual participants in the cohort.*\n<br>\n\n##File Description\n###1. JPG image folder file structure\n![Cancer Image file structure](https:\/\/i.imgur.com\/KtBMlVm.png)\n**File naming:** \n- Folder name: `Subject ID &gt; Study UID &gt; Series UID`\n- File name:  `Series Description &gt; img_0 &gt; 1.jpg`\n\n<br>\n###2. CSV files description\n| CSV File | Description |\n| --- | --- | \n| calc_case(with_jpg_img).csv | This file contains the ***Calcification cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| mass_case(with_jpg_img).csv | This file contains the ***Mass cases patients*** with their *patient_id, breast_density, left or right breast, image view, abnormality id, abnormality type, mass shape, mass margins, assessment, pathology,\tsubtlety, jpg_fullMammo_img_path, jpg_crop_img_path,\tjpg_ROI_img_path* |\n| metadata(with_jpg_img).csv | This file contains both of the ***Mass and Calcification(Calc) patients*** with their *Series UID, Subject ID, Study UID, Series Description, Modality, SOP Class Name, SOP Class UID, Number of Images, jpg_folder_path* |\n\n<br><br>\n\n##Summary\nThe ***CBIS-DDSM (Curated Breast Imaging Subset of DDSM)*** is an updated and standardized version of the  ***Digital Database for Screening Mammography (DDSM)***.  The DDSM is a database of ***2,620 scanned film mammography studies***. It contains ***normal, benign, and malignant*** cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer.  The images have been decompressed and converted to DICOM format.  Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included.  A manuscript describing how to use this dataset in detail is available at https:\/\/www.nature.com\/articles\/sdata2017177.\n\nPublished research results from work in developing decision support systems in mammography are difficult to replicate due to the lack of a standard evaluation data set; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. Few well-curated public datasets have been provided for the mammography community. These include the DDSM, the Mammographic Imaging Analysis Society (MIAS) database, and the Image Retrieval in Medical Applications (IRMA) project. Although these public data sets are useful, they are limited in terms of data set size and accessibility.\n\nFor example, most researchers using the DDSM do not leverage all its images for a variety of historical reasons. When the database was released in 1997, computational resources to process hundreds or thousands of images were not widely available. Additionally, the DDSM images are saved in non-standard compression files that require the use of decompression code that has not been updated or maintained for modern computers. Finally, the ROI annotations for the abnormalities in the DDSM were provided to indicate a general position of lesions, but not a precise segmentation for them. Therefore, many researchers must implement segmentation algorithms for accurate feature extraction. This causes an inability to directly compare the performance of methods or to replicate prior results. The CBIS-DDSM collection addresses that challenge by publicly releasing a curated and standardized version of the DDSM for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography.\n<br><br>\n\n##Source of the Original dataset\nhttps:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n<br><br>\n\n##Citations & Data Usage Policy \nUsers must abide by the TCIA Data Usage Policy and Restrictions. Attribution should include references to the following citations:\n\n<br>\n###CBIS-DDSM Citation\n&gt;Sawyer-Lee, R., Gimenez, F., Hoogi, A., & Rubin, D. (2016). Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) (Version 1) [Data set]. The Cancer Imaging Archive. https:\/\/doi.org\/10.7937\/K9\/TCIA.2016.7O02S9CY\n\n<br>\n###Publication Citation\n&gt;Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M., & Rubin, D. L. (2017). A curated mammography data set for use in computer-aided detection and diagnosis research. In Scientific Data (Vol. 4, Issue 1). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1038\/sdata.2017.177\n\n<br>\n###TCIA Citation\n&gt;Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., & Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. In Journal of Digital Imaging (Vol. 26, Issue 6, pp. 1045\u20131057). Springer Science and Business Media LLC. https:\/\/doi.org\/10.1007\/s10278-013-9622-7\n\n\n<br><br>\nNOTE: ***The CBIS-DDSM dataset lack DICOM image for cropped and ROI images, so there are some cropped and ROI images that may not be found corresponding to their given path, so it is recommended to use the full mammography images from this dataset (only)*** biology data visualization data analytics deep learning cancer","349":" Portrait and 26 Photos Re-identification, 19 GB 27 photos of the same person - face recognition dataset # The Portrait and 26 Photos (`272` people), faces dataset\n\nEach set includes 27 photos of people. Each person provided two types of photos: one photo in profile (portrait_1), and 26 photos from their life (photo_1, photo_2, ..., photo_26).\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 7,300+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, \u2026, photo_26_extension, portrait_1_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, \u2026, photo_26_resolution, portrait_1_resolution** - photo resolution in the dataset\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/portrait-26-photos?utm_source=kaggle&utm_medium=cpc&utm_campaign=portrait-and-30-photos-test) to discuss your requirements, learn about the price and buy the dataset**\n\n## The Portrait\nThe portrait photo is a photo that shows a person in profile. Mandatory conditions for the photo are:\n- The person is pictured alone;\n- Shoulder-length photo;\n- No sunglasses or medical mask on the face;\n- The face is calm, with no smiling or gesturing.\n\n## 26 Photos\nThe rest of the photos are completely different, with one exception being that they show a person from The Portrait. There may be different people in it, taken at different times of life and in different locations. The person may be laughing, wearing a mask, and surrounded by friends.\n \n*keywords: biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, supervised learning dataset, person re-identification, person re-identification dataset, person re-Identification by photo, person re-ID dataset* arts and entertainment people people and society image online communities image classification image segmentation","350":" Surgical Scene Segmentation in Robotic Gastrectomy Surgical scene segmentation in robotic gastrectomy with real and synthetic data # Paper\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html\n\n# Abstract\nThe previous image synthesis research for surgical vision had limited results for real-world applications with simple simulators, including only a few organs and surgical tools and outdated segmentation models to evaluate the quality of the image. Furthermore, none of the research released complete datasets to the public enabling open research. Therefore, we release a new dataset to encourage further study and provide novel methods with extensive experiments for surgical scene segmentation using semantic image synthesis with a more complex virtual surgery environment. First, we created three cross-validation sets of real image data considering demographic and clinical information from 40 cases of real surgical videos of gastrectomy with the da Vinci Surgical System (dVSS). Second, we created a virtual surgery environment in the Unity engine with \ufb01ve organs from real patient CT data and 22 the da Vinci surgical instruments from actual measurements. Third, We converted this environment photo-realistically with representative semantic image synthesis models, SEAN and SPADE. Lastly, we evaluated it with various state-of-the-art instance and semantic segmentation models. We succeeded in highly improving our segmentation models with the help of synthetic training data. More methods, statistics, and visualizations on https:\/\/sisvse.github.io\/.\n\n# The contribution of our work\n- We release the \ufb01rst large-scale instance and semantic segmentation dataset, including both real and synthetic data that can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS\n- We systematically analyzed surgical scene segmentation using semantic image synthesis with state-of-the-art models with ten combinations of real and synthetic data.\n- We found exciting results that synthetic data improved low-performance classes and was very e\ufb00ective for Mask AP improvement while improving the segmentation models overall.\n\n# Data generation\nWe collected 40 cases of real surgical videos of distal gastrectomy for gastric cancer with the da Vinci Surgical System (dVSS), approved by an institutional review board at the medical institution. In order to evaluate generalization performance, we created three cross-validation datasets considering demographic and clinical variations such as gender, age, BMI, operation time, and patient bleeding. Each cross-validation set consists of 30 cases for train\/validation and 10 cases for test data. You can find the overall statistics and demographic and clinical information details [in the paper](https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html).\n\n# Object categories\nWe list \ufb01ve organs (Gallbladder, Liver, Pancreas, Spleen, and Stomach) and 13 surgical instruments that commonly appear from surgeries (Hamonic Ace; HA, Stapler, Cadiere Forceps; CF, Maryland Bipolar Forceps; MBF, Medium-large Clip Applier; MCA, Small Sclip Applier; SCA, Curved Atraumatic Graspers; CAG, Suction, Drain Tube; DT, Endotip, Needle, Specimenbag, Gauze). We classify some rare organs and instruments as \u201cother tissues\u201d and \u201cother instruments\u201d classes. The surgical instruments consist of robotic and laparoscopic instruments and auxiliary tools mainly used for robotic subtotal gastrectomy. In addition, we divide some surgical instruments according to their head, H, wrist; W, and body; B structures, which leads to 24 classes for instruments in total.\n\n# Virtual Surgery Environment and Synthetic Data\nAbdominal computed tomography (CT) DICOM data of a patient and actual measurements of each surgical instrument are used to build a virtual surgery environment. We aim to generate meaningful synthetic data from a sample patient. We annotated \ufb01ve organs listed for real data and reconstructed 3D models by using VTK. In addition, we precisely measured the actual size of each instrument commonly used for laparoscopic and robotic surgery with dVSS. We built 3D models with commercial software such as 3DMax, Zbrush, and Substance Painter. After that, we integrated 3D organ and instrument models into the unity environment for virtual surgery. A user can control a camera and two surgical instruments like actual robotic surgery through a keyboard and mouse in this environment. To reproduce the same camera viewpoint as dVSS, we set the exact parameters of an endoscope used in the surgery. While the user simulates a surgery, a snapshot function projects a 3D scene into a 2D image. According to the projected 2D image, the environment automatically generates corresponding segmentation masks.\n\n# Qualified annotations\nSeven annotators trained for surgical tools and organs annotated six organs and 14 surgical instruments divided into 24 instruments according to head, wrist, and body structures with a web-based computer vision annotation tool (CVAT). We call this real data (R). After that, three medical professionals with clinical experience inspected the annotations to ensure their quality. The three medical professionals also manually simulated virtual surgeries to generate virtual surgical scenes. We call this manual synthetic data (MS). On the other hand, we also use an automatic data generation method called domain randomization, a technique to put objects randomly in a scene to cover the variability in real-world data. We call this domain randomized synthetic data (DRS).\n\nIf the dataset is helpful for your research, [please cite the research](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-031-16449-1_53).\n\n#Contact\n- Email to yjh2020@hutom.io , mkchoi@hutom.io\n- Discussion on https:\/\/www.kaggle.com\/datasets\/yjh4374\/sisvse-dataset\n- Issues on https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n\n# Links\n- https:\/\/sisvse.github.io\n- https:\/\/github.com\/jihun-yoon\/sisvse-dataset\n- https:\/\/conferences.miccai.org\/2022\/papers\/489-Paper2739.html healthcare computer vision image-to-image image segmentation object detection","351":" MFVT Dataset for Real-time Masked Facial Detection A new masked facial dataset with more accurate mask-wearing conventions The dataset introduced in the paper [\"MFVT Dataset for Real-time Masked Facial Detection\"](https:\/\/drive.google.com\/file\/d\/1ZDmdQHI9KFFFDYbhKxun_44QBhsRDNt5\/view)\n\nNowadays, there has been quite a lot of research done on the Masked Facial Detection problem, including the introduction of new datasets and improvements to existing baseline models. However, many of the datasets that have been introduced have some limitations.\n- Some datasets, such as **MAFA**, **Moxa3K**, and **AIZOOTech**, only consider whether a person is wearing a mask or not. The detail that a person is wearing a mask in a proper way as recommended by medical experts has been ignored.\n- For that point, there are also some datasets that take it into account as well, such as **PWMFD** and **FMLD**. However, the way the authors annotate those datasets has some unreasonableness about the material of the mask and the items used alternatively for masks (scarfs, hijabs for example). Furthermore, the bounding boxes of the objects in those datasets are drawn quite carelessly.\n\nTherefore, we introduce the MFVT dataset in order to address those shortcomings of the datasets used for the Masked Facial Detection problem. To build the MFVT dataset, we selected images from previously published datasets and carefully re-annotated the entire dataset with our mask-wearing conventions.\n\nThe MFVT dataset has total of 13507 images. The overall statistics for this dataset are given in the below table:\n<img src=\"https:\/\/i.imgur.com\/V5c1Ipb.jpeg\">\n\nWe provide the COCO annotation format version of this dataset. For the meaning of class indexes, **0**, **1** and **2** correspond to **OK**, **NONE** and **WRONG**. health computer vision deep learning covid19 object detection","352":" Intraretinal Cystoid Fluid The diversified ocular disorder The dataset contains the Optical Coherence Tomography (OCT) images and their masks of Cystoid Macular Edema (CME) ocular disease for image segmentation purposes.\n\nAcquiring OCT data from Diabetic Macular Edema (DME) patients is difficult due to the lack of an open database from any ophthalmic hospital. Therefore, we obtained a database of retinal images available at [Retinal OCT Images](https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/kermany2018) in which we took a dataset of only DME subjects. From this dataset, only 1000 images were chosen by the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, that are trained to identify Cystoid Macular Edema (CME) and its progression and provide a confirmatory diagnosis of CME. Whereas 200 OCT images are taken with the consent of patients from the Institute of Ophthalmology, Jamshoro for the testing set. These OCT images were explicitly selected to contain several CME regions and a wide variety in their shapes and sizes which is utilized for both training and test sets to get better results.\n\nIn OCT images, the approach of semantic segmentation is that each pixel of an image is labeled with a corresponding clinical class representing a particular disease. The manual binary image masking was performed accordingly using the Image Segmentation application in MATLAB software offers a variety of tools (flood fill\/draw freehand) for manually selecting an image region of interest (ROI). This was necessary for the training of our model to identify and isolate the CME and to validate the accuracy of digitally picked ROIs (CME). The pixels whose value is \u201c1\u201d in the mask image indicate the foreground, that is CME, and the pixels whose value is \u201c0\u201d in the mask image indicate the background.\nThese are the actual ground truth label image and for the given input DME image, the model must predict the labels.\n\nThis dataset has been arranged and categorized under the supervision of the medical experts at Liaquat University of Medical and Health Sciences (LUMHS) Jamshoro, Pakistan, that are trained to identify CME and its progression and provide a confirmatory diagnosis of CME.\n\n\n## **Cite Our Work as:** <br>\n#### **Research Paper**\n* Ahmed Z, Panhwar SQ, Baqai A, Umrani FA, Ahmed M, Khan A. Deep learning based automated detection of intraretinal cystoid fluid. Int J Imaging Syst Technol. 2021;1-16. https:\/\/doi.org\/10.1002\/ima.22662 \n\n#### **Dataset**\n* Zeeshan Ahmed, Munawar Ahmed, Attiya Baqai, & Fahim Aziz Umrani. (2022). <i>Intraretinal Cystoid Fluid<\/i> [Data set]. Kaggle. https:\/\/doi.org\/10.34740\/KAGGLE\/DS\/2277068 diseases exploratory data analysis data visualization deep learning eyes and vision","353":" face-mask-detection-dataset   ","354":" Face Mask Detection Dataset - 500 GB of data 301 132 images, 4 types of masks worn, 75 283 unique faces, face dataset  # Face Mask Detection - Faces Dataset\nDataset includes  376 000+ images, 4 types of mask worn on 94 000 unique faces. All images were collected by **[TrainingData.pro](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)**\n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 376 000+ photos of people, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **photo_1_extension, photo_2_extension, photo_3_extension, photo_4_extension** - photo extensions in the dataset\n- **photo_1_resolution, photo_2_resolution, photo_3_extension, photo_4_resolution** - photo resolution in the dataset\n\n# Content\n\nFile with the extension **.csv** includes variables:\n- ID - image id\n- TYPE - image type\n- USER_ID - user id\n- GENDER - gender of a person\n- AGE - person's age\n- name - file name\n- size_mb - image size\n\n## Types of images:\n- **TYPE 1** - There is no mask on the face. \n- **TYPE 2**  - The mask is on, but does not cover the nose or mouth.\n- **TYPE 3** - The mask covers the mouth, but does not cover the nose.\n- **TYPE 4** - The mask is worn correctly, covers the nose and mouth.\n\n![](https:\/\/sun9-10.userapi.com\/impg\/qn0W_s_C3xVYUc_5_IUNEJ6a3xQexHj8GSLlHg\/breQf6Qthzo.jpg?size=2560x988&quality=96&sign=1d633a32909adb9c95eeb5e781e17490&type=album)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1) to learn about the price and buy the dataset**\n\n# **[TrainingData](https:\/\/trainingdata.pro\/datasets\/face-masks-detection?utm_source=kaggle&utm_medium=cpc&utm_campaign=medical-masks-part1)** provides high-quality data annotation tailored to your needs.\n\n*keywords: facial mask detection, face masks detection, face masks classification, face masks recognition, covid-19, re-identification, public safety, health, automatic face mask detection, biometric system, biometric system attacks, biometric dataset, face recognition database, face recognition dataset, face detection dataset, facial analysis, object detection dataset, deep learning datasets, computer vision datset, human images dataset, human faces dataset* people classification deep learning image online communities image-to-image","355":" Kvasir-Instrument Dataset Endoscopy instrument dataset (https:\/\/datasets.simula.no\/kvasir-instrument\/) # Kvasir-Instrument\nGastrointestinal (GI) tract pathologies are screened, biopsied, and resected (if needed) periodically using surgical tools. However, these biopsied and\/or resected areas are not tracked due to which the video analysis for assessing disease burden or the amount of pathology resection remains unknown. To tackle such issues, we have released the novel \u201cKvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy\u201d dataset, which consists of 590 annotated frames comprising of GI procedure tools such as snares, balloons, biopsy forceps, etc. By adding segmentation masks and bounding boxes information to this dataset, we enable computer vision and GI endoscopy researchers to contribute to the field of automated tool segmentation.\n\n# Dataset Details\nThe Kvasir-Instrument dataset (size 170 MB) contains 590 endoscopic tool images and their ground truth mask. The resolution of the image in the dataset varies from 720x576 to 1280x1024. The image file is encoded using jpeg compression. To the best of our knowledge, this is the first attempt to provide the GI tract organ tools dataset. The open-access dataset can be easily downloaded for research and educational purposes. To facilitate the training and testing on the same dataset, we also provide a train-test split so that researchers can build the methods and improve the results using the same dataset. The bounding box information (box coordinates (x, y, width, height)) for the corresponding images are stored in a JSON file. The dataset is designed to push the state-of-the-art solution for the automatic tool segmentation in gastrointestinal endoscopy.\n\n# Applications of the Dataset\nThe Kvasir-Instrument is intended to be used for researching and developing new algorithms for image segmentation, detection, and localization tasks. We have provided a separate file for training and validation which will assist in the development of approaches across the same dataset.\n\n# Annotation Protocol\nWe took a three-step strategy during annotation. First, the selected dataset samples were labeled by two experienced research assistants. These annotations were sent to the expert gastroenterologist for the verification. Finally, the suggested changes were incorporated, and the images were validated for those samples.\n\n# Suggested Metrics for Segmentation\nWe suggest calculating the dice similarity coefficient (DSC) and Jaccard index or Intersection over Union (IoU) for the segmentation task. The other standard metrics for medical image segmentation are precision, recall, and overall accuracy. We also suggest calculating these metrics.\n# \nSuggested Metrics for Detection and Localization\nFor the detection and localization task, we suggest calculating the average precision at different IoU thresholds. Usually, AP at IoU threshold of 50 is taken for evaluation of this dataset. We also recommend calculating overall IoU.\n\n\nFor more information:\nhttps:\/\/datasets.simula.no\/kvasir-instrument\/ health conditions","356":" Medico automatic polyp segmentation dataset polyp seg. data(https:\/\/multimediaeval.github.io\/editions\/2020\/tasks\/medico\/) The \u201cMedico automatic polyp segmentation challenge\u201d aims to develop computer-aided diagnosis systems for automatic polyp segmentation to detect all types of polyps (for example, irregular polyp, smaller or flat polyps) with high efficiency and accuracy. The main goal of the challenge is to benchmark semantic segmentation algorithms on a publicly available dataset, emphasizing robustness, speed, and generalization.\n\nParticipants will get access to a dataset consisting of 1,000 segmented polyp images from the gastrointestinal tract and a separate testing dataset. The challenge consists of two mandatory tasks, each focused on a different requirement for efficient polyp detection. We hope that this task encourages multimedia researchers to apply their vast knowledge to the medical field and make an impact that may affect real lives.\n\n# Data\nThe dataset contains 1,000 polyp images and their corresponding ground truth mask. The datasets were collected from real routine clinical examinations at Vestre Viken Health Trust (VV) in Norway by expert gastroenterologists. The VV is the collaboration of the four hospitals that provide healthcare service to 470,000 peoples. The resolution of images varies from 332\u2715487 to 1920\u27151072 pixels. Some of the images contain green thumbnail in the lower-left corner of the images showing the position marking from the ScopeGuide (Olympus). The training dataset can be downloaded from https:\/\/datasets.simula.no\/kvasir-seg\/.\n\nThe test dataset is now released. It can be downloaded from https:\/\/drive.google.com\/file\/d\/1uP2W2g0iCCS3T6Cf7TPmNdSX4gayOrv2\/view?usp=sharing. health","357":" EndoTect Dataset  polyp segmentation  and GI classification dataset (https:\/\/endotect.com\/) Dataset Details\nLabeled Images. In total, the dataset contains 10,662 labeled images stored using the JPEG format. The images can be found in the images folder. The classes, which each of the images belongto, correspond to the folder they are stored in (e.g., the \"polyp\" folder contains all polyp images, the \"barretts\" folder contains all images of Barrett\u2019s esophagus, etc.). The number of images per class are not balanced, which is a general challenge in the medical field due to the fact that some findings occur more often than others. This adds an additional challenge for researchers, since methods applied to the data should also be able to learn from a small amount of training data. The labeled images represent 23 different classes of findings.\n\nUnlabeled Images. In total, the dataset contains 99,417 unlabeled images. The unlabeled images can be found in the unlabeled folder which is a subfolder in the image folder, together with the other labeled image folders. In addition to the unlabeled image files, we also provide the extracted global features and cluster assignments in the Hyper-Kvasir Github repository as Attribute-Relation File Format (ARFF) files. ARFF files can be opened and processed using, for example, the WEKA machine learning library, or they can easily be converted into comma-separated values (CSV) files.\n\nSegmented Images. We provide the original image, a segmentation mask and a bounding box for 1,000 images from the polyp class. In the mask, the pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain polyp pixels. The bounding box is defined as the outermost pixels of the found polyp. For this segmentation set, we have two folders, one for images and one for masks, each containing 1,000 JPEG-compressed images. The bounding boxes for the corresponding images are stored in a JavaScript Object Notation (JSON) file. The image and its corresponding mask have the same filename. The images and files are stored in the segmented images folder. It is important to point out that the segmented images have duplicates in the images folder of polyps since the images were taken from there.\n\nAnnotated Videos. The dataset contains a total of 373 videos containing different findings and landmarks. This corresponds to approximately 11.62 hours of videos and 1,059,519 video frames that can be converted to images if needed. Each video has been manually assessed by a medical professional working in the field of gastroenterology and resulted in a total of 171 annotated findings.\n\nDevelopment Dataset\nThe dataset can be split into four distinct parts; Labeled image data, unlabeled image data, segmented image data, and annotated video data. Each part is further described below. In total, the dataset contains 110,079 images and 373 videos where it captures anatomical landmarks and pathological and normal findings. The results is more than 1.1 million images and video frames all together.\n\nTest Dataset\nThe dataset is split into two distinct parts; the classification dataset and segmentation dataset. The classification dataset should be used to perform the detection and speed tasks, while the segmentation part should be used for the segmentation task.\n\nMore details can be found here:\nhttps:\/\/endotect.com\/\n biology","358":" Kvasir-SEG Data (Polyp segmentation & detection) Colorectal polyp data(1000 im, GT & BB) (https:\/\/datasets.simula.no\/kvasir-seg\/) # Kvasir-SEG information:\n\nThe Kvasir-SEG dataset (size 46.2 MB) contains 1000 polyp images and their corresponding ground truth from the Kvasir Dataset v2. The images' resolution in Kvasir-SEG varies from 332x487 to 1920x1072 pixels. The images and its corresponding masks are stored in two separate folders with the same filename. The image files are encoded using JPEG compression, facilitating online browsing. The open-access dataset can be easily downloaded for research and educational purposes.\n\n# Applications of the Dataset\nThe Kvasir-SEG dataset is intended to be used for researching and developing new and improved methods for segmentation, detection, localization, and classification of polyps. Multiple datasets are prerequisites for comparing computer vision-based algorithms, and this dataset is useful both as a training dataset or as a validation dataset. These datasets can assist the development of state-of-the-art solutions for images captured by colonoscopes from different manufacturers. Further research in this field has the potential to help reduce the polyp miss rate and thus improve examination quality. The Kvasir-SEG dataset is also suitable for general segmentation and bounding box detection research. In this context, the datasets can accompany several other datasets from a wide range of fields, both medical and otherwise.\n\n# Ground Truth Extraction\nWe uploaded the entire Kvasir polyp class to Labelbox and created all the segmentations using this application. The Labelbox is a tool used for labeling the region of interest (ROI) in image frames, i.e., the polyp regions for our case. We manually annotated and labeled all of the 1000 images with the help of medical experts. After annotation, we exported the files to generate masks for each annotation. The exported JSON file contained all the information about the image and the coordinate points for generating the mask. To create a mask, we used ROI coordinates to draw contours on an empty black image and fill the contours with white color. The generated masks are a 1-bit color depth images. The pixels depicting polyp tissue, the region of interest, are represented by the foreground (white mask), while the background (in black) does not contain positive pixels. Some of the original images contain the image of the endoscope position marking probe, ScopeGuide TM, Olympus Tokyo Japan, located in one of the bottom corners, seen as a small green box. As this information is superfluous for the segmentation task, we have replaced these with black boxes in the Kvasir-SEG dataset.\n\n# Suggested Metrics\nThere are different metrics for evaluating the performance of the architectures on the image segmentation dataset. For medical image segmentation task, the most commonly used ones are Dice coefficient and Intersection over Union (IOU). Based on related work in this field, we have used these metrics for the evaluation of the algorithms. In future work, we encourage the use of these metrics for evaluating the performance of the model. In the future, it might be even better to include as many as possible metrics for the fair comparison of the models.\n\nThe bounding box (coordinate points) for the corresponding images are stored in a JSON file. This dataset is designed to push the state-of-the-art solution for the polyp detection task. Some examples of the dataset. health","359":" Medical Mask detection 6000 images of people wearing masks, scarves, face shields and other accessories ### Context\n\nHumans in the Loop is publishing an open access dataset annotated as a contribution to the worldwide fight against COVID-19. \n\n### Content\n\nThe dataset consists of 6k images acquired from the public domain with an extreme attention to diversity, featuring people of all ethnicities, ages, and regions. In addition, the dataset covers 20 classes of different accessories as well as a classification of faces with a mask, without a mask, or with an incorrectly worn mask (class list is available in meta.json).\n\n### Acknowledgements\n\nThe images were collected and annotated by the refugee workforce of Humans in the Loop in Bulgaria. The platform used for annotation is Supervise.ly. people computer vision image","360":" COVID-QU-Ex Dataset  ### COVID-QU-Ex Dataset\nThe researchers of Qatar University have compiled the COVID-QU-Ex dataset, which consists of 33,920 chest X-ray (CXR) images including:\n-\t11,956 COVID-19\n-\t11,263 Non-COVID infections (Viral or Bacterial Pneumonia)\n-\t10,701 Normal\nGround-truth lung segmentation masks are provided for the entire dataset. This is the largest ever created lung mask dataset. \n\n**If you use  COVID-QU-Ex Dataset in your research, please consider to cite the publications\/dataset below:**\n[1] A. M. Tahir, M. E. H. Chowdhury, A. Khandakar, Y. Qiblawey, U. Khurshid, S. Kiranyaz, N. Ibtehaz, M. S. Rahman, S. Al-Madeed, S. Mahmud, M. Ezeddin, K. Hameed, and T. Hamid, \u201cCOVID-19 Infection Localization and Severity Grading from Chest X-ray Images\u201d, Computers in Biology and Medicine, vol. 139, p. 105002, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.105002.\n[2] Anas M. Tahir, Muhammad E. H. Chowdhury, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Serkan Kiranyaz, Uzair Khurshid, Nabil Ibtehaz, Sakib Mahmud, and Maymouna Ezeddin, \u201cCOVID-QU-Ex .\u201d Kaggle, 2021, https:\/\/doi.org\/10.34740\/kaggle\/dsv\/3122958.\n[3] T. Rahman, A. Khandakar, Y. Qiblawey A. Tahir S. Kiranyaz, S. Abul Kashem, M. Islam, S. Al Maadeed, S. Zughaier, M. Khan, M. Chowdhury, \"Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-rays Images,\" Computers in Biology and Medicine, p. 104319, 2021, https:\/\/doi.org\/10.1016\/j.compbiomed.2021.104319.\n[4] A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. H. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj, \"Covid-19 infection map generation and detection from chest X-ray images,\" Health Inf Sci Syst 9, 15 (2021), https:\/\/doi.org\/10.1007\/s13755-021-00146-8.\n[5] M. E. H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir, Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. A. Emadi, M. B. I. Reaz, M. T. Islam, \"Can AI Help in Screening Viral and COVID-19 Pneumonia?,\" IEEE Access, vol. 8, pp. 132665-132676, 2020, https:\/\/doi.org\/10.1109\/ACCESS.2020.3010287.\n\nTo the best of our knowledge, this is the first study that utilizes both lung and infection segmentation to detect, localize and quantify COVID-19 infection from X-ray images. Therefore, it can assist the medical doctors to better diagnose the severity of COVID-19 pneumonia and follow up the progression of the disease easily.\n\nThe experiments were conducted on two CXR sets, where each set is divided into train, validation and test sets: \n1)\tLung Segmentation Data\n        Entire COVID-QU-Ex dataset (33,920 CXR images with corresponding ground-truth lung masks)\n2)\tCOVID-19 Infection Segmentation Data\n         A subset of COVID-QU-Ex dataset (1,456 Normal and 1,457 Non-COVID-19 CXRs with corresponding lung mask, plus 2,913 COVID-19 CXRs with \n         corresponding lung mask from COVID-QU-Ex dataset and corresponding infections masks from QaTaCov19 dataset).\n### References\nIn COVID-QU-Ex, the X-ray images are collected from the following repositories and studies:\n\u2022\tCOVID-19 Samples: [1- 7].\n\u2022\tNon-COVID Samples: [8- 10].\n\u2022\tNormal Samples: [8- 10].\n\n[1] QaTa-COV19 Database. https:\/\/www.kaggle.com\/aysendegerli\/qatacov19-dataset. Accessed 14 March 2021.\n[2] Covid-19-image-repository. Available: https:\/\/github.com\/ml-workgroup\/covid-19-image-repository\/tree\/master\/png. Accessed 14 March 2021.\n[3] Eurorad. Available: https:\/\/www.eurorad.org\/. Accessed 14 March 2021.\n[4] Covid-chestxray-dataset. Available: https:\/\/github.com\/ieee8023\/covid-chestxray-dataset. Accessed 14 March 2021.\n[5] COVID-19 DATABASE. Available: https:\/\/www.sirm.org\/category\/senza-categoria\/covid-19\/. Accessed 14 March 2021.\n[6] Kaggle. (2020). COVID-19 Radiography Database. Available: https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database. Accessed 14 March 2021.\n[7] GitHub. (2020). COVID-CXNet. Available: https:\/\/github.com\/armiro\/COVID-CXNet. Accessed 14 March 2021.\n[8] RSNA Pneumonia Detection Challenge. Available: https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge\/data. Accessed 14 March 2021.\n[9] Chest X-Ray Images (Pneumonia). Available: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia. Accessed 14 March 2021.\n[10] Medical Imaging Databank of the Valencia Region. PadChest: A large chest x-ray image dataset with multi-label annotated reports. Available: https:\/\/bimcv.cipf.es\/bimcv-projects\/padchest\/. Accessed 14 March 2021. universities and colleges healthcare data visualization deep learning image covid19","361":" COVID-19 Medical Face Mask Detection Dataset  COVID-19 CoronaVirus Face Mask Collection  Please read our paper:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\n### Abstract\n\nDeep learning has shown tremendous potential in many real-life applications in different domains. One of these potentials is object detection. Recent object detection which is based on deep learning models has achieved promising results concerning the finding of an object in images. The objective of this paper is to annotate and localize the medical face mask objects in real-life images. Wearing a medical face mask in public areas, protect people from COVID-19 transmission among them. The proposed model consists of two components. The first component is designed for the feature extraction process based on the ResNet-50 deep transfer learning model. While the second component is designed for the detection of medical face masks based on YOLO v2. Two medical face masks datasets have been combined in one dataset to be investigated through this research. To improve the object detection process, mean IoU has been used to estimate the best number of anchor boxes. The achieved results concluded that the adam optimizer achieved the highest average precision percentage of 81% as a detector. Finally, a comparative result with related work has been presented at the end of the research. The proposed detector achieved higher accuracy and precision than the related work.\n\n\n### Context\n\nThis Dataset conducted its experiments based on two public medical face mask datasets. The first dataset is Medical Masks Dataset (MMD)published by Mikolaj Witkowski (https:\/\/www.kaggle.com\/vtech6\/me dical-masks-dataset). The MMD dataset consists of 682 pictures with over 3k medical masked faces wearing masks. The second public masked face dataset is a Face Mask Dataset (FMD)in (https:\/\/www.kaggle.com\/andrewmvd\/face-mask-detection). The FMD dataset consists of 853 images. We created a new dataset by combining MMD and FMD. The merged dataset contains 1415 images by removing bad quality images and redundancy\n\n\n### Acknowledgements\n\nCite our papers:\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nFighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection,\nSustainable Cities and Society, Volume 65, 2021, https:\/\/doi.org\/10.1016\/j.scs.2020.102600\n\nMohamed Loey, Gunasekaran Manogaran, Mohamed Hamed N. Taha, Nour Eldeen M. Khalifa,\nA hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic,\nMeasurement, Volume 167, 2021, https:\/\/doi.org\/10.1016\/j.measurement.2020.108288\n\nLoey, M., Manogaran, G. & Khalifa, N.E.M. A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images. Neural Comput & Applic (2020). https:\/\/doi.org\/10.1007\/s00521-020-05437-x\nLoey, Mohamed; Smarandache, Florentin; M. Khalifa, Nour E. 2020. \"Within the Lack of Chest COVID-19 X-ray Dataset: A Novel Detection Model Based on GAN and Deep Transfer Learning\" Symmetry 12, no. 4: 651. https:\/\/doi.org\/10.3390\/sym12040651\nKhalifa, N.E.M., Smarandache, F., Manogaran, G. et al. A Study of the Neutrosophic Set Significance on Deep Transfer Learning Models: an Experimental Case on a Limited COVID-19 Chest X-ray Dataset. Cogn Comput (2021). https:\/\/doi.org\/10.1007\/s12559-020-09802-9\n\n### Inspiration\n\nCreating the proposed database presents more challenges\nBenha University\nhttp:\/\/bu.edu.eg\/staff\/mloey\nhttps:\/\/mloey.github.io\/\nhttps:\/\/orcid.org\/0000-0002-3849-4566\nArabic Handwritten Characters Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahcd1\nArabic Handwritten Digits Dataset\nhttps:\/\/www.kaggle.com\/mloey1\/ahdd1 health artificial intelligence classification deep learning image","362":" playing-cards-dataset | YOLO Object detection Comprehensive Playing Cards Dataset generated for YOLO Object Detection Welcome to the \"Playing Cards Dataset\" designed for YOLO Object Detection! This dataset contains high-quality images of playing cards, annotated and ready for training object detection models using the YOLO (You Only Look Once) framework. Whether you're developing an AI for card game analysis, magic tricks detection, or any other application involving playing cards, this dataset will provide the necessary data to build and fine-tune your models.\n\nLabels was generated in source (1):\n`['10', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'J', 'K', 'Q']\n['Clubs', 'Diamonds', 'Hearts', 'Spades']\n['10_Clubs', '10_Diamonds', '10_Hearts', '10_Spades', '2_Clubs', '2_Diamonds', '2_Hearts', '2_Spades', '3_Clubs', '3_Diamonds', '3_Hearts', '3_Spades', '4_Clubs', '4_Diamonds', '4_Hearts', '4_Spades', '5_Clubs', '5_Diamonds', '5_Hearts', '5_Spades', '6_Clubs', '6_Diamonds', '6_Hearts', '6_Spades', '7_Clubs', '7_Diamonds', '7_Hearts', '7_Spades', '8_Clubs', '8_Diamonds', '8_Hearts', '8_Spades', '9_Clubs', '9_Diamonds', '9_Hearts', '9_Spades', 'A_Clubs', 'A_Diamonds', 'A_Hearts', 'A_Spades', 'J_Clubs', 'J_Diamonds', 'J_Hearts', 'J_Spades', 'K_Clubs', 'K_Diamonds', 'K_Hearts', 'K_Spades', 'Q_Clubs', 'Q_Diamonds', 'Q_Hearts', 'Q_Spades'] 52`\n\nFeel free to leave comments on things I should fix or improve games card games image yolo object detection","363":" Bangladeshi Native Vehicle Dataset  Bangladeshi Native Vehicle Dataset: Cataloging Traditional Transportation ## **Overview**\n\nThe success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset\u2019s average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset\u2019s effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union(IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F8436749%2F67e47a3479c81c63a09ab4e3deed6f9c%2FDataset.jpg?generation=1716177287419838&alt=media)\n\n\n**Dataset Details**\nTotal Images: 17,326\nInstances: 81,542\nBBox Per Image - 4.7036\nVehicle Categories: 1. Bicycle 2. Bus 3. Bhotbhoti 4. Car 5. CNG 6. Easybike 7. Leguna 8. Motorbike 9. MPV 10. Pedestrian 11. Pickup 12. PowerTiller 13. Rickshaw 14. ShoppingVan 15. Truck 16. Van 17. Wheelbarrow\n\n## Model Testing Results\nThe dataset has been rigorously tested with YOLO v5-v8 models. The mean Average Precision at 50% Intersection over Union (IoU) is an impressive 84.8%.\n\n| Model        | Dataset          | mAP0.5     | mAP 0.5:0.95   | Precision  | Recall     | Weight                                                                                                                                                                   |\n| --------     | ---------------- | ---------- | -------------- | ---------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **YOLOv5**   | CARL-D           | 0.437      | 0.328          | 0.633      | 0.423      | |   \n|              | DhakaAI          | 0.416      | 0.255          | 0.640      | 0.393      | |\n|              | P2 Dhaka         | 0.655      | 0.400          | 0.804      | 0.581      | |\n|              | PoribohonBD      | 0.981      | 0.743          | 0.939      | 0.948      | |\n|              | **BNVD**         | **0.826**  | **0.609**      | **0.836**  | **0.762**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V5\/weights\/best.pt)      |\n| **YOLOv6**   | CARL-D           | 0.479      | 0.372          | 0.58       | 0.453      | |\n|              | DhakaAI          | 0.420      | 0.262          | 0.311      | 0.548      | |\n|              | P2 Dhaka         | 0.775      | 0.494          | 0.762      | 0.71       | |\n|              | PoribohonBD      | 0.899      | 0.648          | 0.899      | 0.81       | |\n|              | **BNVD**         | **0.837**  | **0.624**      | **0.805**  | **0.76**   | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V6\/weights\/best_ckpt.pt) |\n| **YOLOv7**   | CARL-D           | 0.478      | 0.369          | 0.619      | 0.459      | |\n|              | DhakaAI          | 0.464      | 0.284          | 0.692      | 0.438      | |\n|              | P2 Dhaka         | 0.743      | 0.462          | 0.816      | 0.688      | |\n|              | PoribohonBD      | 0.907      | 0.656          | 0.914      | 0.841      | |\n|              | **BNVD**         | **0.841**  | **0.623**      | **0.83**  | **0.779**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V7\/weights\/best.pt)      |\n| **YOLOv8**   | CARL-D           | 0.478      | 0.359          | 0.602      | 0.446      | |\n|              | DhakaAI          | 0.435      | 0.276          | 0.694      | 0.446      | |\n|              | P2 Dhaka         | 0.69       | 0.449          | 0.798      | 0.604      | | \n|              | PoribohonBD      | 0.889      | 0.658          | 0.898      | 0.823      | |\n|              | **BNVD**         | **0.848**  | **0.643**      | **0.841**  | **0.774**  | [Weight](https:\/\/github.com\/bipin-saha\/BNVD-Bangladeshi-Native-Vehicle-Dataset\/blob\/main\/Cheakpoints\/YOLO%20V8\/weights\/yolov8_ods_new_100e_best.pt)      |\n\n\n\n## Checkpoints\nPre-trained model weight files can be found in the \"Checkpoints\" folder of this repository.\n\n## License\nThis dataset is released under the [insert license type] license.\n\n## Citation\nIf you use this dataset in your work, please consider citing:\n\n[Insert citation information here]\n\n## Contributors\nWe would like to thank the following contributors for their valuable contributions to the development of this dataset.\n\n- Bipin Saha (bipinsaha.bd@gmail.com)\n- Md. Johirul Islam* (johirul@phy.ruet.ac.bd)\n- Shaikh Khaled Mostaque* (misha@ru.ac.bd)\n- Aditya Bhowmik (bhowmik.aditya0@gmail.com)\n- Tapodhir Karmakar Taton (tapodhirtaton@gmail.com)\n- Md Nakib Hayat Chowdhury (nakib2025@gmail.com)\n- Mamun Ibne Bin Reaz* (mamun.reaz@iub.edu.bd) exercise online communities","364":" Car Brand Recognition image datasets training images: 11060 and  test images: 2274 classifies 27 cars  computer vision image multiclass classification image classification object detection","365":" IDD Detection Modified A modified version of IDD-Detection dataset for YOLO object detection pipelines. Modified Version of Indian Driving Dataset to work for SSD and YOLO. \n\nWhat is Indian Driving Dataset?\n\nWhile several datasets for autonomous navigation have become available in recent years, they have tended to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in object or background appearance and strong adherence to traffic rules. We propose a novel dataset for road scene understanding in unstructured environments where the above assumptions are largely not satisfied. It consists of 10,000 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. The label set is expanded in comparison to popular benchmarks such as Cityscapes, to account for new classes.\n\nThe dataset consists of images obtained from a front facing camera attached to a car. The car was driven around Hyderabad, Bangalore cities and their outskirts. The images are mostly of 1080p resolution, but there is also some images with 720p and other resolutions. internet","366":" NDDA Dataset [CVPR 2024] Intriguing Properties of Diffusion Models ## Dataset for the paper entitled \"Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models\" (CVPR 2024) \n\nThe Natural Denoising Diffusion Attack (NDDA) dataset is designed to systematically evaluate the natural attack capability in diffusion models. The latest NDDA dataset consists of the following 15 classes: stop sign, car, dog, hot dog, traffic light, zebra, fire hydrant, frog, horse, bird, boat, air plane, bicycle, cat, and carrot with 6 diffusion models (Dall-E 2, Dall-E 3, Stable Diffusion 2, Deepfloyd IF, Stable Diffusion 1.5, MidJourney, and Google Duet). \n\nThe NDSS dataset is organized into `diffusion parent folder` that separate each diffusion model's set of images, which in turn contains multiple folders for each of the 15 object classes from COCO. Each object class folder then contains multiple subfolders that hold the NDD attack images, and these subfolders' names are the text prompts used to generate the set of NDD attack images. For example, if images were generated using Stable Diffusion 2 with the text prompt, `blue dog`, the path to this prompt subfolder would be `diffusion_output\\_diffusion\\_2\/blue\\_dog`. \n\nMore details are on our website: https:\/\/sites.google.com\/view\/cav-sec\/ndd-attack?authuser=0\n\n\n```bibtex\n@InProceedings{sato2022towards,\n  author    = {Takami Sato and Qi Alfred Chen},\n  title     = {{Towards Driving-Oriented Metric for Lane Detection Models}},\n  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2022}\n}\n```\n earth and nature","367":" FLIR 2024 Dataset Yolov8 Version  Thermal Object Detection for Autonomous Vehicles\nThis dataset provides real-world thermal imagery for training and evaluating object detection algorithms in autonomous vehicles. It  focuses on identifying moving objects crucial for self-driving cars, including:\n\nCars\nBicycles\nPeople\nDogs\nData Summary:\n\nSize: Over 12,000 thermal images\nFocus: Primarily annotated for cars\nApplications:\nTraining object detection models for autonomous vehicles\nResearch on thermal image analysis for self-driving applications\nIf you're working on self-driving car perception systems, this dataset can be valuable for:\n\nDeveloping and refining thermal object detection algorithms\nEvaluating the performance of object detection models in low-light or adverse weather conditions earth and nature","368":" EGYPlate EGYPlate: Egyptian Car License Plate Detection Dataset for Object Detection This meticulously curated dataset is a comprehensive collection of annotated images featuring Egyptian car license plates, tailored specifically for object detection tasks. With a focus on facilitating advanced computer vision research and development, this dataset is an invaluable resource for training, testing, and benchmarking state-of-the-art algorithms in license plate detection. law image yolo object detection yolov8","369":" Roundabout Aerial Images YOLO data  This dataset is created from the data available at https:\/\/www.kaggle.com\/datasets\/javiersanchezsoriano\/roundabout-aerial-images-for-vehicle-detection?select=roundabouts.csv .\n\n\nclasses:\n0-car\n1-cycle\n2-bus\n3-truck\n4-van\n\n\n\n programming beginner image object detection","370":" Lab1_Part3_Dataset  #Carla Object Detection Dataset\n\n##Description:\n\nThe Carla Object Detection Dataset is a labeled dataset designed for object detection tasks within the Carla Simulator environment. The images were captured using the autopilot mode in Carla Simulator across various environments, including Town01, Town02, Town03, Town04, and Town05. Frames were saved at regular intervals during simulation, providing diverse scenes for training and evaluation.\n\n##Labels:\n\nEach image in the dataset is accompanied by annotation files in three different formats:\n\nPascal VOC Format: XML files containing labels are stored in the labels folder.\nYOLO Format: YOLO-formatted label files can be found in the labels_yolo_format folder.\nMS COCO Format: JSON files with annotations in MS COCO format are located in the annotations folder.\nAvailable Classes:\n\nThe dataset includes the following object classes:\n\nVehicle (Car, Truck)\nBike\nMotorbike\nTraffic Light\nTraffic Sign earth and nature","371":" Car Detect Dataset Cars Object Detection Dataset This dataset contains car images for object detection task. The dataset is split into two folders, namely train and test. However, for training purposes, it should be split into three sets necessary for Machine Learning and Deep Learning tasks, namely train, validation, and test splits. The structure of the data is as follows:\n\n- ROOT\n   - train:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - test:\n            - img_file;\n            - img_file;\n            - img_file;\n            - ........\n            - img_file.\n\n   - meta_deta.csv.\n\nFor the object detection task, the bounding box coordinates can be obtained using meta_deta.csv file . artificial intelligence automobiles and vehicles computer vision deep learning object detection","372":" Diverse-LPD - Training Ready License Plate Detection - Ready to Train (YOLO) Diverse License Plate Detection - A combination of images from multiple LPD datasets along with negative samples to make the dataset training-ready.\nThe images are from a large number of datasets from Kaggle as well as general internet sources, as will be listed below.\n\nPurpose:\nTraining a YOLOv8 object detection model for license-plate detection.\nSee https:\/\/github.com\/ultralytics\/ultralytics for pre-trained models.\n\nData:\nThere are about 3100 images of cars with tagged license plates, on the conditions that the plate has at least one recognizable character and is not seen through the glass of another car (or reflection for that matter).\nThere are also about 1500 images which either do not contain cars at all, or contain cars but in such an angle or resolution that their license plate is not detectable. This is to address false positives which I have encountered when using some of the open-source models found on huggingface (ones that were only trained on images with at least one license-plate in them)\n*Update 24\/03\/2024: Dataset contains 5500 images, see Updates for specifications.*\n\nUpdates:\n**Update 24\/03\/2024:**\n- After using a different open-source model from huggingface trained on a dataset from roboflow on yolo5 (dataset in list here) we encountered two items of note.\n  - A. The system yielded much better results when using a Yolo8 model to first identify vehicles, and then detect plates on them\n  - B. The dataset on which the yolov5 model was trained on contained almost no negative examples, and contained a lot of duplications.\n- Added some images from a potholes dataset (listed in Sources) to help with disassociate roads with plates, and also get a few more car images.\n- Added images from the roboflow dataset which included images from a parking garage, allowing for a different angle.\n- Added images from a parking sign dataset, few with car images but mostly to disassociate street and road signs from plates found on actual cars.\n- Trained a Yolo8 model on the dataset, but did not run in production and was only really for testing. However the class loss decreased at a much faster rate than when trained with a much larger dataset consisting of very similar (quality, size, locality) positive images, even by epoch 10.\n- *Current plan moving forward: Run transfer learning on Yolo9 with this dataset for somewhere between 100 and 200 epochs (yay for Programmable Gradient Information) and post some of the model results here.*\n\n**Update 25\/03\/2024** Training update\nSplit dataset into Train, Validation, Test by ratio of 80, 10, 10.\nTrained using the ultralytics library with imgsz=640, batch=16, dropout=0.2 for 150 epochs\nValidation plateaued after about 110 epochs. Out of 100 test photos from real data there were about 5 false positives and 5 false negatives when using this model only, and 2 false positives + 1 false negative when first processing the image through YOLOv8x and filtering for [car, truck, motorcycle, bus]. Currently runs in production on Torchserve.\nFalse positives: Rectangles with lines (like grates)\nFalse negatives: Very low quality images, or images in which the plate was already very small and so disappeared when image was resized to 640.\n\nDiversity:\nThe datasets sampled for this \"diverse\" one contain plates from a variety of countries, but mainly focusing on the ones from this site which are:\n- European (+Russian)\n- American\n- Middle Eastern\n- Indian\n\nSources: (In no particular order)\n- https:\/\/www.kaggle.com\/datasets\/amriteshtiwari20\/truck-licenseplate-dataset\n- https:\/\/www.kaggle.com\/datasets\/gaelcohen\/license-plate-israel\n- https:\/\/www.kaggle.com\/datasets\/kedarsai\/indian-license-plates-with-labels\n- https:\/\/www.kaggle.com\/datasets\/mohamedalitrabelsi\/tunisiania-license-plate-detection\n- https:\/\/www.kaggle.com\/datasets\/mrabduqayum\/license-plate-detection-yolov5\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert\n- https:\/\/www.kaggle.com\/datasets\/balraj98\/stanford-background-dataset\n- https:\/\/www.kaggle.com\/datasets\/mikhailma\/house-rooms-streets-image-dataset\n- https:\/\/www.kaggle.com\/datasets\/paulchambaz\/google-street-view\n- https:\/\/www.kaggle.com\/datasets\/pcmill\/license-plates-on-vehicles\n- https:\/\/www.kaggle.com\/datasets\/psvishnu\/pennfudan-database-for-pedestrian-detection-zip\n- https:\/\/www.kaggle.com\/datasets\/rayanechibani\/dataset\n- https:\/\/www.kaggle.com\/datasets\/yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel\n- https:\/\/www.kaggle.com\/datasets\/amirhoseinahmadnejad\/car-license-plate-detection-iran\n- https:\/\/www.kaggle.com\/datasets\/aritrag\/license\n- https:\/\/www.kaggle.com\/datasets\/marshah\/license-plate-persian-coco\n- https:\/\/www.kaggle.com\/datasets\/narimanjabbar\/dataset-iraq-license-plate\n- https:\/\/www.kaggle.com\/datasets\/nuralitileuov\/car-license-plate\n- https:\/\/www.kaggle.com\/datasets\/saisirishan\/indian-vehicle-dataset\n- https:\/\/www.kaggle.com\/datasets\/sushankghimire\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/trainingdatapro\/license-plates-1-209-438-ocr-plates\n- https:\/\/www.kaggle.com\/datasets\/truongthanh081203\/license-plate\n- https:\/\/www.kaggle.com\/datasets\/andrewmvd\/pothole-detection\n- https:\/\/universe.roboflow.com\/augmented-startups\/vehicle-registration-plates-trudk\/dataset\/1\n- https:\/\/www.kaggle.com\/datasets\/mathurinache\/san-francisco-parking-sign-detection\n- Internet\n\nIt should be noted, with the size of some of these datasets, not all of the images in the datasets were used, depending on the size and variety in each dataset.\n\nLicense:\nAs specified by the datasets (those that did specify a License and did not specify \"Unknown\")\n- amriteshtiwari20\/truck-licenseplate-dataset: MIT\n- kedarsai\/indian-license-plates-with-labels: CC0: Public Domain\n- https:\/\/www.kaggle.com\/datasets\/akhiljethwa\/forest-vs-desert: CC BY-NC-SA 4.0\n- balraj98\/stanford-background-dataset: The dataset is derived from Stanford DAGS Lab's Stanford Background Dataset from their Scene Understanding Datasets, see the Acknowledgements section for credits\n- mikhailma\/house-rooms-streets-image-dataset: CC0: Public Domain\n- paulchambaz\/google-street-view: GNU Lesser General Public License\n- pcmill\/license-plates-on-vehicles: CC0 Public Domain\n- psvishnu\/pennfudan-database-for-pedestrian-detection-zip: Acknowledgements\n- yellowj4acket\/car-license-plate-detection-yolo-orig-by-larxel: CC0 Public Domain\n- amirhoseinahmadnejad\/car-license-plate-detection-iran: Database Contents License (DbCL) v1.0\n- aritrag\/license: CC0 Public Domain\n- marshah\/license-plate-persian-coco: Attribution-NonCommercial 4.0 International\n- trainingdatapro\/license-plates-1-209-438-ocr-plates: Attribution-NonCommercial 4.0 International\n- truongthanh081203\/license-plate: Apache 2.0\n\nNotes on Licenses:\nIf your dataset is included in this list and has a license that could be changed (like CC BY-NC-SA 4.0) please contact me if you'd like your photos removed from the dataset.\n\nAcknowledgements:\nS. Gould, R. Fulton, D. Koller. Decomposing a Scene into Geometric and Semantically Consistent Regions. Proceedings International Conference on Computer Vision (ICCV), 2009.\nhttps:\/\/www.cis.upenn.edu\/~jshi\/ped_html\/ law automobiles and vehicles computer vision image yolo object detection","373":" Object Detection using YOLOV3  YOLO (You Only Look Once) is a popular object detection algorithm that processes images in a single pass through a neural network to simultaneously predict bounding boxes and class probabilities for objects within those boxes. YOLOv3 is one of the versions of this algorithm, known for its balance between speed and accuracy.\n\nFor training and testing YOLOv3, you can use a variety of datasets depending on your application. Some commonly used datasets for object detection tasks, including those suitable for YOLOv3, are:\n\nCOCO (Common Objects in Context): This dataset is widely used for object detection, segmentation, and captioning tasks. It contains over 200,000 labeled images with 80 object categories.\n\nPascal VOC (Visual Object Classes): This dataset consists of images annotated with object bounding boxes and class labels. It includes 20 object categories such as person, car, dog, etc.\n\nOpen Images Dataset: This is a large-scale dataset with millions of images annotated with object bounding boxes and class labels. It covers a wide range of object categories.\n\nKITTI Vision Benchmark Suite: Primarily used for autonomous driving research, this dataset contains images captured from a moving vehicle with annotations for object detection, tracking, and scene understanding.\n\nImageNet: Although primarily known for image classification, ImageNet also contains bounding box annotations for object detection tasks. It consists of millions of images across thousands of categories.\n\nThese datasets provide a diverse range of objects in various contexts, making them suitable for training and evaluating object detection models like YOLOv3. computer science","374":" 26 Class Object detection dataset Comprehensive 26-Class Object Detection Dataset for Urban Scenes \nThe \"26 Class Object Detection Dataset\" comprises a comprehensive collection of images annotated with objects belonging to 26 distinct classes. Each class represents a common urban or outdoor element encountered in various scenarios. The dataset includes the following classes:\n\nBench\nBicycle\nBranch\nBus\nBushes\nCar\nCrosswalk\nDoor\nElevator\nFire Hydrant\nGreen Light\nGun\nMotorcycle\nPerson\nPothole\nRat\nRed Light\nScooter\nStairs\nStop Sign\nTraffic Cone\nTrain\nTree\nTruck\nUmbrella\nYellow Light\nThese classes encompass a wide range of objects commonly encountered in urban and outdoor environments, including transportation vehicles, traffic signs, pedestrian-related elements, and natural features. The dataset serves as a valuable resource for training and evaluating object detection models, particularly those focused on urban scene understanding and safety applications. computer science computer vision image multiclass classification object detection","375":" RSUD20K: Bangladesh Road Scenes Dataset RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving ## RSUD20K\n\n**Concordia University**\n\nHasib Zunair, Shakib Khan, A. Ben Hamza\n\n[[`Paper`](https:\/\/arxiv.org\/abs\/2401.07322)] [[`Dataset`](https:\/\/www.kaggle.com\/datasets\/hasibzunair\/rsud20k-bangladesh-road-scene-understanding)]\n\n**Here's a collage of outputs of RSUD20K trained model in action! Play in 4K for best results.**\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/pdRXa10SrAc\/0.jpg)](https:\/\/www.youtube.com\/watch?v=pdRXa10SrAc)\n\nThis is official code for our **paper under review at ICIP 2024**:<br>\n[RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving](https:\/\/arxiv.org\/abs\/2401.07322)\n<br>\n\nRSUD20K is a new object detection dataset for road scene understanding, comprised of over **20K** high-resolution images from the driving perspective on Bangladesh roads, and includes **130K** bounding box annotations for **13** objects. RSUD20K consists of the following classes for multi-class object detection:\n\n```bash\n# classes.txt\nperson\nrickshaw\nrickshaw van\nauto rickshaw\ntruck\npickup truck\nprivate car\nmotorcycle\nbicycle\nbus\nmicro bus\ncovered van\nhuman hauler\n```\n\nFor details on format, see [here](https:\/\/github.com\/meituan\/YOLOv6\/blob\/main\/docs\/Train_custom_data.md#1-prepare-your-own-dataset).\n\n**Dataset statistics**:\n\n| Name  | Number of images\/label pairs |\n| ------------- | ------------- |\n| `train`  | 3985 |\n| `val`  | 1004 |\n| `test`  | 649 |\n| `pseudo` (used for training)  | 14696 |\n\n\nAll code and pre-trained models for reproducibility are available in [GitHub](https:\/\/github.com\/hasibzunair\/RSUD20K).\n arts and entertainment travel","376":" Nepali Bike and Car images with annotations  The dataset comprises a diverse collection of images, focusing specifically on Nepali motor vehicles, including motorcycles and scooters, along with annotations to facilitate object detection tasks. The dataset is designed to support computer vision and machine learning applications that involve the identification and localization of vehicles, contributing to the development and training of object detection models.\n\nKey features of the dataset include a comprehensive representation of various motorcycle and scooter models commonly found in Nepal, reflecting the unique characteristics and specifications of vehicles used in the region. Additionally, the dataset includes images of cars, providing a broader scope for understanding and detecting different types of motor vehicles on the road.\n\nEach image in the dataset is accompanied by annotations, which typically include information about the bounding boxes around the vehicles and associated class labels. This annotation data is essential for training object detection algorithms, allowing the models to learn and generalize patterns for accurate detection and localization of bikes and cars within images.\n\nResearchers, developers, and data scientists can leverage this dataset to enhance the capabilities of their object detection models, particularly those focused on recognizing and analyzing Nepali motor vehicles. The dataset's specificity to the Nepali context makes it valuable for applications related to traffic monitoring, vehicle counting, and overall transportation-related analysis in the context of Nepal. artificial intelligence computer vision tensorflow nepali object detection","377":" Image Obstacle in Public Spaces Annotated Images of Obstacles in Public Spaces The \"Image Obstacles in Public Spaces\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. The description of this dataset includes several key points:\n\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (JPG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces. It is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset. arts and entertainment computer science automobiles and vehicles computer vision image eyes and vision","378":" Obstacles in Public Spaces for Dist-YOLO Annotated Images of Obstacles for Object Detection in Public Spaces The \"Obstacles in Public Spaces for Dist-YOLO\" dataset is a collection of data focusing on annotated images depicting various types of obstacles that can be encountered in public spaces. This dataset has been curated and annotated with the aim of supporting the development of the Dist-YOLO (You Only Look Once) model for object detection.\nThe description of this dataset includes several key points:\n1. Types of Obstacles: The dataset encompasses various types of obstacles that may be encountered in public spaces, such as Right Turn, Left Turn, Puddle, Street Vendor, Obstacle, Bad Road, Garbage Bin, Chair, Pothole, Car, Motorcycle, Pedestrian, Fence, Gate, Barrier, Roadblock, Door, Tree, Plant, Pot, Drain, Stair, Pole, and Zebra Cross.\n2. Annotation Purpose: Each image in the dataset has been meticulously annotated to identify the location and type of obstacles present in the image.\n3. Data Format: Data in the dataset is typically presented in image formats (e.g., JPG or PNG) that have been annotated with bounding boxes or markers to indicate the location of obstacles.\n4. Dataset Size: The dataset can contain varying numbers of images, depending on research or model development needs. Total images in dataset is 3350 images.\n5. Usage Requirement: This dataset is useful for training and testing Object Detection models, especially models like Dist-YOLO, in recognizing and classifying obstacles in public environments.\n6. Application Fields: The dataset can be utilized in various application fields, including assistive technology for the visually impaired, development of navigation systems for the blind, autonomous vehicle development, and other applications involving object detection in public spaces.\nIt is important to provide proper attribution and references to the dataset when used in research or projects, and to adhere to applicable guidelines and copyrights related to the dataset. arts and entertainment artificial intelligence computer vision deep learning image object detection","379":" Disabled People Sign Detection  This dataset contains car images with disabled people signs. The dataset includes 1241 images.\nCars are annotated in YOLOv8 format.\n\nThe following pre-processing was applied to each image:\n* Auto-orientation of pixel data (with EXIF-orientation stripping)\n* Resize to 416x416 (Stretch)\n\nThe following augmentation was applied to create 2 versions of each source image:\n\nThe following transformations were applied to the bounding boxes of each image:\n* 50% probability of horizontal flip\n* Random rotation of between -15 and +15 degrees computer vision health conditions yolo yolov5 object detection yolov8","380":" Object Detection Carla self-driving Car  CLASSES =10 \nbike, \nmotobike,\n person,\n traffic_light_green, \ntraffic_light_orange, \ntraffic_light_red, \ntraffic_sign_30,\n traffic_sign_60, \ntraffic_sign_90,\n vehicle\n\nCarla Self-Driving Car Simulator\n automobiles and vehicles deep learning gpu object detection yolov8","381":" Processed_data_for_yolov5_20ClassObjectDetection VOC_2012 Ready to use Dataset for yolo 20 class object detection This data was taken from VOC 2012 competition dataset. I have extracted the data from the .xml files into .txt files in the correct directories as required for yolo. There are 20 classes \n--&gt; labels_dictionary = {'person':0, 'car':1, 'chair':2, 'bottle':3, 'pottedplant':4, 'bird':5, 'dog':6,\n'sofa':7, 'bicycle':8, 'horse':9, 'boat':10, 'motorbike':11, 'cat':12, 'tvmonitor':13,\n'cow':14, 'sheep':15, 'aeroplane':16, 'train':17, 'diningtable':18, 'bus':19}. \n earth and nature intermediate cv2 multilabel classification image classification yolov5","382":" CelebsV2_Faces_224 DeepFake dataset celebs v2 images Dataset Name: Celeb-DF Faces Dataset\n\nDescription:\nThe Celeb-DF Faces Dataset is a curated collection of facial images extracted from the Celeb-DF dataset. This dataset focuses on providing a comprehensive set of facial images for research and analysis in the field of deepfake detection and facial image analysis. The images are categorized into two classes: \"Fake\" and \"Real,\" based on the source of the videos.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\n\nSource Folders:\n\nceleb-df-v2\/Celeb-real: Contains authentic facial videos.\nceleb-df-v2\/Celeb-synthesis: Contains synthesized (fake) facial videos.\nceleb-df-v2\/YouTube-real: Contains additional authentic facial videos from YouTube.\nOutput Folder:\n\nceleb_faces_224\/: Contains the extracted and resized facial images.\nMetadata File:\n\nmetadata_celebs.csv: A CSV file storing metadata information for each extracted image with the following columns:\nName: The filename of the extracted image.\nLabel: The label indicating whether the image is \"Fake\" or \"Real.\"\nCreation Process:\n\nVideo Frame Extraction:\n\nThe first frame from each video in the source folders is extracted.\nImage Resizing:\n\nThe extracted frames are resized to 224x224 pixels to ensure uniformity and compatibility with common machine learning models.\nImage Storage:\n\nThe resized images are saved in the celeb_faces_224\/ folder with filenames corresponding to the original video names.\nMetadata Compilation:\n\nA metadata CSV file (metadata_celebs.csv) is created to store the filenames and labels of the images, indicating whether they are from \"Fake\" or \"Real\" videos.\nIntended Use:\nThe dataset is ideal for tasks such as:\n\nDeepfake detection and analysis\nTraining and evaluation of machine learning models for facial image classification\nImage forensics research and development\nNote: This dataset is derived from the Celeb-DF dataset and is intended for research and educational purposes only. ","383":" FF++_Face_224 DeepFake dataset FF++ Dataset Name: FaceForensics++ Faces_224\nDescription:\nThe FaceForensics++ Faces Dataset is a collection of images extracted from the FaceForensics++ dataset, focusing specifically on facial imagery. It comprises two main categories: \"Fake\" and \"Real,\" representing manipulated and authentic facial images, respectively.\n\nDataset Structure:\n\nImage Size: 224x224 pixels\nFolders:\nFF++_Faces_224\/\nContains the extracted and resized images from the FaceForensics++ dataset.\nmetadata_FF++.csv\nA CSV file storing metadata information for each image, including filename (Name) and label (Label) indicating whether the image is \"Fake\" or \"Real.\"\nIntended Use:\nThe dataset is suitable for tasks such as:\n\nFacial image analysis\nDeep learning-based manipulation detection\nImage forensics research and development\nNote: This dataset is derived from the FaceForensics++ dataset and is intended for research and educational purposes only.\n\n football","384":" Contacts, Stores, Products Comprehensive Simulated Business Environment for CRM and Retail Analysis **Contacts Dataset**\nThe Contacts dataset simulates a list of individual contacts with detailed personal and contact information, useful for representing a customer database:\n\nID: A unique identifier for each contact, generated as a UUID.\nEmail: A randomly generated realistic email address.\nCountry: Randomly chosen from a list of six countries (France, US, UK, Germany, Portugal, China), represented by their respective codes.\nCity: A random city, ideally from the chosen country, although this script uses a random city generator without specific country linkage for simplicity.\nPhone: A randomly generated phone number, which should ideally include the country prefix (not implemented in this simple version).\nFirstname: A randomly generated first name.\nBirthdate: A randomly generated birth date for each contact, ranging from 1930 to 2008, making the age of contacts between 15 and 93.\nPostal Code: A postal code corresponding to the randomly chosen city.\nAcquisition Source: The source through which the contact was acquired, chosen randomly from options like Facebook ad, Google ad, promotional email, or a birthday mail campaign.\nCreated At: The timestamp when the contact record was created, ranging from 2019 to the present.\nUpdated At: The timestamp of the last update to the contact record, which is any time between the creation date and the current date.\n\n\n\n**Products Dataset**\nThe Products dataset simulates an inventory of items that might be sold by a company:\n\nID: A unique identifier for each product, generated as a UUID.\nSKU: A unique Stock Keeping Unit code for each product.\nCategories: A list of categories assigned to each product, chosen from predefined options like Electronics, Clothing, Home, Toys, Books. Each product can belong to multiple categories.\nPrice: A randomly generated price for each product, ranging from $10 to $500.\nName: A randomly generated name for the product.\nDescription: A short description for the product, generated randomly.\nParent ID: The ID of a parent product if the current product is a variant; this is left blank in the script for simplicity.\nURL: A fake URL simulating a product page on an e-commerce site.\nImage URL: A fake URL for the product\u2019s image.\nBrand: The brand of the product, either randomly selected from a list or generated.\nCreated At: The timestamp when the product record was created, within the last two years.\nModified At: The timestamp of the last update to the product record.\n\n\n**Stores Dataset**\nThe Stores dataset represents physical or conceptual locations where the company operates:\n\nID: A unique identifier for each store, generated as a UUID.\nName: The name of the store, generated by appending \"Store\" to a randomly generated company name.\nCity: The city where the store is located, chosen randomly.\nCountry: The country where the store is located, chosen randomly.\nPostal Code: A postal code for the store, generated randomly.\nCreated At: The timestamp when the store record was created, within the last two years.\nModified At: The timestamp of the last update to the store record.\n\n\nThese datasets can be used individually or combined to simulate real-world business applications like customer relationship management, inventory tracking, and retail operations. If you need these datasets linked (e.g., linking products to stores, or contacts to purchases), additional scripting would be needed to establish these relationships within the data. business exploratory data analysis data visualization retail and shopping e-commerce services","385":" Asian People - Liveness Detection Video Dataset Face anti spoofing with photos and videos of asian people # Biometric Attack Dataset, Asian People\n\n# The similar dataset that includes all ethnicities - [Anti Spoofing Real Dataset](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)\n\nThe dataset for face anti spoofing and face recognition includes images and videos of asian people. **30,600**+ photos & video of **15,300** people from **32** countries. All people presented in the dataset are  **South Asian, East Asian or Middle Asian**.  The dataset helps in enchancing the performance of the model by providing wider range of data for a specific ethnic group.\n\nThe videos were gathered by capturing faces of genuine individuals presenting spoofs, using facial presentations. Our dataset proposes a novel approach that learns and detects spoofing techniques, extracting features from the genuine facial images to prevent the capturing of such information by fake users. \n\nThe dataset contains images and videos of real humans with various **resolutions, views, and colors**, making it a comprehensive resource for researchers working on anti-spoofing technologies.\n\n### People in the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff545aa561432738d251c09f09e1f5e92%2FFrame%20104.png?generation=1713356643038606&alt=media)\n\n### Types of files in the dataset:\n- **photo** - selfie of the person \n- **video** - real video of the person \n\nOur dataset also explores the use of neural architectures, such as deep neural networks, to facilitate the identification of distinguishing patterns and textures in different regions of the face, increasing the accuracy and generalizability of the anti-spoofing models. \n\n# \ud83d\udcb4 For Commercial Usage: Full version of the dataset includes 30,600 files, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** to buy the dataset\n\n### Metadata for the full dataset:\n- **assignment_id** - unique identifier of the media file\n- **worker_id** - unique identifier of the person\n- **age** - age of the person\n- **true_gender** - gender of the person\n- **country** - country of the person\n- **ethnicity** - ethnicity of the person\n- **video_extension** - video extensions in the dataset\n- **video_resolution** - video resolution in the dataset\n- **video_duration** - video duration in the dataset\n- **video_fps** - frames per second for video in the dataset\n- **photo_extension** - photo extensions in the dataset\n- **photo_resolution** - photo resolution in the dataset\n\n### Statistics for the dataset\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2F6de78d350a9213d8437f766b085d4551%2Fasian_video_liveness.png?generation=1713356627116331&alt=media)\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection) to learn about the price and buy the dataset**\n\n# Content\nThe dataset consists of:\n- **files** - includes 10 folders corresponding to each person and including 1 image and 1 video,\n-  **.csv file** -  contains information about the files and people in the dataset\n\n### File with the extension .csv\n- **id**: id of the person,\n- **selfie_link**: link to access the photo,\n- **video_link**: link to access the video,\n- **age**: age of the person,\n- **country**: country of the person,\n- **gender**: gender of the person,\n- **video_extension**: video extension,\n- **video_resolution**: video resolution,\n- **video_duration**: video duration,\n- **video_fps**: frames per second for video,\n- **photo_extension**: photo extension,\n- **photo_resolution**: photo resolution\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/anti-spoofing-real\/?utm_source=kaggle&utm_medium=cpc&utm_campaign=asian-people-liveness-detection)** provides high-quality data annotation tailored to your needs\n\n*keywords: liveness detection systems, liveness detection dataset, biometric dataset, biometric data dataset, biometric system attacks, anti-spoofing dataset, face liveness detection, deep learning dataset, face spoofing database, face anti-spoofing, ibeta dataset, face anti spoofing, large-scale face anti spoofing, rich annotations anti spoofing dataset, asian people, asian classification, asian image dataset* people computer vision image online communities video classification cyber security","386":" Deepfake-dataset (140k + dataset real or fake)   artificial intelligence image classification","387":" Banknote-authentication-dataset-2024 Build a Model that detect fake Currency Notes Dataset Name: **Banknote Authentication Dataset**\n\n**Description:**\n\nThis dataset contains a collection of features extracted from images of genuine and counterfeit banknotes. It's commonly used to train and evaluate machine learning models for automated banknote authentication, aiming to distinguish real banknotes from forgeries.\n\n**Features:**\n\nEach data point represents a single banknote and includes the following features:\nVariance of Wavelet Transformed image (continuous): Measures image texture variation.\nSkewness of Wavelet Transformed image (continuous): Quantifies image asymmetry.\nCurtosis of Wavelet Transformed image (continuous): Captures image tailedness.\nEntropy of image (continuous): Reflects image randomness or information content.\nClass (categorical): Indicates whether the banknote is genuine (1) or counterfeit (0).\nNumber of Instances:\n\nThe dataset typically contains several hundred banknote images, with approximately equal proportions of genuine and counterfeit examples.\nSource:\n\nThe dataset was originally collected by researchers at the University of Applied Sciences, Ostwestfalen-Lippe, Germany.\nApplications:\n\nDevelop and evaluate machine learning models for banknote authentication.\nCompare the performance of different classification algorithms in this domain.\nExplore feature engineering techniques to improve model accuracy.\nInvestigate the effectiveness of various feature selection methods for identifying the most informative features for authentication.\nAdditional Notes:\n\nThe dataset is often used as a benchmark for classification tasks due to its balanced class distribution and relatively simple feature set.\nIt's essential to consider data preprocessing techniques (e.g., normalization, handling missing values) before model training.\nModel evaluation should involve metrics suitable for imbalanced classes if the distribution of genuine and counterfeit notes is skewed. finance advanced data analytics classification automl","388":" Cebradata for GAN training This datasets(205k images) will be used for GAN models training This datasets(205k images) will be used for GAN models training.\n\nGenerative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.\n\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n\nGANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples. education computer science deep learning image adversarial learning cv2 keras","389":" GENERATED Vietnamese Passports Dataset Images of Vietnamese Passports. ALL DATA IS GENERATED # GENERATED Vietnamese Passports Dataset\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's *robustness and generalization*.\n\nThe dataset contains GENERATED Vietnamese passports, which are replicas of official passports but with randomly generated details, such as *name, date of birth etc*. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to buy the dataset\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to *develop and refine models using simulated passport data without risking privacy leaks*.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12421376%2Ff2778d432611db436f18b9284daec240%2F666.png?generation=1691945421899877&alt=media)\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[https:\/\/trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset\n\n# Passports might be generated in accordance with your requirements.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-vietnamese-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems* people computer vision image image generator vietnamese","390":" Fake or Real Competition Dataset 2023 fake or real image discrimination competition dataset ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3032492%2Ff4201da2a12cae17fed7a8f5a242c78e%2F2023-07-12%20%208.33.10.png?generation=1689161628737196&alt=media)\n\n2023 Fake or Real: AI-generated Image Discrimination Competition dataset is now available on Kaggle!\n---\n\n\nHello, Kagglers\ud83d\udd90\ufe0f\n\nWe are excited to announce the release of the dataset for the 2023 Fake or Real: AI-generated Image Discrimination Competition. The competition was held on AI CONNECT(https:\/\/aiconnect.kr\/) from June 26th to July 6th, 2023, with 768 participants.\n\nIf you're interested in evaluating the performance of your model on the test dataset, we encourage you to visit the [competition page](https:\/\/aiconnect.kr\/competition\/detail\/227\/task\/295\/taskInfo) on AI CONNECT and submit your results. Please note that it supports only Korean yet. Of course we data scientists can always use Chrome translate, and\/or even better translation models\ud83e\udd73. Plus, multilingual service will be provided in the (hopefully near) future, so please stay tuned!\n\n\n# Background\nAs the advancement of generative AI technology has enabled the easy creation of indistinguishable fake information from genuine content, concerns regarding its misuse have surfaced. Image generation AI, in particular, has raised significant alarm due to its potential risks such as identity theft, revenge porn, and political manipulation. In response, it has become imperative to develop technologies that can effectively discern between real and AI-generated fake images.\n\nThe training dataset consists of diffusiondb (https:\/\/huggingface.co\/datasets\/poloclub\/diffusiondb) and Flickr images, with the inclusion of some low-quality fake images. For the test dataset, we took measures to construct it in a manner that closely resembles real-world scenarios involving image misuse. We utilized multiple generative AI models, fine-tuned on diverse photorealistic datasets, and applied negative prompt keywords like 'cartoon' and 'too many fingers' to generate realistic images.\n\nWe hope this dataset encourages the development of robust solutions and stimulates discussions on tackling the challenges associated with AI-generated fake images. \n\nBest Regards,\nAI CONNECT\n\n computer science classification image","391":" GENERATED USA Passports Augmentation Dataset of USA Passports with Augmentation. ALL DATA IS GENERATED # GENERATED USA Passports Dataset\n\n**Data generation** in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative examples that cover a wide range of scenarios, ensuring the model's robustness and generalization.\n\nData augmentation techniques involve applying various transformations to existing data samples to create new ones. These transformations include: *random rotations, translations, scaling, flips, and more*. Augmentation helps in increasing the dataset size, introducing natural variations, and improving model performance by making it more invariant to specific transformations.\n\n# \ud83d\udcb4 For Commercial Usage: To discuss your requirements, learn about the price and buy the dataset, leave a request on **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to buy the dataset\n\nThe dataset contains **GENERATED** USA passports, which are replicas of official passports but with randomly generated details, such as name, date of birth etc. The primary intention of generating these fake passports is to demonstrate the structure and content of a typical passport document and to train the neural network to identify this type of document.\n\nGenerated passports can assist in conducting research without accessing or compromising real user data that is often sensitive and subject to privacy regulations. Synthetic data generation allows researchers to develop and refine models using simulated passport data without risking privacy leaks.\n\n### The dataset is solely for informational or educational purposes and should not be used for any fraudulent or deceptive activities.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F618942%2F30c6650541e63733f9ea0fcdc3bfc2cb%2FMacBook%20Air%20-%201%20(2).png?generation=1688719414649908&alt=media)\n\n# Content\n\n### Folders\n- **original**: includes original generated images of USA passports\n- **augmentation**: contains subfolders, corresponding to the original photos and including 3 black and white generated passport scans with different photo editing.\n\nThe augmentated photos are presented with random rotations, noise and brightness. Augmentation varies depending on the amount of noise and blur in the passport images, from slight (**us_pass_augmentated_1**) to significant (**us_pass_augmentated_3**).\n\n### File with the extension .csv\n\nincludes the following information for each media file:\n\n- **original**: link to access the image of the generated passport,\n- **us_pass_augmentated_1**: link to the first augmentated image, \n- **us_pass_augmentated_2**: link to the second augmentated image, \n- **us_pass_augmentated_3**: link to the third augmentated image\n\n# USA Passeport Photos might be generated in accordance with your requirements.\n\n# \ud83d\udcb4 Buy the Dataset: This is just an example of the data. Leave a request on **[trainingdata.pro\/datasets](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** to discuss your requirements, learn about the price and buy the dataset.\n\n## **[TrainingData](https:\/\/trainingdata.pro\/datasets\/synthetic-data?utm_source=kaggle&utm_medium=cpc&utm_campaign=generated-usa-passeports-dataset)** provides high-quality data annotation tailored to your needs\n\n*keywords: image dataset, generated data, passports, passport designs, machine-readable zone, mrz, synthetic data, synthetic data generation, synthetic dataset , gdpr synthetic data, data augmentation, object detection, computer vision, documents, document security, cybersecurity, information security systems, augmentation dataset, data augmentation, augmented images, image augmentation* people computer vision image image generator","392":" Indian Coin Images Dataset (incl. fake coins) Contains images of Indian coins (real and fake) of denominations Rs 1, 2 and 5.  ![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F8cdd2e3fbfb1a3e97a9ff5733068564c%2Fvending%20machine.jpg?generation=1683391775041836&alt=media)\nVending machines and money changers deal with taking in cash form a user, identifying the denomination of the cash, and detect cash's validity ( i.e is it a real or a counterfeit (fake)). Coins seem to be a major type of cash that are exchanged in these machines. The current methods of identifying the denomination of coins by the these machines commonly involves measuring the weight, size, and thickness of the coin using techniques such as light sensing, and detecting weather the coin is real or fake involves obtaining the composition of the coin to detect if it is metallic or not using techniques such as electromagnetic field transmission. These methods do work, except there are two problems:\n1. In many countries (like India), the versions of coins keep changing overtime and consequently the dimensions and compositions of the coins change too making it difficult to identify and detect coins using the afore mentioned methods.\n2. Fake coins can be made of metallic plates that resemble the size and composition of real coins, when such plates are inserted into the machine, they might get falsely detect as valid coins. \n\nOne potential way to solve these problems would be to use Machine Learning to identify the coin denomination and detect it's validity. But how? An ML model can be trained on a dataset of images of both real and fake coins of various denominations. If the images were captured with a flash, the amount of light reflected off of the surface of metallic coins would be higher compared to that of non-metallic fake coins. Also the higher reflection from metallic surface would produce regions of bright spots only in images of metallic coins.\n\nThe figure below shows some images captured with flash, taken from the dataset, showing some differences present between the images of real and fake coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F5bf4e9119046d51e7c9258eada047ae0%2FRealVsFakeCoins.jpg?generation=1683396232330130&alt=media)\n(a) & (b) are images of a real coin, showing reflected light as bright spots on the coin.\n(c) is the image of a fake non-metallic coin, not showing any apparent bright spots of light.\n(d) is fake too but is a metallic plate and thus showing spots of brightness, but lacks the presence of imprints of denomination, the rupee symbol and artistry.\n(e) is fake, made by printing the image of a real coin on a piece of paper and pasted on a plastic plate, and thus the image contains both the spots of brightness and the information and artistry similar to that of a real coin but lacks the lustrous appearance of a real coin. Also the paper surface appears rougher compared to a real coin.\n\nKeeping in mind these differences, this dataset was created with 1750 images of various real and fake coins captured with flash and the images are classified into:\n1. The front face of real\/valid Indian coins consisting the imprint of common denominations-\nRs 1, Rs 2, and Rs 5.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F3cbee0087a974d872ff3684709e9ad2d%2Freal_coins_front.png?generation=1683302664681760&alt=media)\nThe images of front face of 1 rupee coins are contained in the folder named '1_rupee', images of 2 rupees in folder named '2_rupee' and 5 rupees in '5_rupee'.\n\n2. The back or reverse face of real\/valid coins that do not show their denomination values.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2F23601ab0ffde0cf1465058ea94e03f80%2Freal_coins_back.png?generation=1683302796939179&alt=media)\nThese images are contained in the folder named 'reverse'\n\n3. Fake\/Invalid coins.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fb4adf4b82266250237fcf61722732aa5%2Ffake_coins.png?generation=1683302878176987&alt=media)\nThese images are contained in the folder named 'invalid'\n\n\nThe images were originally captured with a 12MP camera with a resolution of around 2000x4000 pixels and were processed in OpenCV to detect the region of the coin within the image, crop it along the region and resize it to a resolution of 256x256 pixels. \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F12921587%2Fa9b0997b7704c2d679eb12db880e5f5f%2FImageProcessing.png?generation=1683394637379764&alt=media)\nThis preprocessing was done to make it easier for ML models to learn important features. The code for processing and the steps involved in the processing are described in detail in this Github repo.\n\nThus a model trained on this dataset potentially can:\n1. Classify coin denominations based on the features present in the coin images such as the denomination value and designs imprinted on the face of the coin.\n2. Further classify the coin as valid(real) or invalid(fake) based on the features such as presence or absence of information and artistry, the amount of light present, the region of bright spots present or absent in the image, among other factors.\n\n&gt;I have created a notebook in which I have built a ConvNet model to classify the coins in this dataset, you can check it out [here](https:\/\/www.kaggle.com\/code\/mssomanna\/coin-classification-using-cnn\/).\n\n&gt;I have also hosted a project on GitHub with the same dataset, notebook and other relevant items, you can check it out [here](https:\/\/github.com\/ms-somanna\/Coin-Denomination-and-Validity-Detection-using-Neural-Network-and-Coin-Images). india neural networks cnn currencies and foreign exchange image classification","393":" Handwritten Signature Identification Simplified version of handwritten signatures dataset in only 1MB by FCIS-ASU Simplified version of Handwritten Signature datasets where it has 5 classes (PersonA...PersonE) splited into training\/ testing folders. and a `CSV files` folder containing whether its real or forged for each class in training \/ test.\nThe goal here is to train 2 models (1 for identifying the person sig. and 2 for validating that it is a real one).\nThis dataset is part of the Computer Vision course by FCIS-ASU divided into 2 main tasks :\n&gt; 1. Signature Identification : Building a classical vision or DL techniques to identify the person of this signature.\n&gt; 2. Signature Verification : Build a 5 different models to identify for each class if it is a real or fake OR build a 1 siamese network that verifies the signature of the person whether it is a real one or fake one. classification deep learning multiclass classification image classification image feature vector","394":" CIFAKE: Real and AI-Generated Synthetic Images Can Computer Vision detect when images have been generated by AI? # CIFAKE: Real and AI-Generated Synthetic Images\nThe quality of AI-generated images has rapidly increased, leading to concerns of authenticity and trustworthiness.\n\nCIFAKE is a dataset that contains 60,000 synthetically-generated images and 60,000 real images (collected from CIFAR-10). Can computer vision techniques be used to detect when an image is real or has been generated by AI?\n\nFurther information on this dataset can be found here: [Bird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290)\n\n## Dataset details\nThe dataset contains two classes - REAL and FAKE. \n\nFor REAL, we collected the images from Krizhevsky & Hinton's [CIFAR-10 dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)\n\nFor the FAKE images, we generated the equivalent of CIFAR-10 with Stable Diffusion version 1.4\n\nThere are 100,000 images for training (50k per class) and 20,000 for testing (10k per class)\n\n## Papers with Code\nThe dataset and all studies using it are linked using [Papers with Code](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n[https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images](https:\/\/paperswithcode.com\/dataset\/cifake-real-and-ai-generated-synthetic-images)\n\n\n## References\nIf you use this dataset, you **must** cite the following sources\n\n[Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images.](https:\/\/www.cs.toronto.edu\/~kriz\/learning-features-2009-TR.pdfl)\n\nBird, J.J. and Lotfi, A., 2024. CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. IEEE Access.\n\nReal images are from Krizhevsky & Hinton (2009), fake images are from Bird & Lotfi (2024). The Bird & Lotfi study is available [here](https:\/\/ieeexplore.ieee.org\/abstract\/document\/10409290).\n\n## Notes\n\nThe updates to the dataset on the 28th of March 2023 did not change anything; the file formats \".jpeg\" were renamed \".jpg\" and the root folder was uploaded to meet Kaggle's usability requirements.\n\n## License\nThis dataset is published under the [same MIT license as CIFAR-10](https:\/\/github.com\/wichtounet\/cifar-10\/blob\/master\/LICENSE):\n\n*Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:*\n\n*The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.*\n\n*THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.* artificial intelligence computer vision deep learning image image classification","395":" AI recognition dataset a dataset of human generated art  and AI generated art. A Dataset comprised of two parts, images generated by AI image generation models such as DALL-E and Midjourney, and real images known to be made by humans. The majority of AI generated images are artistic works of some type and not photorealistic because it was found that having more artistic works than photos in the human generated set yielded better test results. One major issue found when trying to train classifiers on this set is while a test accuracy as high as 94% was achieved, if the image (regardless of source AI or human) contained noise such as a film grain or fur there was a higher error rate and the image was more likely to be mislabeled as AI generated. My theory is because diffusion image generation models (DALL-E etc.) start with random noise and turn it into an image based on the prompt, so the classifier could be using the noise of the image as a way to detect Ai generated art and by adding noise the model is getting confused. One possible solution to this is using image denoising on the image or edge detection however I have yet to test either. \n\n\n## Benefits over other datasets\n\nThe benefit of this dataset compared to other artificially generated image datasets (such as CIFAKE) is that all images are in there original size and aspect ratio.\n\n arts and entertainment artificial intelligence image image classification image generator","396":" Liveness Detection - Zalo AI Challenge 2022 Replay-attack Video Anti-Spoofing Dataset Problem statement\n\nIn verification services related to face recognition (such as eKYC and face access control), the key question is whether the input face video is real (from a live person present at the point of capture), or fake (from a spoof artifact or lifeless body). Liveness detection is the AI problem to answer that question.\n\nIn this challenge, participants will build a liveness detection model to classify if a given facial video is real or spoofed.\n\n- Input: a video of selfie\/portrait face with a length of 1-5 seconds (you can use any frames you like).\n\n- Output: Liveness score in [0...1] (0 = Fake, 1 = Real).\n\nExample:\n\n- Input: VideoID.mp4\n\n- Output: Predict.csv\n\nfname, liveness_score\nVideoID.mp4, 0.10372\n...\t...\n...\t... image video binary classification image classification video classification","397":" Gemstones  Dataset is consist of 3 different kinds of gemstones\n1. Ruby\n2. Turquoise\n3. Emerald\n\nThe purpose to collect the dataset was to make a model with Convolutional Neural Network Algorithm to identify real or fake gemstones.\nThe Dataset is split into three folders\n1. Train for training the model \n2. Test for Testing the model\n3. Validation for Validating the model\nEach Folders have six subfolders:\n1. Emerald ~ 507 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n2. Fake Emerald ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n3. Ruby ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n4. Fake Ruby ~ 536 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n5. Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n6. Fake Turquoise ~ 500 Images in Train ~ 250 Images in Test ~ 250 Images in Validation\n\nTotal of 6043 Images are in this dataset. It can be used for Classification Problem (Binary and Multiple). I didn't used any prebuild model I suggest you to use pre-trained model with it to increase the accuracy. The highest Accuracy I achieved is 95% and Best of luck for you... categorical advanced classification cnn image","398":" Real vs fake faces Discriminate Real and Fake Face Images **About Dataset**\nThis dataset contains real and fake images of human faces.\nReal and Fake Face Detection\nFake Face Photos by Photoshop Experts\nIntroduction\nWhen using social networks, have you ever encountered a 'fake identity'?\nAnyone can create a fake profile image using image editing tools, or even using deep learning based generators.\nIf you are interested in making the world wide web a better place by recognizing such fake faces, you should check this dataset. computer science computer vision deep learning image online communities","399":" Synthetic Chess Board Images Photorealistic synthetic images representing pictures of chess game states ### Context\n\nData collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.\n\nWith high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. \n\nAs an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.\n\n### Content\n\nThe dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.\n\nThe data has been generated using the Python scripts and .blend file present in [this repository](https:\/\/github.com\/TheFamousRat\/ChessR). The chess board and pieces models that have been used for those renders are not provided with the code.\n\nData characteristics :\n\n- Images : 1280x1280 JPEG images representing pictures of chess game boards.\n- Annotations : JSON files containing two variables : \n    - \"config\", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.\n    - \"corners\", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.\n- config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : \n    - \"cellsCoordinates\", a dictionary associating a cell name to its coordinates on the board. We have for example {\"A1\" : [0,0], \"A2\" : [1,0], ...}\n    - \"piecesTypes\", a list of strings containing the types of pieces present in the renders.\n\nNo distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.\nA proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.\n\n### Acknowledgements\n\nI would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. \n\n### Inspiration\n\nTwo interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : \n\nErroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt (2021) *Fake It Till You Make It: Face analysis in the wild using synthetic data alone* https:\/\/arxiv.org\/abs\/2109.15102\nSalehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) *PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision* https:\/\/arxiv.org\/abs\/2112.09290 board games computer vision classification image","400":" deepfake and real images Detect if any images is real image of deepfake image This dataset contains manipulated images and real images. The manipulated images are the faces which are created by various means. The source for this dataset was [https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd](https:\/\/zenodo.org\/record\/5528418#.YpdlS2hBzDd)\nthis dataset was processed as our will to get maximum outcome out of these images. Each image is a 256 X 256 jpg image of human face either real or fake research people and society image social networks ml ethics","401":" Israeli Parliament (Knesset) Members  All members of the Israeli Parliament with affiliation ### Context\n\nData on the Israeli Knesset from the first Knesset to the 24th. \n\n\n\n### Content\n\nFor Knesset 1-24 this contains:\n- Names of all politicians\n- The Party they are affiliated with\n- The number(s) of parliaments each politician was active in \n- Gender\n- Place of Birth\n- Place of Death\n- image link\n- date of birth\n- primary language \netc.\n\n\nSource:\nhttps:\/\/main.knesset.gov.il\/mk\/Pages\/current.aspx?pg=mklist\nWikipedia\nWikidata\n\n### Inspiration\nDuring 2019-2020 isreal was stuck in a political deadlock with election following election. This contribution was done in the hope to allow easier access to political data which will enable verifying news as fake or real news.  politics middle east","402":" Indian players under 25 in the ISL 23-24 season Find players that can play for India to qualify for the world cup This dataset is extracted from https:\/\/www.indiansuperleague.com on 6\/25\/2024 at 5:15:00 PM  It contains data of Indian players under the age of 25 for the ISL 2023-2024 season. This dataset aims to find players who can play for India in the upcoming years to qualify for the World Cup. football","403":" Euro 2024 Group Stage FBRef Scrape Data In-Depth Data from Euro 2024 Group Stage on FBRef This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures football intermediate text python other","404":" Teams and players stats from FBRef Data from FBRef about top leagues and relative teams and players This dataset contains detailed statistics on soccer players for the 2023-2024 season, sourced from FBRef. FBRef is a renowned source for soccer statistics and analysis, providing comprehensive and accurate data on players and teams from various leagues and international competitions. The data was extracted using web scraping techniques. \nFor more information, please refer to the GitHub [repository](https:\/\/github.com\/dyomed93\/Scraping-Analysis-FbRef). football beginner data analytics text python","405":" Super Lig Player Informations Turkiye Super Lig 2023-2024 Player Informations (Number, Team, Skills, Value...) #**Description**\nThis dataset contains detailed information on players participating in the Super Lig for the 2023-2024 season. It includes players' numbers, teams, positions, preferred foot, attacking (ATT), technique (TEC), tactical (TAC), defense (DEF), creativity (CRE) skill ratings, market values, and nationalities. With a total of 630 rows, this dataset is a valuable resource for analysis and research purposes.\n\n#**Subtitles**\n**Number:**Player's jersey number\n**Team:** The team the player is playing for\n**Position:** The position the player is playing (e.g., forward, midfielder, defender)\n**Preferred Foot:** The player's dominant foot (right, left)\n**ATT:** Attacking skill rating\n**TEC:** Technique skill rating\n**TAC:** Tactical skill rating\n**DEF:** Defense skill rating\n**CRE:** Creativity skill rating\n**Market Value:**The market value of the player (in million \u20ac)\n**Nationality:** The player's nationality\n\n#**File Information**\n**File Name:** Super Lig 2023-2024.csv\n**File Size:** 39.68KB\n**Format:** CSV (Comma-Separated Values)\n\n#**Author**\nThis dataset has been compiled to provide detailed information on players in the Super Lig for the 2023-2024 season for analysis and research purposes. football europe categorical middle east","406":" Fbref Football Leagues Data 2023 2024  **Comprehensive Football Player Statistics: 2023-2024 Season**\nThis dataset contains detailed player statistics from top football leagues for the 2023-2024 season. Sourced from FBref, the dataset includes a wide range of metrics covering various aspects of player performance, such as defense, goalkeeping, passing, and shooting.\n\n**Key Features**\nDetailed Player Metrics: Statistics for individual players across multiple performance areas.\nStructured Data: Organized into tables focusing on different aspects of the game for easy analysis.\nTop Leagues: Includes data from prominent leagues that provide comprehensive detailed stats.\n\nGithub Repository link of the project : https:\/\/github.com\/GuechtouliAnis\/Football-Data-Scraping\n\nBy: Guechtouli Anis football sports","407":" UEFA EURO 2024 - Players Basic info of players from UEFA EURO 2024, collected on Transfermarkt Dataset of all the players that are in the squad of the teams participating in the UEFA EURO 2024. Contains info about clubs, age, height, market value etc. which can be very good for EDA and Data Visualizations. football sports data visualization data analytics data storytelling","408":" 2022\/23 Big 5 Football Leagues Player Stats Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/ All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions football sports data analytics random forest regression","409":" Turkish Super League Turkish Super League 2023\/2024 Mid-Season Player Statistics **Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n football statistical analysis","410":" IPL Auction Dataset Data about Indian Premier League Auction of all teams Context-\nThe data was created from the IPL auction stats displaying funds and players\nSource-\nThe data was scrapped from https:\/\/www.iplt20.com\/auction\/2021 \nAbout-\nEight franchises buy 57 players at IPL 2021 Player Auction The VIVO IPL 2021 Player Auction concluded in Chennai with 57 players filling up the 61 available slots. The eight franchises went for some of the finest talent available, giving them a chance to unleash their potential and make an impact in the upcoming edition of the tournament. football cricket india categorical text english","411":" Football_players_transfer_values_2024 Football players transfer value based on their ratings, skills and attributes This dataset contains detailed information on 3,060 unique football\/soccer players. Each entry provides a comprehensive profile of a player, including personal details, overall performance ratings, specific skills, and various attributes. The dataset is structured to offer insights into player capabilities, potential, and market value. \n\nBelow is a detailed description of the columns included in the dataset:\n\n**ID:** Unique identifier for each player.\n**Age:** Age of the player in years.\n**Height:** Height of the player in centimeters.\n**Weight:** Weight of the player in kilograms.\n**Preferred foot:** The preferred foot of the player (e.g., Left, Right).\n**Overall rating:** The current overall rating of the player, reflecting their performance level.\n**Potential:** The potential rating indicating the highest possible overall rating a player can achieve.\n**Best overall:** The highest overall rating achieved in any position by the player.\n**Best position:** The position where the player performs best.\n**Growth:** The difference between the player's current overall rating and their potential.\n**Value:** The market value of the player, usually in currency (e.g., Euros).\n**Wage:** The weekly wage of the player.\n**Release clause:** The release clause value in the player's contract.\n**Total attacking:** The total score of all attacking attributes combined.\n**Crossing:** Ability to accurately deliver the ball from wide areas.\n**Finishing:** Ability to score goals in one-on-one situations.\n**Heading accuracy:** Accuracy of headers when trying to score or pass the ball.\n**Short passing:** Precision and accuracy of short-distance passes.\n**Volleys:** Technique and power in volley shots.\n**Total skill:** The combined score of all skill-related attributes.\n**Dribbling:** Ability to maintain control of the ball while maneuvering around opponents.\n**Curve:** Ability to curve the ball, typically used in shots and passes.\n**FK Accuracy:** Accuracy in taking free kicks.\n**Long passing:** Precision and accuracy of long-distance passes.\n**Ball control:** Ability to control and manipulate the ball effectively.\n**Total movement:** The combined score of all movement-related attributes.\n**Acceleration:** Quickness of reaching top speed.\n**Sprint speed:** Top speed a player can achieve.\n**Agility:** Ability to quickly change direction and position.\n**Reactions:** Quickness of responding to situations during the game.\n**Balance:** Stability and coordination while moving or under pressure.\n**Total power:** The combined score of all power-related attributes.\n**Shot power:** Strength and power in shots on goal.\n**Jumping:** Ability to jump vertically.\n**Stamina:** Endurance and ability to sustain performance over time.\n**Strength:** Physical strength and ability to win physical duels.\n**Long shots:** Accuracy and power in long-distance shots.\n**Total mentality:** The combined score of all mentality-related attributes.\n**Aggression:** Level of intensity and physicality in play.\n**Interceptions:** Ability to intercept passes and disrupt the opposition's play.\n**Att. Position:** Positioning intelligence when attacking.\n**Vision:** Ability to see and execute key passes and plays.\n**Penalties:** Ability to score from penalty kicks.\n**Composure:** Calmness and performance under pressure.\n**Total defending:** The combined score of all defending-related attributes.\n**Defensive awareness:** Understanding and anticipation of defensive responsibilities.\n**Standing tackle:** Effectiveness in making standing tackles.\n**Sliding tackle:** Effectiveness in making sliding tackles.\n**Total goalkeeping:** The combined score of all goalkeeping attributes.\n**GK Diving:** Ability to dive and save shots.\n**GK Handling:** Ability to catch and control the ball.\n**GK Kicking:** Accuracy and power of goal kicks.\n**GK Positioning:** Positioning intelligence in goalkeeping situations.\n**GK Reflexes:** Quickness of reflexes to make saves.\n**Total stats:** The overall total of all individual attribute scores.\n**Base stats:** The base statistics without modifiers or bonuses.\n**International reputation:** Reputation of the player on the international stage (e.g., 1 to 5 stars).\n**Pace \/ Diving:** Combined score of Pace attributes for outfield players or Diving for goalkeepers.\n**Shooting \/ Handling:** Combined score of Shooting attributes for outfield players or Handling for goalkeepers.\n**Passing \/ Kicking:** Combined score of Passing attributes for outfield players or Kicking for goalkeepers.\n**Dribbling \/ Reflexes:** Combined score of Dribbling attributes for outfield players or Reflexes for goalkeepers.\n**Defending \/ Pace:** Combined score of Defending attributes for outfield players or Pace for goalkeepers.\n\nThis dataset offers a rich resource for analyzing player performance, potential, and market value in the world of football\/soccer. It can be utilized for various purposes, including player scouting, performance analysis, and strategic planning for team management.  football sports beginner data cleaning data analytics","412":" Premier League Player Statistics 2022-2024 Game by game statistics of all current PL players for the past 2 seasons All statistics sourced from FBRef and Transfermarkt and scraped using Ruby on Rails. This dataset contains 3 CSVs, \"Teams\" with basic info (just name and FBRef link), \"Players\" with name and summary statistics for the 2023-2024 season as well as their age and value from Transfermarkt, and \"Match Logs\" which has in-depth information for every Premier League game the player's team took part in over the past 2 seasons (or whatever length of time the player was on said team). football","413":" Football - Soccer - UEFA EURO, 1960 - 2024 Football matches, players & events for EURO 1960-2024 & Nations League 2019-2023 **Dataset will be updated during EURO 2024**<br>\n**You may support this project via the upvotes button**\n\n## Description \nThe dataset contains all players & coaches, all matches & results, and main match events in Football\/Soccer UEFA European Championship\/EURO (1960-2024), and Nations League (2019-2023).\n\n* `matches`:\n * `euro`: `1960.csv` ...  `2024.csv` -  all EURO matches.\n * `nations`: `2019.csv`, `2021.csv`, `2023.csv` - all Nations League matches.\n * `friendly_2021-2024.csv` - all friendly matches from 2021 to 2024.\n * `qualifying_1960-2024.csv` - all qualifying matches from 1960 to 2024.\n* `logos` - contains flags of associations\n*  `euro_coaches.csv` - all coaches from 1960 to 2024\n* `euro_lineups.csv` - all players from 1960 to 2024\n* `euro_summary.csv` - table includes basic information about each EURO.\n\n## Related Datasets\n- [Football - FIFA Men's World Cup, 1930 - 2022](https:\/\/www.kaggle.com\/datasets\/piterfm\/fifa-football-world-cup)\n- [Football - FIFA Women's World Cup, 1991 - 2023](https:\/\/www.kaggle.com\/datasets\/piterfm\/football-fifa-womens-world-cup-1991-2023)\n\n## Table columns\n\n## Updates\n2024-06-16 - add lineups & coaches; add associans flags folder.<br>\n2024-06-09 - add friendly, qualifying & Nations League matches. football sports data analytics tabular","414":" Football_Player_Dataset_for_YOLOV8_Hosted   ","415":" Football Player Dataset for YOLOV8   football","416":" Football Manager England Scout Dataset A scout database for Northampton FC in Football Manager 23(Season 2030-31) This dataset is collected from a simulated save file of the game Football Manager 23. \n\nNorthampton FC has clawed their way upto the second tier of English football, the Championship in 2030-31 season. After a rough season and nearly escaping relagation, they are focusing heavily on their player recruitment to become an established Championship team. \n\nNorthampton finished 21st in the table, securing only 46 points with a goal difference of -21. Their lack of goalscoring ability upfront and leaky defense at the back has made the club rethink their recruitment strategy.\n\nFrom the list of 422 scouted players, they're actively looking for :\n\n- A clinical striker with a keen eye for goal.\n- A creative midfielder to provide high-quality chances\n- A hard working, physical midfielder to provide cover for the creative midfielder.\n- An aerially dominant Center-back who is also sound in defense. \n- A wingback who covers a lot of ground and dishes out quality crosses inside the final third.\n\nTo accomodate the signings, the Northampton FC board has allowed an extra wage budget of 60k\/week.\n\nYour job as a manager is to find the best recruitments within the wage budget. games video games football sports data analytics","417":" 2024 NWSL Womens Soccer League Player Stats FULL 2024 NWSL Season (so far) player stats for 343 players. Updated monthly. 2024 NWSL (so far) complete player stats. The dataset will be updated monthly throughout the season as the season progresses. \n\nThe data is mostly scraped from NWSL.com. It has been cleaned, and additional information has been added, such as position.\n\nThe current file is scraped for the current 2024 season through May 25, 2024, and will be updated monthly with updated information as NWSL posts updates.\n\nThere is currently data for 344 players across 40 variables. \n\nHere is the type of information included:\n\n- team\n- player_name\n- position\n- games_played\n- games_started\n- minutes_played\n- goals\n- accurate_pass_percentage\n- assists\n- total_scoring_attempts\n- on_target_scoring_attempts\n- total_attacking_assists\n- tackles\n- fouls_committed\n- fouls_suffered\n- total_offside\n- yellow_cards\n- red_cards\n- accurate_passes\n- total_passes\n- crosses\n- assists_avg_over_90_mins\n- long_balls\n- successful_short_passes\n- turnovers\n- goals_avg_over_90_mins\n- penalty_kick_goals\n- penalty_kick_taken\n- penalty_kick_percentage\n- accurate_shooting_percentage\n- successful_dribble\n- dribble_percentage\n- goals_and_assists\n- tackles\n- tackles_percentage\n- interceptions\n- headed_duel\n- gk_saves\n- gk_long_ball_percentage\n- gk_total_clearance\n\nUse the data to make interesting visualizations, make your own predictions for the season, power your app, do your own analysis, have fun! Just an NWSL superfan here making information available for you. Please message me for any other ideas or questions.\n football","418":" PSL Season 9 Dataset  For the Pakistan Super League (PSL) Season 9 dataset, you have a couple of good options:\n\n1. Kaggle: You can find a comprehensive dataset for PSL Season 9 on Kaggle. This dataset includes detailed information on matches, players, scores, and other relevant statistics. It's a great resource for analysis and research related to the 2024 season of the PSL. You can access it [here](https:\/\/www.kaggle.com\/datasets\/umerhaddii\/psl-season-9-complete-dataset-2024)\u30105\u2020source\u3011.\n\n2. Open Data Pakistan: Another source for PSL data, including historical data from previous seasons, is available on Open Data Pakistan. This dataset contains extensive records collected from various seasons, including detailed performance metrics. It's available [here](https:\/\/opendata.com.pk\/dataset\/pakistan-super-league-datasets)\u30106\u2020source\u3011\u30107\u2020source\u3011.\n\n football","419":" Pakistan Super League Season 8 & 9 ball-by-ball **# Background**\n\n**The Pakistan Super League (PSL)** is a professional Twenty20 cricket league in Pakistan, founded by the Pakistan Cricket Board (PCB) in 2015. The league was conceived to enhance the cricketing talent pool in Pakistan and provide a platform for local players to compete with international stars.\n**Inception and Early Years (2016-2017):**\nThe inaugural season took place in February 2016, with five franchises representing major cities: Islamabad United, Karachi Kings, Lahore Qalandars, Peshawar Zalmi, and Quetta Gladiators. Matches were held in the United Arab Emirates due to security concerns in Pakistan. Islamabad United won the first season, defeating Quetta Gladiators in the final.\n**Expansion and Domestic Growth (2018-2019):**\nThe PSL expanded in 2018 with the addition of a sixth team, Multan Sultans. This period marked a significant effort by the PCB to bring matches back to Pakistan. In 2018, the final was held in Karachi, and by 2019, more matches, including the playoffs and the final, were held in various Pakistani cities, signaling the return of international cricket to the country.\n**Consolidation and Popularity (2020-Present):**\nBy 2020, the entire PSL season was held in Pakistan for the first time. This move bolstered local support and further integrated the league into the national sports culture. The league continued to attract international players, enhancing its competitiveness and entertainment value.\n**Future Expansion (Post PSL-10):**\nThe PSL has continued to thrive, and after the tenth season, the league is set to expand further. Plans are in place to introduce two additional franchises, increasing the number of teams from six to eight. This expansion aims to bring in more regional representation, further develop cricketing talent, and enhance the league's competitiveness.\n**Impact and Future:**\nThe PSL has significantly impacted Pakistan's cricket, offering a lucrative career path for players and improving the country's cricketing infrastructure. It has also played a crucial role in the revival of international cricket in Pakistan. The league's success has paved the way for potential future expansions and innovations.\n\nOverall, the PSL has grown rapidly since its inception, becoming a prominent fixture in the global cricket calendar and contributing to the resurgence of cricket in Pakistan. The planned expansion after PSL-10 underscores the league's commitment to growth and development in the sport.\nOne of the hallmarks of the PSL is its competitive nature, featuring a dynamic mix of local Pakistani players and international cricket stars. This blend of talent not only raises the level of competition but also enhances the global appeal of the league. Over the years, the PSL has grown into a significant fixture on the cricketing calendar, celebrated for its thrilling matches and the high-quality cricket on display.\n\u2003\n## Gratified\nSeason 8 and Season 9 of the Pakistan Super League (PSL) were captivating cricketing spectacles that highlighted the blend of local and international talent. \n**Season 8 of the Pakistan Super League** played from February 13 to March 19, 2023, captivating fans with its intense matches and high-level performances. The tournament was hosted across multiple cities in Pakistan, showcasing the country's passion for cricket. Lahore Qalandars emerged victorious, clinching their second title by defeating Multan Sultans in a thrilling final. The season was marked by remarkable individual performances, with standout players like Fakhar Zaman and Rashid Khan making significant contributions.\n**Season 9 of the Pakistan Super League** followed, held from February 17 to March 18, 2024. This season continued to build on the excitement and competitive spirit of the PSL, with matches hosted in four cities across Pakistan. Islamabad United secured their third title by triumphing over Multan Sultans in a nail-biting final, underscoring their dominance in the league. The tournament featured 34 matches filled with outstanding performances from both local and international stars. Notable players like Babar Azam and Usama Mir grabbed headlines with their exceptional skills and match-winning contributions.\nFor Season 9, a comprehensive dataset is available, containing detailed summaries of each match as well as ball-by-ball information. This data provides a granular view of the performances and dynamics of the season, offering valuable insights into the intricacies of the game.\n\n**Venues: **Karachi, Lahore, Multan and Rawalpindi\n**Time-period:** 9 February to 19 March 2023 (Season 8) and 17 February to 18 March 2024  (Season 9)  \u2003\n\n## T20 Dataset from Cricsheet.org\nThe T20 dataset available on Cricsheet.org is a comprehensive and detailed collection of cricket match data, focusing specifically on the T20 format. This dataset is a valuable resource for cricket analysts, data scientists, and enthusiasts interested in exploring the intricacies of T20 cricket. The data encompasses a wide range of match details, including player performances, match outcomes, and ball-by-ball events, offering a granular view of the dynamics of each game.\nThis dataset opens up numerous possibilities for data-driven insights and advanced cricket analytics, making it an essential asset for anyone looking to explore the fast-paced world of T20 cricket through a data-centric lens.\nThis dataset is also good source for Data Science students to practice.\n\n football cricket data analytics tabular","420":" Football Manager-23 Championship 30\/31 dataset A dataset of the players in a simulated Championship season(2030-31) ## Context##\n\nThis dataset is collected from the simulated season of 2030-31 in Championship in the game Football Manager 23. \n\nWebscrapping is cool and all. But most of the time, you don't get much from the extracted data from sites like FBref, Opta, and so on. \n\nBut if you're a Football Manager player, you can generate your own data by simulating a full season in the game. And that's exactly what i did.\n\n---\n\nP.S Note: This dataset consists of players in the 2030-31 season. So most of the older players have already retired by now and you'll also stumble upon a lot of newly generated young players in the teams.\n\nHere's all the columns:\n\n---\n\nName  --- Player Name\nNat --- Nationality\nClub --- Playing Club\nPosition --- Playing Position\nAge --- Player Age\nHeight --- Player Height(Feet'inches\")\nPreferred Foot --- Strong foot of the player\nApps --- Appearances\/ Matches played\nStarts --- Starting appearance\nMins --- Minuttes\nGls --- Goals\nAst --- Assists\nWage --- Player Wages\nTransfer_Value --- Estimated transfer value of player\nPens --- Penalties Attempted\nPens_S --- Penalties Scored\nPas_A --- Passes Attempted\nPas_% --- Pass completion rate\nPr_Passes --- Progressive Passes\nPres_A --- Press attempted\nPress_C --- Press completed\nBlk --- Block\nShts Blckd --- Shots Blocked\nClear --- Clearance\nHdrs --- Headers\nItc --- Interception\nTck_A --- Tackles attempted\nTck_W --- Tackled Won\nOff --- Offsides\nGl_Mst --- Mistakes leading to Goal\nK_Tck --- Key Tackles\nDistance --- Distance Covered\nDrb --- Dribbles made\nCr_A --- Crosses attempted\nCr_C% --- Cross Completion Ratio\nShots --- Shots taken\nShts_on_target --- Shots on target\nK_Pas --- Key Passes\nYel --- Yellow Cards\nRed --- Red Cards\nxG --- Expected Goals\nxA --- Expected Assists\n\n---\n**Some of you might get the UTF-8 error(UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte) while defining the dataframe. \n\nIn that case, simply add ''encoding =('ISO-8859-1'),low_memory =False'', after calling the location.\n\nExample:\n\ndf = pd.read_csv('\/kaggle\/input\/football-manager-23-championship-3031-dataset\/Championship_2030-31 Simulation data.csv', encoding =('ISO-8859-1'),low_memory =False)\ndf.head()**\n\n---\n\nThe dataset is not cleaned AT ALL. I'll try to clean it up asap. Until then, it's going to be a bit messy to deal with it. Cheers! video games football data cleaning data analytics simulations","421":" 2023 NWSL Womens Soccer League Player Stats FULL Comprehensive player positions and stats for NWSL athletes - 2023 2023 complete and comprehensive player stats for 347 NWSL athletes\/players.\nData comes from NWSL.com. Data fields have been cleaned, and additional fields, like player position, have been added for NWSL data science fun.\n\nComplete stats include:\n- Attacking\n- Passing\n- Goals\n- Defending\n- PKs\n- Goalkeeping\n\nand more. 38 variables included. \n\nI have left NaNs in the dataset instead of filling them with 0s because 0 is a meaningful stat in this dataset. NaNs means there was no data available. For example, goalkeeping saves, 0 is a meaningful statistical number. But you can drop and clean NaNs for your own purposes on your own.\n\nPercentages are in whole numbers.\n\n2024 season stats and stadium attendance stats will be added soon. \n\nMessage me for other women's sports ideas and variables to include.  football sports","422":" England Premier League (2008-2024) This dataset contains football match data from the English Premier League This dataset contains football match data from the English Premier League for the seasons 2008-2024. Each file includes detailed match statistics, including home and away scores, halftime scores, and various other match details.\n football","423":" Euro 2024 Group Stage FBRef Scrape Data In-Depth Data from Euro 2024 Group Stage on FBRef This dataset contains comprehensive match statistics from the UEFA Euro 2024 Group Stage, covering all matches played until 22.06.2024. The data has been meticulously scraped from FBRef.com, a reputable source for detailed soccer statistics.\n\n**Key features of this dataset include:**\n\nGoals, Assists, and Shots: Detailed statistics on goals scored, assists made, shots taken, and shots on target.\nPassing Metrics: Information on passes completed, attempted, key passes, and progressive passes.\nDefensive Actions: Data on tackles, interceptions, clearances, and blocks.\nGoalkeeping Stats: Insights into goals conceded, saves made, save percentages, and clean sheets.\nAdvanced Metrics: Expected Goals (xG), Expected Assists (xA), and Expected Goals Against (xGA) to provide a deeper understanding of player and team performances.\nPlayer and Team Ratings: Match and team ratings to evaluate overall performance.\nThis dataset will be updated throughout the Euro 2024 Group Stage to ensure the most current data is available for analysis. Whether you're a soccer analyst, data scientist, or fan, this dataset provides valuable insights into the performance of teams and players in the group stage of one of the world's most prestigious soccer tournaments.\n\nSource: Data scraped from FBRef.com\n\nFeel free to explore the dataset, draw insights, and share your findings!\n\nShirt Number\nPos -- Position\n**Position most commonly played by the player**\n- GK - Goalkeepers\n- DF - Defenders\n- MF - Midfielders\n- FW - Forwards\n- FB - Fullbacks\n- LB - Left Backs\n- RB - Right Backs\n- CB - Center Backs\n- DM - Defensive Midfielders\n- CM - Central Midfielders\n- LM - Left Midfielders\n- RM - Right Midfielders\n- WM - Wide Midfielders\n- LW - Left Wingers\n- RW - Right Wingers\n- AM - Attacking Midfielders\n\nAge -- Age on date of match\nAge at season start :Given on August 1 for winter leagues and February 1 for summer leagues.\nMin -- Minutes\n\nfor tables and columns glossary you can visit:\nhttps:\/\/fbref.com\/en\/comps\/676\/schedule\/European-Championship-Scores-and-Fixtures football intermediate text python other","424":" Football Player Performance Data 2017-2024 Football Player Performance Data for Europe's top leagues  These files contain data about football players in Europe's most popular leagues between 2017-2024. There are a wide variety of attributes, the names of teams and players have been properly formatted. If you are unsure what a column is, please visit fbref.com and find the relevant section for a better description of each column. \n\nI used this data to build a Euro 2024 Match predictor (Spoiler: Germany and France Final) using Random Forest Regressor, a machine learning algorithm and I would greatly appreciate any feedback on the project.\nProject link:\nhttps:\/\/github.com\/GurpreetSDeol\/Euro-2024-Match-Predictor-\/tree\/main football sports decision tree","425":" Football Match Data Date Wise From 1960 to 2024   football","426":" The English Women's Football Database Open database of matches played in the top tiers of women's football in England. The English Women's Football (EWF) Database is an open database of matches played in the top tiers of women's football in England. It covers all matches played since the 2011 season for the highest division (the Women's Super League) and since the 2014 season for the second-highest division (the Women's Championship).\n\nThe database contains three datasets:\n\n- ewf_matches contains all matches that have been played and has one observation per match per season.\n- ewf_appearances contains all appearances by a team and has one observation per team per match per season.\n- ewf_standings contains all end-of-the-season division tables and has one observation per team per season.\n\nAll three datasets will be updated with the latest information at the end of each season.\n\nSource: The English Women's Football (EWF) Database, May 2024, https:\/\/github.com\/probjects\/ewf-database.\n\nInspiration has been taken from the Fjelstul English Football Database, a similarly structured dataset that covers men's professional football since 1888.\n\n### Data dictionary\n\n#### ewf_matches\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| home_team_id| The unique ID for the home team. Has the format `T-###-T`.|\n| home_team_name| The name of the home team at the match.|\n| away_team_id| The unique ID for the away team. Has the format `T-###-T`.|\n| away_team_name| The name of the away team at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team_score| The score of the home team.|\n| away_team_score| The score of the away team.|\n| home_team_score_margin| The score margin for the home team, equal to home_team_score minus away_team_score.|\n| away_team_score_margin| The score margin for the away team, equal to away_team_score minus home_team_score.|\n| home_team_win| Whether the home team won the match. Possible values are `1` if the home team won the match and `0` otherwise.|\n| away_team_win| Whether the away team won the match. Possible values are `1` if the away team won the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. Possible values are `1` if the match ended in a draw and `0` otherwise.|\n| result| The result of the match. Possible values are `Home team win`, `Away team win`, and `Draw`.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n\n#### ewf_appearances\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| match_id | The unique ID for the match. Has the format `M-####-####-#-###-M`, where the first number is the year in which the season started, the second number is the year in which the season ended, the third number is the tier, and the fourth number is a counter that is assigned to the data when sorted by the match's date, then by the name of the home team, and then by the name of the away team. References `match_id` in the `ewf_matches` dataset.|\n| match_name| The name of the match, where the name of the home team and the name of the away team is separated by ' vs '.|\n| date| The date of the match, in ``dd\/mm\/yyyy`` format.|\n| attendance| The total crowd attendance at the match. Note that this information is not available for some matches.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team at the match.|\n| opponent_id| The unique ID for the team\u2019s opponent. Has the format `T-###-T`.|\n| opponent_name| The name of the team\u2019s opponent at the match.|\n| score| The score of the match. Has the format `# -- #`, where the first number is the score of the home team and the second number is the score of the away team.|\n| home_team| Whether the team was the home team. Possible value are `1` if the team was the home team and `0` otherwise.|\n| away_team| Whether the team was the away team. Possible value are `1` if the team was the away team and `0` otherwise.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| result| The result of the match. Possible values are `Win`, `Loss`, and `Draw`.|\n| win| Whether the team won the match. The possible values are `1` if the team won the match and `0` otherwise.|\n| loss| Whether the team lost the match. The possible values are `1` if the team lost the match and `0` otherwise.|\n| draw| Whether the match ended in a draw. The possible values are `1` if the match ended in a draw and `0` otherwise.|\n| note| A description of any mitigating circumstances. For example, if the match was not played and the win was instead awarded to the home or away team.|\n| points| The number of points the team earned from the match. A team earns `0` points for a loss, `1` point for a draw, or `3` points for a win.|\n\n#### ewf_standings\n\n| Field      | Description |\n| ----------- | ----------- |\n| season_id | The unique ID for the season. Has the format `S-####-####-#-S`, where the first number is the year in which the season started, the second number is the year in which the season ended, and the third number is the tier.|\n| season | The year(s) that the season started and ended. Has the format `####-####`, where the first number is the year in which the season started and the second number is the year in which the season ended.|\n| tier | The division's tier in English football. Possible values are `1` or `2`.|\n| division | The division name in English football.|\n| position | The team's final position in the season.|\n| team_id| The unique ID for the team. Has the format `T-###-T`.|\n| team_name| The name of the team during the season.|\n| played| The number of matches that the team played.|\n| wins| The number of matches that the team won.|\n| draws| The number of matches that the team drew.|\n| losses| The number of matches that the team lost.|\n| goals_for| The number of goals scored by the team.|\n| goals_against| The number of goals scored against the team.|\n| goal_difference| The number of goals scored by the team minus the number of goals scored against the team.|\n| points| The number of points that the team earned over the whole season (after applying `point_adjustment`).|\n| point_adjustment| The number of points that were deducted by the league due to violations of rules or added by the league due to forfeits.|\n| season_outcome| The outcome for the team following the season. This variable is included to track the movement of teams across seasons more easily. Possible values are `Club folded`, `No change` for when the team remains in their current tier, `Promoted to tier 1` for when the team moves into tier 1 from a lower tier, `Relegated to tier 2` for when the team moves into tier 2 from a higher tier, and `Relegated to tier 3` for when the team moves into tier 3 from a higher tier.| football","427":" 2022\/23 Big 5 Football Leagues Player Stats Complete dataset from all league matches. All data taken from https:\/\/fbref.com\/ All data taken from https:\/\/fbref.com\/\n\nGitHub to my project: https:\/\/github.com\/emreguvenilir\/fifa23-ml-ratingsystem\n\nThere is another statistics dataset here on Kaggle where the data is totally incomplete. So I took the time, mainly because of a final school project, to download the raw data from R. I then cleaned the data to the specifics of my project. The data contains only players from the big 5 leagues (prem, la liga, bundesliga, ligue 1, serie a.) \n\nColumn Description\n\nsquad: The team of a given player\n\ncomp: The league of the team, only includes the \u201cbig 5\u201d\n\nplayer: player name\n\nnation: nationality of the player\n\npos: position of the player\n\nage: age of the player\n\nborn: year born\n\nMP: matches played\n\nMinutes_Played: minutes played in the season\n\nMn_per_MP: minutes per match played\n\nMins_Per_90: minutes per 90 minutes (length of a soccer match)\n\nStarts: matches started\n\nPPM_Team.Success: avg # of point earned by the team from matches in which the player appeared with a minimum of 30 minutes\n\nOnG_Team.Success: goals scored by team while on pitch\n\nonGA_Team.Success: Goals allowed by team while on pitch\nplus_per__minus__Team.Success: goals scored minus allowed while on pitch\n\nGoals: goals scored\n\nAssists: assists that led to goal\n\nGoalsAssists: goals + assists\n\nNonPKG: non penalty kick goals\n\nPK: penalty kicks made\n\nPKatt: penalties attempted\n\nCrdY: yellow cards\n\nCrdR: red cards\n\nxG: expected goals based on all shots taken\n\nxAG: expected assisted goals\n\nnpxG+xAG: non penalty expected goals + assisted goals\n\nPrgC: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nPrgP: progressive carries in the attacking half of the pitch and went at least 10 yards\n\nGls_Per90: goals per 90 minutes\n\nAst_Per90: assists per 90 minutes\n\nG+A_Per90: goals + assists per 90\n\nG_minus_PK_Per: goals excluding penalties per 90\n\nG+A_minus_PK_Per: goals and assists excluding penalties per 90\n\nxG_Per: xG per 90\n\nxAG_Per: xAG per 90\n\nxG+xAG_Per: xG+xAG per 90\n\nShots: shots taken\n\nShots_On_Target: shots on goal frame\n\nSoT_percent: sh\/SoT * 100\n\nG_per_Sh: goals per shot taken\n\nG_per_SoT: goal per shot on target\n\nAvg_Shot_Dist: avg shot dist\n\nFK_Standard: shots from free kicks\n\nG_minus_xG_expected: goals minus expected goals\n\nnp:G_minus_xG_Expected: non penalty goals minus expected goals\n\nPasses_Completed: passes completed\n\nPasses_attempted: passes attempted\n\nPasses_Cmp_percent: pass completion percentage\n\nPrgDist_Total: progressive pass total distance\n\nPasses_Cmp_Short: short passes completed (5 to 15 yds)\n\nPasses_Att_Short: short passes Attempted (5 to 15 yds)\n\nPasses_Cmp_Percent_Short: short passes completed percentage  (5 to 15 yds)\n\nPasses_Cmp_Medium: medium passes completed (15 to 30 yds)\n\nPasses_Att_medium: medium passes Attempted (15 to 30 yds)\n\nPasses_Cmp_Percent_Medium: medium passes completed percentage  (15 to 30 yds)\n\nPasses_Cmp_long: long passes completed (30+ yds)\n\nPasses_Att_long : long passes Attempted (30+ yds)\n\nPasses_Cmp_Percent_long : long passes completed percentage  (30+ yds)\n\nA_minus_xAG_expected: assists minus expected assists\n\nKey_Passes: passes that lead directly to a shot\n\nFinal_third: passes that enter the final third of the field\n\nPPA: passes into the penalty area\n\nCrsPA: crosses into penalty area\n\nTB_pass: through ball passes\n\nCrs_Pass: number of crosses\n\nOffside_passes: passes that resulted in an offside\n\nBlocked_passes: passes blocked by an opponent\n\nShot_Creating_Actions: shot creating actions\n\nSCA_90: shot creating actions per 90\n\nTakeOnTo_Shot: take ons that led to shot\n\nFoulTo_Shot: fouls draw that led to shot\n\nDefAction_Shot: defensive actions that led to a shot (pressing)\n\nGoalCreatingAction: goal creating actions\n\nGCA90: goal creating actions per 90\n\nTakeOn_Goal: take ons that led to a goal\n\nFld_goal: fouls drawn that led to a goal\n\nDefAction_Goal: defensive actions that led to a goal (pressing)\n\nTackles: number of tackles made\n\nTackles_won: tackles won\n\nDef_3rd_Tackles: tackles in the defensive 1\/3 of the pitch\n\nMid_3rd_Tackles: tackles in the middle 1\/3 of the pitch\n\nAtt_3rd_Tackles: tackles in the attacking 1\/3 of the pitch\n\nTkl_percent_won: % of dribblers tackled\n\nLost_challenges: lost challenges, unsuccessful attempts to win the ball\n\nBlocks: # of times blocking the ball by standing in path\n\nSh_blocked: shots blocked\n\nPasses_blocked: number of passes blocked\n\nInterceptions: interceptions\n\nClearances; clearances\n\nErrorsLead_ToShot: errors made leading to a shot\n\nAtt_Take: attacking take ons attempted\n\nSucc:Take: attacking take ons successful\n\nSucc_percent_take: percentage of attacking take ons successfully\n\nTkld_Take: times tackled during a take on\n\nTkld_percent_Take: percentage of times tackled during a take on\n\nTotDist_Carries: total distance carrying the ball in any direction\n\nPrgDist_carries: progressive carry distance total\n\nMiscontrolls: # of times a player failed to get control of the ball\n\nDispossessed: Number of times lost control of ball when being tackled, does not include take ons\n\n2CrdY: second yellow cards\n\nFouls_committed: fouls committed\n\nFouls_drawn: fouls drawn\n\nOffsides: number of times offside\n\nCrosses: crosses (different from other crosses stat)\n\nPK_won: penalty kicks won\n\nPK_conceded: penalty kicks conceded\n\nOwn_goals: own goals\n\nLoose_ball_recoveries: loose ball recoveries made\n\nWon_aerieal: aerial duels won\n\nLost_aerieal: aerial duels Lost\n\n\nWon_percent_aerieal: percentage of aerial duels won\n\nGOALKEEPER SPECIFIC STATS\n\nGoals_against_GK: goals against\n\nSoT_Against_GK: shots on target against\n\nSaves_GK: saves made\n\nSave_percent_GK: save percentage \n\nClean_sheets_GK: clean sheets (no goals allowed during a game)\n\nCS_percent_GK: clean sheet percentage\n\nPK_Faced_GK: penalties faced\n\nPK_Allowed_GK: penalties allowed\n\nPK_Missed_GK: penalties missed\n\nsave_percent_penalty_GK: save percentage on penalties\n\nFK_Goals_GK: free kick goals against \n\nCornerKick_Goals_GK: goals that resulted from corner kick\n\nOG_GK: own goals\n\nPSxG_GK: expected goals based on how likely the goalkeeper is to save a shot\n\nPSxG_per_SoT_GK: expected goals based on how likely the goalkeeper is to save a shot on target per shot on target\n\nPSxG+_per_minus_GK: post shot expected goals minus goals allowed, positive nums good\n\nLaunchedPassesCompleted_GK: passes longer than 40 yds completed\n\nLaunchedPassesAttempted_GK: passes longer than 40 yds attempted\n\ncmp_percent_Launched_GK: percentage of passes longer than 40 yds attempted\n\nAtt_Passes_GK: attempted passes\n\nThrows_GK: throws attempted\n\nLaunch_percent_passes_GK: % of passes that were launched\n\nAvgLen_passes_GK: avg length of a pass\n\nAtt_Goal_kick_GK: goal kicks attempted\n\nAvgLen_GoalKick_GK: avg kick length of a goal kick\n\nCrossesFaced_GK: opponents attempted crosses into penalty area\n\nCrossesStopped_GK: opponents stopped crosses into penalty area\n\nCrossesFaced_perc_GK: % of opponents attempted crosses into penalty area stopped\n\nOPA_Sweeper_GK: defensive actions outside of penalty area by GK\n\nOPA_per_90_sweeper_GK: defensive actions outside of penalty area by GK per 90\n\nAvgDist_sweeper_GK: Avg dist from goal in all defensive actions football sports data analytics random forest regression","428":" Turkish Super League Turkish Super League 2023\/2024 Mid-Season Player Statistics **Turkish Super League 2023\/2024 Mid-Season Player Statistics**\n*Description:*\nThis dataset provides comprehensive and detailed information about players participating in the Turkish Super League for the 2023\/2024 season. It covers a wide range of data points, including personal details, team affiliations, and extensive match statistics. Note that the dataset represents data collected up to April 14, 2024, and thus covers player performances up to the mid-point of the season, not the entire season.\n\n\u2022\tPersonal Details: The dataset includes essential personal information for each player, such as their full name, date of birth, place of birth, nationality, height, and weight. This allows for a thorough understanding of the player's background and physical attributes.\n\n\u2022\tTeam Affiliations: Each player\u2019s current team is recorded, along with unique identifiers for both the player and the team. The dataset also includes URLs to photos of the players and logos of the teams, providing a visual reference for each entry.\n\n\u2022\tMatch Statistics: Extensive match statistics are included to provide a detailed view of each player's performance throughout the season up to the data collection date. \nThis includes:\no\tAppearances and Playing Time: Number of matches played, minutes played, and whether the player was in the starting lineup or substituted.\no\tPosition and Rating: The position played in each match and the player's performance rating.\no\tSubstitutions: Information on whether the player was substituted in or out, or if they spent time on the bench.\no\tShooting Statistics: Total shots, shots on target, goals scored, and assists.\no\tPassing Statistics: Total passes, key passes, and passing accuracy.\no\tDefensive Actions: Tackles, duels, dribbles, fouls committed and drawn, and cards received (yellow, yellow-red, red).\no\tPenalties: Details on penalties won, committed, scored, missed, and saved.\n\n\u2022\tTransferred Players: Columns with a \"2\" suffix contain match statistics for players after they have transferred to a second team during the season. This allows for a comparison of performance across different teams and environments.\n*Data Source:*\nThe dataset was collected from API-Football on April 14, 2024. API-Football is a comprehensive football data provider that aggregates information from official match reports, player statistics, and team data. This dataset represents the most up-to-date statistics available for the 2023\/2024 season up to mid-April, offering real-time insights into player performances, team dynamics, and match outcomes as the season progresses.\n\n\n\n\n*Column Descriptions:*\n\nColumn\tData Type\tDescription\n\nPlayer ID\tint64\tUnique identifier for each player.\nName\tobject\tFull name of the player.\nFirst Name\tobject\tFirst name of the player.\nLast Name\tobject\tLast name of the player.\nAge\tint64\tAge of the player.\nBirth Date\tdatetime64[ns]\tDate of birth of the player.\nBirth Place\tobject\tCity of birth of the player.\nBirth Country\tobject\tCountry of birth of the player.\nNationality\tobject\tNationality of the player.\nHeight\tobject\tHeight of the player in centimeters.\nWeight\tobject\tWeight of the player in kilograms.\nInjured\tbool\tBoolean indicating if the player is injured.\nPhoto\tobject\tURL to the player's photo.\nTeamID\tint64\tUnique identifier for the team.\nTeamName\tobject\tName of the team.\nTeamLogo\tobject\tURL to the team\u2019s logo.\nMatchesPlayed\tobject\tNumber of matches played.\nGamesLineups\tobject\tNumber of times in the starting lineup.\nGamesMin\tobject\tTotal minutes played.\nGamesPosition\tobject\tPosition played.\nGamesRating\tobject\tPlayer's rating.\nGamesCaptain\tobject\tNumber of times served as captain.\nSubsIn\tobject\tNumber of times substituted in.\nSubsOut\tobject\tNumber of times substituted out.\nSubsBench\tobject\tNumber of times on the bench.\nShotsTotal\tobject\tTotal shots.\nShotsOn\tobject\tShots on target.\nShotsGoals\tobject\tGoals scored.\nShotsConceded\tobject\tGoals conceded.\nShotsAssists\tobject\tAssists made.\nShotsSaves\tobject\tSaves made.\nPassesTotal\tobject\tTotal passes.\nPassesKey\tobject\tKey passes.\nPassesAccuracy\tobject\tPassing accuracy percentage.\nTacklesTotal\tobject\tTotal tackles.\nTacklesWon\tobject\tTackles won.\nDribblesAttempted\tobject\tDribbles attempted.\nDribblesSuccess\tobject\tSuccessful dribbles.\nFoulsDrawn\tobject\tFouls drawn.\nFoulsCommitted\tobject\tFouls committed.\nCardsYellow\tobject\tYellow cards received.\nCardsYellowRed\tobject\tYellow-red cards received.\nCardsRed\tobject\tRed cards received.\nPenaltyWon\tobject\tPenalties won.\nPenaltyCommited\tobject\tPenalties committed.\nPenaltyScored\tobject\tPenalties scored.\nPenaltyMissed\tobject\tPenalties missed.\nPenaltySaved\tobject\tPenalties saved. \n\n\n\nColumns with '2'\tCorresponding Type\tStatistics from the player's second team after a transfer.\n\n football statistical analysis","429":" FIFA Worldcup 2022 Final Dataset: ARG vs FRA FIFA Worldcup 2022 FInal Dataset: ARG vs FRA: Event to Event data This dataset contains all the event-by-event actions of World Cup 2022 final match between Argentina vs France. Collected from Statsbomb open data : [https:\/\/github.com\/statsbomb\/open-data](url)\n\nThe dataset contains a total of 4407 recorded events(rows) and 120 columns.\n football","430":" People Slurs Dataset The dataset appears to be a collection of text The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data. psychology nlp text text classification bert","431":" Medicine Dataset Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information global categorical medicine text","432":" Classified NIH Dataset NIH x-ray dataset in classified format. I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n health classification deep learning cnn health conditions tensorflow","433":" phishing-email-classifier-bert BERT Fine-Tuned For Classification on Phishing Email Dataset ### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) classification binary classification email and messaging bert","434":" Stress and Anxiety Posts on Reddit 4,000 Reddit posts from people experiencing stress\/anxiety. This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit. psychology text","435":" IT Skills from Jobs Extract IT skills from Job Descriptions Dataset of Job descriptions and relevant IT skills, soft skills, education and experience required for the job. \nHard skills include IT keywords from job descriptions. It can be used to train LLM models to extract skills from various IT specific job descriptions computer science programming nlp text text classification bert","436":" Labeled IT Job skills labeled IT skills in job descriptions. Contains a small dataset of labeled IT skills, IT Tools, IT technologies to train named entity recognition (NER) model for extracting skills from job description using spaCy. it will be updated weekly to increase the number of rows. beginner nlp classification text json","437":" Dark Pattern Dataset Custom Dark Pattern Dataset - IIT(BHU) This Dataset is a collection of popular dark patterns from different E-Commerce and Online store sites. It can be utilized by creating accurate ML Model to detect Dark Patterns in Online Stores and E-Commerces. earth and nature","438":" Malflow Radare2 disassembled callgraphs, instruction stats & images on Bodmas and MalImg ## Objective\n\n* primary purpose of this dataset is to support the scientific paper submitted to NSS-SocialSec2024 with data, code, and auxiliary information\n* secondary purpose is to offer data to anyone interested in researching malware analysis and\/or family classification domain\n\n## Details\n\n* this is a collection of various resources and metadata regarding 3 datasets:\n  * **MalImg**\n  * **Bodmas**\n  * **IBD** -- Internal Dataset, where we are restricted not to publish md5-family relation\n       * only 14556 MD5 hashes are published, these are public on VirusTotal as well\n       * the full dataset size used in our experiments: 18765 samples\n* each sample was scanned with Radare2 5.8.8 to obtain the static call graph object, saved in a pickle format\n* these call graphs were traversed to obtain instruction information\n* RGB images were generated based on the instruction list, by mapping one instruction into one pixel and shaping the list into fixed dimensions\n\n## Data augmentation -- automated metamorphic variant generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/pymetangine\/general\n\n## Call graph generation\n\n* https:\/\/hub.docker.com\/repository\/docker\/attilamester\/r2-5.8.8\/general\n* https:\/\/github.com\/attilamester\/malflow\n\n\n## References\n\n* MalImg dataset: \n```\n@inproceedings{nataraj2011malware,\n  title={Malware images: visualization and automatic classification},\n  author={Nataraj, Lakshmanan and Karthikeyan, Sreejith and Jacob, Gregoire and Manjunath, Bangalore S},\n  booktitle={Proceedings of the 8th international symposium on visualization for cyber security},\n  pages={1--7},\n  year={2011}\n}\n```\n\n* Bodmas dataset:\n```\n@inproceedings{yang2021bodmas,\n  title={BODMAS: An open dataset for learning based temporal analysis of PE malware},\n  author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},\n  booktitle={2021 IEEE Security and Privacy Workshops (SPW)},\n  pages={78--84},\n  year={2021},\n  organization={IEEE}\n}\n``` data visualization python image classification graph cyber security","439":" Million Song Data Analysis 2 Data Analysis Designed to Predict High Stream Count on Spotify Did We Solve the Problem?\nThe objective of this analysis was to predict high streaming counts on Spotify and perform a detailed cluster analysis to understand user behavior. Here\u2019s a summary of how we addressed each part of the objective:\n\nPrediction of High Streaming Counts:\n\nImplemented Multiple Models: We utilized several machine learning models including Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN).\nComparison and Evaluation: These models were evaluated based on classification metrics like accuracy, precision, recall, and F1-score. The Gradient Boosting and Random Forest models were found to be the most effective in predicting high streaming counts.\nCluster Analysis:\n\nK-means Clustering: We applied K-means clustering to segment users into three clusters based on their listening behavior.\nDetailed Characterization: Each cluster was analyzed to understand the distinct characteristics, such as average playtime, skip rate, offline usage, and shuffle usage.\nVisualizations: Histograms and scatter plots were used to visualize the distributions and relationships within each cluster.\nResults and Insights\nEffective Models: The Gradient Boosting and Random Forest models provided the highest accuracy and balanced performance for predicting high streaming counts.\nUser Segmentation: The cluster analysis revealed three distinct user segments:\nCluster 1: Users with longer playtimes and lower skip rates.\nCluster 2: Users with moderate playtimes and skip rates.\nCluster 3: Users with shorter playtimes and higher skip rates.\nThese insights can be leveraged for targeted marketing, personalized recommendations, and improving user engagement on Spotify.\n\nConclusion\nYes, we solved the problem. We successfully predicted high streaming counts using effective machine learning models and provided a detailed cluster analysis to understand user behavior. The analysis offers valuable insights for enhancing Spotify\u2019s recommendation system and user experience. music","440":" People Slurs Dataset The dataset appears to be a collection of text The dataset appears to be a collection of text entries with associated emotional responses and metadata. Here are some insights:\n\nColumns:\n\nThe dataset includes columns such as text, condition, recalled, slur_source, slur_gender, subj_anger, f_pain, f_fear, f_panic, f_anger, f_guilt, and f_humiliation.\n\nEntries:\nThere are 504 entries in total.\n\nTypes of Data:\nThe data types include text, integers, and floats, indicating a mixture of qualitative and quantitative data. psychology nlp text text classification bert","441":" Medicine Dataset Comprehensive Synthetic Dataset of Medicines: Categorizing 50,000 Unique Entries This comprehensive synthetic dataset of medicines includes 50,000 unique entries, meticulously crafted for modern pharmaceutical research. Each data point features realistic medicine names, categories, dosage forms, strengths, manufacturers, indications, and classifications (Prescription or Over-the-Counter). Designed to aid in the development and testing of machine learning models, this dataset provides a rich and varied collection of medicine-related information global categorical medicine text","442":" Different Colors in challenging lightening Different color classification in different lightening Environment **Overview**\nThis dataset comprises 250 high-resolution images specifically curated for color classification under diverse and challenging lighting conditions. The images are categorized into nine color classes: black, blue, brown, green, orange, purple, red, white, and yellow. The dataset is split into two main subsets for training and testing purposes.\n\n**Training Set**\nNumber of Images: 200\nDescription: The training set consists of 200 images, each labeled with one of the nine color classes. These images capture a range of lighting scenarios, including low light, high contrast, shadows, and mixed lighting conditions.\n**Test Set**\nNumber of Images: 50\nDescription: The test set contains 50 images without labels. It serves as the evaluation dataset for assessing the performance of color classification models trained on the training set. The images in the test set are also captured under varied lighting conditions to simulate real-world scenarios.\nObjectives\nThe primary goal of using this dataset is to develop and evaluate machine learning models capable of accurately classifying colors under challenging lighting conditions. Participants are encouraged to explore various model architectures and preprocessing techniques to achieve robust performance across different lighting scenarios.\n\n**Evaluation**\nSubmissions will be evaluated based on the accuracy of color classifications on the test set. The model's ability to generalize to unseen lighting conditions will be a key factor in determining the final performance metrics.\n\n**Usage**\nResearchers and practitioners interested in color classification, computer vision, and machine learning can utilize this dataset for developing and benchmarking their algorithms. The dataset provides a realistic representation of color classification challenges encountered in practical applications.\n\n**Acknowledgments**\nWe acknowledge the contributors and creators of this dataset for their efforts in collecting and annotating the images, enabling advancements in color classification research and development.\n\n\n**Please cite our paper:**\nN. Maitlo, N. Noonari, S. A. Ghanghro, S. Duraisamy and F. Ahmed, \"Color Recognition in Challenging Lighting Environments: CNN Approach,\" 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 2024, pp. 1-7, doi: 10.1109\/I2CT61223.2024.10543537.\nkeywords: {Image segmentation;Computer vision;Image color analysis;Image edge detection;Neural networks;Lighting;Object segmentation;Deep Learning;Convolutional Neural Network (CNN);Image Segmentation;Color Detection;Object Segmentation}, arts and entertainment","443":" Classified NIH Dataset NIH x-ray dataset in classified format. I took this dataset from (https:\/\/www.kaggle.com\/datasets\/nih-chest-xrays\/data). Because the original dataset was not classified very well so I performed classification and create 2 folders with Normal and Abnormal Chest x-ray images. Original dataset contains around 1 lac+ images in 12 folder (folders were not classified with labels). The csv file does not contains information about all the images present in folders. So I did preprocess over folders and create 2 folders out of it. Classified NIH dataset contains 84999 images. Now we can create a model and train the model very well because this dataset has a lot of images. I created this dataset just to train your model for classification. If you want to do more you can visit the official dataset.\n\nInformation given by NIH:\n\nThis NIH Chest X-ray Dataset is comprised of 84999 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be &gt;90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n\n\nFile contents:\nNormal_chest -  39810 images of 1024*1024\nAbnormal_chest - 46189 images of 1024*10124\n\nData_entry_2017.csv: Class labels and patient data for the entire dataset\n\nImage Index: File name\nFinding Labels: Disease type (Class label)\nFollow-up #\nPatient ID\nPatient Age\nPatient Gender\nView Position: X-ray orientation\nOriginalImageWidth\nOriginalImageHeight\nOriginalImagePixelSpacing_x\nOriginalImagePixelSpacing_y\n health classification deep learning cnn health conditions tensorflow","444":" US Collegiate Sports Dataset US Collegiate Sports Dataset from 2015 to 2019  ## **Content**\n\nThis file contains the comprehensive information on collegiate sports programs across various institutions in the United States. It includes data on student enrollment, sports participation, revenue, and expenditures, categorized by gender and sport. The dataset can be used to analyze trends, financial aspects, and gender disparities in collegiate sports.\n\n***Key Insights***\n\n**Enrollment Data**: The dataset includes the total number of male and female students enrolled in each institution, providing insights into the gender distribution of the student body.\n\n**Sports Participation:** Participation data is broken down by gender and sport, allowing for analysis of gender representation in different sports.\n\n**Financial Data**: Revenue and expenditures for men's and women's sports are detailed, enabling financial analysis of sports programs.\n\n**Institutional Classification**: Institutions are classified by type and sector, which helps in comparing different categories of schools (e.g., NCAA Division I, II, III).\n\n\n## **Context**\n\nGeography: USA\n\nTime period: 2015-  2019\n\nUnit of analysis: US Collegiate Sports Dataset\n\n## **Variables**\n\n| Variable                | Description                                                                 |\n|-------------------------|-----------------------------------------------------------------------------|\n| year                    | Year, which is year: year + 1, e.g., 2015 is 2015 to 2016                    |\n| unitid                  | School ID                                                                   |\n| institution_name        | School name                                                                 |\n| city_txt                | City name                                                                   |\n| state_cd                | State abbreviation                                                          |\n| zip_text                | Zip code of school                                                          |\n| classification_code     | Code for school classification                                              |\n| classification_name     | School classification                                                       |\n| classification_other    | School classification other                                                 |\n| ef_male_count           | Total male students                                                         |\n| ef_female_count         | Total female students                                                       |\n| ef_total_count          | Total students for binary male\/female gender (sum of previous two columns)  |\n| sector_cd               | Sector code                                                                 |\n| sector_name             | Sector name                                                                 |\n| sportscode              | Sport code                                                                  |\n| partic_men              | Participation men                                                           |\n| partic_women            | Participation women                                                         |\n| partic_coed_men         | Participation as coed men                                                   |\n| partic_coed_women       | Participation for coed women                                                |\n| sum_partic_men          | Sum of participation for men                                                |\n| sum_partic_women        | Sum of participation for women                                              |\n| rev_men                 | Revenue in USD for men                                                      |\n| rev_women               | Revenue in USD for women                                                    |\n| total_rev_menwomen      | Total revenue for both                                                      |\n| exp_men                 | Expenditures in USD for men                                                 |\n| exp_women               | Expenditures in USD for women                                               |\n| total_exp_menwomen      | Total expenditures for both                                                 |\n| sports                  | Sport name                                                                  |\n\n\n## **Acknowledgements**\n**Datasource:**  [Equity in Athletics Data Analysis](https:\/\/ope.ed.gov\/athletics\/#\/datafile\/list) , hattip to [Data is Plural](https:\/\/www.data-is-plural.com\/archive\/2020-10-21-edition\/) \n\n**Inspiration:** Additional articles from [US NEWS](https:\/\/www.usnews.com\/news\/sports\/articles\/2021-10-26\/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women's.), [USA Facts](https:\/\/usafacts.org\/articles\/coronavirus-college-football-profit-sec-acc-pac-12-big-ten-millions-fall-2020\/) and [NPR](https:\/\/www.npr.org\/2021\/10\/27\/1049530975\/ncaa-spends-more-on-mens-sports-report-reveals)\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F1e2182a121cc406ef5d604b9f289751a%2Fpic1.png?generation=1719572752072656&alt=media)\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F18335022%2F5850475d99fd60ed2063c1184044b304%2Fpic2.png?generation=1719572764635513&alt=media) sports","445":" phishing-email-classifier-bert BERT Fine-Tuned For Classification on Phishing Email Dataset ### Dataset Description\n\n**Dataset Title**: Fine-Tuned BERT Model for Scam Email Classification\n\n**Directory: scam-email-classifier-bert-uncased**\n- **config.json**: This file contains the configuration parameters for the BERT model architecture, including details about the model layers, attention heads, hidden size, etc. It ensures that the model structure can be correctly instantiated when loaded.\n- **model.safetensors**: This file contains the trained weights of the BERT model in the SafeTensors format. It is used to store and load the model parameters efficiently and safely.\n- **training_args.bin**: This file includes the arguments and hyperparameters used during the training of the BERT model, such as learning rate, batch size, number of training epochs, etc.\n\n**Directory: scam-email-bert-tokenizer**\n- **special_tokens_map.json**: This file maps special tokens (like [CLS], [SEP], [PAD], [UNK], and others) to their corresponding IDs used by the tokenizer.\n- **tokenizer_config.json**: This file contains the configuration parameters for the tokenizer, detailing how text should be processed and tokenized before being fed into the model.\n- **vocab.txt**: This file lists the vocabulary used by the tokenizer, mapping each token to a unique index.\n\nThese files allow users to easily load the tokenizer and model using `BertTokenizer.from_pretrained()` and `BertClassifier.from_pretrained()` respectively.\n\n### Dataset Information\n\nThe BERT model has been fine-tuned on the [Phishing Email Dataset](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) provided by Naser Abdullah Alam. This dataset is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. The dataset includes a collection of phishing and legitimate emails, which has been used to train and evaluate the model.\n\n### Citations\n\nOriginal BERT Model:\n\nDevlin, Jacob, et al. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" [arXiv preprint arXiv:1810.04805, 2018.](https:\/\/arxiv.org\/abs\/1810.04805)\nPhishing Email Dataset:\n\nNaser Abdullah Alam. [\"Phishing Email Dataset.\" Kaggle, 2021.](https:\/\/www.kaggle.com\/datasets\/naserabdullahalam\/phishing-email-dataset\/data?select=phishing_email.csv) classification binary classification email and messaging bert","446":" Classification   ","447":" Cats Dogs Classification dataset contains total of 400 images of cats and dogs for making a classification model. A simple classification dataset which can used to train your very first CNN model, and get better understanding of Deep learning. It contains around 100 images of cats and dogs with each in its own directory. The data is gathered from WWW(World Wide Web) and is free to use for any user.  computer vision classification deep learning image binary classification","448":" ISIC 2020 JPG 256x256 RESIZED  **ISIC 2020 Competition training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine cancer image classification","449":" ISIC 2020 JPG 224x224 RESIZED  **ISIC 2020 Competition training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine classification cancer image classification","450":" ISIC 2019 JPG 256x256 RESIZED  **ISIC 2019 Classification training dataset. Images resized to 256x256 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images | medicine cancer image classification","451":" ISIC 2019 JPG 224x224 RESIZED ISIC 2019 resized dataset **ISIC 2019 Classification training dataset. Images resized to 224x224 for Melanoma Skin Cancer Detection.**\n## Files\n- **train-image\/** - image files for the training set (provided for train only)\n- **train-metadata.csv** - metadata for the training set\n\n## Columns included only in train-metadata.csv\n| Field Name | Description |\n| --- | --- |\n| `target` | Binary class {0: benign, 1: malignant}. |\n| `patient_id` | Randomly generated for formatting |\n| `isic_id` | Unique identifier for images |\n  arts and entertainment medicine cancer image classification","452":" Stress and Anxiety Posts on Reddit 4,000 Reddit posts from people experiencing stress\/anxiety. This dataset contains 4,000 entries from Reddit, focusing on posts related to stress and anxiety. It provides a rich resource for analyzing the textual content and understanding the prevalence and context of stress and anxiety in online discussions.\n\n**Colomns**\n**Text**: The content of the Reddit post.\n**is_stressed\/anxious:**A binary indicator where '1' denotes the presence of stress or anxiety in the post, and '0' denotes the absence.\n\nin this dataset i have merged the title and post content into one filed 'text' .\n\n**Usage**\n\nThis dataset can be used for various purposes, including but not limited to:\n\n- Sentiment analysis to gauge the emotional tone of the posts.\n- Natural language processing tasks such as topic modeling, keyword extraction, and text classification.\n- Studies on mental health trends and the impact of stress and anxiety on social media.\n- Development of machine learning models to predict mental health issues based on textual data.\n\n\n\nThis cleaned dataset offers valuable insights for those interested in exploring the dynamics of stress and anxiety as expressed in online communities, particularly on Reddit. psychology text","453":" Skin Disease Classification   ","454":" 4 Players Chess Pieces Pieces Dataset for training Dataset created for training a neural network that can recognize four-player chess pieces. The dataset consists of photos taken based on the expected design, created with a 3D printer, for easy and straightforward recognition. computer vision classification neural networks cnn tensorflow","455":" Car Classification   automobiles and vehicles","456":" Dresden Image Database  The 'Dresden Image Database' comprises original JPEG images from 73 camera devices across 25 camera models. This dataset is primarily used for Source Camera Device and Model Identification, offering over 14,000 images captured under controlled conditions. \n\nCopyright: \"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and\/or a fee.\"\n\nOriginal Source (Now Working as on 28 June 2024): http:\/\/forensics.inf.tu-dresden.de\/dresden_image_database\/ \n\nPlease Cite the corresponding paper\n\"Gloe, T., & B\u00f6hme, R. (2010, March). The'Dresden Image Database'for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing (pp. 1584-1590).\" computer science computer vision classification image image classification"}}